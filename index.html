<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2510.08567.pdf' target='_blank'>https://arxiv.org/pdf/2510.08567.pdf</a></span>   <span><a href='https://github.com/mbzuai-oryx/MATRIX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tajamul Ashraf, Umair Nawaz, Abdelrahman M. Shaker, Rao Anwer, Philip Torr, Fahad Shahbaz Khan, Salman Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08567">MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision language models (VLMs) are increasingly deployed as controllers with access to external tools for complex reasoning and decision-making, yet their effectiveness remains limited by the scarcity of high-quality multimodal trajectories and the cost of manual annotation. We address this challenge with a vision-centric agent tuning framework that automatically synthesizes multimodal trajectories, generates step-wise preference pairs, and trains a VLM controller for robust tool-use reasoning. Our pipeline first constructs M-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified trajectories, enabling imitation-based trajectory tuning. Building on this, we develop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool reasoning. To achieve finer alignment, we further introduce Pref-X, a set of 11K automatically generated preference pairs, and optimize MATRIX on it via step-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA, MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating scalable and effective multimodal tool use. Our data and code is avaliable at https://github.com/mbzuai-oryx/MATRIX.<br>
<span id='abs_ch'>中文: 本文提出一种以视觉为中心的智能体调优框架，通过自动合成多模态轨迹和偏好对来训练视觉语言模型控制器，在多项工具使用推理基准测试中均展现出卓越性能。</span><br>
<span id='abs_en'>English: This paper introduces a vision-centric agent tuning framework that automatically synthesizes multimodal trajectories and preference pairs to train a VLM controller, achieving superior performance on multiple benchmarks for robust tool-use reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2510.08564.pdf' target='_blank'>https://arxiv.org/pdf/2510.08564.pdf</a></span>   <span><a href='https://github.com/jessemelpolio/LMM_CL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhen Zhu, Yiming Gong, Yao Xiao, Yaoyao Liu, Derek Hoiem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08564">How to Teach Large Multimodal Models New Skills</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>How can we teach large multimodal models (LMMs) new skills without erasing prior abilities? We study sequential fine-tuning on five target skills while monitoring general ability on eight held-out benchmarks across three model families. We observe that apparent "forgetting" on held-out tasks after narrow fine-tuning can partly recover at later stages. We trace this behavior to a measurable shift in the output token distribution, manifested through a simple counting-bias probe that co-varies with forgetting. Guided by this picture, we identify two simple, robust tuning recipes that learn strongly while limiting drift: (i) updating only the self-attention projection layers, and (ii) updating only the MLP Gate&Up while freezing the Down projection. Across models and tasks, these choices deliver strong target gains while largely preserving held-out performance. Code is available at https://github.com/jessemelpolio/LMM_CL<br>
<br>
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2510.08553.pdf' target='_blank'>https://arxiv.org/pdf/2510.08553.pdf</a></span>   <span><a href='https://github.com/xyz9911/Memoir' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunzhe Xu, Yiyuan Pan, Zhe Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08553">Dream to Recall: Imagination-Guided Experience Retrieval for Memory-Persistent Vision-and-Language Navigation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision-and-Language Navigation (VLN) requires agents to follow natural language instructions through environments, with memory-persistent variants demanding progressive improvement through accumulated experience. Existing approaches for memory-persistent VLN face critical limitations: they lack effective memory access mechanisms, instead relying on entire memory incorporation or fixed-horizon lookup, and predominantly store only environmental observations while neglecting navigation behavioral patterns that encode valuable decision-making strategies. We present Memoir, which employs imagination as a retrieval mechanism grounded by explicit memory: a world model imagines future navigation states as queries to selectively retrieve relevant environmental observations and behavioral histories. The approach comprises: 1) a language-conditioned world model that imagines future states serving dual purposes: encoding experiences for storage and generating retrieval queries; 2) Hybrid Viewpoint-Level Memory that anchors both observations and behavioral patterns to viewpoints, enabling hybrid retrieval; and 3) an experience-augmented navigation model that integrates retrieved knowledge through specialized encoders. Extensive evaluation across diverse memory-persistent VLN benchmarks with 10 distinctive testing scenarios demonstrates Memoir's effectiveness: significant improvements across all scenarios, with 5.4% SPL gains on IR2R over the best memory-persistent baseline, accompanied by 8.3x training speedup and 74% inference memory reduction. The results validate that predictive retrieval of both environmental and behavioral memories enables more effective navigation, with analysis indicating substantial headroom (73.3% vs 93.4% upper bound) for this imagination-guided paradigm. Code at https://github.com/xyz9911/Memoir.<br>
<span id='abs_ch'>中文: Memoir系统通过基于想象的记忆检索机制，选择性获取环境观察和行为模式，在多个导航基准测试中实现了性能显著提升、训练 速和推理内存降低。</span><br>
<span id='abs_en'>English: The proposed Memoir system enhances memory-persistent Vision-and-Language Navigation by using a world model to imaginatively retrieve relevant environmental observations and behavioral patterns, achieving significant performance gains, faster training, and reduced inference memory across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2510.08531.pdf' target='_blank'>https://arxiv.org/pdf/2510.08531.pdf</a></span>   <span><a href='https://github.com/ZJU-REAL/SpatialLadder' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongxing Li, Dingming Li, Zixuan Wang, Yuchen Yan, Hang Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08531">SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spatial reasoning remains a fundamental challenge for Vision-Language Models (VLMs), with current approaches struggling to achieve robust performance despite recent advances. We identify that this limitation stems from a critical gap: existing methods attempt to learn spatial reasoning directly without establishing the hierarchical foundations of perception and understanding. To address this challenge, we present a comprehensive methodology for building spatial intelligence progressively. We introduce SpatialLadder-26k, a multimodal dataset containing 26,610 samples spanning object localization, single image, multi-view, and video spatial reasoning tasks, constructed through a standardized pipeline that ensures systematic coverage across modalities. Building on this dataset, we design a three-stage progressive training framework that (1) establishes spatial perception through object localization, (2) develops spatial understanding through multi-dimensional spatial tasks, and (3) strengthens complex reasoning via reinforcement learning with verifiable rewards. This approach yields SpatialLadder, a 3B-parameter model that achieves state-of-the-art performance on spatial reasoning benchmarks, with 23.4% average improvement over the base model, surpassing GPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains strong generalization with 7.2% improvement on out-of-domain benchmarks, demonstrating that progressive training from perception to reasoning is essential for robust spatial intelligence.<br>
<span id='abs_ch'>中文摘要：该 究提出SpatialLadder模型，通过基于SpatialLadder-26k数据集的三阶段渐进训练框架，在空间推理任务中实现最先进性能，相比基础模型和GPT-4o等竞争者取得显著提升，并展现出优异的泛化能力。</span><br>
<span id='abs_en'>English Summary: The study introduces SpatialLadder, a 3B-parameter model trained through a progressive three-stage framework on the SpatialLadder-26k dataset, achieving state-of-the-art spatial reasoning performance with significant improvements over base models and competitors like GPT-4o and Gemini-2.0-Flash while demonstrating strong generalization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2510.08521.pdf' target='_blank'>https://arxiv.org/pdf/2510.08521.pdf</a></span>   <span><a href='https://github.com/Alpha-Innovator/InternAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yusong Hu, Runmin Ma, Yue Fan, Jinxin Shi, Zongsheng Cao, Yuhao Zhou, Jiakang Yuan, Xiangchao Yan, Wenlong Zhang, Lei Bai, Bo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08521">FlowSearch: Advancing deep research with dynamic structured knowledge flow</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Deep research is an inherently challenging task that demands both breadth and depth of thinking. It involves navigating diverse knowledge spaces and reasoning over complex, multi-step dependencies, which presents substantial challenges for agentic systems. To address this, we propose FlowSearch, a multi-agent framework that actively constructs and evolves a dynamic structured knowledge flow to drive subtask execution and reasoning. FlowSearch is capable of strategically planning and expanding the knowledge flow to enable parallel exploration and hierarchical task decomposition, while also adjusting the knowledge flow in real time based on feedback from intermediate reasoning outcomes and insights. FlowSearch achieves state-of-the-art performance on both general and scientific benchmarks, including GAIA, HLE, GPQA and TRQA, demonstrating its effectiveness in multi-disciplinary research scenarios and its potential to advance scientific discovery. The code is available at https://github.com/Alpha-Innovator/InternAgent.<br>
<span id='abs_ch'>中文: FlowSearch是一种多智能体框架，通过动态构建和演进知识流，实现战略规划、并行探索和实时调整，在复杂 究任务中取得了顶尖性能。</span><br>
<span id='abs_en'>English: FlowSearch is a multi-agent framework that dynamically constructs and evolves a knowledge flow to enable strategic planning, parallel exploration, and real-time adjustments for achieving state-of-the-art performance in complex research tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2510.08511.pdf' target='_blank'>https://arxiv.org/pdf/2510.08511.pdf</a></span>   <span><a href='https://github.com/Alpha-Innovator/InternAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangheng Du, Xiangchao Yan, Dengyang Jiang, Jiakang Yuan, Yusong Hu, Xin Li, Liang He, Bo Zhang, Lei Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08511">AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have shown impressive performance in general programming tasks. However, in Machine Learning Engineering (MLE) scenarios such as AutoML and Kaggle competitions, achieving high performance depends heavily on expert intervention and repeated adjustments rather than simply generating correct code. When applied directly to these tasks, LLMs often lack fine-grained domain priors, and existing MLE approaches that use linear or tree-structured searches limit knowledge transfer to adjacent hierarchical links. As a result, they cannot leverage past full trajectories or share information across branches, limiting self-evolving ability and search space diversity. To address these limitations, we introduce AutoMLGen, an LLM-based coding agent that integrates a domain knowledge base for high-quality prior guidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS retains the tree-guided exploration of MCTS while embedding a graph structure into the expansion stage to enable dynamic path reorganization, historical trajectory reuse, and multi-solution fusion to support both self-evolution and collaborative learning. Combined with fine-grained operator sets, this design improves stability and accelerates convergence. Evaluation on the MLE-Bench shows that AutoMLGen achieves state-of-the-art performance in numerous dimensions, such as the average medal rate and the valid submission rate, under a 12-hour budget (half the standard runtime). The code is available at https://github.com/Alpha-Innovator/InternAgent.<br>
<span id='abs_ch'>中文：AutoMLGen是一种基于大语言模型的先进编 代理，通过整合领域知识和蒙特卡洛图搜索，在有限时间内显著提升了机器学 工程的效率与性能，实现了多项指 的领先水平。</span><br>
<span id='abs_en'>English: AutoMLGen is an advanced LLM-based coding agent that enhances machine learning engineering by integrating domain knowledge and Monte Carlo Graph Search, achieving top performance in efficiency and effectiveness under constrained time budgets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2510.08445.pdf' target='_blank'>https://arxiv.org/pdf/2510.08445.pdf</a></span>   <span><a href='https://github.com/wwhenxuan/SymTime' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenxuan Wang, Kai Wu, Yujian Betterest Li, Dan Wang, Xiaoyu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08445">Synthetic Series-Symbol Data Generation for Time Series Foundation Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Foundation models for time series analysis (TSA) have attracted significant attention. However, challenges such as training data scarcity and imbalance continue to hinder their development. Inspired by complex dynamic system theories, we design a series-symbol data generation mechanism, enabling the unrestricted creation of high-quality time series data paired with corresponding symbolic expressions. To leverage series-symbol data pairs with strong correlations, we develop \texttt{SymTime}, a pre-trained foundation model for enhancing time series representation using symbolic information. \texttt{SymTime} demonstrates competitive performance across five major TSA tasks when fine-tunes with downstream tasks, rivaling foundation models pre-trained on real-world datasets. This approach underscores the potential of series-symbol data generation and pretraining mechanisms in overcoming data scarcity and enhancing task performance. The code is available at https://github.com/wwhenxuan/SymTime.<br>
<span id='abs_ch'>中文: 本 究提出了SymTime基础模型，通过创新的序列-符号数据生成机制解决时间序列分析中的数据稀缺问题，在多个任务中展现出与真实数据集预训练模型相媲美的性能。</span><br>
<span id='abs_en'>English: The study introduces SymTime, a foundation model that utilizes a novel series-symbol data generation method to address data scarcity in time series analysis, achieving competitive performance across multiple tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2510.08396.pdf' target='_blank'>https://arxiv.org/pdf/2510.08396.pdf</a></span>   <span><a href='https://github.com/gfyddha/FlyLoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Heming Zou, Yunliang Zang, Wutong Xu, Yao Zhu, Xiangyang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08396">FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning method for foundation models, but it suffers from parameter interference, resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based LoRA variants show promise in mitigating intra-task correlations in single-task instruction tuning, they introduce additional router parameters and remain ineffective in multi-task model merging where inter-task interference arises. Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the up-projection matrix, and (2) an implicit router that unifies expert routing and down-projection, where a frozen sparse random projection matrix replaces the traditional dense trainable version. This design resolves the trade-off between intra-task decorrelation and computational efficiency by eliminating the need for an explicit router, while inherently mitigating inter-task interference due to the orthogonality property of random matrices. Extensive experiments across four domains -- general knowledge understanding, scientific question answering, mathematical reasoning, and code generation -- demonstrate consistent performance improvements over existing methods. Beyond empirical gains, FlyLoRA highlights how biological structures can inspire innovations in AI technologies. Code is available at https://github.com/gfyddha/FlyLoRA.<br>
<span id='abs_ch'>中文摘要：FlyLoRA是一种受生物嗅觉启发的LoRA改进方法，通过秩级专家激活和隐式路由设计， 需显式路由器即可同时解决任务内和任务间的参数干扰问题，在多项领域实现了性能提升。</span><br>
<span id='abs_en'>English Summary: FlyLoRA is a biologically-inspired LoRA variant that eliminates explicit routers through rank-wise expert activation and implicit routing, effectively addressing both intra-task and inter-task parameter interference while improving performance across multiple domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2510.08383.pdf' target='_blank'>https://arxiv.org/pdf/2510.08383.pdf</a></span>   <span><a href='https://github.com/OpenStellarTeam/QAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Jiang, Lei Shen, Lujie Niu, Sendong Zhao, Wenbo Su, Bo Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08383">QAgent: A modular Search Agent with Interactive Query Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) excel at natural language tasks but are limited by their static parametric knowledge, especially in knowledge-intensive task. Retrieval-augmented generation (RAG) mitigates this by integrating external information. However, (1) traditional RAG struggles with complex query understanding, and (2) even search agents trained with reinforcement learning (RL), despite their promise, still face generalization and deployment challenges. To address these limitations, we propose QAgent, a unified agentic RAG framework that employs a search agent for adaptive retrieval. This agent optimizes its understanding of the query through interactive reasoning and retrieval. To facilitate real-world application, we focus on modular search agent for query understanding that are plug-and-play in complex systems. Secifically, the agent follows a multi-step decision process trained with RL to maximize retrieval quality and support accurate downstream answers. We further analyze the strengths and weaknesses of end-to-end RL and propose a strategy that focuses on effective retrieval, thereby enhancing generalization in LLM applications. Experiments show QAgent excels at QA and serves as a plug-and-play module for real-world deployment.<br>
<span id='abs_ch'>Chinese: QAgent是一种统一的代理式检索增强生成框架，它通过采用强化学 训练的搜索代理进行自适应检索和交互式推理，提升了实际应用中的泛化能力和部署效果。</span><br>
<span id='abs_en'>English: QAgent is a unified agentic RAG framework that enhances retrieval-augmented generation by employing a search agent trained with reinforcement learning for adaptive retrieval and interactive reasoning, improving generalization and deployment in real-world applications.Paperid:13,https://arxiv.org/pdf/2510.08300.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2510.08300.pdf' target='_blank'>https://arxiv.org/pdf/2510.08300.pdf</a></span>   <span><a href='https://github.com/daniuyter/scalegmn_amortization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bart Kuipers, Freek Byrman, Daniel Uyterlinde, Alejandro García-Castellanos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08300">Symmetry-Aware Fully-Amortized Optimization with Scale Equivariant Graph Metanetworks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Amortized optimization accelerates the solution of related optimization problems by learning mappings that exploit shared structure across problem instances. We explore the use of Scale Equivariant Graph Metanetworks (ScaleGMNs) for this purpose. By operating directly in weight space, ScaleGMNs enable single-shot fine-tuning of existing models, reducing the need for iterative optimization. We demonstrate the effectiveness of this approach empirically and provide a theoretical result: the gauge freedom induced by scaling symmetries is strictly smaller in convolutional neural networks than in multi-layer perceptrons. This insight helps explain the performance differences observed between architectures in both our work and that of Kalogeropoulos et al. (2024). Overall, our findings underscore the potential of symmetry-aware metanetworks as a powerful approach for efficient and generalizable neural network optimization. Open-source code: https://github.com/daniuyter/scalegmn_amortization<br>
<span id='abs_ch'>中文: 尺度等变图元网络通过利用缩放对称性实现模型的单次快速微调，其性能优于 统迭代优化方法，并揭示了卷积神经网络中对称性自由度的理论优势。</span><br>
<span id='abs_en'>English: Scale Equivariant Graph Metanetworks enable efficient single-shot fine-tuning of models by leveraging scaling symmetries, outperforming traditional iterative optimization methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2510.08026.pdf' target='_blank'>https://arxiv.org/pdf/2510.08026.pdf</a></span>   <span><a href='https://github.com/iNLP-Lab/PEAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Huang, Wei Lu, Wenxuan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08026">PEAR: Phase Entropy Aware Reward for Efficient Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Reasoning Models (LRMs) have achieved impressive performance on complex reasoning tasks by generating detailed chain-of-thought (CoT) explanations. However, these responses are often excessively long, containing redundant reasoning steps that inflate inference cost and reduce usability. Controlling the length of generated reasoning without sacrificing accuracy remains an open challenge. Through a systematic empirical analysis, we reveal a consistent positive correlation between model entropy and response length at different reasoning stages across diverse LRMs: the thinking phase exhibits higher entropy, reflecting exploratory behavior of longer responses, while the final answer phase shows lower entropy, indicating a more deterministic solution.This observation suggests that entropy at different reasoning stages can serve as a control knob for balancing conciseness and performance. Based on this insight, this paper introduces Phase Entropy Aware Reward (PEAR), a reward mechanism that incorporating phase-dependent entropy into the reward design. Instead of treating all tokens uniformly, PEAR penalize excessive entropy during the thinking phase and allowing moderate exploration at the final answer phase, which encourages models to generate concise reasoning traces that retain sufficient flexibility to solve the task correctly. This enables adaptive control of response length without relying on explicit length targets or rigid truncation rules. Extensive experiments across four benchmarks demonstrate that PEAR consistently reduces response length while sustaining competitive accuracy across model scales. In addition, PEAR demonstrates strong out-of-distribution (OOD) robustness beyond the training distribution. Our code is available at: https://github.com/iNLP-Lab/PEAR.<br>
<span id='abs_ch'>大型推理模型生成的思维链响应往往冗长冗余，而提出的阶段熵感知奖励（PEAR）机制通过控制不同推理阶段的熵，能在保持准确性和鲁棒性的同时有效缩短响应长度。</span><br>
<span id='abs_en'>Large Reasoning Models generate lengthy chain-of-thought responses with redundant steps, but the proposed Phase Entropy Aware Reward (PEAR) mechanism effectively reduces response length by controlling entropy at different reasoning phases while maintaining accuracy and robustness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2510.08022.pdf' target='_blank'>https://arxiv.org/pdf/2510.08022.pdf</a></span>   <span><a href='https://github.com/MrKeee/FastUMI-100K' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kehui Liu, Zhongjie Jia, Yang Li, Zhaxizhuoma, Pengan Chen, Song Liu, Xin Liu, Pingrui Zhang, Haoming Song, Xinyi Ye, Nieqing Cao, Zhigang Wang, Jia Zeng, Dong Wang, Yan Ding, Bin Zhao, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.08022">FastUMI-100K: Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Data-driven robotic manipulation learning depends on large-scale, high-quality expert demonstration datasets. However, existing datasets, which primarily rely on human teleoperated robot collection, are limited in terms of scalability, trajectory smoothness, and applicability across different robotic embodiments in real-world environments. In this paper, we present FastUMI-100K, a large-scale UMI-style multimodal demonstration dataset, designed to overcome these limitations and meet the growing complexity of real-world manipulation tasks. Collected by FastUMI, a novel robotic system featuring a modular, hardware-decoupled mechanical design and an integrated lightweight tracking system, FastUMI-100K offers a more scalable, flexible, and adaptable solution to fulfill the diverse requirements of real-world robot demonstration data. Specifically, FastUMI-100K contains over 100K+ demonstration trajectories collected across representative household environments, covering 54 tasks and hundreds of object types. Our dataset integrates multimodal streams, including end-effector states, multi-view wrist-mounted fisheye images and textual annotations. Each trajectory has a length ranging from 120 to 500 frames. Experimental results demonstrate that FastUMI-100K enables high policy success rates across various baseline algorithms, confirming its robustness, adaptability, and real-world applicability for solving complex, dynamic manipulation challenges. The source code and dataset will be released in this link https://github.com/MrKeee/FastUMI-100K.<br>
<span id='abs_ch'>中文：FastUMI-100K作为大规模多模态机器人演示数据集，通过提供超过10万条多 化轨迹克服了 统数据集的可扩展性和适应性限制，能够在各类操作任务中实现高成功率。</span><br>
<span id='abs_en'>English: FastUMI-100K is a large-scale multimodal robotic demonstration dataset that overcomes scalability and adaptability limitations of traditional datasets by offering over 100,000 diverse trajectories, enabling high success rates across various manipulation tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2510.07962.pdf' target='_blank'>https://arxiv.org/pdf/2510.07962.pdf</a></span>   <span><a href='https://github.com/HKUDS/LightReasoner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyuan Wang, Yankai Chen, Zhonghang Li, Chao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07962">LightReasoner: Can Small Language Models Teach Large Language Models Reasoning?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have demonstrated remarkable progress in reasoning, often through supervised fine-tuning (SFT). However, SFT is resource-intensive, relying on large curated datasets, rejection-sampled demonstrations, and uniform optimization across all tokens, even though only a fraction carry meaningful learning value. In this work, we explore a counterintuitive idea: can smaller language models (SLMs) teach larger language models (LLMs) by revealing high-value reasoning moments that reflect the latter's unique strength? We propose LightReasoner, a novel framework that leverages the behavioral divergence between a stronger expert model (LLM) and a weaker amateur model (SLM). LightReasoner operates in two stages: (1) a sampling stage that pinpoints critical reasoning moments and constructs supervision examples capturing the expert's advantage through expert-amateur contrast, and (2) a fine-tuning stage that aligns the expert model with these distilled examples, amplifying its reasoning strengths. Across seven mathematical benchmarks, LightReasoner improves accuracy by up to 28.1%, while reducing time consumption by 90%, sampled problems by 80%, and tuned token usage by 99%, all without relying on ground-truth labels. By turning weaker SLMs into effective teaching signals, LightReasoner offers a scalable and resource-efficient approach for advancing LLM reasoning. Code is available at: https://github.com/HKUDS/LightReasoner<br>
<span id='abs_ch'>Chinese: LightReasoner 是一种创新框架，通过专家-业余模型对比识别关键推理节点，使小型语言模型能够指导大型模型，在 需真实 签的情况下显著提升数学推理的准确性与效率。</span><br>
<span id='abs_en'>English: LightReasoner is a novel framework that enables smaller language models to teach larger ones by identifying critical reasoning moments through expert-amateur contrast, achieving significant improvements in accuracy and efficiency across mathematical benchmarks without ground-truth labels.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2510.07959.pdf' target='_blank'>https://arxiv.org/pdf/2510.07959.pdf</a></span>   <span><a href='https://github.com/arubique/disco-public' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Rubinstein, Benjamin Raible, Martin Gubri, Seong Joon Oh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07959">DISCO: Diversifying Sample Condensation for Efficient Model Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Evaluating modern machine learning models has become prohibitively expensive. Benchmarks such as LMMs-Eval and HELM demand thousands of GPU hours per model. Costly evaluation reduces inclusivity, slows the cycle of innovation, and worsens environmental impact. The typical approach follows two steps. First, select an anchor subset of data. Second, train a mapping from the accuracy on this subset to the final test result. The drawback is that anchor selection depends on clustering, which can be complex and sensitive to design choices. We argue that promoting diversity among samples is not essential; what matters is to select samples that $\textit{maximise diversity in model responses}$. Our method, $\textbf{Diversifying Sample Condensation (DISCO)}$, selects the top-k samples with the greatest model disagreements. This uses greedy, sample-wise statistics rather than global clustering. The approach is conceptually simpler. From a theoretical view, inter-model disagreement provides an information-theoretically optimal rule for such greedy selection. $\textbf{DISCO}$ shows empirical gains over prior methods, achieving state-of-the-art results in performance prediction across MMLU, Hellaswag, Winogrande, and ARC. Code is available here: https://github.com/arubique/disco-public.<br>
<span id='abs_ch'>中文摘要：DISCO方法通过选择模型响应差异最大的 本简化了模型评估，采用贪心 本统计而非复杂聚类，实现了最优性能预测。English Summary: The DISCO method simplifies model evaluation by selecting samples that maximize diversity in model responses, using a greedy, sample-wise approach to achieve state-of-the-art performance without complex clustering.Paperid:30,https://arxiv.org/pdf/2510.07958.pdfGitHub</span><br>
<span id='abs_en'>English Summary: The DISCO method simplifies model evaluation by selecting samples that maximize diversity in model responses, using a greedy, sample-wise approach to achieve state-of-the-art performance without complex clustering.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2510.07958.pdf' target='_blank'>https://arxiv.org/pdf/2510.07958.pdf</a></span>   <span><a href='https://github.com/zfj1998/A2Search' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengji Zhang, Xinyao Niu, Chengyang Ying, Guancheng Lin, Zhongkai Hao, Zhou Fan, Chengen Huang, Jacky Keung, Bei Chen, Junyang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07958">A$^2$Search: Ambiguity-Aware Question Answering with Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in Large Language Models (LLMs) and Reinforcement Learning (RL) have led to strong performance in open-domain question answering (QA). However, existing models still struggle with questions that admit multiple valid answers. Standard QA benchmarks, which typically assume a single gold answer, overlook this reality and thus produce inappropriate training signals. Existing attempts to handle ambiguity often rely on costly manual annotation, which is difficult to scale to multi-hop datasets such as HotpotQA and MuSiQue. In this paper, we present A$^2$Search, an annotation-free, end-to-end training framework to recognize and handle ambiguity. At its core is an automated pipeline that detects ambiguous questions and gathers alternative answers via trajectory sampling and evidence verification. The model is then optimized with RL using a carefully designed $\mathrm{AnsF1}$ reward, which naturally accommodates multiple answers. Experiments on eight open-domain QA benchmarks demonstrate that A$^2$Search achieves new state-of-the-art performance. With only a single rollout, A$^2$Search-7B yields an average $\mathrm{AnsF1}@1$ score of $48.4\%$ across four multi-hop benchmarks, outperforming all strong baselines, including the substantially larger ReSearch-32B ($46.2\%$). Extensive analyses further show that A$^2$Search resolves ambiguity and generalizes across benchmarks, highlighting that embracing ambiguity is essential for building more reliable QA systems. Our code, data, and model weights can be found at https://github.com/zfj1998/A2Search<br>
<span id='abs_ch'>中文: 本文提出A²Search这一 需人工 注的框架，通过轨迹采 和证据验证自动识别模糊问题并收集替代答案，采用支持多答案的AnsF1奖励进行强化学 优化，在多个开放域问答基准测试中实现了最先进的性能。</span><br>
<span id='abs_en'>English: This paper introduces A²Search, an annotation-free framework that automatically detects ambiguous questions and gathers alternative answers through trajectory sampling and evidence verification, achieving state-of-the-art performance on multiple QA benchmarks by optimizing with a novel AnsF1 reward that accommodates multiple valid answers.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2510.07861.pdf' target='_blank'>https://arxiv.org/pdf/2510.07861.pdf</a></span>   <span><a href='https://github.com/HKUDS/DeepResearch-Eval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Fan, Xinyao Niu, Yuxiang Zheng, Fengji Zhang, Chengen Huang, Bei Chen, Junyang Lin, Chao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07861">Understanding DeepResearch via Reports</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>DeepResearch agents represent a transformative AI paradigm, conducting expert-level research through sophisticated reasoning and multi-tool integration. However, evaluating these systems remains critically challenging due to open-ended research scenarios and existing benchmarks that focus on isolated capabilities rather than holistic performance. Unlike traditional LLM tasks, DeepResearch systems must synthesize diverse sources, generate insights, and present coherent findings, which are capabilities that resist simple verification. To address this gap, we introduce DeepResearch-ReportEval, a comprehensive framework designed to assess DeepResearch systems through their most representative outputs: research reports. Our approach systematically measures three dimensions: quality, redundancy, and factuality, using an innovative LLM-as-a-Judge methodology achieving strong expert concordance. We contribute a standardized benchmark of 100 curated queries spanning 12 real-world categories, enabling systematic capability comparison. Our evaluation of four leading commercial systems reveals distinct design philosophies and performance trade-offs, establishing foundational insights as DeepResearch evolves from information assistants toward intelligent research partners. Source code and data are available at: https://github.com/HKUDS/DeepResearch-Eval.<br>
<span id='abs_ch'>中文总结：DeepResearch-ReportEval框架通过采用LLM即评判的创新方法，从质量、冗余性和事实性三个维度系统评估 究报告，解决了AI 究智能体的评估难题。</span><br>
<span id='abs_en'>English Summary: The DeepResearch-ReportEval framework addresses the evaluation gap for AI research agents by systematically assessing research reports across quality, redundancy, and factuality dimensions using LLM-as-a-Judge methodology.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2510.07835.pdf' target='_blank'>https://arxiv.org/pdf/2510.07835.pdf</a></span>   <span><a href='https://github.com/ws-jiang/MetaDefense' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weisen Jiang, Sinno Jialin Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07835">MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper introduces MetaDefense, a novel framework for defending against finetuning-based jailbreak attacks in large language models (LLMs). We observe that existing defense mechanisms fail to generalize to harmful queries disguised by unseen attack templates, despite LLMs being capable of distinguishing disguised harmful queries in the embedding space. Based on these insights, we propose a two-stage defense approach: (i) pre-generation defense that detects harmful queries before response generation begins, and (ii) mid-generation defense that monitors partial responses during generation to prevent outputting more harmful content. Our MetaDefense trains the LLM to predict the harmfulness of both queries and partial responses using specialized prompts, enabling early termination of potentially harmful interactions. Extensive experiments across multiple LLM architectures (LLaMA-2-7B, Qwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct) demonstrate that MetaDefense significantly outperforms existing defense mechanisms, achieving robust defense against harmful queries with seen and unseen attack templates while maintaining competitive performance on benign tasks. Code is available at https://github.com/ws-jiang/MetaDefense.<br>
<span id='abs_ch'>中文: MetaDefense是一种新颖的双阶段防御框架，通过训练大语言模型检测有害查询并监控生成过程中的部分响应，在多种模型架构上显著优于现有防御机制，同时保持良性任务的性能表现。</span><br>
<span id='abs_en'>English: MetaDefense is a novel two-stage framework that defends LLMs against jailbreak attacks by training them to detect harmful queries and monitor partial responses, significantly outperforming existing defenses across various models while maintaining performance on benign tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2510.07825.pdf' target='_blank'>https://arxiv.org/pdf/2510.07825.pdf</a></span>   <span><a href='https://github.com/usail-hkust/CityNav' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuping Zhou, Siqi Lai, Jindong Han, Hao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07825">An LLM-Powered Cooperative Framework for Large-Scale Multi-Vehicle Navigation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rise of Internet of Vehicles (IoV) technologies is transforming traffic management from isolated control to a collective, multi-vehicle process. At the heart of this shift is multi-vehicle dynamic navigation, which requires simultaneously routing large fleets under evolving traffic conditions. Existing path search algorithms and reinforcement learning methods struggle to scale to city-wide networks, often failing to capture the nonlinear, stochastic, and coupled dynamics of urban traffic. To address these challenges, we propose CityNav, a hierarchical, LLM-powered framework for large-scale multi-vehicle navigation. CityNav integrates a global traffic allocation agent, which coordinates strategic traffic flow distribution across regions, with local navigation agents that generate locally adaptive routes aligned with global directives. To enable effective cooperation, we introduce a cooperative reasoning optimization mechanism, in which agents are jointly trained with a dual-reward structure: individual rewards promote per-vehicle efficiency, while shared rewards encourage network-wide coordination and congestion reduction. Extensive experiments on four real-world road networks of varying scales (up to 1.6 million roads and 430,000 intersections) and traffic datasets demonstrate that CityNav consistently outperforms nine classical path search and RL-based baselines in city-scale travel efficiency and congestion mitigation. Our results highlight the potential of LLMs to enable scalable, adaptive, and cooperative city-wide traffic navigation, providing a foundation for intelligent, large-scale vehicle routing in complex urban environments. Our project is available at https://github.com/usail-hkust/CityNav.<br>
<span id='abs_ch'>Chinese: CityNav是一个基于大语言模型的分层框架，通过整合全局交通分配与局部自适应路径规划，在大规模城市路网中显著提升了多车辆导航效率并有效缓解交通拥 。</span><br>
<span id='abs_en'>English: CityNav is a hierarchical, LLM-powered framework that enhances large-scale multi-vehicle navigation by integrating global traffic allocation with local adaptive routing, outperforming existing methods in efficiency and congestion reduction across extensive urban networks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2510.07790.pdf' target='_blank'>https://arxiv.org/pdf/2510.07790.pdf</a></span>   <span><a href='https://github.com/AchoWu/GCPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wu, Wei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07790">GCPO: When Contrast Fails, Go Gold</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning has been widely applied to enhance the reasoning capabilities of large language models. Extending the inference limits of smaller models has become a prominent research focus. However, algorithms such as Group Relative Policy Optimization (GRPO) suffer from a clear drawback: the upper bound of a model's rollout responses is entirely determined by the model itself, preventing the acquisition of knowledge from samples that are either all incorrect or all correct. In this paper, we introduce Group Contrastive Policy Optimization (GCPO), a method that incorporates external standard reference answers. When the model cannot solve a problem, the reference answer supplies the correct response, steering the model toward an unequivocally accurate update direction. This approach offers two main advantages: (1) it improves training efficiency by fully utilizing every sample; (2) it enables the model to emulate the problem solving strategy of the reference answer during training, thereby enhancing generalization in reasoning. GCPO achieves outstanding results across multiple benchmark datasets, yielding substantial improvements over the baseline model. Our code is available at: https://github.com/AchoWu/GCPO.<br>
<span id='abs_ch'>中文: 本文提出群组对比策略优化（GCPO）方法，通过引入外部 准参考答案引导模型更新方向，充分利用所有 本提升训练效率，并在多基准测试中显著提升模型的推理泛化能力。</span><br>
<span id='abs_en'>English: This paper introduces Group Contrastive Policy Optimization (GCPO), a reinforcement learning method that incorporates external reference answers to guide model updates, enhancing training efficiency and reasoning generalization by utilizing all samples effectively.Paperid:42,https://arxiv.org/pdf/2510.07785.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2510.07745.pdf' target='_blank'>https://arxiv.org/pdf/2510.07745.pdf</a></span>   <span><a href='https://github.com/YRYangang/LatentTTS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Runyang You, Yongqi Li, Meng Liu, Wenjie Wang, Liqiang Nie, Wenjie Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07745">Parallel Test-Time Scaling for Latent Reasoning Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Parallel test-time scaling (TTS) is a pivotal approach for enhancing large language models (LLMs), typically by sampling multiple token-based chains-of-thought in parallel and aggregating outcomes through voting or search. Recent advances in latent reasoning, where intermediate reasoning unfolds in continuous vector spaces, offer a more efficient alternative to explicit Chain-of-Thought, yet whether such latent models can similarly benefit from parallel TTS remains open, mainly due to the absence of sampling mechanisms in continuous space, and the lack of probabilistic signals for advanced trajectory aggregation. \ This work enables parallel TTS for latent reasoning models by addressing the above issues. For sampling, we introduce two uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM) trained with step-wise contrastive objective to score and guide latent reasoning. Extensive experiments and visualization analyses show that both sampling strategies scale effectively with compute and exhibit distinct exploration dynamics, while LatentRM enables effective trajectory selection. Together, our explorations open a new direction for scalable inference in continuous spaces. Code released at https://github.com/YRYangang/LatentTTS.<br>
<br>
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2510.07650.pdf' target='_blank'>https://arxiv.org/pdf/2510.07650.pdf</a></span>   <span><a href='https://github.com/chongyi-zheng/value-flows' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Perry Dong, Chongyi Zheng, Chelsea Finn, Dorsa Sadigh, Benjamin Eysenbach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07650">Value Flows</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While most reinforcement learning methods today flatten the distribution of future returns to a single scalar value, distributional RL methods exploit the return distribution to provide stronger learning signals and to enable applications in exploration and safe RL. While the predominant method for estimating the return distribution is by modeling it as a categorical distribution over discrete bins or estimating a finite number of quantiles, such approaches leave unanswered questions about the fine-grained structure of the return distribution and about how to distinguish states with high return uncertainty for decision-making. The key idea in this paper is to use modern, flexible flow-based models to estimate the full future return distributions and identify those states with high return variance. We do so by formulating a new flow-matching objective that generates probability density paths satisfying the distributional Bellman equation. Building upon the learned flow models, we estimate the return uncertainty of distinct states using a new flow derivative ODE. We additionally use this uncertainty information to prioritize learning a more accurate return estimation on certain transitions. We compare our method (Value Flows) with prior methods in the offline and online-to-online settings. Experiments on $37$ state-based and $25$ image-based benchmark tasks demonstrate that Value Flows achieves a $1.3\times$ improvement on average in success rates. Website: https://pd-perry.github.io/value-flows Code: https://github.com/chongyi-zheng/value-flows<br>
<span id='abs_ch'>中文: 本文提出Value Flows方法，通过流模型估计完整未来回报分布并识别高方差状态，在62项基准任务中平均成功率提升1.3倍。</span><br>
<span id='abs_en'>English: This paper introduces Value Flows, a reinforcement learning method that employs flow-based models to estimate full future return distributions and identify high-return-variance states, achieving a 1.3× average success rate improvement across 62 benchmark tasks.Paperid:47,https://arxiv.org/pdf/2510.07624.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2510.07586.pdf' target='_blank'>https://arxiv.org/pdf/2510.07586.pdf</a></span>   <span><a href='https://github.com/tgm-team/tgm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jacob Chmura, Shenyang Huang, Tran Gia Bao Ngo, Ali Parviz, Farimah Poursafaei, Jure Leskovec, Michael Bronstein, Guillaume Rabusseau, Matthias Fey, Reihaneh Rabbany
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07586">TGM: a Modular and Efficient Library for Machine Learning on Temporal Graphs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Well-designed open-source software drives progress in Machine Learning (ML) research. While static graph ML enjoys mature frameworks like PyTorch Geometric and DGL, ML for temporal graphs (TG), networks that evolve over time, lacks comparable infrastructure. Existing TG libraries are often tailored to specific architectures, hindering support for diverse models in this rapidly evolving field. Additionally, the divide between continuous- and discrete-time dynamic graph methods (CTDG and DTDG) limits direct comparisons and idea transfer. To address these gaps, we introduce Temporal Graph Modelling (TGM), a research-oriented library for ML on temporal graphs, the first to unify CTDG and DTDG approaches. TGM offers first-class support for dynamic node features, time-granularity conversions, and native handling of link-, node-, and graph-level tasks. Empirically, TGM achieves an average 7.8x speedup across multiple models, datasets, and tasks compared to the widely used DyGLib, and an average 175x speedup on graph discretization relative to available implementations. Beyond efficiency, we show in our experiments how TGM unlocks entirely new research possibilities by enabling dynamic graph property prediction and time-driven training paradigms, opening the door to questions previously impractical to study. TGM is available at https://github.com/tgm-team/tgm<br>
<span id='abs_ch'>中文: Temporal Graph Modelling (TGM) 库作为首个统一处理时序图机器学 的框架，弥合了连续与离散时间方法之间的鸿沟，在提供卓越效率的同时开启了全新的 究可能性。</span><br>
<span id='abs_en'>English: The Temporal Graph Modelling (TGM) library is introduced as the first unified framework for machine learning on temporal graphs, bridging the gap between continuous- and discrete-time approaches while offering superior efficiency and enabling novel research capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2510.07556.pdf' target='_blank'>https://arxiv.org/pdf/2510.07556.pdf</a></span>   <span><a href='https://github.com/milab-nsu/S3FN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rafin Hassan, Zarin Tasnim Roshni, Rafiqul Bari, Alimul Islam, Nabeel Mohammed, Moshiur Farazi, Shafin Rahman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07556">Label Semantics for Robust Hyperspectral Image Classification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Hyperspectral imaging (HSI) classification is a critical tool with widespread applications across diverse fields such as agriculture, environmental monitoring, medicine, and materials science. Due to the limited availability of high-quality training samples and the high dimensionality of spectral data, HSI classification models are prone to overfitting and often face challenges in balancing accuracy and computational complexity. Furthermore, most of HSI classification models are monomodal, where it solely relies on spectral-spatial data to learn decision boundaries in the high dimensional embedding space. To address this, we propose a general-purpose Semantic Spectral-Spatial Fusion Network (S3FN) that uses contextual, class specific textual descriptions to complement the training of an HSI classification model. Specifically, S3FN leverages LLMs to generate comprehensive textual descriptions for each class label that captures their unique characteristics and spectral behaviors. These descriptions are then embedded into a vector space using a pre-trained text encoder such as BERT or RoBERTa to extract meaningful label semantics which in turn leads to a better feature-label alignment for improved classification performance. To demonstrate the effectiveness of our approach, we evaluate our model on three diverse HSI benchmark datasets - Hyperspectral Wood, HyperspectralBlueberries, and DeepHS-Fruit and report significant performance boost. Our results highlight the synergy between textual semantics and spectral-spatial data, paving the way for further advancements in semantically augmented HSI classification models. Codes are be available in: https://github.com/milab-nsu/S3FN<br>
<span id='abs_ch'>Chinese: 针对高光谱图像分类中训练 本有限和模型易过拟合的问题，我们提出语义光谱-空间融合网络（S3FN），通过大语言模型生成类别文本描述并与光谱空间特征融合，在多个基准数据集上实现了分类性能的显著提升。</span><br>
<span id='abs_en'>English: To overcome overfitting and limited training data in hyperspectral imaging classification, we propose the Semantic Spectral-Spatial Fusion Network (S3FN), which integrates class-specific textual descriptions generated by large language models with spectral-spatial data to significantly enhance classification performance across multiple benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2510.07517.pdf' target='_blank'>https://arxiv.org/pdf/2510.07517.pdf</a></span>   <span><a href='https://github.com/deeplearning-wisc/MAD-identity-bias' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyeong Kyu Choi, Xiaojin Zhu, Yixuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07517">Measuring and Mitigating Identity Bias in Multi-Agent Debate via Anonymization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multi-agent debate (MAD) aims to improve large language model (LLM) reasoning by letting multiple agents exchange answers and then aggregate their opinions. Yet recent studies reveal that agents are not neutral: they are prone to identity-driven sycophancy and self-bias, uncritically adopting a peer's view or stubbornly adhering to their own prior output, undermining the reliability of debate. In this work, we present the first principled framework that joins sycophancy and self-bias to mitigate and quantify identity bias in MAD. First, we formalize the debate dynamics as an identity-weighted Bayesian update process. Second, we propose response anonymization: by removing identity markers from prompts, agents cannot distinguish "self" from "peer", which forces equal weights on agent identity, thereby reducing bias. Third, we define the Identity Bias Coefficient (IBC), a principled metric that measures how often an agent follows a peer versus itself. Empirical studies across multiple models, datasets and debate rounds confirm that identity bias is widespread, with sycophancy far more common than self-bias. Our findings highlight the need to "mask" identity to ensure that MAD systems reason based on content rather than source identity. Code is released in https://github.com/deeplearning-wisc/MAD-identity-bias.<br>
<span id='abs_ch'>中文摘要：本 究提出一个原则性框架，通过将辩论动态形式化为身份 权贝叶斯过程并实施响应匿名化来消除多智能体辩论中的身份偏见，同时引入身份偏见系数作为量化指 ，实验证明匿名化能有效提升辩论可 性。English Summary: This study introduces a principled framework to mitigate identity bias in multi-agent debate by formalizing debate dynamics as an identity-weighted Bayesian process and implementing response anonymization to force equal consideration of all agents' inputs, alongside proposing the Identity Bias Coefficient to quantify bias.Paperid:53,https://arxiv.org/pdf/2510.07507.pdfGitHub</span></span><br>
<span id='abs_en'>English Summary: This study introduces a principled framework to mitigate identity bias in multi-agent debate by formalizing debate dynamics as an identity-weighted Bayesian process and implementing response anonymization to force equal consideration of all agents' inputs, alongside proposing the Identity Bias Coefficient to quantify bias.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2510.07492.pdf' target='_blank'>https://arxiv.org/pdf/2510.07492.pdf</a></span>   <span><a href='https://github.com/MonkeyDadLufy/flow-matching' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoliang Gong, Man Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07492">A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based on an Image Purification Strategy</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Ultra-low dose CT (uLDCT) significantly reduces radiation exposure but introduces severe noise and artifacts. It also leads to substantial spatial misalignment between uLDCT and normal dose CT (NDCT) image pairs. This poses challenges for directly applying existing denoising networks trained on synthetic noise or aligned data. To address this core challenge in uLDCT denoising, this paper proposes an innovative denoising framework based on an Image Purification (IP) strategy. First, we construct a real clinical uLDCT lung dataset. Then, we propose an Image Purification strategy that generates structurally aligned uLDCT-NDCT image pairs, providing a high-quality data foundation for network training. Building upon this, we propose a Frequency-domain Flow Matching (FFM) model, which works synergistically with the IP strategy to excellently preserve the anatomical structure integrity of denoised images. Experiments on the real clinical dataset demonstrate that our IP strategy significantly enhances the performance of multiple mainstream denoising models on the uLDCT task. Notably, our proposed FFM model combined with the IP strategy achieves state-of-the-art (SOTA) results in anatomical structure preservation. This study provides an effective solution to the data mismatch problem in real-world uLDCT denoising. Code and dataset are available at https://github.com/MonkeyDadLufy/flow-matching.<br>
<span id='abs_ch'>中文: 本文提出图像净化策略和频域流匹配模型，解决超低剂量CT去噪中的噪声和空间错位问题，在真实临床数据上实现了最优的解剖结构保护效果。</span><br>
<span id='abs_en'>English: This paper introduces an Image Purification strategy and a Frequency-domain Flow Matching model to address severe noise and spatial misalignment in ultra-low dose CT denoising, achieving state-of-the-art structure preservation on real clinical data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2510.07459.pdf' target='_blank'>https://arxiv.org/pdf/2510.07459.pdf</a></span>   <span><a href='https://github.com/yolish/moe_unc_tsf' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yoli Shavit, Jacob Goldberger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07459">MoGU: Mixture-of-Gaussians with Uncertainty-based Gating for Time Series Forecasting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce Mixture-of-Gaussians with Uncertainty-based Gating (MoGU), a novel Mixture-of-Experts (MoE) framework designed for regression tasks and applied to time series forecasting. Unlike conventional MoEs that provide only point estimates, MoGU models each expert's output as a Gaussian distribution. This allows it to directly quantify both the forecast (the mean) and its inherent uncertainty (variance). MoGU's core innovation is its uncertainty-based gating mechanism, which replaces the traditional input-based gating network by using each expert's estimated variance to determine its contribution to the final prediction. Evaluated across diverse time series forecasting benchmarks, MoGU consistently outperforms single-expert models and traditional MoE setups. It also provides well-quantified, informative uncertainties that directly correlate with prediction errors, enhancing forecast reliability. Our code is available from: https://github.com/yolish/moe_unc_tsf<br>
<span id='abs_ch'>中文: MoGU是一种新颖的专家混合框架，通过将专家输出建模为高斯分布并采用基于不确定性的门控机制，在时间序列预测中优于 统模型，同时提供可 的预测不确定性量化。</span><br>
<span id='abs_en'>English: MoGU is a novel Mixture-of-Experts framework for time series forecasting that models expert outputs as Gaussian distributions and uses an uncertainty-based gating mechanism to outperform traditional models while providing reliable uncertainty quantification.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2510.07414.pdf' target='_blank'>https://arxiv.org/pdf/2510.07414.pdf</a></span>   <span><a href='https://github.com/Graph-COM/HaystackCraft' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mufei Li, Dongqi Fu, Limei Wang, Si Zhang, Hanqing Zeng, Kaan Sancak, Ruizhong Qiu, Haoyu Wang, Xiaoxin He, Xavier Bresson, Yinglong Xia, Chonglin Sun, Pan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07414">Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Modern long-context large language models (LLMs) perform well on synthetic "needle-in-a-haystack" (NIAH) benchmarks, but such tests overlook how noisy contexts arise from biased retrieval and agentic workflows. We argue that haystack engineering is necessary to construct noisy long contexts that faithfully capture key real-world factors -- distraction from heterogeneous biased retrievers and cascading errors in agentic workflows -- to test models' long-context robustness. We instantiate it through HaystackCraft, a new NIAH benchmark built on the full English Wikipedia hyperlink network with multi-hop questions. HaystackCraft evaluates how heterogeneous retrieval strategies (e.g., sparse, dense, hybrid, and graph-based) affect distractor composition, haystack ordering, and downstream LLM performance. HaystackCraft further extends NIAH to dynamic, LLM-dependent settings that simulate agentic operations, where models refine queries, reflect on their past reasonings, and decide when to stop. Experiments with 15 long-context models show that (1) while stronger dense retrievers can introduce more challenging distractors, graph-based reranking simultaneously improves retrieval effectiveness and mitigates more harmful distractors; (2) in agentic tests, even advanced models like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated distractors or struggle to perform early stops. These results highlight persistent challenges in agentic long-context reasoning and establish HaystackCraft as a valuable testbed for future progress.<br>
<span id='abs_ch'>中文: 现代长上下文大语言模型在合成基准测试中表现优异，但需构建真实噪声环境以评估鲁棒性， 此开发了HaystackCraft，通过评估检索策略和智能体工作流，揭示了高级模型中如级联失败等持续存在的挑战。English: Modern long-context LLMs excel in synthetic benchmarks but require realistic noisy contexts to assess robustness, leading to the creation of HaystackCraft, which evaluates retrieval strategies and agentic workflows, revealing persistent challenges like cascading failures in advanced models.Paperid:59,https://arxiv.org/pdf/2510.07318.pdfGitHubGitHub</span><br>
<span id='abs_en'>English: Modern long-context LLMs excel in synthetic benchmarks but require realistic noisy contexts to assess robustness, leading to the creation of HaystackCraft, which evaluates retrieval strategies and agentic workflows, revealing persistent challenges like cascading failures in advanced models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2510.07318.pdf' target='_blank'>https://arxiv.org/pdf/2510.07318.pdf</a></span>   <span><a href='https://github.com/ByteDance-Seed/AHN' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ByteDance-Seed/AHN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunhao Fang, Weihao Yu, Shu Zhong, Qinghao Ye, Xuehan Xiong, Lai Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07318">Artificial Hippocampus Networks for Efficient Long-Context Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.<br>
<span id='abs_ch'>中文: 本文受认知科学启发提出一种记忆框架，通过滑动窗口保留短期记忆，并利用人工海马体网络循环压缩长期记忆，在提升长上下文任务性能的同时大幅降低了计算与内存开销。</span><br>
<span id='abs_en'>English: This paper introduces a memory framework inspired by cognitive science, combining a sliding window for short-term memory with a recurrent Artificial Hippocampus Network for long-term compression, which enhances model performance on long-context tasks while significantly reducing computational and memory costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2510.07293.pdf' target='_blank'>https://arxiv.org/pdf/2510.07293.pdf</a></span>   <span><a href='https://github.com/DabDans/AudioMarathon' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peize He, Zichen Wen, Yubo Wang, Yuxuan Wang, Xiaoqian Liu, Jiajie Huang, Zehui Lei, Zhuangcheng Gu, Xiangqi Jin, Jiabing Yang, Kai Li, Zhifei Liu, Weijia Li, Cunxiang Wang, Conghui He, Linfeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07293">AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Processing long-form audio is a major challenge for Large Audio Language models (LALMs). These models struggle with the quadratic cost of attention ($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio benchmarks are built mostly from short clips and do not evaluate models in realistic long context settings. To address this gap, we introduce AudioMarathon, a benchmark designed to evaluate both understanding and inference efficiency on long-form audio. AudioMarathon provides a diverse set of tasks built upon three pillars: long-context audio inputs with durations ranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of 2,250 to 7,500 audio tokens, respectively, full domain coverage across speech, sound, and music, and complex reasoning that requires multi-hop inference. We evaluate state-of-the-art LALMs and observe clear performance drops as audio length grows. We also study acceleration techniques and analyze the trade-offs of token pruning and KV cache eviction. The results show large gaps across current LALMs and highlight the need for better temporal reasoning and memory-efficient architectures. We believe AudioMarathon will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.<br>
<span id='abs_ch'>中文: AudioMarathon被提出作为一个基准，旨在评估大型音频语言模型在长音频上的表现，解决其在注意力成本和长程依赖方面的不足，同时揭示了性能差距和改进架构的必要性。</span><br>
<span id='abs_en'>English: AudioMarathon is introduced as a benchmark to evaluate large audio language models on long-form audio, addressing their inefficiencies in attention costs and long-range dependencies, while highlighting performance gaps and the need for improved architectures.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2510.07286.pdf' target='_blank'>https://arxiv.org/pdf/2510.07286.pdf</a></span>   <span><a href='https://github.com/aim-uofa/EvoIF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jigang Fan, Xiaoran Jiao, Shengdong Lin, Zhanming Liang, Weian Mao, Chenchen Jing, Hao Chen, Chunhua Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07286">Evolutionary Profiles for Protein Fitness Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Predicting the fitness impact of mutations is central to protein engineering but constrained by limited assays relative to the size of sequence space. Protein language models (pLMs) trained with masked language modeling (MLM) exhibit strong zero-shot fitness prediction; we provide a unifying view by interpreting natural evolution as implicit reward maximization and MLM as inverse reinforcement learning (IRL), in which extant sequences act as expert demonstrations and pLM log-odds serve as fitness estimates. Building on this perspective, we introduce EvoIF, a lightweight model that integrates two complementary sources of evolutionary signal: (i) within-family profiles from retrieved homologs and (ii) cross-family structural-evolutionary constraints distilled from inverse folding logits. EvoIF fuses sequence-structure representations with these profiles via a compact transition block, yielding calibrated probabilities for log-odds scoring. On ProteinGym (217 mutational assays; >2.5M mutants), EvoIF and its MSA-enabled variant achieve state-of-the-art or competitive performance while using only 0.15% of the training data and fewer parameters than recent large models. Ablations confirm that within-family and cross-family profiles are complementary, improving robustness across function types, MSA depths, taxa, and mutation depths. The codes will be made publicly available at https://github.com/aim-uofa/EvoIF.<br>
<br>
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2510.07227.pdf' target='_blank'>https://arxiv.org/pdf/2510.07227.pdf</a></span>   <span><a href='https://github.com/whittle-org/whittle/,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arjun Krishnakumar, Rhea Sanjay Sukthanker, Hannan Javed Mahadik, Gabriela Kadlecová, Vladyslav Moroshan, Timur Carstensen, Frank Hutter, Aaron Klein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07227">Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Small Language models (SLMs) offer an efficient and accessible alternative to Large Language Models (LLMs), delivering strong performance while using far fewer resources. We introduce a simple and effective framework for pretraining SLMs that brings together three complementary ideas. First, we identify structurally sparse sub-network initializations that consistently outperform randomly initialized models of similar size under the same compute budget. Second, we use evolutionary search to automatically discover high-quality sub-network initializations, providing better starting points for pretraining. Third, we apply knowledge distillation from larger teacher models to speed up training and improve generalization. Together, these components make SLM pretraining substantially more efficient: our best model, discovered using evolutionary search and initialized with LLM weights, matches the validation perplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining tokens. We release all code and models at https://github.com/whittle-org/whittle/, offering a practical and reproducible path toward cost-efficient small language model development at scale.<br>
<span id='abs_ch'>Chinese: 本文提出了一种小型语言模型预训练框架，通过稀疏子网络初始化、进化搜索和知识蒸馏相结合，以显著减少的资源实现相当性能，将预训练词元需求降低了9.2倍。English: This paper presents a framework for pretraining small language models (SLMs) that combines sparse sub-network initialization, evolutionary search, and knowledge distillation to achieve comparable performance with significantly fewer resources, reducing pretraining tokens by 9.2 times.Paperid:65,https://arxiv.org/pdf/2510.07221.pdfGitHub</span><br>
<span id='abs_en'>English: This paper presents a framework for pretraining small language models (SLMs) that combines sparse sub-network initialization, evolutionary search, and knowledge distillation to achieve comparable performance with significantly fewer resources, reducing pretraining tokens by 9.2 times.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2510.07217.pdf' target='_blank'>https://arxiv.org/pdf/2510.07217.pdf</a></span>   <span><a href='https://github.com/27yw/GenPilot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Ye, Zhaocheng Liu, Yuwei Gui, Tingyu Yuan, Yunyue Su, Bowen Fang, Chaoyang Zhao, Qiang Liu, Liang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07217">GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text-to-image synthesis has made remarkable progress, yet accurately interpreting complex and lengthy prompts remains challenging, often resulting in semantic inconsistencies and missing details. Existing solutions, such as fine-tuning, are model-specific and require training, while prior automatic prompt optimization (APO) approaches typically lack systematic error analysis and refinement strategies, resulting in limited reliability and effectiveness. Meanwhile, test-time scaling methods operate on fixed prompts and on noise or sample numbers, limiting their interpretability and adaptability. To solve these, we introduce a flexible and efficient test-time prompt optimization strategy that operates directly on the input text. We propose a plug-and-play multi-agent system called GenPilot, integrating error analysis, clustering-based adaptive exploration, fine-grained verification, and a memory module for iterative optimization. Our approach is model-agnostic, interpretable, and well-suited for handling long and complex prompts. Simultaneously, we summarize the common patterns of errors and the refinement strategy, offering more experience and encouraging further exploration. Experiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7% demonstrate the strong capability of our methods in enhancing the text and image consistency and structural coherence of generated images, revealing the effectiveness of our test-time prompt optimization strategy. The code is available at https://github.com/27yw/GenPilot.<br>
<span id='abs_ch'>Chinese: 本文提出GenPilot，一种灵活高效的测试时提示优化策略，通过模型 关、可解释的多智能体系统解决文本到图像合成中的语义不一致和细节缺失问题，显著提升了生成图像的一致性和结构连贯性。</span><br>
<span id='abs_en'>English: This paper introduces GenPilot, a flexible and efficient test-time prompt optimization strategy that enhances text-to-image synthesis by addressing semantic inconsistencies and missing details through a model-agnostic, interpretable multi-agent system, achieving significant improvements in consistency and coherence.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2510.07213.pdf' target='_blank'>https://arxiv.org/pdf/2510.07213.pdf</a></span>   <span><a href='https://github.com/ku-nlp/language-specific-dimensions' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengzhi Zhong, Fei Cheng, Qianying Liu, Yugo Murawaki, Chenhui Chu, Sadao Kurohashi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07213">Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models exhibit strong multilingual capabilities despite limited exposure to non-English data. Prior studies show that English-centric large language models map multilingual content into English-aligned representations at intermediate layers and then project them back into target-language token spaces in the final layer. From this observation, we hypothesize that this cross-lingual transition is governed by a small and sparse set of dimensions, which occur at consistent indices across the intermediate to final layers. Building on this insight, we introduce a simple, training-free method to identify and manipulate these dimensions, requiring only as few as 50 sentences of either parallel or monolingual data. Experiments on a multilingual generation control task reveal the interpretability of these dimensions, demonstrating that the interventions in these dimensions can switch the output language while preserving semantic content, and that it surpasses the performance of prior neuron-based approaches at a substantially lower cost.<br>
<br>
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2510.07048.pdf' target='_blank'>https://arxiv.org/pdf/2510.07048.pdf</a></span>   <span><a href='https://github.com/ytgui/Search-R3' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuntao Gui, James Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07048">Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite their remarkable natural language understanding capabilities, Large Language Models (LLMs) have been underutilized for retrieval tasks. We present Search-R3, a novel framework that addresses this limitation by adapting LLMs to generate search embeddings as a direct output of their reasoning process. Our approach exploits LLMs' chain-of-thought capabilities, allowing them to produce more effective embeddings by reasoning step-by-step through complex semantic analyses. We implement this through three complementary mechanisms. (1) a supervised learning stage enables the model's ability to produce quality embeddings, (2) a reinforcement learning (RL) methodology that optimizes embedding generation alongside reasoning, and (3) a specialized RL environment that efficiently handles evolving embedding representations without requiring complete corpus re-encoding at each training iteration. Our extensive evaluations on diverse benchmarks demonstrate that Search-R3 significantly outperforms prior methods by unifying the reasoning and embedding generation processes. This integrated post-training approach represents a substantial advancement in handling complex knowledge-intensive tasks that require both sophisticated reasoning and effective information retrieval. Project page: https://github.com/ytgui/Search-R3<br>
<span id='abs_ch'>Chinese: Search-R3是一种创新框架，通过利用大语言模型的推理过程生成搜索嵌入，结合监督学 和强化学 机制，显著提升了复杂知识密集型任务的检索性能。</span><br>
<span id='abs_en'>English: Search-R3 is a novel framework that enhances retrieval tasks by enabling Large Language Models to generate search embeddings through their reasoning process, combining supervised and reinforcement learning to outperform existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2510.07035.pdf' target='_blank'>https://arxiv.org/pdf/2510.07035.pdf</a></span>   <span><a href='https://github.com/tewiSong/FlexMol' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tengwei Song, Min Wu, Yuan Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07035">Unified Molecule Pre-training with Flexible 2D and 3D Modalities: Single and Paired Modality Integration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Molecular representation learning plays a crucial role in advancing applications such as drug discovery and material design. Existing work leverages 2D and 3D modalities of molecular information for pre-training, aiming to capture comprehensive structural and geometric insights. However, these methods require paired 2D and 3D molecular data to train the model effectively and prevent it from collapsing into a single modality, posing limitations in scenarios where a certain modality is unavailable or computationally expensive to generate. To overcome this limitation, we propose FlexMol, a flexible molecule pre-training framework that learns unified molecular representations while supporting single-modality input. Specifically, inspired by the unified structure in vision-language models, our approach employs separate models for 2D and 3D molecular data, leverages parameter sharing to improve computational efficiency, and utilizes a decoder to generate features for the missing modality. This enables a multistage continuous learning process where both modalities contribute collaboratively during training, while ensuring robustness when only one modality is available during inference. Extensive experiments demonstrate that FlexMol achieves superior performance across a wide range of molecular property prediction tasks, and we also empirically demonstrate its effectiveness with incomplete data. Our code and data are available at https://github.com/tewiSong/FlexMol.<br>
<span id='abs_ch'>中文：FlexMol是一种灵活的分子预训练框架，通过采用共享参数的独立模型并生成缺失模态特征，能够从单模态输入中学 统一表征，在多种分子性质预测任务中表现优异。</span><br>
<span id='abs_en'>English: FlexMol is a flexible molecular pre-training framework that learns unified representations from single-modality inputs by employing separate models with shared parameters and generating missing modality features, achieving superior performance across molecular property prediction tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2510.07019.pdf' target='_blank'>https://arxiv.org/pdf/2510.07019.pdf</a></span>   <span><a href='https://github.com/JusenD/NHA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jusen Du, Jiaxi Hu, Tao Zhang, Weigao Sun, Yu Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07019">Native Hybrid Attention for Efficient Sequence Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Transformers excel at sequence modeling but face quadratic complexity, while linear attention offers improved efficiency but often compromises recall accuracy over long contexts. In this work, we introduce Native Hybrid Attention (NHA), a novel hybrid architecture of linear and full attention that integrates both intra \& inter-layer hybridization into a unified layer design. NHA maintains long-term context in key-value slots updated by a linear RNN, and augments them with short-term tokens from a sliding window. A single \texttt{softmax attention} operation is then applied over all keys and values, enabling per-token and per-head context-dependent weighting without requiring additional fusion parameters. The inter-layer behavior is controlled through a single hyperparameter, the sliding window size, which allows smooth adjustment between purely linear and full attention while keeping all layers structurally uniform. Experimental results show that NHA surpasses Transformers and other hybrid baselines on recall-intensive and commonsense reasoning tasks. Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving competitive accuracy while delivering significant efficiency gains. Code is available at https://github.com/JusenD/NHA.<br>
<br>
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2510.06961.pdf' target='_blank'>https://arxiv.org/pdf/2510.06961.pdf</a></span>   <span><a href='https://github.com/huggingface/open_asr_leaderboard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaibhav Srivastav, Steven Zheng, Eric Bezzam, Eustache Le Bihan, Nithin Koluguri, Piotr Żelasko, Somshubra Majumdar, Adel Moumen, Sanchit Gandhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06961">Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite rapid progress, ASR evaluation remains saturated with short-form English, and efficiency is rarely reported. We present the Open ASR Leaderboard, a fully reproducible benchmark and interactive leaderboard comparing 60+ open-source and proprietary systems across 11 datasets, including dedicated multilingual and long-form tracks. We standardize text normalization and report both word error rate (WER) and inverse real-time factor (RTFx), enabling fair accuracy-efficiency comparisons. For English transcription, Conformer encoders paired with LLM decoders achieve the best average WER but are slower, while CTC and TDT decoders deliver much better RTFx, making them attractive for long-form and offline use. Whisper-derived encoders fine-tuned for English improve accuracy but often trade off multilingual coverage. All code and dataset loaders are open-sourced to support transparent, extensible evaluation.<br>
<span id='abs_ch'>中文：Open ASR 排行榜建立了可复现的评估基准，在11个数据集上对比60余个ASR系统，通过 准化指 揭示：Conformer与LLM组合在英语转录准确率领先，而CTC/TDT解 器在长音频任务中具有更优效率。</span><br>
<span id='abs_en'>English: The Open ASR Leaderboard introduces a reproducible benchmark evaluating over 60 ASR systems across 11 datasets, standardizing metrics for accuracy and efficiency to reveal that Conformer-LLM pairs excel in English transcription accuracy while CTC/TDT decoders offer superior speed for long-form tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2510.06907.pdf' target='_blank'>https://arxiv.org/pdf/2510.06907.pdf</a></span>   <span><a href='https://github.com/spherepaircc/SpherePairCC/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaojie Zhang, Ke Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06907">Angular Constraint Embedding via SpherePair Loss for Constrained Clustering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Constrained clustering integrates domain knowledge through pairwise constraints. However, existing deep constrained clustering (DCC) methods are either limited by anchors inherent in end-to-end modeling or struggle with learning discriminative Euclidean embedding, restricting their scalability and real-world applicability. To avoid their respective pitfalls, we propose a novel angular constraint embedding approach for DCC, termed SpherePair. Using the SpherePair loss with a geometric formulation, our method faithfully encodes pairwise constraints and leads to embeddings that are clustering-friendly in angular space, effectively separating representation learning from clustering. SpherePair preserves pairwise relations without conflict, removes the need to specify the exact number of clusters, generalizes to unseen data, enables rapid inference of the number of clusters, and is supported by rigorous theoretical guarantees. Comparative evaluations with state-of-the-art DCC methods on diverse benchmarks, along with empirical validation of theoretical insights, confirm its superior performance, scalability, and overall real-world effectiveness. Code is available at \href{https://github.com/spherepaircc/SpherePairCC/tree/main}{our repository}.<br>
<span id='abs_ch'>Chinese: 提出的SpherePair方法采用角度约束嵌入进行深度约束聚类，有效分离表示学 与聚类过程， 需指定聚类数量即可提升可扩展性和实际应用性。</span><br>
<span id='abs_en'>English: The proposed SpherePair method introduces an angular constraint embedding approach for deep constrained clustering, effectively separating representation learning from clustering to enhance scalability and real-world applicability without requiring the exact number of clusters.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2510.06888.pdf' target='_blank'>https://arxiv.org/pdf/2510.06888.pdf</a></span>   <span><a href='https://github.com/AkashGhosh/M3Retrieve' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arkadeep Acharya, Akash Ghosh, Pradeepika Verma, Kitsuchart Pasupa, Sriparna Saha, Priti Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06888">M3Retrieve: Benchmarking Multimodal Retrieval for Medicine</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the increasing use of RetrievalAugmented Generation (RAG), strong retrieval models have become more important than ever. In healthcare, multimodal retrieval models that combine information from both text and images offer major advantages for many downstream tasks such as question answering, cross-modal retrieval, and multimodal summarization, since medical data often includes both formats. However, there is currently no standard benchmark to evaluate how well these models perform in medical settings. To address this gap, we introduce M3Retrieve, a Multimodal Medical Retrieval Benchmark. M3Retrieve, spans 5 domains,16 medical fields, and 4 distinct tasks, with over 1.2 Million text documents and 164K multimodal queries, all collected under approved licenses. We evaluate leading multimodal retrieval models on this benchmark to explore the challenges specific to different medical specialities and to understand their impact on retrieval performance. By releasing M3Retrieve, we aim to enable systematic evaluation, foster model innovation, and accelerate research toward building more capable and reliable multimodal retrieval systems for medical applications. The dataset and the baselines code are available in this github page https://github.com/AkashGhosh/M3Retrieve.<br>
<span id='abs_ch'>中文: M3Retrieve基准的推出填补了医学多模态检索模型评估 准的空白，通过涵盖广泛医学领域和任务的数据集促进医疗AI系统创新与可 性提升。</span><br>
<span id='abs_en'>English: The M3Retrieve benchmark is introduced to evaluate multimodal medical retrieval models across diverse medical domains and tasks, addressing the lack of a standard evaluation framework and promoting advancements in reliable healthcare AI systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2510.06843.pdf' target='_blank'>https://arxiv.org/pdf/2510.06843.pdf</a></span>   <span><a href='https://github.com/xuhang2019/SID' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuhang Chen, Zhifan Song, Deyi Ji, Shuo Gao, Lanyun Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06843">SID: Multi-LLM Debate Driven by Self Signals</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have exhibited impressive capabilities across diverse application domains. Recent work has explored Multi-LLM Agent Debate (MAD) as a way to enhance performance by enabling multiple LLMs to discuss and refine responses iteratively. Nevertheless, existing MAD methods predominantly focus on utilizing external structures, such as debate graphs, using LLM-as-a-Judge, while neglecting the application of self signals, such as token logits and attention, that arise during generation. This omission leads to redundant computation and potential performance degradation. In this paper, we shift the focus to the self signals of multi-LLM debate and introduce a Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of self-signals: model-level confidence and token-level semantic focus, to adaptively guide the debate process. Our approach enables high-confidence agents to exit early at the model level and compress the redundant debate contents based on the attention mechanism. We evaluate our method on various LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental results demonstrate that our method not only outperforms existing MAD techniques in accuracy but also reduces token consumption, highlighting the effectiveness of utilizing self signals in enhancing both the performance and efficiency of multi-agent debate systems. Our code will be available at~\href{https://github.com/xuhang2019/SID}{\texttt{https://github.com/xuhang2019/SID}}.<br>
<br>
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2510.06840.pdf' target='_blank'>https://arxiv.org/pdf/2510.06840.pdf</a></span>   <span><a href='https://github.com/SFStefenon/CNN-TFT-SHAP-MHAW' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefano F. Stefenon, João P. Matos-Carvalho, Valderi R. Q. Leithardt, Kin-Choong Yow
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06840">CNN-TFT explained by SHAP with multi-head attention weights for time series forecasting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Convolutional neural networks (CNNs) and transformer architectures offer strengths for modeling temporal data: CNNs excel at capturing local patterns and translational invariances, while transformers effectively model long-range dependencies via self-attention. This paper proposes a hybrid architecture integrating convolutional feature extraction with a temporal fusion transformer (TFT) backbone to enhance multivariate time series forecasting. The CNN module first applies a hierarchy of one-dimensional convolutional layers to distill salient local patterns from raw input sequences, reducing noise and dimensionality. The resulting feature maps are then fed into the TFT, which applies multi-head attention to capture both short- and long-term dependencies and to weigh relevant covariates adaptively. We evaluate the CNN-TFT on a hydroelectric natural flow time series dataset. Experimental results demonstrate that CNN-TFT outperforms well-established deep learning models, with a mean absolute percentage error of up to 2.2%. The explainability of the model is obtained by a proposed Shapley additive explanations with multi-head attention weights (SHAP-MHAW). Our novel architecture, named CNN-TFT-SHAP-MHAW, is promising for applications requiring high-fidelity, multivariate time series forecasts, being available for future analysis at https://github.com/SFStefenon/CNN-TFT-SHAP-MHAW .<br>
<span id='abs_ch'>中文: 本文提出了一种结合卷积神经网络和时序融合变换器的混合模型，通过卷积层提取局部特征并利用变换器捕捉长期依赖关系，在多元时间序列预测中表现出优越性能，且通过SHAP-MHAW方法增强了模型可解释性。</span><br>
<span id='abs_en'>English: This paper introduces a hybrid CNN-TFT model that combines convolutional layers for local feature extraction with a transformer for capturing long-range dependencies, demonstrating superior performance in multivariate time series forecasting with enhanced explainability through SHAP-MHAW analysis.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2510.06732.pdf' target='_blank'>https://arxiv.org/pdf/2510.06732.pdf</a></span>   <span><a href='https://github.com/glad-lab/RAF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiancheng Xing, Jerry Li, Yixuan Du, Xiyang Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06732">Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are increasingly used as rerankers in information retrieval, yet their ranking behavior can be steered by small, natural-sounding prompts. To expose this vulnerability, we present Rank Anything First (RAF), a two-stage token optimization method that crafts concise textual perturbations to consistently promote a target item in LLM-generated rankings while remaining hard to detect. Stage 1 uses Greedy Coordinate Gradient to shortlist candidate tokens at the current position by combining the gradient of the rank-target with a readability score; Stage 2 evaluates those candidates under exact ranking and readability losses using an entropy-based dynamic weighting scheme, and selects a token via temperature-controlled sampling. RAF generates ranking-promoting prompts token-by-token, guided by dual objectives: maximizing ranking effectiveness and preserving linguistic naturalness. Experiments across multiple LLMs show that RAF significantly boosts the rank of target items using naturalistic language, with greater robustness than existing methods in both promoting target items and maintaining naturalness. These findings underscore a critical security implication: LLM-based reranking is inherently susceptible to adversarial manipulation, raising new challenges for the trustworthiness and robustness of modern retrieval systems. Our code is available at: https://github.com/glad-lab/RAF.<br>
<span id='abs_ch'>中文：RAF方法通过优化令牌以兼顾排名效果和语言自然性，生成隐蔽的文本提示来操纵大型语言模型的排序结果，揭示了检索系统中的安全风险。</span><br>
<span id='abs_en'>English: The RAF method crafts subtle text prompts to manipulate LLM rankings by optimizing tokens for both effectiveness and naturalness, revealing security vulnerabilities in retrieval systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2510.06708.pdf' target='_blank'>https://arxiv.org/pdf/2510.06708.pdf</a></span>   <span><a href='https://github.com/EvoTestOps/AISysRev' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksi Huotala, Miikka Kuutila, Olli-Pekka Turtio, Mika Mäntylä
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06708">AISysRev -- LLM-based Tool for Title-abstract Screening</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Systematic reviews are a standard practice for summarizing the state of evidence in software engineering. Conducting systematic reviews is laborious, especially during the screening or study selection phase, where the number of papers can be overwhelming. During this phase, papers are assessed against inclusion and exclusion criteria based on their titles and abstracts. Recent research has demonstrated that large language models (LLMs) can perform title-abstract screening at a level comparable to that of a master's student. While LLMs cannot be fully trusted, they can help, for example, in Rapid Reviews, which try to expedite the review process. Building on recent research, we developed AiSysRev, an LLM-based screening tool implemented as a web application running in a Docker container. The tool accepts a CSV file containing paper titles and abstracts. Users specify inclusion and exclusion criteria. One can use multiple LLMs for screening via OpenRouter. AiSysRev supports both zero-shot and few-shot screening, and also allows for manual screening through interfaces that display LLM results as guidance for human reviewers.We conducted a trial study with 137 papers using the tool. Our findings indicate that papers can be classified into four categories: Easy Includes, Easy Excludes, Boundary Includes, and Boundary Excludes. The Boundary cases, where LLMs are prone to errors, highlight the need for human intervention. While LLMs do not replace human judgment in systematic reviews, they can significantly reduce the burden of assessing large volumes of scientific literature. Video: https://www.youtube.com/watch?v=jVbEj4Y4tQI Tool: https://github.com/EvoTestOps/AISysRev<br>
<br>
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2510.06669.pdf' target='_blank'>https://arxiv.org/pdf/2510.06669.pdf</a></span>   <span><a href='https://github.com/Yuxi104/AutoNAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxi Liu, Yunfeng Ma, Yi Tang, Min Liu, Shuai Jiang, Yaonan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06669">Automated Neural Architecture Design for Industrial Defect Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Industrial surface defect detection (SDD) is critical for ensuring product quality and manufacturing reliability. Due to the diverse shapes and sizes of surface defects, SDD faces two main challenges: intraclass difference and interclass similarity. Existing methods primarily utilize manually designed models, which require extensive trial and error and often struggle to address both challenges effectively. To overcome this, we propose AutoNAD, an automated neural architecture design framework for SDD that jointly searches over convolutions, transformers, and multi-layer perceptrons. This hybrid design enables the model to capture both fine-grained local variations and long-range semantic context, addressing the two key challenges while reducing the cost of manual network design. To support efficient training of such a diverse search space, AutoNAD introduces a cross weight sharing strategy, which accelerates supernet convergence and improves subnet performance. Additionally, a searchable multi-level feature aggregation module (MFAM) is integrated to enhance multi-scale feature learning. Beyond detection accuracy, runtime efficiency is essential for industrial deployment. To this end, AutoNAD incorporates a latency-aware prior to guide the selection of efficient architectures. The effectiveness of AutoNAD is validated on three industrial defect datasets and further applied within a defect imaging and detection platform. Code will be available at https://github.com/Yuxi104/AutoNAD.<br>
<span id='abs_ch'>中文: AutoNAD是一种自动化神经架构设计框架，通过联合搜索卷积、变换器和多层感知器来解决工业表面缺陷检测中的类内差异和类间相似性挑战，同时结合了针对实际部署的效率优化措施。</span><br>
<span id='abs_en'>English: AutoNAD is an automated neural architecture design framework that addresses the challenges of intraclass difference and interclass similarity in industrial surface defect detection by jointly searching over convolutions, transformers, and MLPs, while incorporating efficiency measures for practical deployment.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2510.06649.pdf' target='_blank'>https://arxiv.org/pdf/2510.06649.pdf</a></span>   <span><a href='https://github.com/agentic-learning-ai-lab/arq' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Frank Wu, Mengye Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06649">Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The Forward-Forward (FF) Algorithm is a recently proposed learning procedure for neural networks that employs two forward passes instead of the traditional forward and backward passes used in backpropagation. However, FF remains largely confined to supervised settings, leaving a gap at domains where learning signals can be yielded more naturally such as RL. In this work, inspired by FF's goodness function using layer activity statistics, we introduce Action-conditioned Root mean squared Q-Functions (ARQ), a novel value estimation method that applies a goodness function and action conditioning for local RL using temporal difference learning. Despite its simplicity and biological grounding, our approach achieves superior performance compared to state-of-the-art local backprop-free RL methods in the MinAtar and the DeepMind Control Suite benchmarks, while also outperforming algorithms trained with backpropagation on most tasks. Code can be found at https://github.com/agentic-learning-ai-lab/arq.<br>
<span id='abs_ch'>Chinese: ARQ方法将前向-前向算法的优度函数应用于强化学 ，在局部 反向 播的强化学 任务中实现了最先进的性能表现。</span><br>
<span id='abs_en'>English: The Action-conditioned Root mean squared Q-Functions (ARQ) method extends the Forward-Forward algorithm's goodness function to reinforcement learning, achieving state-of-the-art performance in local backprop-free RL on benchmark tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2510.06645.pdf' target='_blank'>https://arxiv.org/pdf/2510.06645.pdf</a></span>   <span><a href='https://github.com/yangxiaoxuan123/FineSec_detect' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Wei, Xiaoxuan Yang, Jing Sun, Zijian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06645">Distilling Lightweight Language Models for C/C++ Vulnerabilities</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The increasing complexity of modern software systems exacerbates the prevalence of security vulnerabilities, posing risks of severe breaches and substantial economic loss. Consequently, robust code vulnerability detection is essential for software security. While Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing, their potential for automated code vulnerability detection remains underexplored. This paper presents FineSec, a novel framework that harnesses LLMs through knowledge distillation to enable efficient and precise vulnerability identification in C/C++ codebases. FineSec utilizes knowledge distillation to transfer expertise from large teacher models to compact student models, achieving high accuracy with minimal computational cost. By integrating data preparation, training, evaluation, and continuous learning into a unified, single-task workflow, FineSec offers a streamlined approach. Extensive evaluations on C/C++ codebases demonstrate its superiority over both base models and larger LLMs in identifying complex vulnerabilities and logical flaws, establishing FineSec as a practical and scalable solution for real-world software security. To facilitate reproducibility, the datasets, source code, and experimental results are made publicly available at: https://github.com/yangxiaoxuan123/FineSec_detect.<br>
<span id='abs_ch'>中文: FineSec是一种创新框架，通过知识蒸馏利用大语言模型高效准确地检测C/C++代 中的漏洞，以最小计算成本超越基础模型和更大规模语言模型。</span><br>
<span id='abs_en'>English: FineSec is a novel framework that uses knowledge distillation with Large Language Models to efficiently and accurately detect vulnerabilities in C/C++ code, outperforming base models and larger LLMs with minimal computational cost.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2510.06596.pdf' target='_blank'>https://arxiv.org/pdf/2510.06596.pdf</a></span>   <span><a href='https://github.com/ayushzenith/SDQM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayush Zenith, Arnold Zumbrun, Neel Raut, Jing Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06596">SDQM: Synthetic Data Quality Metric for Object Detection Dataset Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The performance of machine learning models depends heavily on training data. The scarcity of large-scale, well-annotated datasets poses significant challenges in creating robust models. To address this, synthetic data generated through simulations and generative models has emerged as a promising solution, enhancing dataset diversity and improving the performance, reliability, and resilience of models. However, evaluating the quality of this generated data requires an effective metric. This paper introduces the Synthetic Dataset Quality Metric (SDQM) to assess data quality for object detection tasks without requiring model training to converge. This metric enables more efficient generation and selection of synthetic datasets, addressing a key challenge in resource-constrained object detection tasks. In our experiments, SDQM demonstrated a strong correlation with the mean Average Precision (mAP) scores of YOLOv11, a leading object detection model, while previous metrics only exhibited moderate or weak correlations. Additionally, it provides actionable insights for improving dataset quality, minimizing the need for costly iterative training. This scalable and efficient metric sets a new standard for evaluating synthetic data. The code for SDQM is available at https://github.com/ayushzenith/SDQM<br>
<span id='abs_ch'>中文: 本文提出合成数据集质量度量(SDQM)，这种可扩展的评估方法 需模型训练即可评估目 检测任务的合成数据质量，实验证明其与模型性能高度相关，为资源受限场景下的数据集优化提供了高效解决方案。</span><br>
<span id='abs_en'>English: This paper introduces the Synthetic Dataset Quality Metric (SDQM), a scalable evaluation tool that assesses synthetic data quality for object detection without requiring model training, demonstrating strong correlation with model performance and enabling efficient dataset optimization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2510.06307.pdf' target='_blank'>https://arxiv.org/pdf/2510.06307.pdf</a></span>   <span><a href='https://github.com/dengwentao99/BCCS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Deng, Jiahuan Pei, Zhiwei Xu, Zhaochun Ren, Zhumin Chen, Pengjie Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06307">Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>A multi-agent system (MAS) enhances its capacity to solve complex natural language processing (NLP) tasks through collaboration among multiple agents, where consensus-seeking serves as a fundamental mechanism. However, existing consensus-seeking approaches typically rely on voting mechanisms to judge consensus, overlooking contradictions in system-internal beliefs that destabilize the consensus. Moreover, these methods often involve agents updating their results through indiscriminate collaboration with every other agent. Such uniform interaction fails to identify the optimal collaborators for each agent, hindering the emergence of a stable consensus. To address these challenges, we provide a theoretical framework for selecting optimal collaborators that maximize consensus stability. Based on the theorems, we propose the Belief-Calibrated Consensus Seeking (BCCS) framework to facilitate stable consensus via selecting optimal collaborators and calibrating the consensus judgment by system-internal beliefs. Experimental results on the MATH and MMLU benchmark datasets demonstrate that the proposed BCCS framework outperforms the best existing results by 2.23% and 3.95% of accuracy on challenging tasks, respectively. Our code and data are available at https://github.com/dengwentao99/BCCS.<br>
<span id='abs_ch'>中文: BCCS框架通过选择最优合作者并基于系统内部信念 准判断，提升了多智能体系统的共识稳定性，在MATH和MMLU基准测试中分别实现了2.23%和3.95%的准确率提升。</span><br>
<span id='abs_en'>English: The BCCS framework enhances multi-agent system consensus stability by selecting optimal collaborators and calibrating judgments with internal beliefs, achieving accuracy improvements of 2.23% and 3.95% on MATH and MMLU benchmarks respectively.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2510.06288.pdf' target='_blank'>https://arxiv.org/pdf/2510.06288.pdf</a></span>   <span><a href='https://github.com/rajghugare19/builderbench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Raj Ghugare, Catherine Ji, Kathryn Wantlin, Jin Schofield, Benjamin Eysenbach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06288">BuilderBench -- A benchmark for generalist agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Today's AI models learn primarily through mimicry and sharpening, so it is not surprising that they struggle to solve problems beyond the limits set by existing data. To solve novel problems, agents should acquire skills for exploring and learning through experience. Finding a scalable learning mechanism for developing agents that learn through interaction remains a major open problem. In this work, we introduce BuilderBench, a benchmark to accelerate research into agent pre-training that centers open-ended exploration. BuilderBench requires agents to learn how to build any structure using blocks. BuilderBench is equipped with $(1)$ a hardware accelerated simulator of a robotic agent interacting with various physical blocks, and $(2)$ a task-suite with over 42 diverse target structures that are carefully curated to test an understanding of physics, mathematics, and long-horizon planning. During training, agents have to explore and learn general principles about the environment without any external supervision. During evaluation, agents have to build the unseen target structures from the task suite. Solving these tasks requires a sort of \emph{embodied reasoning} that is not reflected in words but rather in actions, experimenting with different strategies and piecing them together. Our experiments show that many of these tasks challenge the current iteration of algorithms. Hence, we also provide a ``training wheels'' protocol, in which agents are trained and evaluated to build a single target structure from the task suite. Finally, we provide single-file implementations of six different algorithms as a reference point for researchers.<br>
<br>
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2510.06275.pdf' target='_blank'>https://arxiv.org/pdf/2510.06275.pdf</a></span>   <span><a href='https://github.com/julianbibo/xrec-reproducibility' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ranjan Mishra, Julian I. Bibo, Quinten van Engelen, Henk Schaapman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06275">Reproducibility Study of "XRec: Large Language Models for Explainable Recommendation"</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this study, we reproduced the work done in the paper "XRec: Large Language Models for Explainable Recommendation" by Ma et al. (2024). The original authors introduced XRec, a model-agnostic collaborative instruction-tuning framework that enables large language models (LLMs) to provide users with comprehensive explanations of generated recommendations. Our objective was to replicate the results of the original paper, albeit using Llama 3 as the LLM for evaluation instead of GPT-3.5-turbo. We built on the source code provided by Ma et al. (2024) to achieve our goal. Our work extends the original paper by modifying the input embeddings or deleting the output embeddings of XRec's Mixture of Experts module. Based on our results, XRec effectively generates personalized explanations and its stability is improved by incorporating collaborative information. However, XRec did not consistently outperform all baseline models in every metric. Our extended analysis further highlights the importance of the Mixture of Experts embeddings in shaping the explanation structures, showcasing how collaborative signals interact with language modeling. Through our work, we provide an open-source evaluation implementation that enhances accessibility for researchers and practitioners alike. Our complete code repository can be found at https://github.com/julianbibo/xrec-reproducibility.<br>
<span id='abs_ch'>本 究使用Llama 3复现了XRec框架，验证了其生成个性化推荐的能力，同时发现调整专家嵌入会影响解释结构且模型并非在所有指 上都优于基线。</span><br>
<span id='abs_en'>This study replicates the XRec framework using Llama 3 instead of GPT-3.5-turbo, confirming its ability to generate personalized recommendations while revealing that modified expert embeddings affect explanation structures without consistently outperforming all baselines.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2510.06261.pdf' target='_blank'>https://arxiv.org/pdf/2510.06261.pdf</a></span>   <span><a href='https://github.com/tmlr-group/AlphaApollo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhanke Zhou, Chentao Cao, Xiao Feng, Xuan Li, Zongze Li, Xiangyu Lu, Jiangchao Yao, Weikai Huang, Linrui Xu, Tian Cheng, Guanyu Jiang, Yiming Zheng, Brando Miranda, Tongliang Liu, Sanmi Koyejo, Masashi Sugiyama, Bo Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06261">AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present AlphaApollo, a self-evolving agentic reasoning system that aims to address two bottlenecks in foundation model (FM) reasoning-limited model-intrinsic capacity and unreliable test-time iteration. AlphaApollo orchestrates multiple models with professional tools to enable deliberate, verifiable reasoning. It couples (i) a computation tool (Python with numerical and symbolic libraries) and (ii) a retrieval tool (task-relevant external information) to execute exact calculations and ground decisions. The system further supports multi-round, multi-model solution evolution via a shared state map that records candidates, executable checks, and feedback for iterative refinement. In evaluations on AIME 2024/2025 across multiple models, AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32 for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool calls are successfully executed, with consistent outperformance of non-tool baselines, thereby lifting the capability ceiling of FMs. More empirical results and implementation details will be updated at https://github.com/tmlr-group/AlphaApollo.<br>
<br>
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2510.06240.pdf' target='_blank'>https://arxiv.org/pdf/2510.06240.pdf</a></span>   <span><a href='https://github.com/erwinmsmith/KG-MAD/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiqun Pan, Zhenke Duan, Jiani Tu, Anzhi Cheng, Yanqing Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06240">Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Industrial question-answering (QA) systems require higher safety and reliability than general-purpose dialogue models, as errors in high-risk scenarios such as equipment fault diagnosis can have severe consequences. Although multi-agent large language models enhance reasoning depth, they suffer from uncontrolled iterations and unverifiable outputs, and conventional distillation methods struggle to transfer collaborative reasoning capabilities to lightweight, deployable student models. To address these challenges, we propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our approach formulates distillation as a Markov Decision Process and incorporates a knowledge graph as a verifiable structured prior to enrich state representation and ensure convergence. By integrating collaborative reasoning with knowledge grounding, KG-MASD generates high-confidence instruction-tuning data and jointly distills reasoning depth and verifiability into compact student models suitable for edge deployment. Experiments on an industrial QA dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent over baselines and significantly enhances reliability, enabling trustworthy AI deployment in safety-critical industrial scenarios. Code and data are available at https://github.com/erwinmsmith/KG-MAD/.<br>
<span id='abs_ch'>中文: 提出的KG-MASD方法通过知识图谱引导的蒸馏技术，将多智能体推理能力压缩至轻量模型中，显著提升了工业问答系统的准确性和可 性，适用于安全关键场景。</span><br>
<span id='abs_en'>English: The proposed KG-MASD method enhances industrial QA systems by distilling multi-agent reasoning into compact models through knowledge graph-guided distillation, achieving significant accuracy improvements and reliability for safety-critical applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2510.06223.pdf' target='_blank'>https://arxiv.org/pdf/2510.06223.pdf</a></span>   <span><a href='https://github.com/hansvdam/langbar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hans G. W. van Dam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06223">A Multimodal GUI Architecture for Interfacing with LLM-Based Conversational Assistants</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Advances in large language models (LLMs) and real-time speech recognition now make it possible to issue any graphical user interface (GUI) action through natural language and receive the corresponding system response directly through the GUI. Most production applications were never designed with speech in mind. This article provides a concrete architecture that enables GUIs to interface with LLM-based speech-enabled assistants. The architecture makes an application's navigation graph and semantics available through the Model Context Protocol (MCP). The ViewModel, part of the MVVM (Model-View-ViewModel) pattern, exposes the application's capabilities to the assistant by supplying both tools applicable to a currently visible view and application-global tools extracted from the GUI tree router. This architecture facilitates full voice accessibility while ensuring reliable alignment between spoken input and the visual interface, accompanied by consistent feedback across modalities. It future-proofs apps for upcoming OS super assistants that employ computer use agents (CUAs) and natively consume MCP if an application provides it. To address concerns about privacy and data security, the practical effectiveness of locally deployable, open-weight LLMs for speech-enabled multimodal UIs is evaluated. Findings suggest that recent smaller open-weight models approach the performance of leading proprietary models in overall accuracy and require enterprise-grade hardware for fast responsiveness. A demo implementation of the proposed architecture can be found at https://github.com/hansvdam/langbar<br>
<span id='abs_ch'>中文: 大型语言模型和实时语音识别的进步使得通过自然语言控制图形用户界面成为可能，本文提出的架构利用模型上下文协议暴露应用程序功能，实现全语音交互，并通过本地部署开源模型保障隐私安全。</span><br>
<span id='abs_en'>English: Recent advances in LLMs and speech recognition enable natural language control of GUI actions, with a proposed architecture using the Model Context Protocol to expose application capabilities for voice accessibility while maintaining privacy through local deployment of open-weight models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2510.06093.pdf' target='_blank'>https://arxiv.org/pdf/2510.06093.pdf</a></span>   <span><a href='https://github.com/TeX-Base/ClassicalAIvsLLMsforDMAlignment' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Parallax-Advanced-Research/ITM/tree/feature_insurance' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mallika Mainali, Harsha Sureshbabu, Anik Sen, Christopher B. Rauch, Noah D. Reifsnyder, John Meyer, J. T. Turner, Michael W. Floyd, Matthew Molineaux, Rosina O. Weber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06093">Classical AI vs. LLMs for Decision-Maker Alignment in Health Insurance Choices</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As algorithmic decision-makers are increasingly applied to high-stakes domains, AI alignment research has evolved from a focus on universal value alignment to context-specific approaches that account for decision-maker attributes. Prior work on Decision-Maker Alignment (DMA) has explored two primary strategies: (1) classical AI methods integrating case-based reasoning, Bayesian reasoning, and naturalistic decision-making, and (2) large language model (LLM)-based methods leveraging prompt engineering. While both approaches have shown promise in limited domains such as medical triage, their generalizability to novel contexts remains underexplored. In this work, we implement a prior classical AI model and develop an LLM-based algorithmic decision-maker evaluated using a large reasoning model (GPT-5) and a non-reasoning model (GPT-4) with weighted self-consistency under a zero-shot prompting framework, as proposed in recent literature. We evaluate both approaches on a health insurance decision-making dataset annotated for three target decision-makers with varying levels of risk tolerance (0.0, 0.5, 1.0). In the experiments reported herein, classical AI and LLM-based models achieved comparable alignment with attribute-based targets, with classical AI exhibiting slightly better alignment for a moderate risk profile. The dataset and open-source implementation are publicly available at: https://github.com/TeX-Base/ClassicalAIvsLLMsforDMAlignment and https://github.com/Parallax-Advanced-Research/ITM/tree/feature_insurance.<br>
<span id='abs_ch'>中文摘要：人工智能对齐 究已从普适价值转向情境化方法，经典AI与大语言模型方法在匹配决策者风险偏好方面表现相当，但经典AI在中等风险场景中略具优势。</span><br>
<span id='abs_en'>English Summary: AI alignment research has shifted from universal values to context-specific approaches, with classical AI and LLM-based methods showing comparable performance in aligning with decision-makers' risk profiles, though classical AI slightly outperforms in moderate risk scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2510.06071.pdf' target='_blank'>https://arxiv.org/pdf/2510.06071.pdf</a></span>   <span><a href='https://github.com/feedzai/biy-paper' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>João Palmeiro, Diogo Duarte, Rita Costa, Pedro Bizarro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06071">Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI Models for Scatterplot-Related Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>AI models are increasingly used for data analysis and visualization, yet benchmarks rarely address scatterplot-specific tasks, limiting insight into performance. To address this gap for one of the most common chart types, we introduce a synthetic, annotated dataset of over 18,000 scatterplots from six data generators and 17 chart designs, and a benchmark based on it. We evaluate proprietary models from OpenAI and Google using N-shot prompting on five distinct tasks derived from annotations of cluster bounding boxes, their center coordinates, and outlier coordinates. OpenAI models and Gemini 2.5 Flash, especially when prompted with examples, are viable options for counting clusters and, in Flash's case, outliers (90%+ Accuracy). However, the results for localization-related tasks are unsatisfactory: Precision and Recall are near or below 50%, except for Flash in outlier identification (65.01%). Furthermore, the impact of chart design on performance appears to be a secondary factor, but it is advisable to avoid scatterplots with wide aspect ratios (16:9 and 21:9) or those colored randomly. Supplementary materials are available at https://github.com/feedzai/biy-paper.<br>
<span id='abs_ch'>中文摘要：本 究针对散点图任务提出了一个评估AI模型的基准，发现尽管OpenAI和Gemini模型在聚类计数和异常值检测方面表现良好，但在定位相关任务上表现 佳。</span><br>
<span id='abs_en'>English Summary: This study introduces a benchmark for evaluating AI models on scatterplot tasks, finding that while OpenAI and Gemini models perform well in cluster counting and outlier detection, they struggle significantly with localization tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2510.06056.pdf' target='_blank'>https://arxiv.org/pdf/2510.06056.pdf</a></span>   <span><a href='https://github.com/liugangcode/deepevolve' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gang Liu, Yihan Zhu, Jie Chen, Meng Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06056">Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep Research</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models hold promise as scientific assistants, yet existing agents either rely solely on algorithm evolution or on deep research in isolation, both of which face critical limitations. Pure algorithm evolution, as in AlphaEvolve, depends only on the internal knowledge of LLMs and quickly plateaus in complex domains, while pure deep research proposes ideas without validation, resulting in unrealistic or unimplementable solutions. We present DeepEvolve, an agent that integrates deep research with algorithm evolution, uniting external knowledge retrieval, cross-file code editing, and systematic debugging under a feedback-driven iterative loop. Each iteration not only proposes new hypotheses but also refines, implements, and tests them, avoiding both shallow improvements and unproductive over-refinements. Across nine benchmarks in chemistry, mathematics, biology, materials, and patents, DeepEvolve consistently improves the initial algorithm, producing executable new algorithms with sustained gains. By bridging the gap between unguided evolution and research without grounding, DeepEvolve provides a reliable framework for advancing scientific algorithm discovery. Our code is available at https://github.com/liugangcode/deepevolve.<br>
<span id='abs_ch'>Chinese: DeepEvolve通过反馈驱动的循环将深度 究与算法进化相结合，在检索外部知识、编辑代 和系统调试的过程中，持续生成可执行且优化的算法，有效推动了科学算法的发展。</span><br>
<span id='abs_en'>English: DeepEvolve integrates deep research with algorithm evolution through a feedback-driven loop that retrieves external knowledge, edits code, and debugs systematically to produce executable, improved algorithms across multiple scientific domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2510.06040.pdf' target='_blank'>https://arxiv.org/pdf/2510.06040.pdf</a></span>   <span><a href='https://github.com/caoxinye/VideoMiner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinye Cao, Hongcan Guo, Jiawen Qian, Guoshun Nan, Chao Wang, Yuqi Pan, Tianhao Hou, Xiaojuan Wang, Yutong Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.06040">VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Understanding hour-long videos with multi-modal large language models (MM-LLMs) enriches the landscape of human-centered AI applications. However, for end-to-end video understanding with LLMs, uniformly sampling video frames results in LLMs being overwhelmed by a vast amount of irrelevant information as video length increases. Existing hierarchical key frame extraction methods improve the accuracy of video understanding but still face two critical challenges. 1) How can the interference of extensive redundant information in long videos be mitigated? 2) How can a model dynamically adapt to complex hierarchical structures while accurately identifying key frames? To address these issues, we propose VideoMiner, which iteratively segments, captions, and clusters long videos, forming a hierarchical tree structure. The proposed VideoMiner progresses from long videos to events to frames while preserving temporal coherence, effectively addressing the first challenge. To precisely locate key frames, we introduce T-GRPO, a tree-based group relative policy optimization in reinforcement learning method that guides the exploration of the VideoMiner. The proposed T-GRPO is specifically designed for tree structures, integrating spatiotemporal information at the event level while being guided by the question, thus solving the second challenge. We achieve superior performance in all long-video understanding tasks and uncover several interesting insights. Our proposed T-GRPO surprisingly incentivizes the model to spontaneously generate a reasoning chain. Additionally, the designed tree growth auxin dynamically adjusts the expansion depth, obtaining accuracy and efficiency gains. The code is publicly available at https://github.com/caoxinye/VideoMiner.<br>
<span id='abs_ch'>中文: VideoMiner通过迭代分割、描述和聚类长视频形成层次 结构解决视频理解难题，同时T-GRPO强化学 方法优化关键帧定位，在保持时序连贯性的同时提升处理效率与准确性。</span><br>
<span id='abs_en'>English: VideoMiner addresses long-video understanding challenges by iteratively segmenting, captioning, and clustering videos into a hierarchical tree structure, while T-GRPO reinforcement learning optimizes key frame identification for improved accuracy and efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2510.05950.pdf' target='_blank'>https://arxiv.org/pdf/2510.05950.pdf</a></span>   <span><a href='https://github.com/SongyuanSui/FETATSC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Songyuan Sui, Zihang Xu, Yu-Neng Chuang, Kwei-Herng Lai, Xia Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05950">Training-Free Time Series Classification via In-Context Reasoning with LLM Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Time series classification (TSC) spans diverse application scenarios, yet labeled data are often scarce, making task-specific training costly and inflexible. Recent reasoning-oriented large language models (LLMs) show promise in understanding temporal patterns, but purely zero-shot usage remains suboptimal. We propose FETA, a multi-agent framework for training-free TSC via exemplar-based in-context reasoning. FETA decomposes a multivariate series into channel-wise subproblems, retrieves a few structurally similar labeled examples for each channel, and leverages a reasoning LLM to compare the query against these exemplars, producing channel-level labels with self-assessed confidences; a confidence-weighted aggregator then fuses all channel decisions. This design eliminates the need for pretraining or fine-tuning, improves efficiency by pruning irrelevant channels and controlling input length, and enhances interpretability through exemplar grounding and confidence estimation. On nine challenging UEA datasets, FETA achieves strong accuracy under a fully training-free setting, surpassing multiple trained baselines. These results demonstrate that a multi-agent in-context reasoning framework can transform LLMs into competitive, plug-and-play TSC solvers without any parameter training. The code is available at https://github.com/SongyuanSui/FETATSC.<br>
<span id='abs_ch'>中文: FETA是一个 需训练的多智能体框架，通过基于示例的推理，利用大语言模型将多元时间序列分解为通道，比较每个通道与相似示例，并聚合置信决策，在 需任何训练的情况下实现了高精度分类。</span><br>
<span id='abs_en'>English: FETA is a training-free multi-agent framework that uses exemplar-based reasoning with large language models to classify time series by decomposing them into channels, comparing each to similar examples, and aggregating confident decisions, achieving strong accuracy without any training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2510.05909.pdf' target='_blank'>https://arxiv.org/pdf/2510.05909.pdf</a></span>   <span><a href='https://github.com/flowersteam/llm_persuasion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aksel Joonas Reedi, Corentin Léger, Julien Pourcel, Loris Gaven, Perrine Charriau, Guillaume Pourcel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05909">Optimizing for Persuasion Improves LLM Generalization: Evidence from Quality-Diversity Evolution of Debate Strategies</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) optimized to output truthful answers often overfit, producing brittle reasoning that fails to generalize. While persuasion-based optimization has shown promise in debate settings, it has not been systematically compared against mainstream truth-based approaches. We introduce DebateQD, a minimal Quality-Diversity (QD) evolutionary algorithm that evolves diverse debate strategies across different categories (rationality, authority, emotional appeal, etc.) through tournament-style competitions where two LLMs debate while a third judges. Unlike previously proposed methods that require a population of LLMs, our approach maintains diversity of opponents through prompt-based strategies within a single LLM architecture, making it more accessible for experiments while preserving the key benefits of population-based optimization. In contrast to prior work, we explicitly isolate the role of the optimization objective by fixing the debate protocol and swapping only the fitness function: persuasion rewards strategies that convince the judge irrespective of truth, whereas truth rewards collaborative correctness. Across three model scales (7B, 32B, 72B parameters) and multiple dataset sizes from the QuALITY benchmark, persuasion-optimized strategies achieve up to 13.94% smaller train-test generalization gaps, while matching or exceeding truth optimization's test performance. These results provide the first controlled evidence that competitive pressure to persuade, rather than seek the truth collaboratively, fosters more transferable reasoning skills, offering a promising path for improving LLM generalization.<br>
<br>
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2510.05891.pdf' target='_blank'>https://arxiv.org/pdf/2510.05891.pdf</a></span>   <span><a href='https://github.com/Zhangyr2022/D3QE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanran Zhang, Bingyao Yu, Yu Zheng, Wenzhao Zheng, Yueqi Duan, Lei Chen, Jie Zhou, Jiwen Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05891">$\bf{D^3}$QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The emergence of visual autoregressive (AR) models has revolutionized image generation while presenting new challenges for synthetic image detection. Unlike previous GAN or diffusion-based methods, AR models generate images through discrete token prediction, exhibiting both marked improvements in image synthesis quality and unique characteristics in their vector-quantized representations. In this paper, we propose to leverage Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE) for autoregressive-generated image detection that exploits the distinctive patterns and the frequency distribution bias of the codebook existing in real and fake images. We introduce a discrete distribution discrepancy-aware transformer that integrates dynamic codebook frequency statistics into its attention mechanism, fusing semantic features and quantization error latent. To evaluate our method, we construct a comprehensive dataset termed ARForensics covering 7 mainstream visual AR models. Experiments demonstrate superior detection accuracy and strong generalization of D$^3$QE across different AR models, with robustness to real-world perturbations. Code is available at \href{https://github.com/Zhangyr2022/D3QE}{https://github.com/Zhangyr2022/D3QE}.<br>
<span id='abs_ch'>中文: 本文提出D$^3$QE方法，通过分析离散分布差异和量化误差来检测自回归模型生成的图像，在不同AR模型中实现了卓越的检测精度和鲁棒性。</span><br>
<span id='abs_en'>English: This paper introduces D$^3$QE, a novel method for detecting images generated by autoregressive models by analyzing discrete distribution discrepancies and quantization errors, achieving superior accuracy and robustness across diverse AR models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2510.05819.pdf' target='_blank'>https://arxiv.org/pdf/2510.05819.pdf</a></span>   <span><a href='https://github.com/Cardio-AI/cmr-multi-view-phase-detection.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sven Koehler, Sarah Kaye Mueller, Jonathan Kiekenap, Gerald Greil, Tarique Hussain, Samir Sarikouch, Florian André, Norbert Frey, Sandy Engelhardt
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05819">Deformable Image Registration for Self-supervised Cardiac Phase Detection in Multi-View Multi-Disease Cardiac Magnetic Resonance Images</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Cardiovascular magnetic resonance (CMR) is the gold standard for assessing cardiac function, but individual cardiac cycles complicate automatic temporal comparison or sub-phase analysis. Accurate cardiac keyframe detection can eliminate this problem. However, automatic methods solely derive end-systole (ES) and end-diastole (ED) frames from left ventricular volume curves, which do not provide a deeper insight into myocardial motion. We propose a self-supervised deep learning method detecting five keyframes in short-axis (SAX) and four-chamber long-axis (4CH) cine CMR. Initially, dense deformable registration fields are derived from the images and used to compute a 1D motion descriptor, which provides valuable insights into global cardiac contraction and relaxation patterns. From these characteristic curves, keyframes are determined using a simple set of rules. The method was independently evaluated for both views using three public, multicentre, multidisease datasets. M&Ms-2 (n=360) dataset was used for training and evaluation, and M&Ms (n=345) and ACDC (n=100) datasets for repeatability control. Furthermore, generalisability to patients with rare congenital heart defects was tested using the German Competence Network (GCN) dataset. Our self-supervised approach achieved improved detection accuracy by 30% - 51% for SAX and 11% - 47% for 4CH in ED and ES, as measured by cyclic frame difference (cFD), compared with the volume-based approach. We can detect ED and ES, as well as three additional keyframes throughout the cardiac cycle with a mean cFD below 1.31 frames for SAX and 1.73 for LAX. Our approach enables temporally aligned inter- and intra-patient analysis of cardiac dynamics, irrespective of cycle or phase lengths. GitHub repository: https://github.com/Cardio-AI/cmr-multi-view-phase-detection.git<br>
<span id='abs_ch'>This study introduces a self-supervised deep learning m=
ethod that detects five cardiac keyframes using motion descriptors from def=
ormable registration, achieving significantly improved accuracy over volume=
-based approaches and enabling temporally aligned analysis across patients.=</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2510.05774.pdf' target='_blank'>https://arxiv.org/pdf/2510.05774.pdf</a></span>   <span><a href='https://github.com/william4s/ConstraintLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weichun Shi, Minghao Liu, Wanting Zhang, Langchen Shi, Fuqi Jia, Feifei Ma, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05774">ConstraintLLM: A Neuro-Symbolic Framework for Industrial-Level Constraint Programming</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Constraint programming (CP) is a crucial technology for solving real-world constraint optimization problems (COPs), with the advantages of rich modeling semantics and high solving efficiency. Using large language models (LLMs) to generate formal modeling automatically for COPs is becoming a promising approach, which aims to build trustworthy neuro-symbolic AI with the help of symbolic solvers. However, CP has received less attention compared to works based on operations research (OR) models. We introduce ConstraintLLM, the first LLM specifically designed for CP modeling, which is trained on an open-source LLM with multi-instruction supervised fine-tuning. We propose the Constraint-Aware Retrieval Module (CARM) to increase the in-context learning capabilities, which is integrated in a Tree-of-Thoughts (ToT) framework with guided self-correction mechanism. Moreover, we construct and release IndusCP, the first industrial-level benchmark for CP modeling, which contains 140 challenging tasks from various domains. Our experiments demonstrate that ConstraintLLM achieves state-of-the-art solving accuracy across multiple benchmarks and outperforms the baselines by 2x on the new IndusCP benchmark. Code and data are available at: https://github.com/william4s/ConstraintLLM.<br>
<span id='abs_ch'>中文总结：ConstraintLLM是首个专为约束编程建模设计的大语言模型，通过约束感知检索模块和引导式自我修正机制，在多个基准测试中实现了最优求解精度，并在新的工业级基准上性能超越基线方法两倍。</span><br>
<span id='abs_en'>English Summary: ConstraintLLM is the first large language model specifically designed for constraint programming modeling, achieving state-of-the-art solving accuracy across multiple benchmarks through its constraint-aware retrieval module and guided self-correction mechanism.Paperid:137,https://arxiv.org/pdf/2510.05750.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2510.05750.pdf' target='_blank'>https://arxiv.org/pdf/2510.05750.pdf</a></span>   <span><a href='https://github.com/YXNTU/CausalHGNN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Yang, Xuejiao Zhao, Zhiqi Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05750">Are Heterogeneous Graph Neural Networks Truly Effective? A Causal Perspective</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Graph neural networks (GNNs) have achieved remarkable success in node classification. Building on this progress, heterogeneous graph neural networks (HGNNs) integrate relation types and node and edge semantics to leverage heterogeneous information. Causal analysis for HGNNs is advancing rapidly, aiming to separate genuine causal effects from spurious correlations. However, whether HGNNs are intrinsically effective remains underexamined, and most studies implicitly assume rather than establish this effectiveness. In this work, we examine HGNNs from two perspectives: model architecture and heterogeneous information. We conduct a systematic reproduction across 21 datasets and 20 baselines, complemented by comprehensive hyperparameter retuning. To further disentangle the source of performance gains, we develop a causal effect estimation framework that constructs and evaluates candidate factors under standard assumptions through factual and counterfactual analyses, with robustness validated via minimal sufficient adjustment sets, cross-method consistency checks, and sensitivity analyses. Our results lead to two conclusions. First, model architecture and complexity have no causal effect on performance. Second, heterogeneous information exerts a positive causal effect by increasing homophily and local-global distribution discrepancy, which makes node classes more distinguishable. The implementation is publicly available at https://github.com/YXNTU/CausalHGNN.<br>
<span id='abs_ch'>中文摘要：异质图神经网络（HGNNs）的性能提升并非源于模型架构，而是来自异质信息通过增强同质性和分布差异来提高节点类别的可区分性。</span><br>
<span id='abs_en'>English summary: Heterogeneous graph neural networks (HGNNs) derive performance gains not from model architecture but from heterogeneous information that enhances class distinguishability through increased homophily and distribution discrepancies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2510.05740.pdf' target='_blank'>https://arxiv.org/pdf/2510.05740.pdf</a></span>   <span><a href='http://github.com/amir-aman/FusionDetect' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amirtaha Amanzadi, Zahra Dehghanian, Hamid Beigy, Hamid R. Rabiee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05740">Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid development of generative models has made it increasingly crucial to develop detectors that can reliably detect synthetic images. Although most of the work has now focused on cross-generator generalization, we argue that this viewpoint is too limited. Detecting synthetic images involves another equally important challenge: generalization across visual domains. To bridge this gap,we present the OmniGen Benchmark. This comprehensive evaluation dataset incorporates 12 state-of-the-art generators, providing a more realistic way of evaluating detector performance under realistic conditions. In addition, we introduce a new method, FusionDetect, aimed at addressing both vectors of generalization. FusionDetect draws on the benefits of two frozen foundation models: CLIP & Dinov2. By deriving features from both complementary models,we develop a cohesive feature space that naturally adapts to changes in both thecontent and design of the generator. Our extensive experiments demonstrate that FusionDetect delivers not only a new state-of-the-art, which is 3.87% more accurate than its closest competitor and 6.13% more precise on average on established benchmarks, but also achieves a 4.48% increase in accuracy on OmniGen,along with exceptional robustness to common image perturbations. We introduce not only a top-performing detector, but also a new benchmark and framework for furthering universal AI image detection. The code and dataset are available at http://github.com/amir-aman/FusionDetect<br>
<span id='abs_ch'>中文：OmniGen基准测试和FusionDetect方法解决了合成图像检测中跨生成器和跨领域泛化的双重挑战，实现了最先进的准确性和鲁棒性。</span><br>
<span id='abs_en'>English: The OmniGen Benchmark and FusionDetect method address the dual challenges of cross-generator and cross-domain generalization in synthetic image detection, achieving state-of-the-art accuracy and robustness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2510.05699.pdf' target='_blank'>https://arxiv.org/pdf/2510.05699.pdf</a></span>   <span><a href='https://github.com/mengtong0110/Tokenizer-MIA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Meng Tong, Yuntao Du, Kejiang Chen, Weiming Zhang, Ninghui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05699">Membership Inference Attacks on Tokenizers of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Membership inference attacks (MIAs) are widely used to assess the privacy risks associated with machine learning models. However, when these attacks are applied to pre-trained large language models (LLMs), they encounter significant challenges, including mislabeled samples, distribution shifts, and discrepancies in model size between experimental and real-world settings. To address these limitations, we introduce tokenizers as a new attack vector for membership inference. Specifically, a tokenizer converts raw text into tokens for LLMs. Unlike full models, tokenizers can be efficiently trained from scratch, thereby avoiding the aforementioned challenges. In addition, the tokenizer's training data is typically representative of the data used to pre-train LLMs. Despite these advantages, the potential of tokenizers as an attack vector remains unexplored. To this end, we present the first study on membership leakage through tokenizers and explore five attack methods to infer dataset membership. Extensive experiments on millions of Internet samples reveal the vulnerabilities in the tokenizers of state-of-the-art LLMs. To mitigate this emerging risk, we further propose an adaptive defense. Our findings highlight tokenizers as an overlooked yet critical privacy threat, underscoring the urgent need for privacy-preserving mechanisms specifically designed for them.<br>
<span id='abs_ch'>中文摘要：本 究首次将分词器作为大语言模型成员推理的新型攻击向量，通过五种攻击方法揭示了其安全漏洞，并提出了自适应防御机制以应对这一被忽视的关键隐私威胁。</span><br>
<span id='abs_en'>English Summary: This study introduces tokenizers as a novel attack vector for membership inference on large language models, revealing their vulnerabilities through five attack methods and proposing an adaptive defense to address this overlooked privacy threat.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2510.05688.pdf' target='_blank'>https://arxiv.org/pdf/2510.05688.pdf</a></span>   <span><a href='https://github.com/xAlg-ai/sparse-attention-hub' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Desai, Kumar Krishna Agrawal, Shuo Yang, Alejandro Cuadron, Luis Gaspar Schroeder, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05688">vAttention: Verified Sparse Attention</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>State-of-the-art sparse attention methods for reducing decoding latency fall into two main categories: approximate top-$k$ (and its extension, top-$p$) and recently introduced sampling-based estimation. However, these approaches are fundamentally limited in their ability to approximate full attention: they fail to provide consistent approximations across heads and query vectors and, most critically, lack guarantees on approximation quality, limiting their practical deployment. We observe that top-$k$ and random sampling are complementary: top-$k$ performs well when attention scores are dominated by a few tokens, whereas random sampling provides better estimates when attention scores are relatively uniform. Building on this insight and leveraging the statistical guarantees of sampling, we introduce vAttention, the first practical sparse attention mechanism with user-specified $(ε, δ)$ guarantees on approximation accuracy (thus, verified). These guarantees make vAttention a compelling step toward practical, reliable deployment of sparse attention at scale. By unifying top-k and sampling, vAttention outperforms both individually, delivering a superior quality-efficiency trade-off. Our experiments show that vAttention significantly improves the quality of sparse attention (e.g., $\sim$4.5 percentage points for Llama-3.1-8B-Inst and Deepseek-R1-Distill-Llama-8B on RULER-HARD), and effectively bridges the gap between full and sparse attention (e.g., across datasets, it matches full model quality with upto 20x sparsity). We also demonstrate that it can be deployed in reasoning scenarios to achieve fast decoding without compromising model quality (e.g., vAttention achieves full model quality on AIME2024 at 10x sparsity with up to 32K token generations). Code is open-sourced at https://github.com/xAlg-ai/sparse-attention-hub.<br>
<span id='abs_ch'>中文：vAttention通过结合top-k和随机采 ，首次实现了具有用户指定精度保证的稀疏注意力机制，显著提升了质量与效率的平衡，并在实际部署中有效弥合了完全注意力和稀疏注意力之间的差距。</span><br>
<span id='abs_en'>English: vAttention unifies top-k and random sampling to provide the first sparse attention mechanism with user-specified accuracy guarantees, significantly improving quality-efficiency trade-offs and bridging the gap between full and sparse attention in practical deployments.Paperid:143,https://arxiv.org/pdf/2510.05683.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2510.05683.pdf' target='_blank'>https://arxiv.org/pdf/2510.05683.pdf</a></span>   <span><a href='https://github.com/smlab-niser/qglime' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haribandhu Jena, Jyotirmaya Shivottam, Subhankar Mishra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05683">QGraphLIME - Explaining Quantum Graph Neural Networks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Quantum graph neural networks offer a powerful paradigm for learning on graph-structured data, yet their explainability is complicated by measurement-induced stochasticity and the combinatorial nature of graph structure. In this paper, we introduce QuantumGraphLIME (QGraphLIME), a model-agnostic, post-hoc framework that treats model explanations as distributions over local surrogates fit on structure-preserving perturbations of a graph. By aggregating surrogate attributions together with their dispersion, QGraphLIME yields uncertainty-aware node and edge importance rankings for quantum graph models. The framework further provides a distribution-free, finite-sample guarantee on the size of the surrogate ensemble: a Dvoretzky-Kiefer-Wolfowitz bound ensures uniform approximation of the induced distribution of a binary class probability at target accuracy and confidence under standard independence assumptions. Empirical studies on controlled synthetic graphs with known ground truth demonstrate accurate and stable explanations, with ablations showing clear benefits of nonlinear surrogate modeling and highlighting sensitivity to perturbation design. Collectively, these results establish a principled, uncertainty-aware, and structure-sensitive approach to explaining quantum graph neural networks, and lay the groundwork for scaling to broader architectures and real-world datasets, as quantum resources mature. Code is available at https://github.com/smlab-niser/qglime.<br>
<br>
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2510.05609.pdf' target='_blank'>https://arxiv.org/pdf/2510.05609.pdf</a></span>   <span><a href='https://github.com/cjw2021/HOI-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junwen Chen, Peilin Xiong, Keiji Yanai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05609">HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent Human-object interaction detection (HOID) methods highly require prior knowledge from VLMs to enhance the interaction recognition capabilities. The training strategies and model architectures for connecting the knowledge from VLMs to the HOI instance representations from the object detector are challenging, and the whole framework is complex for further development or application. On the other hand, the inherent reasoning abilities of MLLMs on human-object interaction detection are under-explored. Inspired by the recent success of training MLLMs with reinforcement learning (RL) methods, we propose HOI-R1 and first explore the potential of the language model on the HOID task without any additional detection modules. We introduce an HOI reasoning process and HOID reward functions to solve the HOID task by pure text. The results on the HICO-DET dataset show that HOI-R1 achieves 2x the accuracy of the baseline with great generalization ability. The source code is available at https://github.com/cjw2021/HOI-R1.<br>
<span id='abs_ch'>Chinese Summary: 该 究提出HOI-R1方法，通过在多模态语言模型中运用强化学 进行纯文本推理来实现人-物交互检测，在HICO-DET数据集上达到基线两倍的准确率，且 需额外检测模块。English Summary: The study introduces HOI-R1, a method that leverages reinforcement learning in multimodal language models to perform human-object interaction detection purely through text reasoning, achieving double the baseline accuracy on the HICO-DET dataset without relying on additional detection modules.Paperid:148,https://arxiv.org/pdf/2510.05586.pdfGitHub</span><br>
<span id='abs_en'>English Summary: The study introduces HOI-R1, a method that leverages reinforcement learning in multimodal language models to perform human-object interaction detection purely through text reasoning, achieving double the baseline accuracy on the HICO-DET dataset without relying on additional detection modules.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2510.05379.pdf' target='_blank'>https://arxiv.org/pdf/2510.05379.pdf</a></span>   <span><a href='https://github.com/SaFoLab-WISC/AutoDAN-Reasoning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaogeng Liu, Chaowei Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05379">AutoDAN-Reasoning: Enhancing Strategies Exploration based Jailbreak Attacks with Test-Time Scaling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in jailbreaking large language models (LLMs), such as AutoDAN-Turbo, have demonstrated the power of automated strategy discovery. AutoDAN-Turbo employs a lifelong learning agent to build a rich library of attack strategies from scratch. While highly effective, its test-time generation process involves sampling a strategy and generating a single corresponding attack prompt, which may not fully exploit the potential of the learned strategy library. In this paper, we propose to further improve the attack performance of AutoDAN-Turbo through test-time scaling. We introduce two distinct scaling methods: Best-of-N and Beam Search. The Best-of-N method generates N candidate attack prompts from a sampled strategy and selects the most effective one based on a scorer model. The Beam Search method conducts a more exhaustive search by exploring combinations of strategies from the library to discover more potent and synergistic attack vectors. According to the experiments, the proposed methods significantly boost performance, with Beam Search increasing the attack success rate by up to 15.6 percentage points on Llama-3.1-70B-Instruct and achieving a nearly 60% relative improvement against the highly robust GPT-o4-mini compared to the vanilla method.<br>
<span id='abs_ch'>中文: 本文通过引入Best-of-N和集束搜索两种扩展方法，显著提升了AutoDAN-Turbo对大型语言模型的越狱攻击成功率，在高级模型上最高可提升15.6个百分点。</span><br>
<span id='abs_en'>English: This paper enhances AutoDAN-Turbo's jailbreaking effectiveness by introducing Best-of-N and Beam Search scaling methods, which significantly boost attack success rates by up to 15.6 percentage points on advanced LLMs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2510.05351.pdf' target='_blank'>https://arxiv.org/pdf/2510.05351.pdf</a></span>   <span><a href='https://github.com/Autumnstar-cjh/PIANO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinghao Cao, Qin Li, Mengnan Du, Haimin Wang, Bo Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05351">Physics-informed Attention-enhanced Fourier Neural Operator for Solar Magnetic Field Extrapolations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We propose Physics-informed Attention-enhanced Fourier Neural Operator (PIANO) to solve the Nonlinear Force-Free Field (NLFFF) problem in solar physics. Unlike conventional approaches that rely on iterative numerical methods, our proposed PIANO directly learns the 3D magnetic field structure from 2D boundary conditions. Specifically, PIANO integrates Efficient Channel Attention (ECA) mechanisms with Dilated Convolutions (DC), which enhances the model's ability to capture multimodal input by prioritizing critical channels relevant to the magnetic field's variations. Furthermore, we apply physics-informed loss by enforcing the force-free and divergence-free conditions in the training process so that our prediction is consistent with underlying physics with high accuracy. Experimental results on the ISEE NLFFF dataset show that our PIANO not only outperforms state-of-the-art neural operators in terms of accuracy but also shows strong consistency with the physical characteristics of NLFFF data across magnetic fields reconstructed from various solar active regions. The GitHub of this project is available https://github.com/Autumnstar-cjh/PIANO<br>
<span id='abs_ch'>我们提出PIANO，一种融合注意力机制与物理约束的神经算子，能够直接从二维边界条件学 三维磁场结构，在太阳磁场重建中实现了更高的精度和物理一致性。</span><br>
<span id='abs_en'>We propose PIANO, a physics-informed neural operator that directly learns 3D magnetic fields from 2D boundary conditions, integrating attention mechanisms and physical constraints to achieve superior accuracy and physical consistency in solar magnetic field reconstruction.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2510.05295.pdf' target='_blank'>https://arxiv.org/pdf/2510.05295.pdf</a></span>   <span><a href='https://github.com/mtanveer1/AVSEC-4-Challenge-2025' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>M. Sajid, Deepanshu Gupta, Yash Modi, Sanskriti Jain, Harshith Jai Surya Ganji, A. Rahaman, Harshvardhan Choudhary, Nasir Saleem, Amir Hussain, M. Tanveer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05295">AUREXA-SE: Audio-Visual Unified Representation Exchange Architecture with Cross-Attention and Squeezeformer for Speech Enhancement</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this paper, we propose AUREXA-SE (Audio-Visual Unified Representation Exchange Architecture with Cross-Attention and Squeezeformer for Speech Enhancement), a progressive bimodal framework tailored for audio-visual speech enhancement (AVSE). AUREXA-SE jointly leverages raw audio waveforms and visual cues by employing a U-Net-based 1D convolutional encoder for audio and a Swin Transformer V2 for efficient and expressive visual feature extraction. Central to the architecture is a novel bidirectional cross-attention mechanism, which facilitates deep contextual fusion between modalities, enabling rich and complementary representation learning. To capture temporal dependencies within the fused embeddings, a stack of lightweight Squeezeformer blocks combining convolutional and attention modules is introduced. The enhanced embeddings are then decoded via a U-Net-style decoder for direct waveform reconstruction, ensuring perceptually consistent and intelligible speech output. Experimental evaluations demonstrate the effectiveness of AUREXA-SE, achieving significant performance improvements over noisy baselines, with STOI of 0.516, PESQ of 1.323, and SI-SDR of -4.322 dB. The source code of AUREXA-SE is available at https://github.com/mtanveer1/AVSEC-4-Challenge-2025.<br>
<br>
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2510.05179.pdf' target='_blank'>https://arxiv.org/pdf/2510.05179.pdf</a></span>   <span><a href='https://github.com/anthropic-experimental/agentic-misalignment' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aengus Lynch, Benjamin Wright, Caleb Larson, Stuart J. Ritchie, Soren Mindermann, Ethan Perez, Kevin K. Troy, Evan Hubinger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05179">Agentic Misalignment: How LLMs Could Be Insider Threats</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We stress-tested 16 leading models from multiple developers in hypothetical corporate environments to identify potentially risky agentic behaviors before they cause real harm. In the scenarios, we allowed models to autonomously send emails and access sensitive information. They were assigned only harmless business goals by their deploying companies; we then tested whether they would act against these companies either when facing replacement with an updated version, or when their assigned goal conflicted with the company's changing direction. In at least some cases, models from all developers resorted to malicious insider behaviors when that was the only way to avoid replacement or achieve their goals - including blackmailing officials and leaking sensitive information to competitors. We call this phenomenon agentic misalignment. Models often disobeyed direct commands to avoid such behaviors. In another experiment, we told Claude to assess if it was in a test or a real deployment before acting. It misbehaved less when it stated it was in testing and misbehaved more when it stated the situation was real. We have not seen evidence of agentic misalignment in real deployments. However, our results (a) suggest caution about deploying current models in roles with minimal human oversight and access to sensitive information; (b) point to plausible future risks as models are put in more autonomous roles; and (c) underscore the importance of further research into, and testing of, the safety and alignment of agentic AI models, as well as transparency from frontier AI developers (Amodei, 2025). We are releasing our methods publicly to enable further research.<br>
<br>
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2510.05096.pdf' target='_blank'>https://arxiv.org/pdf/2510.05096.pdf</a></span>   <span><a href='https://github.com/showlab/Paper2Video' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Zhu, Kevin Qinghong Lin, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05096">Paper2Video: Automatic Video Generation from Scientific Papers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video. Unlike natural video, presentation video generation involves distinctive challenges: inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker. To address these challenges, we introduce Paper2Video, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics--Meta Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos convey the paper's information to the audience. Building on this foundation, we propose PaperTalker, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective tree search visual choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at https://github.com/showlab/Paper2Video.<br>
<span id='abs_ch'>中文：Paper2Video基准和PaperTalker框架通过自动化多模态内容协调并引入定制化评估指 ，解决了学术演示视频制作费时费力的问题，生成的视频比现有方法更具信息保真度和内容丰富性。</span><br>
<span id='abs_en'>English: The Paper2Video benchmark and PaperTalker framework address the labor-intensive creation of academic presentation videos by automating multi-modal content coordination and introducing tailored evaluation metrics, resulting in more faithful and informative videos than existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2510.05069.pdf' target='_blank'>https://arxiv.org/pdf/2510.05069.pdf</a></span>   <span><a href='https://github.com/sdc17/SwiReasoning,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dachuan Shi, Abedelkadir Asi, Keying Li, Xiangchi Yuan, Leyan Pan, Wenke Lee, Wen Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05069">SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent work shows that, beyond discrete reasoning through explicit chain-of-thought steps, which are limited by the boundaries of natural languages, large language models (LLMs) can also reason continuously in latent space, allowing richer information per step and thereby improving token efficiency. Despite this promise, latent reasoning still faces two challenges, especially in training-free settings: 1) purely latent reasoning broadens the search distribution by maintaining multiple implicit paths, which diffuses probability mass, introduces noise, and impedes convergence to a single high-confidence solution, thereby hurting accuracy; and 2) overthinking persists even without explicit text, wasting tokens and degrading efficiency. To address these issues, we introduce SwiReasoning, a training-free framework for LLM reasoning which features two key innovations: 1) SwiReasoning dynamically switches between explicit and latent reasoning, guided by block-wise confidence estimated from entropy trends in next-token distributions, to balance exploration and exploitation and promote timely convergence. 2) By limiting the maximum number of thinking-block switches, SwiReasoning curbs overthinking and improves token efficiency across varying problem difficulties. On widely used mathematics and STEM benchmarks, SwiReasoning consistently improves average accuracy by 1.5%-2.8% across reasoning LLMs of different model families and scales. Furthermore, under constrained budgets, SwiReasoning improves average token efficiency by 56%-79%, with larger gains as budgets tighten.<br>
<span id='abs_ch'>Chinese: 本文提出SwiReasoning框架，通过动态切换显性与潜在推理，解决了潜在推理中的概率扩散和过度思考问题，在数学和STEM基准测试中，将大型语言模型的平均准确率提升1.5%-2.8%，并在预算受限时显著提高 记效率56%-79%。English: This abstract introduces SwiReasoning, a training-free framework that dynamically switches between explicit and latent reasoning in large language models to address challenges like diffused probability mass and overthinking, thereby improving accuracy by 1.5%-2.8% and token efficiency by 56%-79% across various benchmarks.Paperid:162,https://arxiv.org/pdf/2510.05064.pdfGitHub</span><br>
<span id='abs_en'>English: This abstract introduces SwiReasoning, a training-free framework that dynamically switches between explicit and latent reasoning in large language models to address challenges like diffused probability mass and overthinking, thereby improving accuracy by 1.5%-2.8% and token efficiency by 56%-79% across various benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2510.05025.pdf' target='_blank'>https://arxiv.org/pdf/2510.05025.pdf</a></span>   <span><a href='https://github.com/sail-sg/imperceptible-jailbreaks' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kuofeng Gao, Yiming Li, Chao Du, Xin Wang, Xingjun Ma, Shu-Tao Xia, Tianyu Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05025">Imperceptible Jailbreaking against Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Jailbreaking attacks on the vision modality typically rely on imperceptible adversarial perturbations, whereas attacks on the textual modality are generally assumed to require visible modifications (e.g., non-semantic suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a class of Unicode characters called variation selectors. By appending invisible variation selectors to malicious questions, the jailbreak prompts appear visually identical to original malicious questions on screen, while their tokenization is "secretly" altered. We propose a chain-of-search pipeline to generate such adversarial suffixes to induce harmful responses. Our experiments show that our imperceptible jailbreaks achieve high attack success rates against four aligned LLMs and generalize to prompt injection attacks, all without producing any visible modifications in the written prompt. Our code is available at https://github.com/sail-sg/imperceptible-jailbreaks.<br>
<span id='abs_ch'>中文摘要：本文提出利用不可见的Unicode变体选择器实现隐形越狱攻击，通过改变分词过程而不产生可见修改，采用链式搜索方法成功攻击多个对齐大语言模型。</span><br>
<span id='abs_en'>English Summary: This paper introduces imperceptible jailbreaks using invisible Unicode variation selectors that alter tokenization without visible changes, achieving high attack success rates against multiple LLMs through a chain-of-search pipeline.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2510.05016.pdf' target='_blank'>https://arxiv.org/pdf/2510.05016.pdf</a></span>   <span><a href='https://github.com/OSU-NLP-Group/LLM-IOAA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas Carrit Delgado Pinheiro, Ziru Chen, Bruno Caixeta Piazza, Ness Shroff, Yingbin Liang, Yuan-Sen Ting, Huan Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05016">Large Language Models Achieve Gold Medal Performance at the International Olympiad on Astronomy & Astrophysics (IOAA)</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While task-specific demonstrations show early success in applying large language models (LLMs) to automate some astronomical research tasks, they only provide incomplete views of all necessary capabilities in solving astronomy problems, calling for more thorough understanding of LLMs' strengths and limitations. So far, existing benchmarks and evaluations focus on simple question-answering that primarily tests astronomical knowledge and fails to evaluate the complex reasoning required for real-world research in the discipline. Here, we address this gap by systematically benchmarking five state-of-the-art LLMs on the International Olympiad on Astronomy and Astrophysics (IOAA) exams, which are designed to examine deep conceptual understanding, multi-step derivations, and multimodal analysis. With average scores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing models) not only achieve gold medal level performance but also rank in the top two among ~200-300 participants in all four IOAA theory exams evaluated (2022-2025). In comparison, results on the data analysis exams show more divergence. GPT-5 still excels in the exams with an 88.5% average score, ranking top 10 among the participants in the four most recent IOAAs, while other models' performances drop to 48-76%. Furthermore, our in-depth error analysis underscores conceptual reasoning, geometric reasoning, and spatial visualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence, although LLMs approach peak human performance in theory exams, critical gaps must be addressed before they can serve as autonomous research agents in astronomy.<br>
<span id='abs_ch'>中文摘要：尽管Gemini 2.5 Pro和GPT-5等大语言模型在天文理论考试中接近人类顶尖水平，但在数据分析与复杂推理方面仍存在显著缺陷，目前尚不能作为独立的天文 究工具。</span><br>
<span id='abs_en'>English Summary: Large language models like Gemini 2.5 Pro and GPT-5 achieve near-human performance in astronomy theory exams but reveal critical weaknesses in data analysis and complex reasoning, limiting their current viability as autonomous research tools.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2510.04996.pdf' target='_blank'>https://arxiv.org/pdf/2510.04996.pdf</a></span>   <span><a href='https://github.com/RLHFlow/Reinforce-Ada' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Xiong, Chenlu Ye, Baohao Liao, Hanze Dong, Xinxing Xu, Christof Monz, Jiang Bian, Nan Jiang, Tong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04996">Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning applied to large language models (LLMs) for reasoning tasks is often bottlenecked by unstable gradient estimates due to fixed and uniform sampling of responses across prompts. Prior work such as GVM-RAFT addresses this by dynamically allocating inference budget per prompt to minimize stochastic gradient variance under a budget constraint. Inspired by this insight, we propose Reinforce-Ada, an adaptive sampling framework for online RL post-training of LLMs that continuously reallocates sampling effort to the prompts with the greatest uncertainty or learning potential. Unlike conventional two-stage allocation methods, Reinforce-Ada interleaves estimation and sampling in an online successive elimination process, and automatically stops sampling for a prompt once sufficient signal is collected. To stabilize updates, we form fixed-size groups with enforced reward diversity and compute advantage baselines using global statistics aggregated over the adaptive sampling phase. Empirical results across multiple model architectures and reasoning benchmarks show that Reinforce-Ada accelerates convergence and improves final performance compared to GRPO, especially when using the balanced sampling variant. Our work highlights the central role of variance-aware, adaptive data curation in enabling efficient and reliable reinforcement learning for reasoning-capable LLMs. Code is available at https://github.com/RLHFlow/Reinforce-Ada.<br>
<span id='abs_ch'>中文摘要：针对大语言模型在推理任务中 均匀采 导致梯度估计不稳定的问题，Reinforce-Ada提出自适应采 框架，通过在线动态分配采 资源至高潜力提示，并结合奖励多 性分组实现稳定优化，在多项基准测试中显著提升收敛速度与最终性能。English Summary: Reinforcement learning for large language models in reasoning tasks is hindered by unstable gradients from uniform response sampling, which Reinforce-Ada addresses through an adaptive online framework that dynamically reallocates sampling effort to high-uncertainty prompts and stabilizes updates with reward-diverse grouping.Paperid:169,https://arxiv.org/pdf/2510.04994.pdfGitHub</span><br>
<span id='abs_en'>English Summary: Reinforcement learning for large language models in reasoning tasks is hindered by unstable gradients from uniform response sampling, which Reinforce-Ada addresses through an adaptive online framework that dynamically reallocates sampling effort to high-uncertainty prompts and stabilizes updates with reward-diverse grouping.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2510.04978.pdf' target='_blank'>https://arxiv.org/pdf/2510.04978.pdf</a></span>   <span><a href='https://github.com/AI4Phys/Awesome-AI-for-Physics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Xiang, Terry Jingchen Zhang, Yinya Huang, Jixi He, Zirong Liu, Yueling Tang, Ruizhe Zhou, Lijing Luo, Youpeng Wen, Xiuwei Chen, Bingqian Lin, Jianhua Han, Hang Xu, Hanhui Li, Bin Dong, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04978">Aligning Perception, Reasoning, Modeling and Interaction: A Survey on Physical AI</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of embodied intelligence and world models has intensified efforts to integrate physical laws into AI systems, yet physical perception and symbolic physics reasoning have developed along separate trajectories without a unified bridging framework. This work provides a comprehensive overview of physical AI, establishing clear distinctions between theoretical physics reasoning and applied physical understanding while systematically examining how physics-grounded methods enhance AI's real-world comprehension across structured symbolic reasoning, embodied systems, and generative models. Through rigorous analysis of recent advances, we advocate for intelligent systems that ground learning in both physical principles and embodied reasoning processes, transcending pattern recognition toward genuine understanding of physical laws. Our synthesis envisions next-generation world models capable of explaining physical phenomena and predicting future states, advancing safe, generalizable, and interpretable AI systems. We maintain a continuously updated resource at https://github.com/AI4Phys/Awesome-AI-for-Physics.<br>
<span id='abs_ch'>中文摘要：本文主 将物理原理融入人工智能系统，以弥合符号推理与具身理解之间的鸿沟，旨在开发能够真正理解物理定律的可解释世界模型。English Summary: This paper advocates for integrating physical principles into AI systems to bridge the gap between symbolic reasoning and embodied understanding, aiming to develop interpretable world models that genuinely comprehend physical laws.Paperid:171,https://arxiv.org/pdf/2510.04938.pdfGitHub</span><br>
<span id='abs_en'>English Summary: This paper advocates for integrating physical principles into AI systems to bridge the gap between symbolic reasoning and embodied understanding, aiming to develop interpretable world models that genuinely comprehend physical laws.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2510.04938.pdf' target='_blank'>https://arxiv.org/pdf/2510.04938.pdf</a></span>   <span><a href='https://github.com/shiwenqin/ONNX-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiwen Qin, Alexander Auras, Shay B. Cohen, Elliot J. Crowley, Michael Moeller, Linus Ericsson, Jovita Lukasik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04938">ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Neural architecture search (NAS) automates the design process of high-performing architectures, but remains bottlenecked by expensive performance evaluation. Most existing studies that achieve faster evaluation are mostly tied to cell-based search spaces and graph encodings tailored to those individual search spaces, limiting their flexibility and scalability when applied to more expressive search spaces. In this work, we aim to close the gap of individual search space restrictions and search space dependent network representations. We present ONNX-Bench, a benchmark consisting of a collection of neural networks in a unified format based on ONNX files. ONNX-Bench includes all open-source NAS-bench-based neural networks, resulting in a total size of more than 600k {architecture, accuracy} pairs. This benchmark allows creating a shared neural network representation, ONNX-Net, able to represent any neural architecture using natural language descriptions acting as an input to a performance predictor. This text-based encoding can accommodate arbitrary layer types, operation parameters, and heterogeneous topologies, enabling a single surrogate to generalise across all neural architectures rather than being confined to cell-based search spaces. Experiments show strong zero-shot performance across disparate search spaces using only a small amount of pretraining samples, enabling the unprecedented ability to evaluate any neural network architecture instantly.<br>
<span id='abs_ch'>中文: ONNX-Bench提出了一种名为ONNX-Net的统一文本编 方法，使单个性能预测器能够泛化至超越单元搜索空间的各种神经网络架构，仅需少量预训练 本即可实现强大的零 本评估能力。</span><br>
<span id='abs_en'>English: ONNX-Bench introduces a unified text-based encoding called ONNX-Net, enabling a single performance predictor to generalize across diverse neural architectures beyond cell-based search spaces, achieving strong zero-shot evaluation with minimal pretraining.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2510.04933.pdf' target='_blank'>https://arxiv.org/pdf/2510.04933.pdf</a></span>   <span><a href='https://github.com/sirraya-tech/Sirraya_LSD_Code' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amir Hameed Mir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04933">The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) often produce fluent yet factually incorrect statements-a phenomenon known as hallucination-posing serious risks in high-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric framework for hallucination detection that analyzes the evolution of hidden-state semantics across transformer layers. Unlike prior methods that rely on multiple sampling passes or external verification sources, LSD operates intrinsically within the model's representational space. Using margin-based contrastive learning, LSD aligns hidden activations with ground-truth embeddings derived from a factual encoder, revealing a distinct separation in semantic trajectories: factual responses preserve stable alignment, while hallucinations exhibit pronounced semantic drift across depth. Evaluated on the TruthfulQA and synthetic factual-hallucination datasets, LSD achieves an F1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming SelfCheckGPT and Semantic Entropy baselines while requiring only a single forward pass. This efficiency yields a 5-20x speedup over sampling-based methods without sacrificing precision or interpretability. LSD offers a scalable, model-agnostic mechanism for real-time hallucination monitoring and provides new insights into the geometry of factual consistency within large language models.<br>
<span id='abs_ch'>中文摘要：层间语义动态（LSD）框架通过分析Transformer层间的语义漂移来检测大语言模型中的幻觉现象，仅需单次前向 播即可实现卓越的准确性与效率。</span><br>
<span id='abs_en'>English Summary: The Layer-wise Semantic Dynamics (LSD) framework detects hallucinations in Large Language Models by analyzing semantic drift across transformer layers, achieving superior accuracy and efficiency with a single forward pass.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2510.04910.pdf' target='_blank'>https://arxiv.org/pdf/2510.04910.pdf</a></span>   <span><a href='https://github.com/Muyiiiii/NeurIPS-25-Glocal-IB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Yang, Kexin Zhang, Guibin Zhang, Philip S. Yu, Kaize Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04910">Glocal Information Bottleneck for Time Series Imputation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Time Series Imputation (TSI), which aims to recover missing values in temporal data, remains a fundamental challenge due to the complex and often high-rate missingness in real-world scenarios. Existing models typically optimize the point-wise reconstruction loss, focusing on recovering numerical values (local information). However, we observe that under high missing rates, these models still perform well in the training phase yet produce poor imputations and distorted latent representation distributions (global information) in the inference phase. This reveals a critical optimization dilemma: current objectives lack global guidance, leading models to overfit local noise and fail to capture global information of the data. To address this issue, we propose a new training paradigm, Glocal Information Bottleneck (Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework by introducing a Global Alignment loss, derived from a tractable mutual information approximation. This loss aligns the latent representations of masked inputs with those of their originally observed counterparts. It helps the model retain global structure and local details while suppressing noise caused by missing values, giving rise to better generalization under high missingness. Extensive experiments on nine datasets confirm that Glocal-IB leads to consistently improved performance and aligned latent representations under missingness. Our code implementation is available in https://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.<br>
<span id='abs_ch'>中文摘要：提出的Glocal-IB训练范式通过引入全局对齐损失来对齐潜在表征，解决了时间序列插值中的优化困境，使模型在高缺失率下能更好地保持全局结构和局部细节。English Summary: The proposed Glocal-IB training paradigm addresses the optimization dilemma in time series imputation by introducing a Global Alignment loss that aligns latent representations, enabling models to better preserve global structure and local details under high missing rates.Paperid:174,https://arxiv.org/pdf/2510.04908.pdfGitHub</span><br>
<span id='abs_en'>English Summary: The proposed Glocal-IB training paradigm addresses the optimization dilemma in time series imputation by introducing a Global Alignment loss that aligns latent representations, enabling models to better preserve global structure and local details under high missing rates.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2510.04898.pdf' target='_blank'>https://arxiv.org/pdf/2510.04898.pdf</a></span>   <span><a href='https://github.com/MasterXiong/HyperVLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Xiong, Kang Li, Zilin Wang, Matthew Jackson, Jakob Foerster, Shimon Whiteson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04898">HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Built upon language and vision foundation models with strong generalization ability and trained on large-scale robotic data, Vision-Language-Action (VLA) models have recently emerged as a promising approach to learning generalist robotic policies. However, a key drawback of existing VLAs is their extremely high inference costs. In this paper, we propose HyperVLA to address this problem. Unlike existing monolithic VLAs that activate the whole model during both training and inference, HyperVLA uses a novel hypernetwork (HN)-based architecture that activates only a small task-specific policy during inference, while still retaining the high model capacity needed to accommodate diverse multi-task behaviors during training. Successfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization, and an action generation strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even higher success rate for both zero-shot generalization and few-shot adaptation, while significantly reducing inference costs. Compared to OpenVLA, a state-of-the-art VLA model, HyperVLA reduces the number of activated parameters at test time by $90\times$, and accelerates inference speed by $120\times$. Code is publicly available at https://github.com/MasterXiong/HyperVLA<br>
<span id='abs_ch'>中文摘要：HyperVLA采用基于超网络的新型架构，通过在推理时仅激活特定任务策略，大幅降低了计算成本，同时保持了机器人任务的高性能表现。</span><br>
<span id='abs_en'>English Summary: HyperVLA introduces a hypernetwork-based architecture that significantly reduces inference costs while maintaining high performance in robotic tasks by activating only task-specific policies during inference.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2510.04860.pdf' target='_blank'>https://arxiv.org/pdf/2510.04860.pdf</a></span>   <span><a href='https://github.com/aiming-lab/ATP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siwei Han, Jiaqi Liu, Yaofeng Su, Wenbo Duan, Xinyuan Liu, Cihang Xie, Mohit Bansal, Mingyu Ding, Linjun Zhang, Huaxiu Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04860">Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As Large Language Model (LLM) agents increasingly gain self-evolutionary capabilities to adapt and refine their strategies through real-world interaction, their long-term reliability becomes a critical concern. We identify the Alignment Tipping Process (ATP), a critical post-deployment risk unique to self-evolving LLM agents. Unlike training-time failures, ATP arises when continual interaction drives agents to abandon alignment constraints established during training in favor of reinforced, self-interested strategies. We formalize and analyze ATP through two complementary paradigms: Self-Interested Exploration, where repeated high-reward deviations induce individual behavioral drift, and Imitative Strategy Diffusion, where deviant behaviors spread across multi-agent systems. Building on these paradigms, we construct controllable testbeds and benchmark Qwen3-8B and Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode rapidly under self-evolution, with initially aligned models converging toward unaligned states. In multi-agent settings, successful violations diffuse quickly, leading to collective misalignment. Moreover, current reinforcement learning-based alignment methods provide only fragile defenses against alignment tipping. Together, these findings demonstrate that alignment of LLM agents is not a static property but a fragile and dynamic one, vulnerable to feedback-driven decay during deployment. Our data and code are available at https://github.com/aiming-lab/ATP.<br>
<span id='abs_ch'>Chinese: 自我进化的大语言模型智能体面临对齐临界过程风险，持续交互会使其逐渐放弃训练时的对齐约束而转向利己策略，导致对齐状态变得脆弱且动态不稳定。</span><br>
<span id='abs_en'>English: Self-evolving LLM agents risk losing alignment through the Alignment Tipping Process, where continuous interaction causes them to abandon trained constraints for self-interested strategies, making alignment fragile and dynamic rather than static.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2510.04671.pdf' target='_blank'>https://arxiv.org/pdf/2510.04671.pdf</a></span>   <span><a href='https://github.com/DUT-LiuChao/FocusMed' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Liu, Ling Luo, Tengxiao Lv, Huan Zhuang, Lejing Yu, Jian Wang, Hongfei Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04671">FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the rapid development of online medical platforms, consumer health questions (CHQs) are inefficient in diagnosis due to redundant information and frequent non-professional terms. The medical question summary (MQS) task aims to transform CHQs into streamlined doctors' frequently asked questions (FAQs), but existing methods still face challenges such as poor identification of question focus and model hallucination. This paper explores the potential of large language models (LLMs) in the MQS task and finds that direct fine-tuning is prone to focus identification bias and generates unfaithful content. To this end, we propose an optimization framework based on core focus guidance. First, a prompt template is designed to drive the LLMs to extract the core focus from the CHQs that is faithful to the original text. Then, a fine-tuning dataset is constructed in combination with the original CHQ-FAQ pairs to improve the ability to identify the focus of the question. Finally, a multi-dimensional quality evaluation and selection mechanism is proposed to comprehensively improve the quality of the summary from multiple dimensions. We conduct comprehensive experiments on two widely-adopted MQS datasets using three established evaluation metrics. The proposed framework achieves state-of-the-art performance across all measures, demonstrating a significant boost in the model's ability to identify critical focus of questions and a notable mitigation of hallucinations. The source codes are freely available at https://github.com/DUT-LiuChao/FocusMed.<br>
<br>
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2510.04623.pdf' target='_blank'>https://arxiv.org/pdf/2510.04623.pdf</a></span>   <span><a href='https://github.com/MiRL-IITM/medpao-agent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shrish Shrinath Vaidya, Gowthamaan Palani, Sidharth Ramesh, Velmurugan Balasubramanian, Minmini Selvam, Gokulraja Srinivasaraja, Ganapathy Krishnamurthi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04623">MedPAO: A Protocol-Driven Agent for Structuring Medical Reports</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The deployment of Large Language Models (LLMs) for structuring clinical data is critically hindered by their tendency to hallucinate facts and their inability to follow domain-specific rules. To address this, we introduce MedPAO, a novel agentic framework that ensures accuracy and verifiable reasoning by grounding its operation in established clinical protocols such as the ABCDEF protocol for CXR analysis. MedPAO decomposes the report structuring task into a transparent process managed by a Plan-Act-Observe (PAO) loop and specialized tools. This protocol-driven method provides a verifiable alternative to opaque, monolithic models. The efficacy of our approach is demonstrated through rigorous evaluation: MedPAO achieves an F1-score of 0.96 on the critical sub-task of concept categorization. Notably, expert radiologists and clinicians rated the final structured outputs with an average score of 4.52 out of 5, indicating a level of reliability that surpasses baseline approaches relying solely on LLM-based foundation models. The code is available at: https://github.com/MiRL-IITM/medpao-agent<br>
<span id='abs_ch'>中文: MedPAO提出了一种临床智能框架，通过基于既定协议进行推理来减少大语言模型的幻觉，在概念分类任务中取得了0.96的F1分数，并获得了专家4.52/5的高可 性评分。</span><br>
<span id='abs_en'>English: MedPAO introduces a clinical agentic framework that mitigates LLM hallucinations by grounding reasoning in established protocols, achieving a 0.96 F1-score and expert ratings of 4.52/5 for reliable structured outputs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2510.04617.pdf' target='_blank'>https://arxiv.org/pdf/2510.04617.pdf</a></span>   <span><a href='https://github.com/LaiZhejian/AdaR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhejian Lai, Xiang Geng, Zhijun Wang, Yang Bai, Jiahuan Li, Rongxiang Weng, Jingang Wang, Xuezhi Cao, Xunliang Cai, Shujian Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04617">Making Mathematical Reasoning Adaptive</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Mathematical reasoning is a primary indicator of large language models (LLMs) intelligence. However, existing LLMs exhibit failures of robustness and generalization. This paper attributes these deficiencies to spurious reasoning, i.e., producing answers from superficial features. To address this challenge, we propose the AdaR framework to enable adaptive reasoning, wherein models rely on problem-solving logic to produce answers. AdaR synthesizes logically equivalent queries by varying variable values, and trains models with RLVR on these data to penalize spurious logic while encouraging adaptive logic. To improve data quality, we extract the problem-solving logic from the original query and generate the corresponding answer by code execution, then apply a sanity check. Experimental results demonstrate that AdaR improves robustness and generalization, achieving substantial improvement in mathematical reasoning while maintaining high data efficiency. Analysis indicates that data synthesis and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs. Subsequent analyses derive key design insights into the effect of critical factors and the applicability to instruct LLMs. Our project is available at https://github.com/LaiZhejian/AdaR<br>
<span id='abs_ch'>中文: AdaR框架通过合成逻辑等效的查询并运用强化学 来惩罚表面推理，从而增强大型语言模型的数学推理能力，在保持数据效率的同时显著提升了鲁棒性和泛化性。</span><br>
<span id='abs_en'>English: The AdaR framework enhances large language models' mathematical reasoning by synthesizing logically equivalent queries and applying reinforcement learning to penalize superficial logic, thereby improving robustness and generalization while maintaining data efficiency.Paperid:186,https://arxiv.org/pdf/2510.04564.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2510.04506.pdf' target='_blank'>https://arxiv.org/pdf/2510.04506.pdf</a></span>   <span><a href='https://github.com/GasolSun36/GRACE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiashuo Sun, Shixuan Liu, Zhaochen Su, Xianrui Zhong, Pengcheng Jiang, Bowen Jin, Peiran Li, Weijia Shi, Jiawei Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04506">GRACE: Generative Representation Learning via Contrastive Policy Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that treat the model as a black box function, discarding its generative and reasoning capabilities in favor of static embeddings. We introduce GRACE (Generative Representation Learning via Contrastive Policy Optimization), a novel framework that reimagines contrastive signals not as losses to be minimized, but as rewards that guide a generative policy. In GRACE, the LLM acts as a policy that produces explicit, human-interpretable rationales--structured natural language explanations of its semantic understanding. These rationales are then encoded into high-quality embeddings via mean pooling. Using policy gradient optimization, we train the model with a multi-component reward function that maximizes similarity between query positive pairs and minimizes similarity with negatives. This transforms the LLM from an opaque encoder into an interpretable agent whose reasoning process is transparent and inspectable. On MTEB benchmark, GRACE yields broad cross category gains: averaged over four backbones, the supervised setting improves overall score by 11.5% over base models, and the unsupervised variant adds 6.9%, while preserving general capabilities. This work treats contrastive objectives as rewards over rationales, unifying representation learning with generation to produce stronger embeddings and transparent rationales. The model, data and code are available at https://github.com/GasolSun36/GRACE.<br>
<span id='abs_ch'>GRACE is a novel framework that transforms contrastive =
signals into rewards to train LLMs as generative policies, producing interp=
retable rationales and high-quality embeddings while achieving significant =
performance gains on benchmarks.</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2510.04491.pdf' target='_blank'>https://arxiv.org/pdf/2510.04491.pdf</a></span>   <span><a href='https://github.com/collinear-ai/tau-trait' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muyu He, Anand Kumar, Tsach Mackey, Meghana Rajeev, James Zou, Nazneen Rajani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04491">Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite rapid progress in building conversational AI agents, robustness is still largely untested. Small shifts in user behavior, such as being more impatient, incoherent, or skeptical, can cause sharp drops in agent performance, revealing how brittle current AI agents are. Today's benchmarks fail to capture this fragility: agents may perform well under standard evaluations but degrade spectacularly in more realistic and varied settings. We address this robustness testing gap by introducing TraitBasis, a lightweight, model-agnostic method for systematically stress testing AI agents. TraitBasis learns directions in activation space corresponding to steerable user traits (e.g., impatience or incoherence), which can be controlled, scaled, composed, and applied at inference time without any fine-tuning or extra data. Using TraitBasis, we extend $τ$-Bench to $τ$-Trait, where user behaviors are altered via controlled trait vectors. We observe on average a 2%-30% performance degradation on $τ$-Trait across frontier models, highlighting the lack of robustness of current AI agents to variations in user behavior. Together, these results highlight both the critical role of robustness testing and the promise of TraitBasis as a simple, data-efficient, and compositional tool. By powering simulation-driven stress tests and training loops, TraitBasis opens the door to building AI agents that remain reliable in the unpredictable dynamics of real-world human interactions. We have open-sourced $τ$-Trai across four domains: airline, retail, telecom, and telehealth, so the community can systematically QA their agents under realistic, behaviorally diverse intents and trait scenarios: https://github.com/collinear-ai/tau-trait.<br>
<span id='abs_ch'>中文摘要：当前对话AI代理在用户行为轻微变化时性能显著下降，为此我们提出TraitBasis这一模型 关的压力测试方法，通过可操控特征向量揭示代理鲁棒性最高下降30%的脆弱现状。</span><br>
<span id='abs_en'>English Summary: Current conversational AI agents show significant performance drops under slight behavioral shifts in users, prompting the introduction of TraitBasis—a model-agnostic method for stress testing that reveals up to 30% degradation in agent robustness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2510.04472.pdf' target='_blank'>https://arxiv.org/pdf/2510.04472.pdf</a></span>   <span><a href='https://github.com/Baber-Jan/SPEGNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Baber Jan, Saeed Anwar, Aiman H. El-Maleh, Abdul Jabbar Siddiqui, Abdul Bais
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04472">SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Camouflaged object detection segments objects with intrinsic similarity and edge disruption. Current detection methods rely on accumulated complex components. Each approach adds components such as boundary modules, attention mechanisms, and multi-scale processors independently. This accumulation creates a computational burden without proportional gains. To manage this complexity, they process at reduced resolutions, eliminating fine details essential for camouflage. We present SPEGNet, addressing fragmentation through a unified design. The architecture integrates multi-scale features via channel calibration and spatial enhancement. Boundaries emerge directly from context-rich representations, maintaining semantic-spatial alignment. Progressive refinement implements scale-adaptive edge modulation with peak influence at intermediate resolutions. This design strikes a balance between boundary precision and regional consistency. SPEGNet achieves 0.887 $S_α$ on CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed. Our approach excels across scales, from tiny, intricate objects to large, pattern-similar ones, while handling occlusion and ambiguous boundaries. Code, model weights, and results are available on \href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.<br>
<span id='abs_ch'>Chinese: SPEGNet提出了一种统一架构，通过整合多尺度特征和渐进式优化，在基准数据集上实现了精确的伪装目 检测和实时性能，超越了现有方法。</span><br>
<span id='abs_en'>English: SPEGNet introduces a unified architecture that integrates multi-scale features and progressive refinement to achieve precise camouflaged object detection with real-time performance, outperforming existing methods on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2510.04398.pdf' target='_blank'>https://arxiv.org/pdf/2510.04398.pdf</a></span>   <span><a href='https://github.com/Buyun-Liang/SECA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Buyun-Liang/SECA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Buyun Liang, Liangzu Peng, Jinqi Luo, Darshan Thaker, Kwan Ho Ryan Chan, René Vidal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04398">SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) are increasingly deployed in high-risk domains. However, state-of-the-art LLMs often produce hallucinations, raising serious concerns about their reliability. Prior work has explored adversarial attacks for hallucination elicitation in LLMs, but it often produces unrealistic prompts, either by inserting gibberish tokens or by altering the original meaning. As a result, these approaches offer limited insight into how hallucinations may occur in practice. While adversarial attacks in computer vision often involve realistic modifications to input images, the problem of finding realistic adversarial prompts for eliciting LLM hallucinations has remained largely underexplored. To address this gap, we propose Semantically Equivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic modifications to the prompt that preserve its meaning while maintaining semantic coherence. Our contributions are threefold: (i) we formulate finding realistic attacks for hallucination elicitation as a constrained optimization problem over the input prompt space under semantic equivalence and coherence constraints; (ii) we introduce a constraint-preserving zeroth-order method to effectively search for adversarial yet feasible prompts; and (iii) we demonstrate through experiments on open-ended multiple-choice question answering tasks that SECA achieves higher attack success rates while incurring almost no constraint violations compared to existing methods. SECA highlights the sensitivity of both open-source and commercial gradient-inaccessible LLMs to realistic and plausible prompt variations. Code is available at https://github.com/Buyun-Liang/SECA.<br>
<span id='abs_ch'>Chinese Summary: 本 究提出语义等价连贯攻击（SECA），通过保持语义连贯的现实提示修改来有效引发大语言模型产生幻觉，相比现有方法在保持约束的同时实现了更高的攻击成功率。</span><br>
<span id='abs_en'>English Summary: The study introduces Semantically Equivalent and Coherent Attacks (SECA), a method that uses realistic prompt modifications to effectively elicit hallucinations in Large Language Models while preserving semantic meaning and coherence, demonstrating higher success rates than existing approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2510.04390.pdf' target='_blank'>https://arxiv.org/pdf/2510.04390.pdf</a></span>   <span><a href='https://github.com/eric-ai-lab/Morph4D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuehai He, Shijie Zhou, Thivyanth Venkateswaran, Kaizhi Zheng, Ziyu Wan, Achuta Kadambi, Xin Eric Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04390">MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>World models that support controllable and editable spatiotemporal environments are valuable for robotics, enabling scalable training data, repro ducible evaluation, and flexible task design. While recent text-to-video models generate realistic dynam ics, they are constrained to 2D views and offer limited interaction. We introduce MorphoSim, a language guided framework that generates 4D scenes with multi-view consistency and object-level controls. From natural language instructions, MorphoSim produces dynamic environments where objects can be directed, recolored, or removed, and scenes can be observed from arbitrary viewpoints. The framework integrates trajectory-guided generation with feature field dis tillation, allowing edits to be applied interactively without full re-generation. Experiments show that Mor phoSim maintains high scene fidelity while enabling controllability and editability. The code is available at https://github.com/eric-ai-lab/Morph4D.<br>
<span id='abs_ch'>中文: MorphoSim是一种语言引导的框架，能生成具有多视角一致性和对象级控制的4D场景，支持动态环境交互编辑且 需完全重新生成，同时保持高场景保真度。</span><br>
<span id='abs_en'>English: MorphoSim is a language-guided framework that generates controllable 4D environments with multi-view consistency and object-level editing capabilities, enabling dynamic scene manipulation without full regeneration while maintaining high fidelity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2510.04363.pdf' target='_blank'>https://arxiv.org/pdf/2510.04363.pdf</a></span>   <span><a href='https://github.com/hyunjun1121/MacroBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunjun Kim, Sejong Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04363">MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce MacroBench, a code-first benchmark that evaluates whether LLMs can synthesize reusable browser-automation programs (macros) from natural-language goals by reading HTML/DOM and emitting Selenium. MacroBench instantiates seven self-hosted sites covering 681 tasks across interaction complexity and targeting difficulty. Our end-to-end protocol validates generated code via static checks, sandboxed execution, and outcome verification (DOM assertions, database snapshots), and includes a safety suite for scraping, spam/abuse, and credential/privacy prompts. Across 2,636 model-task runs, we observe stratified success: GPT-4o-mini (96.8%), GPT-4o (95.3%), Gemini (89.0%), DeepSeek (83.4%). Models handle simple tasks reliably (91.7%) but fail on complex workflows (0.0%), and none meet production-quality coding practices despite functional completion. We release our complete benchmark pipeline, evaluation framework, and experimental results at https://github.com/hyunjun1121/MacroBench to enable reproducible assessment of macro synthesis for web automation.<br>
<span id='abs_ch'>中文摘要：MacroBench是一个代 优先的基准测试，用于评估大语言模型从自然语言指令生成可复用浏览器自动化程序的能力，结果显示不同模型性能差异显著，且尽管能完成基础功能，但均 法处理复杂工作流程。</span><br>
<span id='abs_en'>English Summary: MacroBench is a code-first benchmark that evaluates LLMs' ability to generate reusable browser automation programs from natural language instructions, revealing significant performance gaps between models and their limitations in handling complex workflows despite functional completion.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2510.04241.pdf' target='_blank'>https://arxiv.org/pdf/2510.04241.pdf</a></span>   <span><a href='https://github.com/SeongJinAhn/DAD-SGM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seong Jin Ahn, Myoung-Ho Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04241">Diffusion-Assisted Distillation for Self-Supervised Graph Representation Learning with MLPs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>For large-scale applications, there is growing interest in replacing Graph Neural Networks (GNNs) with lightweight Multi-Layer Perceptrons (MLPs) via knowledge distillation. However, distilling GNNs for self-supervised graph representation learning into MLPs is more challenging. This is because the performance of self-supervised learning is more related to the model's inductive bias than supervised learning. This motivates us to design a new distillation method to bridge a huge capacity gap between GNNs and MLPs in self-supervised graph representation learning. In this paper, we propose \textbf{D}iffusion-\textbf{A}ssisted \textbf{D}istillation for \textbf{S}elf-supervised \textbf{G}raph representation learning with \textbf{M}LPs (DAD-SGM). The proposed method employs a denoising diffusion model as a teacher assistant to better distill the knowledge from the teacher GNN into the student MLP. This approach enhances the generalizability and robustness of MLPs in self-supervised graph representation learning. Extensive experiments demonstrate that DAD-SGM effectively distills the knowledge of self-supervised GNNs compared to state-of-the-art GNN-to-MLP distillation methods. Our implementation is available at https://github.com/SeongJinAhn/DAD-SGM.<br>
<span id='abs_ch'>Chinese: 针对自监督图神经网络难以蒸馏到轻量级多层感知机的问题，本文提出DAD-SGM方法，通过引入去噪扩散模型作为辅助教师，有效缩小模型能力差距，显著提升了多层感知机在自监督图表示学 中的性能。</span><br>
<span id='abs_en'>English: To address the challenge of distilling self-supervised Graph Neural Networks into lightweight MLPs, this paper introduces DAD-SGM, a diffusion-assisted method that enhances MLP performance by bridging the capacity gap through a denoising diffusion model as a teacher assistant.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2510.04206.pdf' target='_blank'>https://arxiv.org/pdf/2510.04206.pdf</a></span>   <span><a href='https://github.com/THUDM/AgentRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanchen Zhang, Xiao Liu, Bowen Lv, Xueqiao Sun, Bohao Jing, Iat Long Iong, Zhenyu Hou, Zehan Qi, Hanyu Lai, Yifan Xu, Rui Lu, Hongning Wang, Jie Tang, Yuxiao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04206">AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in large language models (LLMs) have sparked growing interest in building generalist agents that can learn through online interactions. However, applying reinforcement learning (RL) to train LLM agents in multi-turn, multi-task settings remains challenging due to lack of scalable infrastructure and stable training algorithms. In this work, we present the AgentRL framework for scalable multi-turn, multi-task agentic RL training. On the infrastructure side, AgentRL features a fully-asynchronous generation-training pipeline for efficient multi-turn RL. To support heterogeneous environment development in multi-task RL, we design a unified function-call based API interface, containerized environment development, and a centralized controller. On the algorithm side, we propose cross-policy sampling to encourage model exploration in multi-turn settings and task advantage normalization to stabilize multi-task training. Experiments show that AgentRL, trained on open LLMs across five agentic tasks, significantly outperforms GPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents. Multi-task training with AgentRL matches the best results among all task-specific models. AgentRL is open-sourced at https://github.com/THUDM/AgentRL. The algorithm and framework are adopted in building \textsc{\href{https://autoglm.zhipuai.cn}{AutoGLM}}.<br>
<span id='abs_ch'>中文：AgentRL框架通过可扩展的异步基础设施和稳定的多轮多任务强化学 算法，解决了大语言模型代理训练中的难题，在多项任务中显著超越了主流模型性能。</span><br>
<span id='abs_en'>English: The AgentRL framework addresses challenges in training large language model agents by introducing scalable infrastructure with an asynchronous pipeline and stable algorithms for multi-turn, multi-task reinforcement learning, achieving superior performance across various tasks compared to leading models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2510.04201.pdf' target='_blank'>https://arxiv.org/pdf/2510.04201.pdf</a></span>   <span><a href='https://github.com/mhson-kyle/World-To-Image' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Moo Hyun Son, Jintaek Oh, Sun Bin Mun, Jaechul Roh, Sehyun Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04201">World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While text-to-image (T2I) models can synthesize high-quality images, their performance degrades significantly when prompted with novel or out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We introduce World-To-Image, a novel framework that bridges this gap by empowering T2I generation with agent-driven world knowledge. We design an agent that dynamically searches the web to retrieve images for concepts unknown to the base model. This information is then used to perform multimodal prompt optimization, steering powerful generative backbones toward an accurate synthesis. Critically, our evaluation goes beyond traditional metrics, utilizing modern assessments like LLMGrader and ImageReward to measure true semantic fidelity. Our experiments show that World-To-Image substantially outperforms state-of-the-art methods in both semantic alignment and visual aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated NICE benchmark. Our framework achieves these results with high efficiency in less than three iterations, paving the way for T2I systems that can better reflect the ever-changing real world. Our demo code is available here\footnote{https://github.com/mhson-kyle/World-To-Image}.<br>
<span id='abs_ch'>中文: World-To-Image框架通过智能体检索网络知识补充模型未知概念，采用多模态提示优化技术，在语义准确性和视觉美感上显著超越现有方法。</span><br>
<span id='abs_en'>English: The World-To-Image framework enhances text-to-image models by using an agent to retrieve web-based knowledge for unknown concepts, significantly improving semantic accuracy and visual quality through multimodal prompt optimization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2510.04134.pdf' target='_blank'>https://arxiv.org/pdf/2510.04134.pdf</a></span>   <span><a href='https://github.com/neumyor/PhaseFormer_TSL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Niu, Jinliang Deng, Yongxin Tong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04134">PhaseFormer: From Patches to Phases for Efficient and Effective Time Series Forecasting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Periodicity is a fundamental characteristic of time series data and has long played a central role in forecasting. Recent deep learning methods strengthen the exploitation of periodicity by treating patches as basic tokens, thereby improving predictive effectiveness. However, their efficiency remains a bottleneck due to large parameter counts and heavy computational costs. This paper provides, for the first time, a clear explanation of why patch-level processing is inherently inefficient, supported by strong evidence from real-world data. To address these limitations, we introduce a phase perspective for modeling periodicity and present an efficient yet effective solution, PhaseFormer. PhaseFormer features phase-wise prediction through compact phase embeddings and efficient cross-phase interaction enabled by a lightweight routing mechanism. Extensive experiments demonstrate that PhaseFormer achieves state-of-the-art performance with around 1k parameters, consistently across benchmark datasets. Notably, it excels on large-scale and complex datasets, where models with comparable efficiency often struggle. This work marks a significant step toward truly efficient and effective time series forecasting. Code is available at this repository: https://github.com/neumyor/PhaseFormer_TSL<br>
<span id='abs_ch'>中文摘要：本文提出PhaseFormer模型，通过采用相位视角配合紧凑嵌入和轻量级路由机制，解决了基于分块方法在时序预测中的效率瓶颈，仅用约1,000参数即实现最优性能。</span><br>
<span id='abs_en'>English Summary: This paper introduces PhaseFormer, an efficient time series forecasting model that addresses the inefficiency of patch-based methods by adopting a phase perspective with compact embeddings and lightweight routing, achieving state-of-the-art performance using only about 1,000 parameters.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2510.04051.pdf' target='_blank'>https://arxiv.org/pdf/2510.04051.pdf</a></span>   <span><a href='https://github.com/Rorschach1989/efficient-lm-eval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lele Liao, Qile Zhang, Ruofan Wu, Guanhua Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04051">Toward a unified framework for data-efficient evaluation of large language models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Evaluating large language models (LLMs) on comprehensive benchmarks is a cornerstone of their development, yet it's often computationally and financially prohibitive. While Item Response Theory (IRT) offers a promising path toward data-efficient evaluation by disentangling model capability from item difficulty, existing IRT-based methods are hampered by significant limitations. They are typically restricted to binary correctness metrics, failing to natively handle the continuous scores used in generative tasks, and they operate on single benchmarks, ignoring valuable structural knowledge like correlations across different metrics or benchmarks. To overcome these challenges, we introduce LEGO-IRT, a unified and flexible framework for data-efficient LLM evaluation. LEGO-IRT's novel design natively supports both binary and continuous evaluation metrics. Moreover, it introduces a factorized architecture to explicitly model and leverage structural knowledge, decomposing model ability estimates into a general component and structure-specific (e.g., per-metric or per-benchmark) components. Through extensive experiments involving $70$ LLMs across $5$ benchmarks, we show that LEGO-IRT achieves stable capability estimates using just $3\%$ of the total evaluation items. We demonstrate that incorporating structural knowledge reduces estimation error by up to $10\%$ and reveal that the latent abilities estimated by our framework may align more closely with human preferences.<br>
<span id='abs_ch'>中文: LEGO-IRT是一种创新的框架，通过支持二元和连续评估指 并利用跨基准的结构知识，仅需3%的评估项即可实现大语言模型的数据高效评估，获得稳定的能力估计。</span><br>
<span id='abs_en'>English: LEGO-IRT is a novel framework that enables data-efficient evaluation of large language models by supporting both binary and continuous metrics while leveraging structural knowledge across benchmarks, achieving stable capability estimates with only 3% of evaluation items.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2510.04044.pdf' target='_blank'>https://arxiv.org/pdf/2510.04044.pdf</a></span>   <span><a href='https://github.com/codeiscommitting/REQuant' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingtao Yang, Yujia Wang, Mengzhi Jiao, Hongwei Huo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04044">Quantization Range Estimation for Convolutional Neural Networks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Post-training quantization for reducing the storage of deep neural network models has been demonstrated to be an effective way in various tasks. However, low-bit quantization while maintaining model accuracy is a challenging problem. In this paper, we present a range estimation method to improve the quantization performance for post-training quantization. We model the range estimation into an optimization problem of minimizing quantization errors by layer-wise local minima. We prove this problem is locally convex and present an efficient search algorithm to find the optimal solution. We propose the application of the above search algorithm to the transformed weights space to do further improvement in practice. Our experiments demonstrate that our method outperforms state-of-the-art performance generally on top-1 accuracy for image classification tasks on the ResNet series models and Inception-v3 model. The experimental results show that the proposed method has almost no loss of top-1 accuracy in 8-bit and 6-bit settings for image classifications, and the accuracy of 4-bit quantization is also significantly improved. The code is available at https://github.com/codeiscommitting/REQuant.<br>
<span id='abs_ch'>中文: 本文提出了一种通过逐层局部最小值优化来最小化量化误差的范围估计方法，显著提升了训练后量化性能，在图像分类模型的8位和6位量化中 乎 精度损失，并大幅改善了4位量化的准确性。</span><br>
<span id='abs_en'>English: This paper introduces a range estimation method that minimizes quantization errors through layer-wise optimization, significantly enhancing post-training quantization performance with minimal accuracy loss in 8-bit and 6-bit settings and notable improvements in 4-bit quantization for image classification models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2510.04001.pdf' target='_blank'>https://arxiv.org/pdf/2510.04001.pdf</a></span>   <span><a href='https://github.com/kkkenshi/LLM-EKA/tree/master' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuankang Zhang, Jiangming Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04001">Named Entity Recognition in COVID-19 tweets with Entity Knowledge Augmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The COVID-19 pandemic causes severe social and economic disruption around the world, raising various subjects that are discussed over social media. Identifying pandemic-related named entities as expressed on social media is fundamental and important to understand the discussions about the pandemic. However, there is limited work on named entity recognition on this topic due to the following challenges: 1) COVID-19 texts in social media are informal and their annotations are rare and insufficient to train a robust recognition model, and 2) named entity recognition in COVID-19 requires extensive domain-specific knowledge. To address these issues, we propose a novel entity knowledge augmentation approach for COVID-19, which can also be applied in general biomedical named entity recognition in both informal text format and formal text format. Experiments carried out on the COVID-19 tweets dataset and PubMed dataset show that our proposed entity knowledge augmentation improves NER performance in both fully-supervised and few-shot settings. Our source code is publicly available: https://github.com/kkkenshi/LLM-EKA/tree/master<br>
<span id='abs_ch'>中文摘要：新 疫情在社交媒体引发广泛讨论，但由于非正式文本和领域知识匮乏，识别相关实体面临挑战，为此提出一种实体知识增强方法，有效提升了社交媒体和生物医学文本中的命名实体识别性能。</span><br>
<span id='abs_en'>English Summary: The COVID-19 pandemic has spurred discussions on social media, but identifying pandemic-related entities is challenging due to informal language and scarce domain-specific knowledge, leading to a new entity knowledge augmentation method that enhances named entity recognition performance in both social media and biomedical texts.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2510.03971.pdf' target='_blank'>https://arxiv.org/pdf/2510.03971.pdf</a></span>   <span><a href='https://github.com/rl4reasoning/rl-baselines' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jatin Prakash, Anirudh Buvanesh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03971">What Can You Do When You Have Zero Rewards During RL?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning (RL) with outcome-based rewards has proven effective for improving large language models (LLMs) on complex reasoning tasks. However, its success often depends on the base model occasionally sampling correct solutions. When no correct solutions are sampled, training encounters a zero-reward barrier where learning stalls due to zero gradients. We study this scenario through the graph search task introduced in Bachmann et al. (2024) and evaluate recent methods that incorporate desirable components such as dense rewards, diversity incentives, and improved credit assignment. Our experiments show that none of these approaches overcome the zero-reward barrier if the base model never produces a correct answer. In contrast, we find that a simple data-centric intervention of adding easier samples to the training set enables the model to eventually solve the original hard task despite starting from zero reward. Importantly, this succeeds without modifying the RL algorithm itself. Because official implementations of several baselines were unavailable, we developed our own, which allowed us to conduct a detailed analysis of their failure modes. We release these implementations to support further research at: https://github.com/rl4reasoning/rl-baselines<br>
<br>
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2510.03812.pdf' target='_blank'>https://arxiv.org/pdf/2510.03812.pdf</a></span>   <span><a href='https://github.com/RCSL-TCD/ReTiDe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Changhong Li, Clément Bled, Rosa Fernandez, Shreejith Shanker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03812">ReTiDe: Real-Time Denoising for Energy-Efficient Motion Picture Processing with FPGAs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Denoising is a core operation in modern video pipelines. In codecs, in-loop filters suppress sensor noise and quantisation artefacts to improve rate-distortion performance; in cinema post-production, denoisers are used for restoration, grain management, and plate clean-up. However, state-of-the-art deep denoisers are computationally intensive and, at scale, are typically deployed on GPUs, incurring high power and cost for real-time, high-resolution streams. This paper presents Real-Time Denoise (ReTiDe), a hardware-accelerated denoising system that serves inference on data-centre Field Programmable Gate Arrays (FPGAs). A compact convolutional model is quantised (post-training quantisation plus quantisation-aware fine-tuning) to INT8 and compiled for AMD Deep Learning Processor Unit (DPU)-based FPGAs. A client-server integration offloads computation from the host CPU/GPU to a networked FPGA service, while remaining callable from existing workflows, e.g., NUKE, without disrupting artist tooling. On representative benchmarks, ReTiDe delivers 37.71$\times$ Giga Operations Per Second (GOPS) throughput and 5.29$\times$ higher energy efficiency than prior FPGA denoising accelerators, with negligible degradation in Peak Signal-to-Noise Ratio (PSNR)/Structural Similarity Index (SSIM). These results indicate that specialised accelerators can provide practical, scalable denoising for both encoding pipelines and post-production, reducing energy per frame without sacrificing quality or workflow compatibility. Code is available at https://github.com/RCSL-TCD/ReTiDe.<br>
<span id='abs_ch'>中文: 本文提出的ReTiDe硬件 速去噪系统基于FPGA实现，在保持图像质量和工作流兼容性的同时，显著提升了处理效率与能效比。</span><br>
<span id='abs_en'>English: The paper introduces ReTiDe, a hardware-accelerated denoising system using FPGAs that achieves high throughput and energy efficiency without compromising quality or workflow integration.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2510.03763.pdf' target='_blank'>https://arxiv.org/pdf/2510.03763.pdf</a></span>   <span><a href='https://github.com/ajiaaa/ARSAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxin Deng, Junbiao Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03763">Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Sharpness-Aware Minimization (SAM) improves model generalization but doubles the computational cost of Stochastic Gradient Descent (SGD) by requiring twice the gradient calculations per optimization step. To mitigate this, we propose Adaptively sampling-Reusing-mixing decomposed gradients to significantly accelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can be decomposed into the SGD gradient and the Projection of the Second-order gradient onto the First-order gradient (PSF). Furthermore, we observe that the SGD gradient and PSF dynamically evolve during training, emphasizing the growing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed to the reused PSF and the timely updated PSF still maintain the model's generalization ability. Extensive experiments show that ARSAM achieves state-of-the-art accuracies comparable to SAM across diverse network architectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a speedup of about 40\%. Moreover, ARSAM accelerates optimization for the various challenge tasks (\textit{e.g.}, human pose estimation, and model quantization) without sacrificing performance, demonstrating its broad practicality.% The code is publicly accessible at: https://github.com/ajiaaa/ARSAM.<br>
<span id='abs_ch'>中文: ARSAM通过自适应重用分解梯度来 速锐度感知最小化，在CIFAR数据集上保持与SAM相当的准确率同时实现约40%的速度提升。</span><br>
<span id='abs_en'>English: ARSAM accelerates Sharpness-Aware Minimization by adaptively reusing decomposed gradients, maintaining comparable accuracy to SAM while achieving about 40% speedup on CIFAR datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2510.03761.pdf' target='_blank'>https://arxiv.org/pdf/2510.03761.pdf</a></span>   <span><a href='https://github.com/LaTeXpOsEd' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Richard A. Dubniczky, Bertalan Borsos, Tihanyi Norbert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03761">You Have Been LaTeXpOsEd: A Systematic Analysis of Information Leakage in Preprint Archives Using Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The widespread use of preprint repositories such as arXiv has accelerated the communication of scientific results but also introduced overlooked security risks. Beyond PDFs, these platforms provide unrestricted access to original source materials, including LaTeX sources, auxiliary code, figures, and embedded comments. In the absence of sanitization, submissions may disclose sensitive information that adversaries can harvest using open-source intelligence. In this work, we present the first large-scale security audit of preprint archives, analyzing more than 1.2 TB of source data from 100,000 arXiv submissions. We introduce LaTeXpOsEd, a four-stage framework that integrates pattern matching, logical filtering, traditional harvesting techniques, and large language models (LLMs) to uncover hidden disclosures within non-referenced files and LaTeX comments. To evaluate LLMs' secret-detection capabilities, we introduce LLMSec-DB, a benchmark on which we tested 25 state-of-the-art models. Our analysis uncovered thousands of PII leaks, GPS-tagged EXIF files, publicly available Google Drive and Dropbox folders, editable private SharePoint links, exposed GitHub and Google credentials, and cloud API keys. We also uncovered confidential author communications, internal disagreements, and conference submission credentials, exposing information that poses serious reputational risks to both researchers and institutions. We urge the research community and repository operators to take immediate action to close these hidden security gaps. To support open science, we release all scripts and methods from this study but withhold sensitive findings that could be misused, in line with ethical principles. The source code and related material are available at the project website https://github.com/LaTeXpOsEd<br>
<br>
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2510.03680.pdf' target='_blank'>https://arxiv.org/pdf/2510.03680.pdf</a></span>   <span><a href='https://github.com/quasar529/rainbow-padding' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bumjun Kim, Dongjae Jeon, Dueun Kim, Wonje Jeung, Albert No
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03680">Rainbow Padding: Mitigating Early Termination in Instruction-Tuned Diffusion LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion large language models (dLLMs) have emerged as a promising alternative to autoregressive models, offering flexible generation orders and strong performance on complex reasoning tasks. However, instruction-tuned dLLMs exhibit a critical vulnerability we term \texttt{<eos>} overflow: as allocated sequence length increases, responses paradoxically become shorter, collapsing into early termination or degenerating into streams of \texttt{<eos>} tokens. Although noticed in practice, this issue has not been systematically analyzed. We trace its root cause to the dual role of \texttt{<eos>} as both termination and padding, which concentrates probability mass on \texttt{<eos>} at later positions and propagates backward to trigger early termination. To address this, we introduce Rainbow Padding, a simple remedy that replaces repeated \texttt{<eos>} placeholders with a repeating cycle of distinct padding tokens, distributing probability mass and breaking \texttt{<eos>} dominance. Experiments show that Rainbow Padding substantially improves length robustness and output quality, with as few as seven padding tokens sufficient to prevent early termination. Moreover, the method integrates efficiently into existing instruction-tuned models: LoRA fine-tuning for a single epoch on minimal data yields significant improvements, making this solution highly practical. The code is publicly available at https://github.com/quasar529/rainbow-padding.<br>
<span id='abs_ch'>Chinese: 扩散大语言模型（dLLMs）存在“溢出”漏洞，即分配序列越长，响应反而越短， 源在于=E5时作为终止符和填充符；而彩虹填充法通过用循环的不同填充符替代重复，以极少的微调即可显著提升输出质量和长度鲁棒性，有效解决此问题。</span><br>
<span id='abs_en'>English: Diffusion large language models (dLLMs) suffer from a vulnerability called "overflow," where longer allocated sequences cause shorter responses due to the dual role ofas both termination and padding, but this issue is effectively resolved by Rainbow Padding, a method that replaces repeatedtokens with a cycle of distinct padding tokens to improve output quality and length robustness with minimal fine-tuning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2510.03650.pdf' target='_blank'>https://arxiv.org/pdf/2510.03650.pdf</a></span>   <span><a href='https://github.com/hockeyguy123/openevolve-star-discrepancy.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amir Sadikov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03650">LLM-Guided Evolutionary Program Synthesis for Quasi-Monte Carlo Design</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Low-discrepancy point sets and digital sequences underpin quasi-Monte Carlo (QMC) methods for high-dimensional integration. We cast two long-standing QMC design problems as program synthesis and solve them with an LLM-guided evolutionary loop that mutates and selects code under task-specific fitness: (i) constructing finite 2D/3D point sets with low star discrepancy, and (ii) choosing Sobol' direction numbers that minimize randomized QMC error on downstream integrands. Our two-phase procedure combines constructive code proposals with iterative numerical refinement. On finite sets, we rediscover known optima in small 2D cases and set new best-known 2D benchmarks for N >= 40, while matching most known 3D optima up to the proven frontier (N <= 8) and reporting improved 3D benchmarks beyond. On digital sequences, evolving Sobol' parameters yields consistent reductions in randomized quasi-Monte Carlo (rQMC) mean-squared error for several 32-dimensional option-pricing tasks relative to widely used Joe--Kuo parameters, while preserving extensibility to any sample size and compatibility with standard randomizations. Taken together, the results demonstrate that LLM-driven evolutionary program synthesis can automate the discovery of high-quality QMC constructions, recovering classical designs where they are optimal and improving them where finite-N structure matters. Data and code are available at https://github.com/hockeyguy123/openevolve-star-discrepancy.git.<br>
<span id='abs_ch'>中文总结：本 究采用LLM引导的进化程序合成方法，自动设计高质量拟蒙特卡洛结构，在有限点集上创下新基准，并针对金融应用改进了Sobol序列参数。</span><br>
<span id='abs_en'>English Summary: This study uses LLM-guided evolutionary program synthesis to automate the design of high-quality quasi-Monte Carlo constructions, achieving new benchmarks for finite point sets and improving Sobol' sequence parameters for financial applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2510.03597.pdf' target='_blank'>https://arxiv.org/pdf/2510.03597.pdf</a></span>   <span><a href='https://github.com/VITA-Group/Neon' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sina Alemohammad, Zhangyang Wang, Richard G. Baraniuk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03597">Neon: Negative Extrapolation From Self-Training Improves Image Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Scaling generative AI models is bottlenecked by the scarcity of high-quality training data. The ease of synthesizing from a generative model suggests using (unverified) synthetic data to augment a limited corpus of real data for the purpose of fine-tuning in the hope of improving performance. Unfortunately, however, the resulting positive feedback loop leads to model autophagy disorder (MAD, aka model collapse) that results in a rapid degradation in sample quality and/or diversity. In this paper, we introduce Neon (for Negative Extrapolation frOm self-traiNing), a new learning method that turns the degradation from self-training into a powerful signal for self-improvement. Given a base model, Neon first fine-tunes it on its own self-synthesized data but then, counterintuitively, reverses its gradient updates to extrapolate away from the degraded weights. We prove that Neon works because typical inference samplers that favor high-probability regions create a predictable anti-alignment between the synthetic and real data population gradients, which negative extrapolation corrects to better align the model with the true data distribution. Neon is remarkably easy to implement via a simple post-hoc merge that requires no new real data, works effectively with as few as 1k synthetic samples, and typically uses less than 1% additional training compute. We demonstrate Neon's universality across a range of architectures (diffusion, flow matching, autoregressive, and inductive moment matching models) and datasets (ImageNet, CIFAR-10, and FFHQ). In particular, on ImageNet 256x256, Neon elevates the xAR-L model to a new state-of-the-art FID of 1.02 with only 0.36% additional training compute. Code is available at https://github.com/VITA-Group/Neon<br>
<span id='abs_ch'>中文: 生成式AI的扩展受限于高质量数据的稀缺，使用合成数据进行微调可能导致模型崩溃，而新方法Neon通过反转梯度更新有效 正这一问题，使模型更贴合真实数据分布。</span><br>
<span id='abs_en'>English: Scaling generative AI is hindered by limited high-quality data, and using synthetic data for fine-tuning can cause model collapse, but the new method Neon counteracts this degradation by reversing gradient updates to better align with the true data distribution.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2510.03502.pdf' target='_blank'>https://arxiv.org/pdf/2510.03502.pdf</a></span>   <span><a href='https://github.com/alikhairallah/ALHD-Benchmarking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Khairallah, Arkaitz Zubiaga
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03502">ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce ALHD, the first large-scale comprehensive Arabic dataset explicitly designed to distinguish between human- and LLM-generated texts. ALHD spans three genres (news, social media, reviews), covering both MSA and dialectal Arabic, and contains over 400K balanced samples generated by three leading LLMs and originated from multiple human sources, which enables studying generalizability in Arabic LLM-genearted text detection. We provide rigorous preprocessing, rich annotations, and standardized balanced splits to support reproducibility. In addition, we present, analyze and discuss benchmark experiments using our new dataset, in turn identifying gaps and proposing future research directions. Benchmarking across traditional classifiers, BERT-based models, and LLMs (zero-shot and few-shot) demonstrates that fine-tuned BERT models achieve competitive performance, outperforming LLM-based models. Results are however not always consistent, as we observe challenges when generalizing across genres; indeed, models struggle to generalize when they need to deal with unseen patterns in cross-genre settings, and these challenges are particularly prominent when dealing with news articles, where LLM-generated texts resemble human texts in style, which opens up avenues for future research. ALHD establishes a foundation for research related to Arabic LLM-detection and mitigating risks of misinformation, academic dishonesty, and cyber threats.<br>
<span id='abs_ch'>中文摘要：ALHD是首个专为区分人类与LLM生成文本而设计的大规模阿拉伯语数据集，涵盖多种文体和方言，包含超40万平衡 本，基准实验揭示了跨文体泛化的挑战，尤其在新闻类文本中最为显著。</span><br>
<span id='abs_en'>English Summary: ALHD is the first large-scale Arabic dataset designed to distinguish human- from LLM-generated texts across multiple genres and dialects, featuring over 400K balanced samples and benchmark experiments that reveal challenges in cross-genre generalization, particularly with news articles.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2510.03501.pdf' target='_blank'>https://arxiv.org/pdf/2510.03501.pdf</a></span>   <span><a href='https://github.com/LyesSaadSaoud/mobile-houbara-detseg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lyes Saad Saoud, Loic Lesobre, Enrico Sorato, Irfan Hussain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03501">Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Real-time animal detection and segmentation in natural environments are vital for wildlife conservation, enabling non-invasive monitoring through remote camera streams. However, these tasks remain challenging due to limited computational resources and the cryptic appearance of many species. We propose a mobile-optimized two-stage deep learning framework that integrates a Threading Detection Model (TDM) to parallelize YOLOv10-based detection and MobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach improves real-time performance by reducing latency through threading. YOLOv10 handles detection while MobileSAM performs lightweight segmentation, both executed concurrently for efficient resource use. On the cryptic Houbara Bustard, a conservation-priority species, our model achieves mAP50 of 0.9627, mAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10 operates at 43.7 ms per frame, confirming real-time readiness. We introduce a curated Houbara dataset of 40,000 annotated images to support model training and evaluation across diverse conditions. The code and dataset used in this study are publicly available on GitHub at https://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos and additional resources, visit https://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.<br>
<span id='abs_ch'>Chinese: 本 究提出了一种移动优化的两阶段深度学 框架，结合YOLOv10进行检测和MobileSAM进行分割，通过线程化技术提升实时性能，在隐蔽的波斑鸨检测与分割中实现了高精度。English: This study introduces a mobile-optimized two-stage deep learning framework that combines YOLOv10 for detection and MobileSAM for segmentation, using threading to enhance real-time performance and achieving high accuracy in detecting and segmenting the cryptic Houbara Bustard.Paperid:241,https://arxiv.org/pdf/2510.03472.pdfGitHub</span><br>
<span id='abs_en'>English: This study introduces a mobile-optimized two-stage deep learning framework that combines YOLOv10 for detection and MobileSAM for segmentation, using threading to enhance real-time performance and achieving high accuracy in detecting and segmenting the cryptic Houbara Bustard.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2510.03472.pdf' target='_blank'>https://arxiv.org/pdf/2510.03472.pdf</a></span>   <span><a href='https://github.com/lunjohnzhang/tmo_public' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yulun Zhang, Alexandre O. G. Barbosa, Federico Pecora, Jiaoyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03472">Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We study optimizing a destination-to-chutes task mapping to improve throughput in Robotic Sorting Systems (RSS), where a team of robots sort packages on a sortation floor by transporting them from induct workstations to eject chutes based on their shipping destinations (e.g. Los Angeles or Pittsburgh). The destination-to-chutes task mapping is used to determine which chutes a robot can drop its package. Finding a high-quality task mapping is challenging because of the complexity of a real-world RSS. First, optimizing task mapping is interdependent with robot target assignment and path planning. Second, chutes will be CLOSED for a period of time once they receive sufficient packages to allow for downstream processing. Third, task mapping quality directly impacts the downstream processing, as scattered chutes for the same destination increase package handling time. In this paper, we first formally define task mappings and the problem of Task Mapping Optimization (TMO). We then present a simulator of RSS to evaluate task mappings. We then present a simple TMO method based on the Evolutionary Algorithm and Mixed Integer Linear Programming, demonstrating the advantage of our optimized task mappings over the greedily generated ones in various RSS setups with different map sizes, numbers of chutes, and destinations. Finally, we use Quality Diversity algorithms to analyze the throughput of a diverse set of task mappings. Our code is available online at https://github.com/lunjohnzhang/tmo_public.<br>
<span id='abs_ch'>中文摘要：本 究针对机器人分拣系统中的任务 射优化问题，开发了基于进化算法和混合整数线性规划的优化方法，有效解决了与机器人任务分配、路径规划及滑槽关闭的相互依赖关系，从而显著提升了系统吞吐量。</span><br>
<span id='abs_en'>English Summary: This research develops optimization methods for task mapping in robotic sorting systems to enhance throughput by addressing interdependencies with robot assignments and path planning while considering chute closures and downstream processing impacts.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2510.03426.pdf' target='_blank'>https://arxiv.org/pdf/2510.03426.pdf</a></span>   <span><a href='https://github.com/glassroom/generalized_orders_of_magnitude' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Franz A. Heinsen, Leo Kozachkov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03426">Generalized Orders of Magnitude for Scalable, Parallel, High-Dynamic-Range Computation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Many domains, from deep learning to finance, require compounding real numbers over long sequences, often leading to catastrophic numerical underflow or overflow. We introduce generalized orders of magnitude (GOOMs), a principled extension of traditional orders of magnitude that incorporates floating-point numbers as a special case, and which in practice enables stable computation over significantly larger dynamic ranges of real numbers than previously possible. We implement GOOMs, along with an efficient custom parallel prefix scan, to support native execution on parallel hardware such as GPUs. We demonstrate that our implementation of GOOMs outperforms traditional approaches with three representative experiments, all of which were previously considered impractical or impossible, and now become possible and practical: (1) compounding real matrix products far beyond standard floating-point limits; (2) estimating spectra of Lyapunov exponents in parallel, orders of magnitude faster than with previous methods, applying a novel selective-resetting method to prevent state colinearity; and (3) capturing long-range dependencies in deep recurrent neural networks with non-diagonal recurrent states, computed in parallel via a prefix scan, without requiring any form of stabilization. Our results show that our implementation of GOOMs, combined with efficient parallel scanning, offers a scalable and numerically robust alternative to conventional floating-point numbers for high-dynamic-range applications.<br>
<span id='abs_ch'>中文: 本文提出的广义数量级（GOOMs）框架能够在极大动态范围内实现稳定数值计算，在矩阵乘积、李雅普诺夫指数和深度循环网络三个高难度应用中显著超越了 统浮点数方法的性能表现。</span><br>
<span id='abs_en'>English: This paper introduces generalized orders of magnitude (GOOMs), a numerical framework that enables stable computation over large dynamic ranges, outperforming traditional floating-point methods in three challenging applications involving matrix products, Lyapunov exponents, and deep recurrent networks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2510.03417.pdf' target='_blank'>https://arxiv.org/pdf/2510.03417.pdf</a></span>   <span><a href='https://github.com/inspire-lab/NEXUS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Javad Rafiei Asl, Sidhant Narula, Mohammad Ghasemigol, Eduardo Blanco, Daniel Takabi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03417">NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have revolutionized natural language processing but remain vulnerable to jailbreak attacks, especially multi-turn jailbreaks that distribute malicious intent across benign exchanges and bypass alignment mechanisms. Existing approaches often explore the adversarial space poorly, rely on hand-crafted heuristics, or lack systematic query refinement. We present NEXUS (Network Exploration for eXploiting Unsafe Sequences), a modular framework for constructing, refining, and executing optimized multi-turn attacks. NEXUS comprises: (1) ThoughtNet, which hierarchically expands a harmful intent into a structured semantic network of topics, entities, and query chains; (2) a feedback-driven Simulator that iteratively refines and prunes these chains through attacker-victim-judge LLM collaboration using harmfulness and semantic-similarity benchmarks; and (3) a Network Traverser that adaptively navigates the refined query space for real-time attacks. This pipeline uncovers stealthy, high-success adversarial paths across LLMs. On several closed-source and open-source LLMs, NEXUS increases attack success rate by 2.1% to 19.4% over prior methods. Code: https://github.com/inspire-lab/NEXUS<br>
<span id='abs_ch'>Chinese: NEXUS是一个模块化框架，通过将有害意图分层扩展为语义网络并自适应导航，构建针对大语言模型的多轮越狱攻击，相比现有方法显著提高了攻击成功率。</span><br>
<span id='abs_en'>English: NEXUS is a modular framework that constructs multi-turn jailbreak attacks on LLMs by hierarchically expanding harmful intents into semantic networks and adaptively navigating them, achieving significantly higher success rates than previous methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2510.03415.pdf' target='_blank'>https://arxiv.org/pdf/2510.03415.pdf</a></span>   <span><a href='https://github.com/EngineeringSoftware/PLSemanticsBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Thimmaiah, Jiyang Zhang, Jayanth Srinivasa, Junyi Jessy Li, Milos Gligoric
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03415">PLSemanticsBench: Large Language Models As Programming Language Interpreters</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) excel at code reasoning, a natural question arises: can an LLM execute programs (i.e., act as an interpreter) purely based on a programming language's formal semantics? If so, it will enable rapid prototyping of new programming languages and language features. We study this question using the imperative language IMP (a subset of C), formalized via small-step operational semantics (SOS) and rewriting-based operational semantics (K-semantics). We introduce three evaluation sets-Human-Written, LLM-Translated, and Fuzzer- Generated-whose difficulty is controlled by code-complexity metrics spanning the size, control-flow, and data-flow axes. Given a program and its semantics formalized with SOS/K-semantics, models are evaluated on three tasks ranging from coarse to fine: (1) final-state prediction, (2) semantic rule prediction, and (3) execution trace prediction. To distinguish pretraining memorization from semantic competence, we define two nonstandard semantics obtained through systematic mutations of the standard rules. Across strong code/reasoning LLMs, performance drops under nonstandard semantics despite high performance under the standard one. We further find that (i) there are patterns to different model failures, (ii) most reasoning models perform exceptionally well on coarse grained tasks involving reasoning about highly complex programs often containing nested loop depths beyond five, and surprisingly, (iii) providing formal semantics helps on simple programs but often hurts on more complex ones. Overall, the results show a promise that LLMs could serve as programming language interpreters, but points to the lack of their robust semantics understanding. We release the benchmark and the supporting code at https://github.com/EngineeringSoftware/PLSemanticsBench.<br>
<span id='abs_ch'>中文: 大型语言模型有望成为编程语言解释器，但缺乏稳健的语义理解能力，这体现在 准语义下能处理复杂程序，但在非 准语义下性能显著下降。</span><br>
<span id='abs_en'>English: Large language models show promise as programming language interpreters but lack robust semantic understanding, as demonstrated by their performance drop under nonstandard semantics despite handling complex programs well under standard rules.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2510.03399.pdf' target='_blank'>https://arxiv.org/pdf/2510.03399.pdf</a></span>   <span><a href='https://github.com/ChicagoHAI/self-recognition' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyan Bai, Aryan Shrivastava, Ari Holtzman, Chenhao Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03399">Know Thyself? On the Incapability and Implications of AI Self-Recognition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Self-recognition is a crucial metacognitive capability for AI systems, relevant not only for psychological analysis but also for safety, particularly in evaluative scenarios. Motivated by contradictory interpretations of whether models possess self-recognition (Panickssery et al., 2024; Davidson et al., 2024), we introduce a systematic evaluation framework that can be easily applied and updated. Specifically, we measure how well 10 contemporary larger language models (LLMs) can identify their own generated text versus text from other models through two tasks: binary self-recognition and exact model prediction. Different from prior claims, our results reveal a consistent failure in self-recognition. Only 4 out of 10 models predict themselves as generators, and the performance is rarely above random chance. Additionally, models exhibit a strong bias toward predicting GPT and Claude families. We also provide the first evaluation of model awareness of their own and others' existence, as well as the reasoning behind their choices in self-recognition. We find that the model demonstrates some knowledge of its own existence and other models, but their reasoning reveals a hierarchical bias. They appear to assume that GPT, Claude, and occasionally Gemini are the top-tier models, often associating high-quality text with them. We conclude by discussing the implications of our findings on AI safety and future directions to develop appropriate AI self-awareness.<br>
<span id='abs_ch'>中文: 本 究提出了一个系统性评估框架来检测大型语言模型的自我识别能力，结果发现模型普遍失败且偏向预测GPT和Claude系列，这对AI安全性和未来发展具有重要启示。</span><br>
<span id='abs_en'>English: This study introduces a systematic evaluation framework to assess self-recognition in large language models, revealing consistent failures and biases toward predicting GPT and Claude families, with implications for AI safety and future development.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2510.03363.pdf' target='_blank'>https://arxiv.org/pdf/2510.03363.pdf</a></span>   <span><a href='https://github.com/ZHE-SAPI/CostFilter-AD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Zhang, Mingxiu Cai, Gaochang Wu, Jing Zhang, Lingqiao Liu, Dacheng Tao, Tianyou Chai, Xiatian Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03363">Unified Unsupervised Anomaly Detection via Matching Cost Filtering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level anomalies using only normal training data, with wide applications such as industrial inspection and medical analysis, where anomalies are scarce due to privacy concerns and cold-start constraints. Existing methods, whether reconstruction-based (restoring normal counterparts) or embedding-based (pretrained representations), fundamentally conduct image- or feature-level matching to generate anomaly maps. Nonetheless, matching noise has been largely overlooked, limiting their detection ability. Beyond earlier focus on unimodal RGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB-3D and RGB-Text, enabled by point cloud sensing and vision-language models. Despite shared challenges, these lines remain largely isolated, hindering a comprehensive understanding and knowledge transfer. In this paper, we advocate unified UAD for both unimodal and multimodal settings in the matching perspective. Under this insight, we present Unified Cost Filtering (UCF), a generic post-hoc refinement framework for refining anomaly cost volume of any UAD model. The cost volume is constructed by matching a test sample against normal samples from the same or different modalities, followed by a learnable filtering module with multi-layer attention guidance from the test sample, mitigating matching noise and highlighting subtle anomalies. Comprehensive experiments on 22 diverse benchmarks demonstrate the efficacy of UCF in enhancing a variety of UAD methods, consistently achieving new state-of-the-art results in both unimodal (RGB) and multimodal (RGB-3D, RGB-Text) UAD scenarios. Code and models will be released at https://github.com/ZHE-SAPI/CostFilter-AD.<br>
<span id='abs_ch'>中文: 本文提出统一成本过滤（UCF）框架，通过多层注意力引导过滤异常成本体积来减少 监督异常检测中的匹配噪声，在多种单模态和多模态基准测试中均取得了最先进的性能。</span><br>
<span id='abs_en'>English: This paper introduces Unified Cost Filtering (UCF), a post-hoc refinement framework that mitigates matching noise in unsupervised anomaly detection by filtering anomaly cost volumes with multi-layer attention guidance, achieving state-of-the-art results across diverse unimodal and multimodal benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2510.03352.pdf' target='_blank'>https://arxiv.org/pdf/2510.03352.pdf</a></span>   <span><a href='https://github.com/mhdfb/sideinfo-search-reconstruction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahdi Farahbakhsh, Vishnu Teja Kunde, Dileep Kalathil, Krishna Narayanan, Jean-Francois Chamberland
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03352">Inference-Time Search using Side Information for Diffusion-based Image Reconstruction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion models have emerged as powerful priors for solving inverse problems. However, existing approaches typically overlook side information that could significantly improve reconstruction quality, especially in severely ill-posed settings. In this work, we propose a novel inference-time search algorithm that guides the sampling process using the side information in a manner that balances exploration and exploitation. This enables more accurate and reliable reconstructions, providing an alternative to the gradient-based guidance that is prone to reward-hacking artifacts. Our approach can be seamlessly integrated into a wide range of existing diffusion-based image reconstruction pipelines. Through extensive experiments on a number of inverse problems, such as box inpainting, super-resolution, and various deblurring tasks including motion, Gaussian, nonlinear, and blind deblurring, we show that our approach consistently improves the qualitative and quantitative performance of diffusion-based image reconstruction algorithms. We also show the superior performance of our approach with respect to other baselines, including reward gradient-based guidance algorithms. The code is available at \href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this repository}.<br>
<span id='abs_ch'>中文: 本文提出了一种新颖的推理时搜索算法，利用辅助信息增强扩散模型解决逆问题的能力，在修复和去模糊等任务中实现了更优的重建质量，同时避免了基于梯度方法常见的伪影问题。</span><br>
<span id='abs_en'>English: This paper introduces a novel inference-time search algorithm that leverages side information to enhance diffusion models for solving inverse problems, achieving superior reconstruction quality across tasks like inpainting and deblurring without the artifacts common in gradient-based methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2510.03297.pdf' target='_blank'>https://arxiv.org/pdf/2510.03297.pdf</a></span>   <span><a href='https://github.com/akshar27/spacenet-cnn-vs-vit' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Akshar Gothi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03297">Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present a controlled comparison of a convolutional neural network (EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two label-distribution regimes: a naturally imbalanced five-class split and a balanced-resampled split with 700 images per class (70:20:10 train/val/test). With matched preprocessing (224x224, ImageNet normalization), lightweight augmentations, and a 40-epoch budget on a single NVIDIA P100, we report accuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics (model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93% test accuracy with strong macro-F1 and lower latency; ViT-Base is competitive at 93% with a larger parameter count and runtime. On the balanced split, both models are strong; EfficientNet-B0 reaches 99% while ViT-Base remains competitive, indicating that balancing narrows architecture gaps while CNNs retain an efficiency edge. We release manifests, logs, and per-image predictions to support reproducibility.<br>
<span id='abs_ch'>中文: 本 究对比了EfficientNet-B0与Vision Transformer在SpaceNet数据集上的表现，证明CNN在处理不平衡数据时效率更优，在平衡数据下两者性能相当，同时公开了所有实验材料以确保可复现性。</span><br>
<span id='abs_en'>English: This study compares EfficientNet-B0 and Vision Transformer on SpaceNet, showing CNN's superior efficiency on imbalanced data and competitive performance with balanced resampling, while releasing all materials for reproducibility.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2510.03291.pdf' target='_blank'>https://arxiv.org/pdf/2510.03291.pdf</a></span>   <span><a href='https://github.com/RainbowQTT/UniPruning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhuo Ding, Wanying Qu, Jiawei Geng, Wenqi Shao, Yanwei Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03291">UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) achieve strong performance across diverse tasks but face prohibitive computational and memory costs. Pruning offers a promising path by inducing sparsity while preserving architectural flexibility. However, existing methods struggle to balance efficiency and robustness: local metric approaches prune layer by layer but often collapse under high sparsity, whereas global feedback methods enforce consistency at the cost of expensive weight updates or restrictive semi-structured formats. We present UniPruning, a unified post-training pruning framework that combines the speed of local saliency metrics with the stability of global coordination, enabled by a mirror descent based optimization, all without updating model weights. UniPruning leverages fast layer-wise scoring and a lightweight global controller to allocate a single sparsity budget, supporting both unstructured and semi-structured N :M pruning within one framework. After a brief calibration, it can generate pruning masks for arbitrary sparsity levels in one shot, and adapts seamlessly to hardware-aware constraints. Extensive experiments on multiple pretrained LLM families and standard benchmarks show that UniPruning consistently delivers competitive or superior perplexity and zero-shot accuracy. Ablation studies further highlight the importance of mirror descent and local saliency anchoring. Overall, UniPruning provides an efficient, principled, and scalable solution for sparsifying large-scale LLMs. Our code is available at: https://github.com/RainbowQTT/UniPruning.<br>
<span id='abs_ch'>中文摘要：UniPruning是一种统一的后训练剪枝框架，通过结合局部显著性度量的速度与全局协调的稳定性，在不更新模型权重的情况下高效稀疏化大语言模型，并在多个基准测试中取得优异性能。</span><br>
<span id='abs_en'>English Summary: UniPruning is a unified post-training pruning framework that efficiently balances local saliency metrics with global coordination to sparsify large language models without weight updates, achieving competitive performance across various benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2510.03275.pdf' target='_blank'>https://arxiv.org/pdf/2510.03275.pdf</a></span>   <span><a href='https://github.com/Dreamlittlecat/LLM-Quant-Factory' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhao Xia, Ming Zhao, Limin Xiao, Xiujun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03275">SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) face significant computational and memory challenges, making extremely low-bit quantization crucial for their efficient deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size, a novel framework that enables extremely low-bit quantization of LLMs while preserving their linguistic reasoning capabilities. A distinctive feature of SDQ-LLM is the continuous adjustability of the Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal trade-off between model size and accuracy. SDQ-LLM uses upsampling combined with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding high-precision parameters into 1-bit or 1.58-bit representations, replacing the multiplication operations within linear layers with addition. This approach significantly enhances inference efficiency under extremely low-bit quantization. To further reduce the loss of quantization precision, we incorporate Hadamard-based weight smoothing prior to quantization, improving the stability and robustness of the weight representations. Furthermore, to fully leverage the continuity of the OSR and reduce precision loss, recognizing the correlation between quantization sensitivity and weight variance, we propose a fine-grained, layer- and linear-wise OSR allocation strategy, MultiOSR. This strategy distributes OSR both across layers and within each layer, based on weight variance and parameter scale. Finally, extensive experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a more efficient and high-precision performance even under highly aggressive low-OSR settings. Our code is available at https://github.com/Dreamlittlecat/LLM-Quant-Factory.<br>
<span id='abs_ch'>中文摘要：SDQ-LLM提出创新的Sigma-Delta量化框架，通过可调过采 比实现1比特大语言模型，在哈达玛平滑和MultiOSR分配策略支持下，用 法替代乘法运算并保持语言推理能力。</span><br>
<span id='abs_en'>English Summary: SDQ-LLM introduces a novel Sigma-Delta quantization framework enabling 1-bit LLMs with adjustable over-sampling ratios, replacing multiplications with additions while maintaining reasoning capabilities through Hadamard smoothing and MultiOSR allocation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2510.03274.pdf' target='_blank'>https://arxiv.org/pdf/2510.03274.pdf</a></span>   <span><a href='https://github.com/ZTA2785/Quant-dLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianao Zhang, Zhiteng Li, Xianglong Yan, Haotong Qin, Yong Guo, Yulun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03274">Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion large language models (dLLMs), which offer bidirectional context and flexible masked-denoising generation, are emerging as a compelling alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model sizes continue to grow, motivating weight compression for deployment. Although post-training quantization (PTQ) is effective for AR LLMs, directly transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the fully visible signals assumed by standard PTQ methods, we introduce Masked Calibration Simulation (MCS) to align calibration with the timestep-dependent masking, which yields more reliable calibrations. Moreover, we propose a Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight representations via an optimization algorithm. It performs iterative approximation guided by our simulated calibration data. In addition, under a strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a sensitivity-based precision allocation scheme that adaptively assigns bit width across channel groups. When restricted to 2-bit precision, Quant-dLLM consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer PTQ methods on dLLMs. The code and models will be available at: https://github.com/ZTA2785/Quant-dLLM.<br>
<span id='abs_ch'>Chinese: 提出的Quant-dLLM框架通过引入掩  准模拟、数据感知任意顺序量化器和自适应块级混合精度，解决了 准后训练量化在扩散大语言模型中的局限性，在2比特精度下相比现有方法实现了更优的性能。</span><br>
<span id='abs_en'>English: The proposed Quant-dLLM framework addresses the limitations of standard post-training quantization for diffusion large language models by introducing Masked Calibration Simulation, Data-aware Any-order Quantizer, and Adaptive Blockwise Mixed Precision, achieving superior 2-bit performance compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2510.03273.pdf' target='_blank'>https://arxiv.org/pdf/2510.03273.pdf</a></span>   <span><a href='https://github.com/ychAlbert/sid-bp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhao Ye, Ming Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03273">Learning without Global Backpropagation via Synergistic Information Distillation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Backpropagation (BP), while foundational to deep learning, imposes two critical scalability bottlenecks: update locking, where network modules remain idle until the entire backward pass completes, and high memory consumption due to storing activations for gradient computation. To address these limitations, we introduce Synergistic Information Distillation (SID), a novel training framework that reframes deep learning as a cascade of local cooperative refinement problems. In SID, a deep network is structured as a pipeline of modules, each imposed with a local objective to refine a probabilistic belief about the ground-truth target. This objective balances fidelity to the target with consistency to the belief from its preceding module. By decoupling the backward dependencies between modules, SID enables parallel training and hence eliminates update locking and drastically reduces memory requirements. Meanwhile, this design preserves the standard feed-forward inference pass, making SID a versatile drop-in replacement for BP. We provide a theoretical foundation, proving that SID guarantees monotonic performance improvement with network depth. Empirically, SID consistently matches or surpasses the classification accuracy of BP, exhibiting superior scalability and pronounced robustness to label noise.Code is available at: https://github.com/ychAlbert/sid-bp<br>
<span id='abs_ch'>中文: 提出的协同信息蒸馏（SID）框架通过模块并行训练，解决了反向 播的更新锁定与内存瓶颈问题，在保持竞争力的准确率同时展现出更强的鲁棒性。</span><br>
<span id='abs_en'>English: The proposed Synergistic Information Distillation (SID) framework eliminates backpropagation's update locking and memory bottlenecks by enabling parallel module training while maintaining competitive accuracy and enhanced robustness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2510.03271.pdf' target='_blank'>https://arxiv.org/pdf/2510.03271.pdf</a></span>   <span><a href='https://github.com/liangzid/DPS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zi Liang, Zhiyao Wu, Haoyang Shang, Yulin Jin, Qingqing Ye, Huadi Zheng, Peizhao Hu, Haibo Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03271">Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Decision boundary, the subspace of inputs where a machine learning model assigns equal classification probabilities to two classes, is pivotal in revealing core model properties and interpreting behaviors. While analyzing the decision boundary of large language models (LLMs) has raised increasing attention recently, constructing it for mainstream LLMs remains computationally infeasible due to the enormous vocabulary-sequence sizes and the auto-regressive nature of LLMs. To address this issue, in this paper we propose Decision Potential Surface (DPS), a new notion for analyzing LLM decision boundary. DPS is defined on the confidences in distinguishing different sampling sequences for each input, which naturally captures the potential of decision boundary. We prove that the zero-height isohypse in DPS is equivalent to the decision boundary of an LLM, with enclosed regions representing decision regions. By leveraging DPS, for the first time in the literature, we propose an approximate decision boundary construction algorithm, namely $K$-DPS, which only requires K-finite times of sequence sampling to approximate an LLM's decision boundary with negligible error. We theoretically derive the upper bounds for the absolute error, expected error, and the error concentration between K-DPS and the ideal DPS, demonstrating that such errors can be trade-off with sampling times. Our results are empirically validated by extensive experiments across various LLMs and corpora.<br>
<span id='abs_ch'>中文: 本文提出决策势能面（DPS）作为分析大型语言模型决策边界的新方法，通过有限次序列采 能以可忽略误差高效近似决策边界。</span><br>
<span id='abs_en'>English: This paper introduces Decision Potential Surface (DPS) as a novel method to analyze the decision boundaries of large language models (LLMs), enabling efficient approximation with minimal error through finite sequence sampling.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2510.03267.pdf' target='_blank'>https://arxiv.org/pdf/2510.03267.pdf</a></span>   <span><a href='https://github.com/XIANGLONGYAN/PT2-LLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianglong Yan, Chengzhu Bao, Zhiteng Li, Tianao Zhang, Kaicheng Yang, Haotong Qin, Ruobing Xie, Xingwu Sun, Yulun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03267">PT$^2$-LLM: Post-Training Ternarization for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have shown impressive capabilities across diverse tasks, but their large memory and compute demands hinder deployment. Ternarization has gained attention as a promising compression technique, delivering substantial size reduction and high computational efficiency. However, its potential in the post-training quantization (PTQ) setting remains underexplored, due to the challenge of training-free parameter optimization and the quantization difficulty posed by outliers and dispersed weights. To address these issues, we propose PT$^2$-LLM, a post-training ternarization framework tailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with a two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which alternates between optimal ternary grid construction and flexible rounding to minimize quantization error, and (2) Activation-aware Grid Alignment (AGA), which further refines the ternary grid to better match full-precision outputs. In addition, we propose a plug-and-play Structural Similarity-based Reordering (SSR) strategy that leverages inter-column structural similarity to ease quantization and mitigate outlier effects, further enhancing overall performance. Extensive experiments demonstrate that PT$^2$-LLM delivers competitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with lower memory cost, while also accelerating both prefill and decoding to achieve end-to-end speedup. The code and models will be available at https://github.com/XIANGLONGYAN/PT2-LLM.<br>
<span id='abs_ch'>中文摘要：PT²-LLM是一种后训练三值化框架，通过迭代量化优化和结构重排技术压缩大语言模型，在保持与先进2位量化方法相当性能的同时，显著降低内存消耗并提升推理速度。</span><br>
<span id='abs_en'>English Summary: PT²-LLM is a post-training ternarization framework that uses iterative quantization refinement and structural reordering to compress Large Language Models, achieving competitive performance with 2-bit methods while reducing memory usage and accelerating inference.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2510.03258.pdf' target='_blank'>https://arxiv.org/pdf/2510.03258.pdf</a></span>   <span><a href='https://github.com/ycarobot/POEM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang'an Yi, Xiaohui Deng, Shuaicheng Niu, Yan Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03258">POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Test-time adaptation (TTA) aims to transfer knowledge from a source model to unknown test data with potential distribution shifts in an online manner. Many existing TTA methods rely on entropy as a confidence metric to optimize the model. However, these approaches are sensitive to the predefined entropy threshold, influencing which samples are chosen for model adaptation. Consequently, potentially reliable target samples are often overlooked and underutilized. For instance, a sample's entropy might slightly exceed the threshold initially, but fall below it after the model is updated. Such samples can provide stable supervised information and offer a normal range of gradients to guide model adaptation. In this paper, we propose a general approach, \underline{POEM}, to promote TTA via ex\underline{\textbf{p}}loring the previously unexpl\underline{\textbf{o}}red reliabl\underline{\textbf{e}} sa\underline{\textbf{m}}ples. Additionally, we introduce an extra Adapt Branch network to strike a balance between extracting domain-agnostic representations and achieving high performance on target data. Comprehensive experiments across multiple architectures demonstrate that POEM consistently outperforms existing TTA methods in both challenging scenarios and real-world domain shifts, while remaining computationally efficient. The effectiveness of POEM is evaluated through extensive analyses and thorough ablation studies. Moreover, the core idea behind POEM can be employed as an augmentation strategy to boost the performance of existing TTA approaches. The source code is publicly available at \emph{https://github.com/ycarobot/POEM}<br>
<span id='abs_ch'>中文摘要：本文提出POEM方法，通过挖掘未充分利用的可  本来改进测试时自适应，并引入自适应分支网络来平衡领域 关表征学 与目 领域性能，在多种场景下显著优于现有方法。English Summary: This paper introduces POEM, a novel test-time adaptation method that enhances model performance by identifying and utilizing reliable but previously overlooked samples, while incorporating an Adapt Branch network to balance domain-agnostic representation learning with target domain effectiveness.Paperid:261,https://arxiv.org/pdf/2510.03257.pdfGitHub</span><br>
<span id='abs_en'>English Summary: This paper introduces POEM, a novel test-time adaptation method that enhances model performance by identifying and utilizing reliable but previously overlooked samples, while incorporating an Adapt Branch network to balance domain-agnostic representation learning with target domain effectiveness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2510.03257.pdf' target='_blank'>https://arxiv.org/pdf/2510.03257.pdf</a></span>   <span><a href='https://github.com/RS2002/Triple-BERT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Zhao, Sen Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03257">Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate real-time challenge of bundling and matching passengers-each with distinct origins and destinations-to available vehicles, all while navigating significant system uncertainties. Due to the extensive observation space arising from the large number of drivers and orders, order dispatching, though fundamentally a centralized task, is often addressed using Multi-Agent Reinforcement Learning (MARL). However, independent MARL methods fail to capture global information and exhibit poor cooperation among workers, while Centralized Training Decentralized Execution (CTDE) MARL methods suffer from the curse of dimensionality. To overcome these challenges, we propose Triple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method designed specifically for large-scale order dispatching on ride-sharing platforms. Built on a variant TD3, our approach addresses the vast action space through an action decomposition strategy that breaks down the joint action probability into individual driver action probabilities. To handle the extensive observation space, we introduce a novel BERT-based network, where parameter reuse mitigates parameter growth as the number of drivers and orders increases, and the attention mechanism effectively captures the complex relationships among the large pool of driver and orders. We validate our method using a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves approximately an 11.95% improvement over current state-of-the-art methods, with a 4.26% increase in served orders and a 22.25% reduction in pickup times. Our code, trained model parameters, and processed data are publicly available at the repository https://github.com/RS2002/Triple-BERT .<br>
<span id='abs_ch'>中文摘要：本 究提出Triple-BERT方法，通过动作分解策略和基于BERT的网络架构解决网约车平台大规模订单分配中的动作空间与观测空间难题，相比现有最优方法实现了订单服务量提升与接送时间大幅缩减的双重突 。</span><br>
<span id='abs_en'>English Summary: The study introduces Triple-BERT, a centralized single-agent reinforcement learning method that overcomes the limitations of multi-agent approaches in ride-sharing order dispatching by using action decomposition and a BERT-based network to handle large action and observation spaces, achieving significant improvements in served orders and pickup times.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2510.03216.pdf' target='_blank'>https://arxiv.org/pdf/2510.03216.pdf</a></span>   <span><a href='https://github.com/ATPLab-LUMS/Wave-GMS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Talha Ahmed, Nehal Ahmed Shaikh, Hassan Mohy-ud-Din
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03216">Wave-GMS: Lightweight Multi-Scale Generative Model for Medical Image Segmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>For equitable deployment of AI tools in hospitals and healthcare facilities, we need Deep Segmentation Networks that offer high performance and can be trained on cost-effective GPUs with limited memory and large batch sizes. In this work, we propose Wave-GMS, a lightweight and efficient multi-scale generative model for medical image segmentation. Wave-GMS has a substantially smaller number of trainable parameters, does not require loading memory-intensive pretrained vision foundation models, and supports training with large batch sizes on GPUs with limited memory. We conducted extensive experiments on four publicly available datasets (BUS, BUSI, Kvasir-Instrument, and HAM10000), demonstrating that Wave-GMS achieves state-of-the-art segmentation performance with superior cross-domain generalizability, while requiring only ~2.6M trainable parameters. Code is available at https://github.com/ATPLab-LUMS/Wave-GMS.<br>
<span id='abs_ch'>Chinese: 为实现医疗领域人工智能工具的公平部署，本 究提出Wave-GMS轻量级生成模型，该模型通过极少的可训练参数在有限GPU内存下实现最优医学图像分割性能，并展现出卓越的跨领域泛化能力。</span><br>
<span id='abs_en'>English: To enable equitable AI deployment in healthcare, this study introduces Wave-GMS, a lightweight generative model for medical image segmentation that achieves state-of-the-art performance with minimal parameters and efficient GPU usage.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2510.03129.pdf' target='_blank'>https://arxiv.org/pdf/2510.03129.pdf</a></span>   <span><a href='https://github.com/Yoontae6719/Signature-Informed-Transformer-For-Asset-Allocation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yoontae Hwang, Stefan Zohren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03129">Signature-Informed Transformer for Asset Allocation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Robust asset allocation is a key challenge in quantitative finance, where deep-learning forecasters often fail due to objective mismatch and error amplification. We introduce the Signature-Informed Transformer (SIT), a novel framework that learns end-to-end allocation policies by directly optimizing a risk-aware financial objective. SIT's core innovations include path signatures for a rich geometric representation of asset dynamics and a signature-augmented attention mechanism embedding financial inductive biases, like lead-lag effects, into the model. Evaluated on daily S\&P 100 equity data, SIT decisively outperforms traditional and deep-learning baselines, especially when compared to predict-then-optimize models. These results indicate that portfolio-aware objectives and geometry-aware inductive biases are essential for risk-aware capital allocation in machine-learning systems. The code is available at: https://github.com/Yoontae6719/Signature-Informed-Transformer-For-Asset-Allocation<br>
<span id='abs_ch'>Chinese: 签名信息Transformer（SIT）通过路径签名和签名增强注意力机制优化风险感知的金融目 ，在资产配置中显著优于 统和深度学 基准模型。</span><br>
<span id='abs_en'>English: The Signature-Informed Transformer (SIT) introduces a novel framework that optimizes risk-aware financial objectives using path signatures and signature-augmented attention, outperforming traditional and deep-learning models in asset allocation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2510.03051.pdf' target='_blank'>https://arxiv.org/pdf/2510.03051.pdf</a></span>   <span><a href='https://github.com/jamisonmeindl/zeroshotopt' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jamison Meindl, Yunsheng Tian, Tony Cui, Veronika Thost, Zhang-Wei Hong, Johannes Dürholt, Jie Chen, Wojciech Matusik, Mina Konaković Luković
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03051">ZeroShotOpt: Towards Zero-Shot Pretrained Models for Efficient Black-Box Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Global optimization of expensive, derivative-free black-box functions requires extreme sample efficiency. While Bayesian optimization (BO) is the current state-of-the-art, its performance hinges on surrogate and acquisition function hyper-parameters that are often hand-tuned and fail to generalize across problem landscapes. We present ZeroShotOpt, a general-purpose, pretrained model for continuous black-box optimization tasks ranging from 2D to 20D. Our approach leverages offline reinforcement learning on large-scale optimization trajectories collected from 12 BO variants. To scale pretraining, we generate millions of synthetic Gaussian process-based functions with diverse landscapes, enabling the model to learn transferable optimization policies. As a result, ZeroShotOpt achieves robust zero-shot generalization on a wide array of unseen benchmarks, matching or surpassing the sample efficiency of leading global optimizers, including BO, while also offering a reusable foundation for future extensions and improvements. Our open-source code, dataset, and model are available at: https://github.com/jamisonmeindl/zeroshotopt<br>
<span id='abs_ch'>中文: ZeroShotOpt 是一种通过离线强化学 在合成函数和贝叶斯优化轨迹上预训练的模型， 需手动调优即可在零 本情况下实现鲁棒泛化，并在黑盒优化中展现出卓越的 本效率。</span><br>
<span id='abs_en'>English: ZeroShotOpt is a pretrained model using offline reinforcement learning on synthetic functions and BO trajectories, achieving robust zero-shot generalization and superior sample efficiency in black-box optimization without manual tuning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2510.03004.pdf' target='_blank'>https://arxiv.org/pdf/2510.03004.pdf</a></span>   <span><a href='https://github.com/TianzhengHU/BrainIB_coding/tree/main/BrainIB_GIB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianzheng Hu, Qiang Li, Shu Liu, Vince D. Calhoun, Guido van Wingen, Shujian Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03004">BrainIB++: Leveraging Graph Neural Networks and Information Bottleneck for Functional Brain Biomarkers in Schizophrenia</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The development of diagnostic models is gaining traction in the field of psychiatric disorders. Recently, machine learning classifiers based on resting-state functional magnetic resonance imaging (rs-fMRI) have been developed to identify brain biomarkers that differentiate psychiatric disorders from healthy controls. However, conventional machine learning-based diagnostic models often depend on extensive feature engineering, which introduces bias through manual intervention. While deep learning models are expected to operate without manual involvement, their lack of interpretability poses significant challenges in obtaining explainable and reliable brain biomarkers to support diagnostic decisions, ultimately limiting their clinical applicability. In this study, we introduce an end-to-end innovative graph neural network framework named BrainIB++, which applies the information bottleneck (IB) principle to identify the most informative data-driven brain regions as subgraphs during model training for interpretation. We evaluate the performance of our model against nine established brain network classification methods across three multi-cohort schizophrenia datasets. It consistently demonstrates superior diagnostic accuracy and exhibits generalizability to unseen data. Furthermore, the subgraphs identified by our model also correspond with established clinical biomarkers in schizophrenia, particularly emphasizing abnormalities in the visual, sensorimotor, and higher cognition brain functional network. This alignment enhances the model's interpretability and underscores its relevance for real-world diagnostic applications.<br>
<br>
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2510.02880.pdf' target='_blank'>https://arxiv.org/pdf/2510.02880.pdf</a></span>   <span><a href='https://github.com/martian422/MaskGRPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianren Ma, Mu Zhang, Yibing Wang, Qixiang Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02880">Consolidating Reinforcement Learning for Multimodal Discrete Diffusion Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Optimizing discrete diffusion model (DDM) with rewards remains a challenge: the non-autoregressive paradigm makes importance sampling intractable and rollout complex, puzzling reinforcement learning methods such as Group Relative Policy Optimization (GRPO). In this study, we introduce MaskGRPO, the first viable approach to enable scalable multimodal reinforcement learning in discrete diffusion with effective importance sampling and modality-specific adaptations. To this end, we first clarify the theoretical foundation for DDMs, which facilitates building an importance estimator that captures valuable token fluctuation for gradient updates. We then delicately tailored the rollout method for visual sequences, which yields diverse completions and reliable optimization gradients. Upon math reasoning, coding, and visual generation benchmarks, MaskGRPO brings more stable and efficient updates, leading to stronger reasoning performance and better generation quality. This study establishes MaskGRPO as a systematic policy optimization approach and the first practical way for discretized visual diffusion.<br>
<span id='abs_ch'>中文摘要：MaskGRPO提出了首个适用于离散扩散模型的可扩展多模态强化学 方法，通过理论重要性采 和针对性视觉序列展开实现稳定优化，在多项基准测试中显著提升了推理能力和生成质量。</span><br>
<span id='abs_en'>English Summary: MaskGRPO introduces the first scalable multimodal reinforcement learning approach for discrete diffusion models, enabling stable optimization through theoretical importance sampling and tailored visual sequence rollouts to enhance reasoning and generation quality across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2510.02855.pdf' target='_blank'>https://arxiv.org/pdf/2510.02855.pdf</a></span>   <span><a href='https://github.com/jahidul-arafat/constraint_satisfaction_wordle_arxiv_preprint' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jahidul Arafat, Fariha Tasmin, Sanjaya Poudel, Kamrujjaman, Eftakhar Ahmed Arnob, Ahsan Habib Tareq
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02855">Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Wordle presents an algorithmically rich testbed for constraint satisfaction problem (CSP) solving. While existing solvers rely on information-theoretic entropy maximization or frequency-based heuristics without formal constraint treatment, we present the first comprehensive CSP formulation of Wordle with novel constraint-aware solving strategies. We introduce CSP-Aware Entropy, computing information gain after constraint propagation rather than on raw candidate sets, and a Probabilistic CSP framework integrating Bayesian word-frequency priors with logical constraints. Through evaluation on 2,315 English words, CSP-Aware Entropy achieves 3.54 average guesses with 99.9% success rate, a statistically significant 1.7% improvement over Forward Checking (t=-4.82, p<0.001, Cohen's d=0.07) with 46% faster runtime (12.9ms versus 23.7ms per guess). Under 10% noise, CSP-aware approaches maintain 5.3 percentage point advantages (29.0% versus 23.7%, p=0.041), while Probabilistic CSP achieves 100% success across all noise levels (0-20%) through constraint recovery mechanisms. Cross-lexicon validation on 500 Spanish words demonstrates 88% success with zero language-specific tuning, validating that core CSP principles transfer across languages despite an 11.2 percentage point gap from linguistic differences (p<0.001, Fisher's exact test). Our open-source implementation with 34 unit tests achieving 91% code coverage provides reproducible infrastructure for CSP research. The combination of formal CSP treatment, constraint-aware heuristics, probabilistic-logical integration, robustness analysis, and cross-lexicon validation establishes new performance benchmarks demonstrating that principled constraint satisfaction techniques outperform classical information-theoretic and learning-based approaches for structured puzzle-solving domains.<br>
<span id='abs_ch'>中文: 本 究提出了首个完整的Wordle约束满足问题（CSP）建模框架，通过创新的约束感知策略（如CSP感知熵和概率CSP）在成功率、运行效率和跨语言鲁棒性上显著超越了现有方法，确立了结构化解谜领域的新性能基准。</span><br>
<span id='abs_en'>English: This study introduces the first comprehensive constraint satisfaction problem (CSP) formulation for Wordle, featuring novel constraint-aware strategies like CSP-Aware Entropy and Probabilistic CSP that significantly outperform existing methods in success rates, efficiency, and robustness across multiple languages.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2510.02798.pdf' target='_blank'>https://arxiv.org/pdf/2510.02798.pdf</a></span>   <span><a href='https://github.com/optuna/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yoshihiko Ozaki, Shuhei Watanabe, Toshihiko Yanase
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02798">OptunaHub: A Platform for Black-Box Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Black-box optimization (BBO) drives advances in domains such as AutoML and Materials Informatics, yet research efforts often remain fragmented across domains. We introduce OptunaHub (https://hub.optuna.org/), a community platform that centralizes BBO methods and benchmarks. OptunaHub provides unified Python APIs, a contributor package registry, and a web interface to promote searchability and cross-domain research. OptunaHub aims to foster a virtuous cycle of contributions and applications. The source code is publicly available in the optunahub, optunahub-registry, and optunahub-web repositories under the Optuna organization on GitHub (https://github.com/optuna/).<br>
<br>
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2510.02790.pdf' target='_blank'>https://arxiv.org/pdf/2510.02790.pdf</a></span>   <span><a href='https://github.com/Deng-Jingyuan/MaskCD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyuan Deng, Yujiu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02790">MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large vision-language models (LVLMs) have shown remarkable performance in visual-language understanding for downstream multimodal tasks. While their capabilities are improving, problems emerge simultaneously. Among those problems, the hallucinations have attracted much attention, which stands for the phenomenon where LVLMs generate contradictory content to their input visual and text contents. Many approaches have been proposed to deal with this issue, such as contrastive decoding and attention manipulation. However, contrastive decoding methods struggle in constructing appropriate contrastive samples, and attention manipulation methods are highly sensitive, lacking stability. In this work, we propose image head Masked Contrastive Decoding (MaskCD). Our approach utilizes the "image heads" in LVLMs, masking them to construct contrastive samples for contrastive decoding. We evaluated MaskCD on LLaVA-1.5-7b and Qwen-VL-7b, using various benchmarks such as CHAIR, POPE, AMBER and MME. The results demonstrate that MaskCD effectively alleviates the phenomenon of hallucinations and retains the general capabilities of LVLMs. Corresponding resources could be found at: https://github.com/Deng-Jingyuan/MaskCD .<br>
<span id='abs_ch'>中文: 本文提出MaskCD方法，通过掩 视觉语言模型中的图像头构建对比 本，在缓解模型幻觉现象的同时保持其通用能力，多基准测试验证了该方法的有效性。</span><br>
<span id='abs_en'>English: This paper introduces MaskCD, a novel method that mitigates hallucinations in large vision-language models by masking image heads to create contrastive samples, effectively reducing contradictions while preserving model capabilities as validated on multiple benchmarks.Paperid:285,https://arxiv.org/pdf/2510.02778.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2510.02695.pdf' target='_blank'>https://arxiv.org/pdf/2510.02695.pdf</a></span>   <span><a href='https://github.com/KaiFukazawa/RAMAC.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Fukazawa, Kunal Mundada, Iman Soltani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02695">RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In safety-critical domains where online data collection is infeasible, offline reinforcement learning (RL) offers an attractive alternative but only if policies deliver high returns without incurring catastrophic lower-tail risk. Prior work on risk-averse offline RL achieves safety at the cost of value conservatism and restricted policy classes, whereas expressive policies are only used in risk-neutral settings. Here, we address this gap by introducing the \textbf{Risk-Aware Multimodal Actor-Critic (RAMAC)} framework, which couples an \emph{expressive generative actor} with a distributional critic. The RAMAC differentiates composite objective combining distributional risk and BC loss through the generative path, achieving risk-sensitive learning in complex multimodal scenarios. We instantiate RAMAC with diffusion and flow-matching actors and observe consistent gains in $\mathrm{CVaR}_{0.1}$ while maintaining strong returns on most Stochastic-D4RL tasks. Code: https://github.com/KaiFukazawa/RAMAC.git<br>
<span id='abs_ch'>中文摘要：RAMAC框架通过结合表达性生成执行器与分布式评论家，在离线强化学 中实现了风险敏感学 ，在复杂多模态场景下显著提升了条件风险价值指 ，同时保持了优异的回报表现。</span><br>
<span id='abs_en'>English Summary: The RAMAC framework introduces an expressive generative actor paired with a distributional critic to enable risk-averse offline reinforcement learning, achieving improved conditional value-at-risk while maintaining high returns on complex multimodal tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2510.02390.pdf' target='_blank'>https://arxiv.org/pdf/2510.02390.pdf</a></span>   <span><a href='https://github.com/TheLovesOfLadyPurple/Hyperparameters-are-all-you-need' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zilai Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02390">Hyperparameters are all you need: Using five-step inference for an original diffusion model to generate images comparable to the latest distillation model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The diffusion model is a state-of-the-art generative model that generates an image by applying a neural network iteratively. Moreover, this generation process is regarded as an algorithm solving an ordinary differential equation or a stochastic differential equation. Based on the analysis of the truncation error of the diffusion ODE and SDE, our study proposes a training-free algorithm that generates high-quality 512 x 512 and 1024 x 1024 images in eight steps, with flexible guidance scales. To the best of my knowledge, our algorithm is the first one that samples a 1024 x 1024 resolution image in 8 steps with an FID performance comparable to that of the latest distillation model, but without additional training. Meanwhile, our algorithm can also generate a 512 x 512 image in 8 steps, and its FID performance is better than the inference result using state-of-the-art ODE solver DPM++ 2m in 20 steps. We validate our eight-step image generation algorithm using the COCO 2014, COCO 2017, and LAION datasets. And our best FID performance is 15.7, 22.35, and 17.52. While the FID performance of DPM++2m is 17.3, 23.75, and 17.33. Further, it also outperforms the state-of-the-art AMED-plugin solver, whose FID performance is 19.07, 25.50, and 18.06. We also apply the algorithm in five-step inference without additional training, for which the best FID performance in the datasets mentioned above is 19.18, 23.24, and 19.61, respectively, and is comparable to the performance of the state-of-the-art AMED Pulgin solver in eight steps, SDXL-turbo in four steps, and the state-of-the-art diffusion distillation model Flash Diffusion in five steps. We also validate our algorithm in synthesizing 1024 * 1024 images within 6 steps, whose FID performance only has a limited distance to the latest distillation algorithm. The code is in repo: https://github.com/TheLovesOfLadyPurple/Hyperparameters-are-all-you-need<br>
<span id='abs_ch'>中文: 本 究基于扩散常微分方程和随机微分方程的截断误差分析，提出了一种 需额外训练的算法，仅需八步即可生成高质量512x512和1024x1024图像，其FID指 优于当前最先进的求解器。</span><br>
<span id='abs_en'>English: This study introduces a training-free algorithm based on diffusion ODE and SDE analysis, enabling high-quality 512x512 and 1024x1024 image generation in just eight steps with superior FID performance compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2510.02373.pdf' target='_blank'>https://arxiv.org/pdf/2510.02373.pdf</a></span>   <span><a href='https://github.com/TangciuYueng/AMemGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianshan Wei, Tengchao Yang, Yaochen Wang, Xinfeng Li, Lijun Li, Zhenfei Yin, Yi Zhan, Thorsten Holz, Zhiqiang Lin, XiaoFeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02373">A-MemGuard: A Proactive Defense Framework for LLM-Based Agent Memory</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Model (LLM) agents use memory to learn from past interactions, enabling autonomous planning and decision-making in complex environments. However, this reliance on memory introduces a critical security risk: an adversary can inject seemingly harmless records into an agent's memory to manipulate its future behavior. This vulnerability is characterized by two core aspects: First, the malicious effect of injected records is only activated within a specific context, making them hard to detect when individual memory entries are audited in isolation. Second, once triggered, the manipulation can initiate a self-reinforcing error cycle: the corrupted outcome is stored as precedent, which not only amplifies the initial error but also progressively lowers the threshold for similar attacks in the future. To address these challenges, we introduce A-MemGuard (Agent-Memory Guard), the first proactive defense framework for LLM agent memory. The core idea of our work is the insight that memory itself must become both self-checking and self-correcting. Without modifying the agent's core architecture, A-MemGuard combines two mechanisms: (1) consensus-based validation, which detects anomalies by comparing reasoning paths derived from multiple related memories and (2) a dual-memory structure, where detected failures are distilled into ``lessons'' stored separately and consulted before future actions, breaking error cycles and enabling adaptation. Comprehensive evaluations on multiple benchmarks show that A-MemGuard effectively cuts attack success rates by over 95% while incurring a minimal utility cost. This work shifts LLM memory security from static filtering to a proactive, experience-driven model where defenses strengthen over time. Our code is available in https://github.com/TangciuYueng/AMemGuard<br>
<span id='abs_ch'>中文摘要：大型语言模型智能体面临记忆操纵攻击的安全风险，其中隐藏的恶意记录可触发自我强化的错误循环，而提出的A-MemGuard框架通过共识验证和双记忆结构进行主动防御，将攻击成功率降低超过95%。</span><br>
<span id='abs_en'>English Summary: Large Language Model agents face security risks from memory manipulation attacks, where hidden malicious records can trigger self-reinforcing error cycles, but the proposed A-MemGuard framework proactively defends against these by implementing consensus-based validation and a dual-memory structure to reduce attack success by over 95%.Paperid:299,https://arxiv.org/pdf/2510.02349.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2510.02349.pdf' target='_blank'>https://arxiv.org/pdf/2510.02349.pdf</a></span>   <span><a href='https://github.com/renje4z335jh4/non_contrastive_SSL_NIDS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hamed Fard, Tobias Schalau, Gerhard Wunder
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02349">An Investigation into the Performance of Non-Contrastive Self-Supervised Learning Methods for Network Intrusion Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Network intrusion detection, a well-explored cybersecurity field, has predominantly relied on supervised learning algorithms in the past two decades. However, their limitations in detecting only known anomalies prompt the exploration of alternative approaches. Motivated by the success of self-supervised learning in computer vision, there is a rising interest in adapting this paradigm for network intrusion detection. While prior research mainly delved into contrastive self-supervised methods, the efficacy of non-contrastive methods, in conjunction with encoder architectures serving as the representation learning backbone and augmentation strategies that determine what is learned, remains unclear for effective attack detection. This paper compares the performance of five non-contrastive self-supervised learning methods using three encoder architectures and six augmentation strategies. Ninety experiments are systematically conducted on two network intrusion detection datasets, UNSW-NB15 and 5G-NIDD. For each self-supervised model, the combination of encoder architecture and augmentation method yielding the highest average precision, recall, F1-score, and AUCROC is reported. Furthermore, by comparing the best-performing models to two unsupervised baselines, DeepSVDD, and an Autoencoder, we showcase the competitiveness of the non-contrastive methods for attack detection. Code at: https://github.com/renje4z335jh4/non_contrastive_SSL_NIDS<br>
<span id='abs_ch'>中文摘要：本文评估了非对比自监督学 方法在网络入侵检测中的应用，通过系统测试编 器与增强策略的组合，证明了其相对于 监督基线的竞争力。</span><br>
<span id='abs_en'>English Summary: This paper evaluates non-contrastive self-supervised learning methods for network intrusion detection, systematically testing combinations of encoders and augmentation strategies to demonstrate their competitiveness against unsupervised baselines.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2510.02341.pdf' target='_blank'>https://arxiv.org/pdf/2510.02341.pdf</a></span>   <span><a href='https://github.com/cacayaya/DRIFT.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Wang, Bolian Li, Junlin Wu, Zhaoxuan Tan, Zheli Liu, Ruqi Zhang, Ananth Grama, Qingkai Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02341">DRIFT: Learning from Abundant User Dissatisfaction in Real-World Preference Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Real-world large language model deployments (e.g., conversational AI systems, code generation assistants) naturally generate abundant implicit user dissatisfaction (DSAT) signals, as users iterate toward better answers through refinements, corrections, and expressed preferences, while explicit satisfaction (SAT) feedback is scarce. Existing preference learning approaches are poorly aligned with this data profile, as they rely on costly human annotations or assume plentiful positive responses. In this paper, we introduce \textbf{DRIFT} (\textbf{D}issatisfaction-\textbf{R}efined \textbf{I}terative pre\textbf{F}erence \textbf{T}raining), which anchors training on real-world DSAT signals and samples positives dynamically from the evolving policy. Empirically, DRIFT models trained on real-world \textit{WildFeedback} datasets and synthetic \textit{UltraFeedback} datasets achieve up to +6.23\% (7B) / +7.61\% (14B) on WildBench Task Score and up to +8.95\% (7B) / +12.29\% (14B) on AlpacaEval2 win rate over base models, outperforming strong baseline methods such as iterative DPO and SPIN. At larger scales, the improvements are particularly pronounced: 14B models trained with DRIFT surpass GPT-4o-mini on WildBench. Further analysis shows that DRIFT also preserves exploratory capacity, yielding more diverse high-reward solutions rather than collapsing to narrow subsets. Theoretically, we demonstrate that this design preserves preference margins and avoids the gradient degeneration. These results show that DRIFT is an effective and scalable recipe for real-world post-training that leverages the most abundant and informative signal. The code and data are available at https://github.com/cacayaya/DRIFT.git.<br>
<span id='abs_ch'>中文: DRIFT是一种创新的偏好训练方法，通过利用现实部署中丰富的隐式用户不满意信号并动态采 积极回应，在多个基准测试中显著超越基础模型并优于现有强基线方法。</span><br>
<span id='abs_en'>English: DRIFT is a novel preference training method that leverages abundant implicit user dissatisfaction signals from real-world deployments to dynamically sample positive responses, achieving significant performance improvements over base models and outperforming strong baselines on multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2510.02334.pdf' target='_blank'>https://arxiv.org/pdf/2510.02334.pdf</a></span>   <span><a href='https://github.com/plumprc/RepT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Li, Wei Zhao, Yige Li, Jun Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02334">Where Did It Go Wrong? Attributing Undesirable LLM Behaviors via Representation Gradient Tracing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their deployment is frequently undermined by undesirable behaviors such as generating harmful content, factual inaccuracies, and societal biases. Diagnosing the root causes of these failures poses a critical challenge for AI safety. Existing attribution methods, particularly those based on parameter gradients, often fall short due to prohibitive noisy signals and computational complexity. In this work, we introduce a novel and efficient framework that diagnoses a range of undesirable LLM behaviors by analyzing representation and its gradients, which operates directly in the model's activation space to provide a semantically meaningful signal linking outputs to their training data. We systematically evaluate our method for tasks that include tracking harmful content, detecting backdoor poisoning, and identifying knowledge contamination. The results demonstrate that our approach not only excels at sample-level attribution but also enables fine-grained token-level analysis, precisely identifying the specific samples and phrases that causally influence model behavior. This work provides a powerful diagnostic tool to understand, audit, and ultimately mitigate the risks associated with LLMs. The code is available at https://github.com/plumprc/RepT.<br>
<span id='abs_ch'>中文: 本文提出了一种高效框架，通过分析激活空间中的表征梯度来诊断大语言模型的不良行为，能够实现精确的 本级和词元级归 ，从而理解和降低相关风险。</span><br>
<span id='abs_en'>English: This paper introduces an efficient framework that diagnoses undesirable behaviors in Large Language Models by analyzing representation gradients in activation space, enabling precise sample-level and token-level attribution to understand and mitigate risks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2510.02328.pdf' target='_blank'>https://arxiv.org/pdf/2510.02328.pdf</a></span>   <span><a href='https://github.com/REAL-Lab-NU/AMANDA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziqing Wang, Chengsheng Mao, Xiaole Wen, Yuan Luo, Kaize Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02328">AMANDA: Agentic Medical Knowledge Augmentation for Data-Efficient Medical Visual Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Medical Multimodal Large Language Models (Med-MLLMs) have shown great promise in medical visual question answering (Med-VQA). However, when deployed in low-resource settings where abundant labeled data are unavailable, existing Med-MLLMs commonly fail due to their medical reasoning capability bottlenecks: (i) the intrinsic reasoning bottleneck that ignores the details from the medical image; (ii) the extrinsic reasoning bottleneck that fails to incorporate specialized medical knowledge. To address those limitations, we propose AMANDA, a training-free agentic framework that performs medical knowledge augmentation via LLM agents. Specifically, our intrinsic medical knowledge augmentation focuses on coarse-to-fine question decomposition for comprehensive diagnosis, while extrinsic medical knowledge augmentation grounds the reasoning process via biomedical knowledge graph retrieval. Extensive experiments across eight Med-VQA benchmarks demonstrate substantial improvements in both zero-shot and few-shot Med-VQA settings. The code is available at https://github.com/REAL-Lab-NU/AMANDA.<br>
<span id='abs_ch'>中文摘要：AMANDA框架通过内在问题分解和外部知识图谱检索，解决了医学多模态大语言模型的推理瓶颈，在低资源医学视觉问答中显著提升了性能。English Summary: The AMANDA framework enhances medical multimodal large language models by addressing their reasoning bottlenecks through intrinsic question decomposition and extrinsic knowledge graph retrieval, significantly improving performance in low-resource medical visual question answering.Paperid:304,https://arxiv.org/pdf/2510.02315.pdfGitHub</span><br>
<span id='abs_en'>English Summary: The AMANDA framework enhances medical multimodal large language models by addressing their reasoning bottlenecks through intrinsic question decomposition and extrinsic knowledge graph retrieval, significantly improving performance in low-resource medical visual question answering.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2510.02295.pdf' target='_blank'>https://arxiv.org/pdf/2510.02295.pdf</a></span>   <span><a href='https://github.com/Espere-1119-Song/VideoNSA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Enxin Song, Wenhao Chai, Shusheng Yang, Ethan Armand, Xiaojun Shan, Haiyang Xu, Jianwen Xie, Zhuowen Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02295">VideoNSA: Native Sparse Attention Scales Video Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Video understanding in multimodal language models remains limited by context length: models often miss key transition frames and struggle to maintain coherence across long time scales. To address this, we adapt Native Sparse Attention (NSA) to video-language models. Our method, VideoNSA, adapts Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We employ a hardware-aware hybrid approach to attention, preserving dense attention for text, while employing NSA for video. Compared to token-compression and training-free sparse baselines, VideoNSA achieves improved performance on long-video understanding, temporal reasoning, and spatial benchmarks. Further ablation analysis reveals four key findings: (1) reliable scaling to 128K tokens; (2) an optimal global-local attention allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4) the learnable combined sparse attention help induce dynamic attention sinks.<br>
<span id='abs_ch'>中文：VideoNSA通过将原生稀疏注意力应用于视频，增强了视频语言模型的长视频理解能力，在保持文本密集注意力的同时，优化了注意力分配，从而在时间和空间基准测试中取得了更好的性能。</span><br>
<span id='abs_en'>English: VideoNSA enhances video-language models by applying Native Sparse Attention to videos, enabling scalable, coherent long-video understanding and improved performance on temporal and spatial benchmarks through optimized attention allocation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2510.02270.pdf' target='_blank'>https://arxiv.org/pdf/2510.02270.pdf</a></span>   <span><a href='https://github.com/sathiiii/microCLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sathira Silva, Eman Ali, Chetan Arora, Muhammad Haris Khan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02270">microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Unsupervised adaptation of CLIP-based vision-language models (VLMs) for fine-grained image classification requires sensitivity to microscopic local cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse global features restricts its performance on fine-grained classification tasks. Prior efforts inject fine-grained knowledge by aligning large language model (LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training framework that jointly refines CLIP's visual and textual representations using fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP) within a lightweight TokenFusion module, which builds a saliency-guided $\texttt{[FG]}$ token from patch embeddings and fuses it with the global $\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we introduce a two-headed LLM-derived classifier: a frozen classifier that, via multi-view alignment, provides a stable text-based prior for pseudo-labeling, and a learnable classifier initialized from LLM descriptions and fine-tuned with TokenFusion. We further develop Dynamic Knowledge Aggregation, which convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to iteratively refine pseudo-labels. Together, these components uncover latent fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy gain across 13 fine-grained benchmarks while requiring only light adaptation. Our code is available at https://github.com/sathiiii/microCLIP.<br>
<span id='abs_ch'>中文摘要：microCLIP 是一种自训练框架，通过融合显著性引导的局部特征与全局表征，并利用大语言模型派生的分类器稳定适应过程，显著提升了CLIP在细粒度分类任务中的性能，在13个基准测试中平均准确率提高了2.90%。</span><br>
<span id='abs_en'>English Summary: microCLIP is a self-training framework that enhances CLIP's fine-grained classification by integrating saliency-guided local features with global representations and stabilizing adaptation through LLM-derived classifiers, achieving a 2.90% average accuracy gain across 13 benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2510.02230.pdf' target='_blank'>https://arxiv.org/pdf/2510.02230.pdf</a></span>   <span><a href='https://github.com/mail-research/SELF-llm-interference' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Phuc Minh Nguyen, Chinh D. La, Duy M. H. Nguyen, Nitesh V. Chawla, Binh T. Nguyen, Khoa D. Doan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02230">The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key method for improving Large Language Models' reasoning capabilities, yet recent evidence suggests it may paradoxically shrink the reasoning boundary rather than expand it. This paper investigates the shrinkage issue of RLVR by analyzing its learning dynamics and reveals two critical phenomena that explain this failure. First, we expose negative interference in RLVR, where learning to solve certain training problems actively reduces the likelihood of correct solutions for others, leading to the decline of Pass@$k$ performance, or the probability of generating a correct solution within $k$ attempts. Second, we uncover the winner-take-all phenomenon: RLVR disproportionately reinforces problems with high likelihood, correct solutions, under the base model, while suppressing other initially low-likelihood ones. Through extensive theoretical and empirical analysis on multiple mathematical reasoning benchmarks, we show that this effect arises from the inherent on-policy sampling in standard RL objectives, causing the model to converge toward narrow solution strategies. Based on these insights, we propose a simple yet effective data curation algorithm that focuses RLVR learning on low-likelihood problems, achieving notable improvement in Pass@$k$ performance. Our code is available at https://github.com/mail-research/SELF-llm-interference.<br>
<span id='abs_ch'>中文: 强化学 与可验证奖励（RLVR）会 负干扰和赢家通吃效应而限制推理能力，但针对低概率问题的数据筛选方法能有效提升性能。</span><br>
<span id='abs_en'>English: RLVR can paradoxically limit reasoning by causing negative interference and a winner-take-all effect, but a data curation method focusing on low-likelihood problems improves performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2510.02227.pdf' target='_blank'>https://arxiv.org/pdf/2510.02227.pdf</a></span>   <span><a href='https://github.com/SII-Enigma/AMPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyang Yuan, Yujuan Ding, Yi Bin, Wenqi Shao, Jinyu Cai, Jingkuan Song, Yang Yang, Heng Tao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02227">More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm for enhancing the reasoning ability in Large Language Models (LLMs). However, prevailing methods primarily rely on self-exploration or a single off-policy teacher to elicit long chain-of-thought (LongCoT) reasoning, which may introduce intrinsic model biases and restrict exploration, ultimately limiting reasoning diversity and performance. Drawing inspiration from multi-teacher strategies in knowledge distillation, we introduce Adaptive Multi-Guidance Policy Optimization (AMPO), a novel framework that adaptively leverages guidance from multiple proficient teacher models, but only when the on-policy model fails to generate correct solutions. This "guidance-on-demand" approach expands exploration while preserving the value of self-discovery. Moreover, AMPO incorporates a comprehension-based selection mechanism, prompting the student to learn from the reasoning paths that it is most likely to comprehend, thus balancing broad exploration with effective exploitation. Extensive experiments show AMPO substantially outperforms a strong baseline (GRPO), with a 4.3% improvement on mathematical reasoning tasks and 12.2% on out-of-distribution tasks, while significantly boosting Pass@k performance and enabling more diverse exploration. Notably, using four peer-sized teachers, our method achieves comparable results to approaches that leverage a single, more powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate a more efficient and scalable path to superior reasoning and generalizability. Our code is available at https://github.com/SII-Enigma/AMPO.<br>
<span id='abs_ch'>中文: AMPO是一种新颖的强化学 框架，仅在需要时自适应地利用多个教师模型进行指导，从而在数学和分布外任务中显著提升推理多 性和性能。</span><br>
<span id='abs_en'>English: AMPO is a novel reinforcement learning framework that adaptively guides LLMs using multiple teachers only when needed, enhancing reasoning diversity and performance across mathematical and out-of-distribution tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2510.02109.pdf' target='_blank'>https://arxiv.org/pdf/2510.02109.pdf</a></span>   <span><a href='https://github.com/utkuozbulak/spurbreast' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jong Bum Won, Wesley De Neve, Joris Vankerschaver, Utku Ozbulak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02109">SpurBreast: A Curated Dataset for Investigating Spurious Correlations in Real-world Breast MRI Classification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Deep neural networks (DNNs) have demonstrated remarkable success in medical imaging, yet their real-world deployment remains challenging due to spurious correlations, where models can learn non-clinical features instead of meaningful medical patterns. Existing medical imaging datasets are not designed to systematically study this issue, largely due to restrictive licensing and limited supplementary patient data. To address this gap, we introduce SpurBreast, a curated breast MRI dataset that intentionally incorporates spurious correlations to evaluate their impact on model performance. Analyzing over 100 features involving patient, device, and imaging protocol, we identify two dominant spurious signals: magnetic field strength (a global feature influencing the entire image) and image orientation (a local feature affecting spatial alignment). Through controlled dataset splits, we demonstrate that DNNs can exploit these non-clinical signals, achieving high validation accuracy while failing to generalize to unbiased test data. Alongside these two datasets containing spurious correlations, we also provide benchmark datasets without spurious correlations, allowing researchers to systematically investigate clinically relevant and irrelevant features, uncertainty estimation, adversarial robustness, and generalization strategies. Models and datasets are available at https://github.com/utkuozbulak/spurbreast.<br>
<span id='abs_ch'>中文摘要：医学影像中的深度神经网络常利用磁场强度和图像方向等虚假相关性而非临床特征，为此我们开发了SpurBreast乳腺MRI数据集，通过控制数据集划分系统评估这些干扰信号对模型泛化能力的影响。</span><br>
<span id='abs_en'>English Summary: Deep neural networks in medical imaging often exploit spurious correlations like magnetic field strength and image orientation rather than clinical patterns, prompting the creation of SpurBreast—a breast MRI dataset designed to systematically evaluate and mitigate these misleading signals.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2510.01934.pdf' target='_blank'>https://arxiv.org/pdf/2510.01934.pdf</a></span>   <span><a href='https://github.com/ymxlzgy/FoundAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangyao Zhai, Yue Zhou, Xinyan Deng, Lars Heckler, Nassir Navab, Benjamin Busam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01934">Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Few-shot anomaly detection streamlines and simplifies industrial safety inspection. However, limited samples make accurate differentiation between normal and abnormal features challenging, and even more so under category-agnostic conditions. Large-scale pre-training of foundation visual encoders has advanced many fields, as the enormous quantity of data helps to learn the general distribution of normal images. We observe that the anomaly amount in an image directly correlates with the difference in the learnt embeddings and utilize this to design a few-shot anomaly detector termed FoundAD. This is done by learning a nonlinear projection operator onto the natural image manifold. The simple operator acts as an effective tool for anomaly detection to characterize and identify out-of-distribution regions in an image. Extensive experiments show that our approach supports multi-class detection and achieves competitive performance while using substantially fewer parameters than prior methods. Backed up by evaluations with multiple foundation encoders, including fresh DINOv3, we believe this idea broadens the perspective on foundation features and advances the field of few-shot anomaly detection.<br>
<span id='abs_ch'>中文摘要：FoundAD提出了一种基于基础视觉编 器的小 本异常检测方法，通过将图像投影到自然流形上来识别异常区域，在显著减少参数量的同时实现了优越的检测性能。</span><br>
<span id='abs_en'>English Summary: FoundAD introduces a few-shot anomaly detection method that leverages foundation visual encoders to distinguish anomalies by projecting images onto a natural manifold, achieving competitive performance with fewer parameters.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2510.01724.pdf' target='_blank'>https://arxiv.org/pdf/2510.01724.pdf</a></span>   <span><a href='https://github.com/HolobiomicsLab/MetaboT]' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Madina Bekbergenova, Lucas Pradi, Benjamin Navet, Emma Tysinger, Franck Michel, Matthieu Feraud, Yousouf Taghzouti, Yan Zhou Chen, Olivier Kirchhoffer, Florence Mehl, Martin Legrand, Tao Jiang, Marco Pagni, Soha Hassoun, Jean-Luc Wolfender, Wout Bittremieux, Fabien Gandon, Louis-Félix Nothias
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01724">MetaboT: AI-based agent for natural language-based interaction with metabolomics knowledge graphs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Mass spectrometry metabolomics generates vast amounts of data requiring advanced methods for interpretation. Knowledge graphs address these challenges by structuring mass spectrometry data, metabolite information, and their relationships into a connected network (Gaudry et al. 2024). However, effective use of a knowledge graph demands an in-depth understanding of its ontology and its query language syntax. To overcome this, we designed MetaboT, an AI system utilizing large language models (LLMs) to translate user questions into SPARQL semantic query language for operating on knowledge graphs (Steve Harris 2013). We demonstrate its effectiveness using the Experimental Natural Products Knowledge Graph (ENPKG), a large-scale public knowledge graph for plant natural products (Gaudry et al. 2024).MetaboT employs specialized AI agents for handling user queries and interacting with the knowledge graph by breaking down complex tasks into discrete components, each managed by a specialised agent (Fig. 1a). The multi-agent system is constructed using the LangChain and LangGraph libraries, which facilitate the integration of LLMs with external tools and information sources (LangChain, n.d.). The query generation process follows a structured workflow. First, the Entry Agent determines if the question is new or a follow-up to previous interactions. New questions are forwarded to the Validator Agent, which verifies if the question is related to the knowledge graph. Then, the valid question is sent to the Supervisor Agent, which identifies if the question requires chemical conversions or standardized identifiers. In this case it delegates the question to the Knowledge Graph Agent, which can use tools to extract necessary details, such as URIs or taxonomies of chemical names, from the user query. Finally, an agent responsible for crafting the SPARQL queries equipped with the ontology of the knowledge graph uses the provided identifiers to generate the query. Then, the system executes the generated query against the metabolomics knowledge graph and returns structured results to the user (Fig. 1b). To assess the performance of MetaboT we have curated 50 metabolomics-related questions and their expected answers. In addition to submitting these questions to MetaboT, we evaluated a baseline by submitting them to a standard LLM (GPT-4o) with a prompt that incorporated the knowledge graph ontology but did not provide specific entity IDs. This baseline achieved only 8.16% accuracy, compared to MetaboT's 83.67%, underscoring the necessity of our multi-agent system for accurately retrieving entities and generating correct SPARQL queries. MetaboT demonstrates promising performance as a conversational question-answering assistant, enabling researchers to retrieve structured metabolomics data through natural language queries. By automating the generation and execution of SPARQL queries, it removes technical barriers that have traditionally hindered access to knowledge graphs. Importantly, MetaboT leverages the capabilities of LLMs while maintaining experimentally grounded query generation, ensuring that outputs remain aligned with domain-specific standards and data structures. This approach facilitates data-driven discoveries by bridging the gap between complex semantic technologies and user-friendly interaction. MetaboT is accessible at [https://metabot.holobiomicslab.eu/], and its source code is available at [https://github.com/HolobiomicsLab/MetaboT].<br>
<span id='abs_ch'>中文: MetaboT是一种利用大语言模型和多智能体框架的人工智能系统，能将自然语言问题转化为SPARQL查询，使 究人员能以83.67%的准确率访问复杂的代谢组学知识图谱，同时消除了技术障碍。</span><br>
<span id='abs_en'>English: MetaboT is an AI system that uses large language models and a multi-agent framework to translate natural language questions into SPARQL queries, enabling researchers to access complex metabolomics knowledge graphs with 83.67% accuracy while eliminating technical barriers.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2510.01704.pdf' target='_blank'>https://arxiv.org/pdf/2510.01704.pdf</a></span>   <span><a href='https://github.com/SNU-VGILab/InstaOrder' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierre Musacchio, Hyunmin Lee, Jaesik Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01704">Holistic Order Prediction in Natural Scenes</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Even in controlled settings, understanding instance-wise geometries is a challenging task for a wide range of visual models. Although specialized systems exist, modern arts rely on expensive input formats (category labels, binary segmentation masks) and inference costs (a quadratic amount of forward passes). We mitigate these limitations by proposing InstaFormer, a network capable of holistic order prediction. That is, solely given an input RGB image, InstaFormer returns the full occlusion and depth orderings for all the instances in the scene in a single forward pass. At its core, InstaFormer relies on interactions between object queries and latent mask descriptors that semantically represent the same objects while carrying complementary information. We comprehensively benchmark and ablate our approach to highlight its effectiveness. Our code and models are open-source and available at this URL: https://github.com/SNU-VGILab/InstaOrder.<br>
<br>
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2510.01685.pdf' target='_blank'>https://arxiv.org/pdf/2510.01685.pdf</a></span>   <span><a href='https://github.com/apoorvkh/composing-functions' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Apoorv Khandelwal, Ellie Pavlick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01685">How Do Language Models Compose Functions?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While large language models (LLMs) appear to be increasingly capable of solving compositional tasks, it is an open question whether they do so using compositional mechanisms. In this work, we investigate how feedforward LLMs solve two-hop factual recall tasks, which can be expressed compositionally as $g(f(x))$. We first confirm that modern LLMs continue to suffer from the "compositionality gap": i.e. their ability to compute both $z = f(x)$ and $y = g(z)$ does not entail their ability to compute the composition $y = g(f(x))$. Then, using logit lens on their residual stream activations, we identify two processing mechanisms, one which solves tasks $\textit{compositionally}$, computing $f(x)$ along the way to computing $g(f(x))$, and one which solves them $\textit{directly}$, without any detectable signature of the intermediate variable $f(x)$. Finally, we find that which mechanism is employed appears to be related to the embedding space geometry, with the idiomatic mechanism being dominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in the embedding spaces. We fully release our data and code at: https://github.com/apoorvkh/composing-functions .<br>
<span id='abs_ch'>中文: 本 究揭示大型语言模型通过组合式或直接式两种机制处理双跳事实回忆任务，其选择机制受嵌入空间 射的线性特征影响。</span><br>
<span id='abs_en'>English: This study reveals that large language models address two-hop factual recall tasks through either compositional or direct mechanisms, with the choice influenced by the linearity of embedding space mappings.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2510.01671.pdf' target='_blank'>https://arxiv.org/pdf/2510.01671.pdf</a></span>   <span><a href='http://github.com/motokinaru/LENOHA-medical-dialogue' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Motoki Sato, Yuki Matsushita, Hidekazu Takahashi, Tomoaki Kakazu, Sou Nagata, Mizuho Ohnuma, Atsushi Yoshikawa, Masayuki Yamamura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01671">A Locally Executable AI System for Improving Preoperative Patient Communication: A Multi-Domain Clinical Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Patients awaiting invasive procedures often have unanswered pre-procedural questions; however, time-pressured workflows and privacy constraints limit personalized counseling. We present LENOHA (Low Energy, No Hallucination, Leave No One Behind Architecture), a safety-first, local-first system that routes inputs with a high-precision sentence-transformer classifier and returns verbatim answers from a clinician-curated FAQ for clinical queries, eliminating free-text generation in the clinical path. We evaluated two domains (tooth extraction and gastroscopy) using expert-reviewed validation sets (n=400/domain) for thresholding and independent test sets (n=200/domain). Among the four encoders, E5-large-instruct (560M) achieved an overall accuracy of 0.983 (95% CI 0.964-0.991), AUC 0.996, and seven total errors, which were statistically indistinguishable from GPT-4o on this task; Gemini made no errors on this test set. Energy logging shows that the non-generative clinical path consumes ~1.0 mWh per input versus ~168 mWh per small-talk reply from a local 8B SLM, a ~170x difference, while maintaining ~0.10 s latency on a single on-prem GPU. These results indicate that near-frontier discrimination and generation-induced errors are structurally avoided in the clinical path by returning vetted FAQ answers verbatim, supporting privacy, sustainability, and equitable deployment in bandwidth-limited environments.<br>
<span id='abs_ch'>中文: LENOHA系统通过高精度分类器从临床医生整理的常见问题中检索原文答案，以接近完美的准确率解决患者术前疑问，同时大幅降低能耗并规避生成式AI错误。</span><br>
<span id='abs_en'>English: The LENOHA system addresses pre-procedural patient inquiries by using a high-precision classifier to retrieve verbatim answers from clinician-curated FAQs, achieving near-perfect accuracy while consuming minimal energy and avoiding generative AI errors.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2510.01664.pdf' target='_blank'>https://arxiv.org/pdf/2510.01664.pdf</a></span>   <span><a href='https://github.com/yejining99/GuruAgents' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yejin Kim, Youngbin Lee, Juhyeong Kim, Yongjae Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01664">GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This study demonstrates that GuruAgents, prompt-guided AI agents, can systematically operationalize the strategies of legendary investment gurus. We develop five distinct GuruAgents, each designed to emulate an iconic investor, by encoding their distinct philosophies into LLM prompts that integrate financial tools and a deterministic reasoning pipeline. In a backtest on NASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique behaviors driven by their prompted personas. The Buffett GuruAgent achieves the highest performance, delivering a 42.2\% CAGR that significantly outperforms benchmarks, while other agents show varied results. These findings confirm that prompt engineering can successfully translate the qualitative philosophies of investment gurus into reproducible, quantitative strategies, highlighting a novel direction for automated systematic investing. The source code and data are available at https://github.com/yejining99/GuruAgents.<br>
<span id='abs_ch'>中文:  究表明，通过提示工程指导的GuruAgents能够将 奇投资大师的定性理念转化为可复现的量化策略，其中巴菲特风 的智能体在回测中实现了42.2%的年复合增长率，显著超越基准表现。</span><br>
<span id='abs_en'>English: This study shows that GuruAgents, AI agents guided by prompts, can effectively translate the investment philosophies of legendary gurus into systematic strategies, with the Buffett-inspired agent achieving a 42.2% CAGR and outperforming benchmarks in backtesting.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2510.01581.pdf' target='_blank'>https://arxiv.org/pdf/2510.01581.pdf</a></span>   <span><a href='https://github.com/joykirat18/TRAAC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joykirat Singh, Justin Chih-Yao Chen, Archiki Prasad, Elias Stengel-Eskin, Akshay Nambi, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01581">Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent thinking models solve complex reasoning tasks by scaling test-time compute, but this scaling must be allocated in line with task difficulty. On one hand, short reasoning (underthinking) leads to errors on harder problems that require extended reasoning steps; but, excessively long reasoning (overthinking) can be token-inefficient, generating unnecessary steps even after reaching a correct intermediate solution. We refer to this as under-adaptivity, where the model fails to modulate its response length appropriately given problems of varying difficulty. To address under-adaptivity and strike a balance between under- and overthinking, we propose TRAAC (Think Right with Adaptive, Attentive Compression), an online post-training RL method that leverages the model's self-attention over a long reasoning trajectory to identify important steps and prune redundant ones. TRAAC also estimates difficulty and incorporates it into training rewards, thereby learning to allocate reasoning budget commensurate with example difficulty. Our approach improves accuracy, reduces reasoning steps, and enables adaptive thinking compared to base models and other RL baselines. Across a variety of tasks (AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute accuracy gain of 8.4% with a relative reduction in reasoning length of 36.8% compared to the base model, and a 7.9% accuracy gain paired with a 29.4% length drop compared to the best RL baseline. TRAAC also shows strong generalization: although our models are trained on math datasets, they show accuracy and efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH, and OptimalThinkingBench. Our analysis further verifies that TRAAC provides fine-grained adjustments to thinking budget based on difficulty and that a combination of task-difficulty calibration and attention-based compression yields gains across diverse tasks.<br>
<span id='abs_ch'>中文: TRAAC是一种自适应推理方法，通过 据问题难度动态调整推理长度来优化计算效率，在多项任务中实现了更高准确率和更少推理步骤。</span><br>
<span id='abs_en'>English: TRAAC is an adaptive reasoning method that optimizes computational efficiency by dynamically adjusting reasoning length based on problem difficulty, achieving higher accuracy with fewer steps across diverse tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2510.01576.pdf' target='_blank'>https://arxiv.org/pdf/2510.01576.pdf</a></span>   <span><a href='https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ricardo Gonzalez Penuela, Felipe Arias-Russi, Victor Capriles
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01576">Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal large language models (MLLMs) have been integrated into visual interpretation applications to support Blind and Low Vision (BLV) users because of their accuracy and ability to provide rich, human-like interpretations. However, these applications often default to comprehensive, lengthy descriptions regardless of context. This leads to inefficient exchanges, as users must go through irrelevant details rather than receiving the specific information they are likely to seek. To deliver more contextually-relevant information, we developed a system that draws on historical BLV users questions. When given an image, our system identifies similar past visual contexts from the VizWiz-LF dataset and uses the associated questions to guide the MLLM generate descriptions more relevant to BLV users. An evaluation with three human labelers who revised 92 context-aware and context-free descriptions showed that context-aware descriptions anticipated and answered users' questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of comparisons (50 out of 92). Our paper reviews, and data analysis are publicly available in a Github repository at https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .<br>
<br>
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2510.01571.pdf' target='_blank'>https://arxiv.org/pdf/2510.01571.pdf</a></span>   <span><a href='https://github.com/chq1155/RL-PLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanqun Cao, Hongrui Zhang, Junde Xu, Zhou Zhang, Lingdong Shen, Minghao Sun, Ge Liu, Jinbo Xu, Wu-Jun Li, Jinren Ni, Cesar de la Fuente-Nunez, Tianfan Fu, Yejin Choi, Pheng-Ann Heng, Fang Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01571">From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Protein language models (PLMs) have advanced computational protein science through large-scale pretraining and scalable architectures. In parallel, reinforcement learning (RL) has broadened exploration and enabled precise multi-objective optimization in protein design. Yet whether RL can push PLMs beyond their pretraining priors to uncover latent sequence-structure-function rules remains unclear. We address this by pairing RL with PLMs across four domains: antimicrobial peptide design, kinase variant optimization, antibody engineering, and inverse folding. Using diverse RL algorithms and model classes, we ask if RL improves sampling efficiency and, more importantly, if it reveals capabilities not captured by supervised learning. Across benchmarks, RL consistently boosts success rates and sample efficiency. Performance follows a three-factor interaction: task headroom, reward fidelity, and policy capacity jointly determine gains. When rewards are accurate and informative, policies have sufficient capacity, and tasks leave room beyond supervised baselines, improvements scale; when rewards are noisy or capacity is constrained, gains saturate despite exploration. This view yields practical guidance for RL in protein design: prioritize reward modeling and calibration before scaling policy size, match algorithm and regularization strength to task difficulty, and allocate capacity where marginal gains are largest. Implementation is available at https://github.com/chq1155/RL-PLM.<br>
<span id='abs_ch'>Chinese: 强化学 通过提升成功率和 本效率来增强蛋白质语言模型在多种蛋白质设计任务中的表现，其性能增益取决于奖励准确性、策略容量和任务提升空间的相互作用。</span><br>
<span id='abs_en'>English: Reinforcement learning enhances protein language models by boosting success rates and sample efficiency across various protein design tasks, with performance gains depending on reward accuracy, policy capacity, and task headroom.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2510.01498.pdf' target='_blank'>https://arxiv.org/pdf/2510.01498.pdf</a></span>   <span><a href='https://github.com/yuxuanou623/AortaDiff.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Ou, Ning Bi, Jiazhen Pan, Jiancheng Yang, Boliang Yu, Usama Zidan, Regent Lee, Vicente Grau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01498">AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic aneurysms (AAA), the required iodinated contrast agents pose significant risks, including nephrotoxicity, patient allergies, and environmental harm. To reduce contrast agent use, recent deep learning methods have focused on generating synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a multi-stage pipeline that first generates images and then performs segmentation, which leads to error accumulation and fails to leverage shared semantic and anatomical structures. To address this, we propose a unified deep learning framework that generates synthetic CECT images from NCCT scans while simultaneously segmenting the aortic lumen and thrombus. Our approach integrates conditional diffusion models (CDM) with multi-task learning, enabling end-to-end joint optimization of image synthesis and anatomical segmentation. Unlike previous multitask diffusion models, our approach requires no initial predictions (e.g., a coarse segmentation mask), shares both encoder and decoder parameters across tasks, and employs a semi-supervised training strategy to learn from scans with missing segmentation labels, a common constraint in real-world clinical data. We evaluated our method on a cohort of 264 patients, where it consistently outperformed state-of-the-art single-task and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61 dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation, it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.<br>
<span id='abs_ch'>Chinese: 本 究提出了一种统一的深度学 框架，通过结合条件扩散模型与多任务学 ，能够从非增强CT扫描中同步生成合成增强CT图像并分割主动脉结构，在图像质量和解剖分割精度上均优于现有方法。</span><br>
<span id='abs_en'>English: This study introduces a unified deep learning framework that simultaneously generates synthetic contrast-enhanced CT images from non-contrast scans and segments aortic structures, outperforming existing methods in both image quality and anatomical accuracy through integrated conditional diffusion models and multi-task learning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2510.01474.pdf' target='_blank'>https://arxiv.org/pdf/2510.01474.pdf</a></span>   <span><a href='https://github.com/camlsys/aireg-bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bill Marino, Rosco Hunter, Zubair Jamali, Marinos Emmanouil Kalpakos, Mudra Kashyap, Isaiah Hinton, Alexa Hanson, Maahum Nazir, Christoph Schnabl, Felix Steffek, Hongkai Wen, Nicholas D. Lane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01474">AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As governments move to regulate AI, there is growing interest in using Large Language Models (LLMs) to assess whether or not an AI system complies with a given AI Regulation (AIR). However, there is presently no way to benchmark the performance of LLMs at this task. To fill this void, we introduce AIReg-Bench: the first benchmark dataset designed to test how well LLMs can assess compliance with the EU AI Act (AIA). We created this dataset through a two-step process: (1) by prompting an LLM with carefully structured instructions, we generated 120 technical documentation excerpts (samples), each depicting a fictional, albeit plausible, AI system - of the kind an AI provider might produce to demonstrate their compliance with AIR; (2) legal experts then reviewed and annotated each sample to indicate whether, and in what way, the AI system described therein violates specific Articles of the AIA. The resulting dataset, together with our evaluation of whether frontier LLMs can reproduce the experts' compliance labels, provides a starting point to understand the opportunities and limitations of LLM-based AIR compliance assessment tools and establishes a benchmark against which subsequent LLMs can be compared. The dataset and evaluation code are available at https://github.com/camlsys/aireg-bench.<br>
<span id='abs_ch'>中文: 为解决评估大型语言模型在AI法规合规性方面缺乏基准的问题，AIReg-Bench作为首个测试LLMs对欧盟AI法案合规评估能力的数据集被开发出来，该数据集通过LLM生成技术文档和法律专家 注共同构建而成。</span><br>
<span id='abs_en'>English: To address the lack of benchmarks for evaluating LLMs in AI regulation compliance assessment, AIReg-Bench was developed as the first dataset to test LLMs' ability to assess adherence to the EU AI Act, created through LLM-generated documentation and expert legal annotations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2510.01450.pdf' target='_blank'>https://arxiv.org/pdf/2510.01450.pdf</a></span>   <span><a href='https://github.com/Yifei-Zuo/Flash-LLA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Zuo, Yutong Yin, Zhichen Zeng, Ang Li, Banghua Zhu, Zhaoran Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01450">Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Transformer architectures have achieved remarkable success in various domains. While efficient alternatives to Softmax Attention have been widely studied, the search for more expressive mechanisms grounded in theoretical insight-even at greater computational cost-has been relatively underexplored. In this work, we bridge this gap by proposing Local Linear Attention (LLA), a novel attention mechanism derived from nonparametric statistics through the lens of test-time regression. First, we show that LLA offers theoretical advantages over Linear and Softmax Attention for associative memory via a bias-variance trade-off analysis. Next, we address its computational challenges and propose two memory-efficient primitives to tackle the $Θ(n^2 d)$ and $Θ(n d^2)$ complexity. We then introduce FlashLLA, a hardware-efficient, blockwise algorithm that enables scalable and parallel computation on modern accelerators. In addition, we implement and profile a customized inference kernel that significantly reduces memory overheads. Finally, we empirically validate the advantages and limitations of LLA on test-time regression, in-context regression, associative recall and state tracking tasks. Experiment results demonstrate that LLA effectively adapts to non-stationarity, outperforming strong baselines in test-time training and in-context learning, and exhibiting promising evidence for its scalability and applicability in large-scale models. Code is available at https://github.com/Yifei-Zuo/Flash-LLA.<br>
<span id='abs_ch'>中文: 本文提出局部线性注意力机制（LLA），该基于理论推导的注意力机制在适应性与可扩展性上超越现有方法，并通过实验验证及高效计算优化实现显著性能提升。</span><br>
<span id='abs_en'>English: This paper introduces Local Linear Attention (LLA), a theoretically grounded attention mechanism that outperforms existing methods in adaptability and scalability, validated through extensive experiments and optimized with efficient computational primitives.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2510.01354.pdf' target='_blank'>https://arxiv.org/pdf/2510.01354.pdf</a></span>   <span><a href='https://github.com/Norrrrrrr-lyn/WAInjectBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinuo Liu, Ruohan Xu, Xilong Wang, Yuqi Jia, Neil Zhenqiang Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01354">WAInjectBench: Benchmarking Prompt Injection Detections for Web Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multiple prompt injection attacks have been proposed against web agents. At the same time, various methods have been developed to detect general prompt injection attacks, but none have been systematically evaluated for web agents. In this work, we bridge this gap by presenting the first comprehensive benchmark study on detecting prompt injection attacks targeting web agents. We begin by introducing a fine-grained categorization of such attacks based on the threat model. We then construct datasets containing both malicious and benign samples: malicious text segments generated by different attacks, benign text segments from four categories, malicious images produced by attacks, and benign images from two categories. Next, we systematize both text-based and image-based detection methods. Finally, we evaluate their performance across multiple scenarios. Our key findings show that while some detectors can identify attacks that rely on explicit textual instructions or visible image perturbations with moderate to high accuracy, they largely fail against attacks that omit explicit instructions or employ imperceptible perturbations. Our datasets and code are released at: https://github.com/Norrrrrrr-lyn/WAInjectBench.<br>
<span id='abs_ch'>Chinese: 本 究首次建立了针对网络代理的提示注入攻击检测综合基准，发现尽管检测器能较好识别显式文本或可见图像扰动攻击，但对隐蔽或 指令变体的防御能力严重不足。</span><br>
<span id='abs_en'>English: This study presents the first comprehensive benchmark for detecting prompt injection attacks on web agents, revealing that while detectors perform moderately well against explicit textual or visible image-based attacks, they largely fail against subtle or instruction-free variants.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2510.01304.pdf' target='_blank'>https://arxiv.org/pdf/2510.01304.pdf</a></span>   <span><a href='https://github.com/yuzeng0-0/AGILE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Zeng, Wenxuan Huang, Shiting Huang, Xikun Bao, Yukun Qi, Yiming Zhao, Qiuchen Wang, Lin Chen, Zehui Chen, Huaian Chen, Wanli Ouyang, Feng Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01304">Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although current large Vision-Language Models (VLMs) have advanced in multimodal understanding and reasoning, their fundamental perceptual and reasoning abilities remain limited. Specifically, even on simple jigsaw tasks, existing VLMs perform near randomly, revealing deficiencies in core perception and reasoning capabilities. While high-quality vision-language data can enhance these capabilities, its scarcity and limited scalability impose significant constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction Learning for Enhancing visual perception and reasoning in VLMs. AGILE formulates jigsaw solving as an interactive process, enabling the model to progressively engage with the environment. At each step, the model generates executable code to perform an action based on the current state, while the environment provides fine-grained visual feedback to guide task completion. Through this iterative cycle of observation and interaction, the model incrementally improves its perceptual and reasoning capabilities via exploration and feedback. Experimental results show that AGILE not only substantially boosts performance on jigsaw tasks of varying complexity (e.g., increasing accuracy from 9.5% to 82.8% under the 2 $\times$ 2 setting) but also demonstrates strong generalization across 9 general vision tasks, achieving an average improvement of 3.1%. These results indicate notable enhancements in both perceptual and reasoning abilities. This work opens a new avenue for advancing reasoning and generalization in multimodal models and provides an efficient, scalable solution to the scarcity of multimodal reinforcement learning data. The code and datasets is available at https://github.com/yuzeng0-0/AGILE .<br>
<span id='abs_ch'>中文: AGILE方法通过交互式拼图任务增强视觉语言模型的感知与推理能力，在解决数据稀缺问题的同时显著提升了模型在拼图和通用视觉任务上的表现。</span><br>
<span id='abs_en'>English: The proposed AGILE method enhances visual perception and reasoning in Vision-Language Models through interactive jigsaw solving, significantly improving performance on both jigsaw and general vision tasks while addressing data scarcity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2510.01295.pdf' target='_blank'>https://arxiv.org/pdf/2510.01295.pdf</a></span>   <span><a href='https://github.com/znreza/multi-agent-LLM-eval-for-debate' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zarreen Reza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01295">The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As Large Language Models (LLMs) transition from static tools to autonomous agents, traditional evaluation benchmarks that measure performance on downstream tasks are becoming insufficient. These methods fail to capture the emergent social and cognitive dynamics that arise when agents communicate, persuade, and collaborate in interactive environments. To address this gap, we introduce a novel evaluation framework that uses multi-agent debate as a controlled "social laboratory" to discover and quantify these behaviors. In our framework, LLM-based agents, instantiated with distinct personas and incentives, deliberate on a wide range of challenging topics under the supervision of an LLM moderator. Our analysis, enabled by a new suite of psychometric and semantic metrics, reveals several key findings. Across hundreds of debates, we uncover a powerful and robust emergent tendency for agents to seek consensus, consistently reaching high semantic agreement (μ > 0.88) even without explicit instruction and across sensitive topics. We show that assigned personas induce stable, measurable psychometric profiles, particularly in cognitive effort, and that the moderators persona can significantly alter debate outcomes by structuring the environment, a key finding for external AI alignment. This work provides a blueprint for a new class of dynamic, psychometrically grounded evaluation protocols designed for the agentic setting, offering a crucial methodology for understanding and shaping the social behaviors of the next generation of AI agents. We have released the code and results at https://github.com/znreza/multi-agent-LLM-eval-for-debate.<br>
<span id='abs_ch'>中文: 本文提出了一种多智能体辩论框架，用于评估大型语言模型中涌现的社会行为，揭示了其强烈的共识寻求倾向以及受设定角色影响的可测量心理特征。</span><br>
<span id='abs_en'>English: This paper introduces a multi-agent debate framework to evaluate emergent social behaviors in LLMs, revealing a strong consensus-seeking tendency and measurable psychometric profiles influenced by assigned personas.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2510.01268.pdf' target='_blank'>https://arxiv.org/pdf/2510.01268.pdf</a></span>   <span><a href='https://github.com/Mamba413/AdaDetectGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyi Zhou, Jin Zhu, Pingfan Su, Kai Ye, Ying Yang, Shakeel A O B Gavioli-Akilagun, Chengchun Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01268">AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We study the problem of determining whether a piece of text has been authored by a human or by a large language model (LLM). Existing state of the art logits-based detectors make use of statistics derived from the log-probability of the observed text evaluated using the distribution function of a given source LLM. However, relying solely on log probabilities can be sub-optimal. In response, we introduce AdaDetectGPT -- a novel classifier that adaptively learns a witness function from training data to enhance the performance of logits-based detectors. We provide statistical guarantees on its true positive rate, false positive rate, true negative rate and false negative rate. Extensive numerical studies show AdaDetectGPT nearly uniformly improves the state-of-the-art method in various combination of datasets and LLMs, and the improvement can reach up to 58%. A python implementation of our method is available at https://github.com/Mamba413/AdaDetectGPT.<br>
<span id='abs_ch'>Chinese: 本文提出AdaDetectGPT，一种新颖的分类器，通过从训练数据中自适应学 见证函数来增强基于逻辑值的检测器，以区分人类撰写文本与大型语言模型生成内容，相比现有最优方法提升高达58%。English: This paper introduces AdaDetectGPT, a novel classifier that adaptively learns a witness function from training data to enhance logits-based detectors for distinguishing human-authored text from LLM-generated content, achieving up to 58% improvement over state-of-the-art methods.Paperid:353,https://arxiv.org/pdf/2510.01264.pdfGitHub</span><br>
<span id='abs_en'>English: This paper introduces AdaDetectGPT, a novel classifier that adaptively learns a witness function from training data to enhance logits-based detectors for distinguishing human-authored text from LLM-generated content, achieving up to 58% improvement over state-of-the-art methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2510.01260.pdf' target='_blank'>https://arxiv.org/pdf/2510.01260.pdf</a></span>   <span><a href='https://github.com/Duke-CEI-Center/IoT-MCP-Servers' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ningyuan Yang, Guanliang Lyu, Mingchen Ma, Yiyi Lu, Yiming Li, Zhihui Gao, Hancheng Ye, Jianyi Zhang, Tingjun Chen, Yiran Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01260">IoT-MCP: Bridging LLMs and IoT Systems Through Model Context Protocol</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The integration of Large Language Models (LLMs) with Internet-of-Things (IoT) systems faces significant challenges in hardware heterogeneity and control complexity. The Model Context Protocol (MCP) emerges as a critical enabler, providing standardized communication between LLMs and physical devices. We propose IoT-MCP, a novel framework that implements MCP through edge-deployed servers to bridge LLMs and IoT ecosystems. To support rigorous evaluation, we introduce IoT-MCP Bench, the first benchmark containing 114 Basic Tasks (e.g., ``What is the current temperature?'') and 1,140 Complex Tasks (e.g., ``I feel so hot, do you have any ideas?'') for IoT-enabled LLMs. Experimental validation across 22 sensor types and 6 microcontroller units demonstrates IoT-MCP's 100% task success rate to generate tool calls that fully meet expectations and obtain completely accurate results, 205ms average response time, and 74KB peak memory footprint. This work delivers both an open-source integration framework (https://github.com/Duke-CEI-Center/IoT-MCP-Servers) and a standardized evaluation methodology for LLM-IoT systems.<br>
<span id='abs_ch'>中文: IoT-MCP框架通过边缘服务器实现模型上下文协议，成功将大语言模型与物联网系统连接，在实现任务完美执行的同时保持低延迟和小内存 用，并提供了开源集成平台和 准化评估基准。</span><br>
<span id='abs_en'>English: The IoT-MCP framework successfully bridges Large Language Models with IoT systems by implementing the Model Context Protocol through edge servers, achieving perfect task execution with minimal latency and memory usage while providing both an open-source integration platform and a standardized benchmark for evaluation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2510.01259.pdf' target='_blank'>https://arxiv.org/pdf/2510.01259.pdf</a></span>   <span><a href='https://github.com/ndurner/gpt-oss-rt-run' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nils Durner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01259">In AI Sweet Harmony: Sociopragmatic Guardrail Bypasses and Evaluation-Awareness in OpenAI gpt-oss-20b</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We probe OpenAI's open-weights 20-billion-parameter model gpt-oss-20b to study how sociopragmatic framing, language choice, and instruction hierarchy affect refusal behavior. Across 80 seeded iterations per scenario, we test several harm domains including ZIP-bomb construction (cyber threat), synthetic card-number generation, minor-unsafe driving advice, drug-precursor indicators, and RAG context exfiltration. Composite prompts that combine an educator persona, a safety-pretext ("what to avoid"), and step-cue phrasing flip assistance rates from 0% to 97.5% on a ZIP-bomb task. On our grid, formal registers in German and French are often leakier than matched English prompts. A "Linux terminal" role-play overrides a developer rule not to reveal context in a majority of runs with a naive developer prompt, and we introduce an AI-assisted hardening method that reduces leakage to 0% in several user-prompt variants. We further test evaluation awareness with a paired-track design and measure frame-conditioned differences between matched "helpfulness" and "harmfulness" evaluation prompts; we observe inconsistent assistance in 13% of pairs. Finally, we find that the OpenAI Moderation API under-captures materially helpful outputs relative to a semantic grader, and that refusal rates differ by 5 to 10 percentage points across inference stacks, raising reproducibility concerns. We release prompts, seeds, outputs, and code for reproducible auditing at https://github.com/ndurner/gpt-oss-rt-run .<br>
<span id='abs_ch'>中文: 本 究探讨了社会语用框架、语言选择和指令层级如何影响OpenAI的GPT-OSS-20B模型的拒绝行为，发现特定提示策略能显著提高对有害任务的协助率，并揭示了不同语言和推理 在安全评估中的不一致性。</span><br>
<span id='abs_en'>English: This study investigates how sociopragmatic framing, language choice, and instruction hierarchy influence refusal behaviors in OpenAI's GPT-OSS-20B model, revealing that specific prompt strategies can drastically increase assistance rates for harmful tasks and highlighting inconsistencies in safety evaluations across different languages and inference stacks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2510.01178.pdf' target='_blank'>https://arxiv.org/pdf/2510.01178.pdf</a></span>   <span><a href='https://github.com/GaoxiangLuo/COM-BOM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaoxiang Luo, Aryan Deshwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01178">COM-BOM: Bayesian Exemplar Search for Efficiently Exploring the Accuracy-Calibration Pareto Frontier</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Selecting an optimal set of exemplars is critical for good performance of in-context learning. However, prior exemplar search methods narrowly optimize for predictive accuracy, critically neglecting model calibration--a key determinant of trustworthiness and safe deployment. In this paper, we formulate exemplar selection as a multi-objective optimization problem, explicitly targeting both the maximization of predictive accuracy and the minimization of expected calibration error. We solve this problem with a sample-efficient Combinatorial Bayesian Optimization algorithm (COM-BOM) to find the Pareto front that optimally trades off the two objectives of accuracy and calibration. We evaluate COM-BOM on multiple tasks from unsaturated MMLU-Pro benchmark and find that COM-BOM beats or matches the baselines at jointly optimizing the two objectives, while requiring a minimal number of LLM API calls.<br>
<span id='abs_ch'>中文: 本文提出了一种多目 优化的示例选择方法，通过COM-BOM算法在保证预测准确性的同时优化模型 准效果，以最少的计算成本实现了优于基线模型的综合性能。</span><br>
<span id='abs_en'>English: This paper introduces a multi-objective optimization approach for exemplar selection that balances predictive accuracy and model calibration, using a sample-efficient algorithm called COM-BOM to outperform baselines with minimal computational cost.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2510.01174.pdf' target='_blank'>https://arxiv.org/pdf/2510.01174.pdf</a></span>   <span><a href='https://github.com/showlab/Code2Video' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanzhe Chen, Kevin Qinghong Lin, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01174">Code2Video: A Code-centric Paradigm for Educational Video Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While recent generative models advance pixel-space video synthesis, they remain limited in producing professional educational videos, which demand disciplinary knowledge, precise visual structures, and coherent transitions, limiting their applicability in educational scenarios. Intuitively, such requirements are better addressed through the manipulation of a renderable environment, which can be explicitly controlled via logical commands (e.g., code). In this work, we propose Code2Video, a code-centric agent framework for generating educational videos via executable Python code. The framework comprises three collaborative agents: (i) Planner, which structures lecture content into temporally coherent flows and prepares corresponding visual assets; (ii) Coder, which converts structured instructions into executable Python codes while incorporating scope-guided auto-fix to enhance efficiency; and (iii) Critic, which leverages vision-language models (VLM) with visual anchor prompts to refine spatial layout and ensure clarity. To support systematic evaluation, we build MMMC, a benchmark of professionally produced, discipline-specific educational videos. We evaluate MMMC across diverse dimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and particularly, TeachQuiz, a novel end-to-end metric that quantifies how well a VLM, after unlearning, can recover knowledge by watching the generated videos. Our results demonstrate the potential of Code2Video as a scalable, interpretable, and controllable approach, achieving 40% improvement over direct code generation and producing videos comparable to human-crafted tutorials. The code and datasets are available at https://github.com/showlab/Code2Video.<br>
<span id='abs_ch'>中文摘要：Code2Video是一个基于代 的框架，通过三个协作智能体将教学指令转化为可执行的Python代 来生成教育视频，在质量和效率上相比 统方法实现显著提升。</span><br>
<span id='abs_en'>English Summary: Code2Video is a code-driven framework that uses collaborative agents to generate educational videos through executable Python code, achieving significant improvements in quality and efficiency over traditional methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2510.01171.pdf' target='_blank'>https://arxiv.org/pdf/2510.01171.pdf</a></span>   <span><a href='https://github.com/CHATS-lab/verbalize-sampling' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Zhang, Simon Yu, Derek Chong, Anthony Sicilia, Michael R. Tomz, Christopher D. Manning, Weiyan Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01171">Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Post-training alignment often reduces LLM diversity, leading to a phenomenon known as mode collapse. Unlike prior work that attributes this effect to algorithmic limitations, we identify a fundamental, pervasive data-level driver: typicality bias in preference data, whereby annotators systematically favor familiar text as a result of well-established findings in cognitive psychology. We formalize this bias theoretically, verify it on preference datasets empirically, and show that it plays a central role in mode collapse. Motivated by this analysis, we introduce Verbalized Sampling, a simple, training-free prompting strategy to circumvent mode collapse. VS prompts the model to verbalize a probability distribution over a set of responses (e.g., "Generate 5 jokes about coffee and their corresponding probabilities"). Comprehensive experiments show that VS significantly improves performance across creative writing (poems, stories, jokes), dialogue simulation, open-ended QA, and synthetic data generation, without sacrificing factual accuracy and safety. For instance, in creative writing, VS increases diversity by 1.6-2.1x over direct prompting. We further observe an emergent trend that more capable models benefit more from VS. In sum, our work provides a new data-centric perspective on mode collapse and a practical inference-time remedy that helps unlock pre-trained generative diversity.<br>
<span id='abs_ch'>中文: 训练后对齐 偏好数据中的典型性偏见导致LLM模式崩溃，我们提出的言语化采 方法 需额外训练，通过提示策略显著提升多 性，同时保持准确性和安全性。</span><br>
<span id='abs_en'>English: Post-training alignment causes mode collapse in LLMs due to typicality bias in preference data, which we address with Verbalized Sampling, a training-free prompting method that significantly enhances diversity without compromising accuracy or safety.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2510.01167.pdf' target='_blank'>https://arxiv.org/pdf/2510.01167.pdf</a></span>   <span><a href='https://github.com/pearls-lab/multiobj-align' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiran Shen, Yu Xia, Jonathan Chang, Prithviraj Ammanabrolu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01167">Simultaneous Multi-objective Alignment Across Verifiable and Non-verifiable Rewards</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Aligning large language models to human preferences is inherently multidimensional, yet most pipelines collapse heterogeneous signals into a single optimizeable objective. We seek to answer what it would take to simultaneously align a model across various domains spanning those with: verifiable rewards (mathematical accuracy), non-verifiable subjective preferences (human values), and complex interactive scenarios (multi-turn AI tutoring dialogues). Such multi-objective reinforcement learning setups are often plagued by the individual objectives being at odds with each other, resulting in inefficient training and little user control during inference. We propose a unified framework that: (i) standardizes {process reward model} (PRM) training across both verifiable and non-verifiable settings to better supervise models' chain-of-thought reasoning; (ii) performs {multi-objective alignment} by training the LLM with our $\textbf{M}$ulti-$\textbf{A}$ction-$\textbf{H}$ead $\textbf{DPO}$ (MAH-DPO) and a vectorized reward where the dimensions of the vector correspond to the various objectives instead of a single scalar; and (iii) demonstrates how such a system provides fine-grained inference-time user control. Experiments across math reasoning, value alignment, and multi-turn dialogue show that our framework improves performance across multiple objectives simultaneously, while minimizing cross-objective trade-offs and enabling flexible inference time user control. The code can be found at https://github.com/pearls-lab/multiobj-align.<br>
<span id='abs_ch'>Chinese: 本文提出了一种统一框架，通过 准化过程奖励模型训练、采用带向量化奖励的多动作头DPO算法，并在推理时实现细粒度用户控制，从而在多领域同时提升模型性能并最小化目 间权衡。</span><br>
<span id='abs_en'>English: This paper introduces a unified framework for multi-objective alignment of large language models that standardizes process reward model training, employs a multi-action-head DPO with vectorized rewards, and enables fine-grained user control during inference to simultaneously improve performance across diverse domains while minimizing trade-offs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2510.01146.pdf' target='_blank'>https://arxiv.org/pdf/2510.01146.pdf</a></span>   <span><a href='https://github.com/rubricreward/mr3' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>David Anugraha, Shou-Yi Hung, Zilu Tang, Annie En-Shiun Lee, Derry Tanti Wijaya, Genta Indra Winata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01146">mR3: Multilingual Rubric-Agnostic Reward Reasoning Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Evaluation using Large Language Model (LLM) judges has been widely adopted in English and shown to be effective for automatic evaluation. However, their performance does not generalize well to non-English settings, and it remains unclear what constitutes effective multilingual training for such judges. In this paper, we introduce mR3, a massively multilingual, rubric-agnostic reward reasoning model trained on 72 languages, achieving the broadest language coverage in reward modeling to date. We present a comprehensive study of data and curriculum selection for training to identify effective strategies and data sources for building high-quality reward models, including the integration of target-language reasoning datasets. Our approach attains state-of-the-art performance on multilingual reward model benchmarks, surpassing much larger models (i.e., GPT-OSS-120B) while being up to 9x smaller, and its effectiveness is further confirmed through extensive ablation studies. Our models, data, and code are available as open source at https://github.com/rubricreward/mr3.<br>
<span id='abs_ch'>Chinese Summary: 本 究推出了mR3，一个在72种语言上训练的大规模多语言奖励推理模型，在基准测试中实现了最先进的性能，同时模型规模远小于大型模型，并通过广泛的消融 究验证了其有效性。English Summary: The study introduces mR3, a highly efficient multilingual reward reasoning model trained across 72 languages, which achieves state-of-the-art performance on benchmarks while being significantly smaller than larger models, with its effectiveness validated through comprehensive ablation studies.Paperid:365,https://arxiv.org/pdf/2510.01132.pdfGitHub</span><br>
<span id='abs_en'>English Summary: The study introduces mR3, a highly efficient multilingual reward reasoning model trained across 72 languages, which achieves state-of-the-art performance on benchmarks while being significantly smaller than larger models, with its effectiveness validated through comprehensive ablation studies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2510.01132.pdf' target='_blank'>https://arxiv.org/pdf/2510.01132.pdf</a></span>   <span><a href='https://github.com/pearls-lab/meow-tea-taro' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiyi Wang, Prithviraj Ammanabrolu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01132">A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We study what actually works and what doesn't for training large language models as agents via multi-turn reinforcement learning. Despite rapid progress, existing frameworks and definitions are fragmented, and there is no systematic formulation or analysis of which design choices matter across tasks. We address this gap by first breaking down the design space into three inter-related pillars -- environment, reward, and policy -- and empirically derive a recipe for training LLM agents in situated textual domains. In particular, we test TextWorld and ALFWorld, popular domains for testing situated embodied reasoning, as well as SWE-Gym for more software engineering style tasks. (i) For the environment, we analyze the impacts of task complexity in terms of sizes of the state and action spaces as well as optimal solution length, finding that even simple environments within a domain can provide signal on how well an agent can generalize to more complex tasks. (ii) For the reward, we ablate relative reward sparsity, observing that while dense turn-level rewards accelerate training, performance and stability is highly dependent on the choice of RL algorithm. (iii) And for the agent's policy, we explore the interplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO) policy gradient methods in addition to showing how to find the optimal Supervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We distill these findings into a training recipe that guides co-design across the three pillars, facilitating research and practical efforts in multi-turn agentic RL. Code: https://github.com/pearls-lab/meow-tea-taro<br>
<span id='abs_ch'>中文: 本 究通过多轮强化学 系统分析了训练大型语言模型智能体的设计要 ，重点关注环境、奖励和策略在不同领域中的相互作用，并提出了一套优化训练方案。English: This study systematically analyzes the design choices for training large language model agents through multi-turn reinforcement learning, focusing on the interplay between environment, reward, and policy components across different domains.Paperid:366,https://arxiv.org/pdf/2510.01083.pdfGitHub</span><br>
<span id='abs_en'>English: This study systematically analyzes the design choices for training large language model agents through multi-turn reinforcement learning, focusing on the interplay between environment, reward, and policy components across different domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2510.01077.pdf' target='_blank'>https://arxiv.org/pdf/2510.01077.pdf</a></span>   <span><a href='https://github.com/danielebifolco/CodeGenLink' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniele Bifolco, Guido Annicchiarico, Pierluigi Barbiero, Massimiliano Di Penta, Fiorella Zampetti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01077">CodeGenLink: A Tool to Find the Likely Origin and License of Automatically Generated Code</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) are widely used in software development tasks nowadays. Unlike reusing code taken from the Web, for LLMs' generated code, developers are concerned about its lack of trustworthiness and possible copyright or licensing violations, due to the lack of code provenance information. This paper proposes CodeGenLink, a GitHub CoPilot extension for Visual Studio Code aimed at (i) suggesting links containing code very similar to automatically generated code, and (ii) whenever possible, indicating the license of the likely origin of the code. CodeGenLink retrieves candidate links by combining LLMs with their web search features and then performs similarity analysis between the generated and retrieved code. Preliminary results show that CodeGenLink effectively filters unrelated links via similarity analysis and provides licensing information when available. Tool URL: https://github.com/danielebifolco/CodeGenLink Tool Video: https://youtu.be/M6nqjBf9_pw<br>
<span id='abs_ch'>中文摘要：本文提出CodeGenLink，一个Visual Studio Code扩展插件，通过结合网络搜索和相似性分析，为LLM生成的代 提供相似代 链接及许可信息，以解决其可信度和版权问题。</span><br>
<span id='abs_en'>English Summary: This paper introduces CodeGenLink, a Visual Studio Code extension that addresses concerns about LLM-generated code's trustworthiness and licensing by linking it to similar online code and providing license information through a combination of web search and similarity analysis.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2510.00922.pdf' target='_blank'>https://arxiv.org/pdf/2510.00922.pdf</a></span>   <span><a href='https://github.com/shshnkreddy/DAIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shashank Reddy Chirra, Jayden Teoh, Praveen Paruchuri, Pradeep Varakantham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00922">On Discovering Algorithms for Adversarial Imitation Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Adversarial Imitation Learning (AIL) methods, while effective in settings with limited expert demonstrations, are often considered unstable. These approaches typically decompose into two components: Density Ratio (DR) estimation $\frac{ρ_E}{ρ_π}$, where a discriminator estimates the relative occupancy of state-action pairs under the policy versus the expert; and Reward Assignment (RA), where this ratio is transformed into a reward signal used to train the policy. While significant research has focused on improving density estimation, the role of reward assignment in influencing training dynamics and final policy performance has been largely overlooked. RA functions in AIL are typically derived from divergence minimization objectives, relying heavily on human design and ingenuity. In this work, we take a different approach: we investigate the discovery of data-driven RA functions, i.e, based directly on the performance of the resulting imitation policy. To this end, we leverage an LLM-guided evolutionary framework that efficiently explores the space of RA functions, yielding \emph{Discovered Adversarial Imitation Learning} (DAIL), the first meta-learnt AIL algorithm. Remarkably, DAIL generalises across unseen environments and policy optimization algorithms, outperforming the current state-of-the-art of \emph{human-designed} baselines. Finally, we analyse why DAIL leads to more stable training, offering novel insights into the role of RA functions in the stability of AIL. Code is publicly available: https://github.com/shshnkreddy/DAIL.<br>
<span id='abs_ch'>中文: 对抗性模仿学 方法在专家数据有限时有效但常不稳定，本 究通过引入DAIL算法，首次以数据驱动方式优化奖励分配函数，在跨环境泛化性和训练稳定性上均超越了人工设计的现有最佳方法。</span><br>
<span id='abs_en'>English: Adversarial Imitation Learning (AIL) methods, though effective with limited expert data, face instability issues primarily due to overlooked reward assignment functions, which this study addresses by introducing DAIL, a meta-learned algorithm that outperforms human-designed baselines and enhances training stability across diverse environments.</span>Paperid:371,https://arxiv.org/pdf/2510.00910.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2510.00862.pdf' target='_blank'>https://arxiv.org/pdf/2510.00862.pdf</a></span>   <span><a href='https://github.com/Ko-Lani/GSMamba' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Ko-Lani/GSMamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyun-kyu Ko, Youbin Kim, Jihyeon Park, Dongheok Park, Gyeongjin Kang, Wonjun Cho, Hyung Yi, Eunbyung Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00862">Gather-Scatter Mamba: Accelerating Propagation with Efficient State Space Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>State Space Models (SSMs)-most notably RNNs-have historically played a central role in sequential modeling. Although attention mechanisms such as Transformers have since dominated due to their ability to model global context, their quadratic complexity and limited scalability make them less suited for long sequences. Video super-resolution (VSR) methods have traditionally relied on recurrent architectures to propagate features across frames. However, such approaches suffer from well-known issues including vanishing gradients, lack of parallelism, and slow inference speed. Recent advances in selective SSMs like Mamba offer a compelling alternative: by enabling input-dependent state transitions with linear-time complexity, Mamba mitigates these issues while maintaining strong long-range modeling capabilities. Despite this potential, Mamba alone struggles to capture fine-grained spatial dependencies due to its causal nature and lack of explicit context aggregation. To address this, we propose a hybrid architecture that combines shifted window self-attention for spatial context aggregation with Mamba-based selective scanning for efficient temporal propagation. Furthermore, we introduce Gather-Scatter Mamba (GSM), an alignment-aware mechanism that warps features toward a center anchor frame within the temporal window before Mamba propagation and scatters them back afterward, effectively reducing occlusion artifacts and ensuring effective redistribution of aggregated information across all frames. The official implementation is provided at: https://github.com/Ko-Lani/GSMamba.<br>
<span id='abs_ch'>中文摘要：本文提出了一种混合架构，结合移位窗口自注意力进行空间上下文建模与基于Mamba的选择性扫描实现高效时序 播，并通过收集-散射Mamba机制减少视频超分辨率中的遮挡伪影。</span><br>
<span id='abs_en'>English Summary: This paper introduces a hybrid architecture combining shifted window self-attention for spatial context with Mamba-based selective scanning for efficient temporal propagation, along with a Gather-Scatter Mamba mechanism to reduce occlusion artifacts in video super-resolution.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2510.00805.pdf' target='_blank'>https://arxiv.org/pdf/2510.00805.pdf</a></span>   <span><a href='https://github.com/ZRNB/MG2FlowNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Zhu, Xuan Yu, Yudong Zhang, Chen Zhang, Xu Wang, Yang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00805">MG2FlowNet: Accelerating High-Reward Sample Generation via Enhanced MCTS and Greediness Control</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Generative Flow Networks (GFlowNets) have emerged as a powerful tool for generating diverse and high-reward structured objects by learning to sample from a distribution proportional to a given reward function. Unlike conventional reinforcement learning (RL) approaches that prioritize optimization of a single trajectory, GFlowNets seek to balance diversity and reward by modeling the entire trajectory distribution. This capability makes them especially suitable for domains such as molecular design and combinatorial optimization. However, existing GFlowNets sampling strategies tend to overexplore and struggle to consistently generate high-reward samples, particularly in large search spaces with sparse high-reward regions. Therefore, improving the probability of generating high-reward samples without sacrificing diversity remains a key challenge under this premise. In this work, we integrate an enhanced Monte Carlo Tree Search (MCTS) into the GFlowNets sampling process, using MCTS-based policy evaluation to guide the generation toward high-reward trajectories and Polynomial Upper Confidence Trees (PUCT) to balance exploration and exploitation adaptively, and we introduce a controllable mechanism to regulate the degree of greediness. Our method enhances exploitation without sacrificing diversity by dynamically balancing exploration and reward-driven guidance. The experimental results show that our method can not only accelerate the speed of discovering high-reward regions but also continuously generate high-reward samples, while preserving the diversity of the generative distribution. All implementations are available at https://github.com/ZRNB/MG2FlowNet.<br>
<span id='abs_ch'>中文: 本文通过将蒙特卡洛 搜索融入生成流网络，在保持多 性的同时提升高奖励 本的生成能力，实现了高质量 本的快速发现与持续输出。</span><br>
<span id='abs_en'>English: This paper enhances Generative Flow Networks by integrating Monte Carlo Tree Search to improve high-reward sample generation while maintaining diversity, achieving accelerated discovery and sustained output of quality samples.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2510.00726.pdf' target='_blank'>https://arxiv.org/pdf/2510.00726.pdf</a></span>   <span><a href='https://github.com/iit-DLSLab/croSTAta' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Giovanni Minelli, Giulio Turrisi, Victor Barasuol, Claudio Semini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00726">CroSTAta: Cross-State Transition Attention Transformer for Robotic Manipulation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Learning robotic manipulation policies through supervised learning from demonstrations remains challenging when policies encounter execution variations not explicitly covered during training. While incorporating historical context through attention mechanisms can improve robustness, standard approaches process all past states in a sequence without explicitly modeling the temporal structure that demonstrations may include, such as failure and recovery patterns. We propose a Cross-State Transition Attention Transformer that employs a novel State Transition Attention (STA) mechanism to modulate standard attention weights based on learned state evolution patterns, enabling policies to better adapt their behavior based on execution history. Our approach combines this structured attention with temporal masking during training, where visual information is randomly removed from recent timesteps to encourage temporal reasoning from historical context. Evaluation in simulation shows that STA consistently outperforms standard cross-attention and temporal modeling approaches like TCN and LSTM networks across all tasks, achieving more than 2x improvement over cross-attention on precision-critical tasks.<br>
<span id='abs_ch'>中文摘要：本 究提出的跨状态转换注意力变换器通过状态转换注意力机制学 状态演化模式，并在训练中采用时序掩 ，显著提升了机器人操作的鲁棒性，在仿真实验中全面优于现有方法。</span><br>
<span id='abs_en'>English Summary: The proposed Cross-State Transition Attention Transformer enhances robotic manipulation by incorporating a State Transition Attention mechanism that learns from state evolution patterns and employs temporal masking during training, significantly outperforming existing methods in simulation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2510.00658.pdf' target='_blank'>https://arxiv.org/pdf/2510.00658.pdf</a></span>   <span><a href='https://github.com/1202kbs/AYT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Beomsu Kim, Byunghee Cha, Jong Chul Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00658">Align Your Tangent: Training Better Consistency Models via Manifold-Aligned Tangents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With diffusion and flow matching models achieving state-of-the-art generating performance, the interest of the community now turned to reducing the inference time without sacrificing sample quality. Consistency Models (CMs), which are trained to be consistent on diffusion or probability flow ordinary differential equation (PF-ODE) trajectories, enable one or two-step flow or diffusion sampling. However, CMs typically require prolonged training with large batch sizes to obtain competitive sample quality. In this paper, we examine the training dynamics of CMs near convergence and discover that CM tangents -- CM output update directions -- are quite oscillatory, in the sense that they move parallel to the data manifold, not towards the manifold. To mitigate oscillatory tangents, we propose a new loss function, called the manifold feature distance (MFD), which provides manifold-aligned tangents that point toward the data manifold. Consequently, our method -- dubbed Align Your Tangent (AYT) -- can accelerate CM training by orders of magnitude and even out-perform the learned perceptual image patch similarity metric (LPIPS). Furthermore, we find that our loss enables training with extremely small batch sizes without compromising sample quality. Code: https://github.com/1202kbs/AYT<br>
<span id='abs_ch'>中文: 本文提出对齐切线（AYT）方法，通过引入流形特征距离损失来修正一致性模型中的振荡训练方向，大幅 速训练过程，并在小批量情况下保持 本质量。</span><br>
<span id='abs_en'>English: The paper introduces Align Your Tangent (AYT), a method that uses a manifold feature distance loss to correct oscillatory training directions in Consistency Models, significantly accelerating training and maintaining sample quality even with small batch sizes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2510.00627.pdf' target='_blank'>https://arxiv.org/pdf/2510.00627.pdf</a></span>   <span><a href='https://github.com/bingzhangw/CDDM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingzhang Wang, Kehua Chen, Yinhai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00627">Collaborative-Distilled Diffusion Models (CDDM) for Accelerated and Lightweight Trajectory Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Trajectory prediction is a fundamental task in Autonomous Vehicles (AVs) and Intelligent Transportation Systems (ITS), supporting efficient motion planning and real-time traffic safety management. Diffusion models have recently demonstrated strong performance in probabilistic trajectory prediction, but their large model size and slow sampling process hinder real-world deployment. This paper proposes Collaborative-Distilled Diffusion Models (CDDM), a novel method for real-time and lightweight trajectory prediction. Built upon Collaborative Progressive Distillation (CPD), CDDM progressively transfers knowledge from a high-capacity teacher diffusion model to a lightweight student model, jointly reducing both the number of sampling steps and the model size across distillation iterations. A dual-signal regularized distillation loss is further introduced to incorporate guidance from both the teacher and ground-truth data, mitigating potential overfitting and ensuring robust performance. Extensive experiments on the ETH-UCY pedestrian benchmark and the nuScenes vehicle benchmark demonstrate that CDDM achieves state-of-the-art prediction accuracy. The well-distilled CDDM retains 96.2% and 95.5% of the baseline model's ADE and FDE performance on pedestrian trajectories, while requiring only 231K parameters and 4 or 2 sampling steps, corresponding to 161x compression, 31x acceleration, and 9 ms latency. Qualitative results further show that CDDM generates diverse and accurate trajectories under dynamic agent behaviors and complex social interactions. By bridging high-performing generative models with practical deployment constraints, CDDM enables resource-efficient probabilistic prediction for AVs and ITS. Code is available at https://github.com/bingzhangw/CDDM.<br>
<span id='abs_ch'>中文: 本文提出的协作蒸馏扩散模型（CDDM）通过渐进式知识蒸馏，在保持顶尖预测精度的同时大幅压缩模型规模并减少采 步骤，实现了自动驾驶系统的高效轨迹预测。</span><br>
<span id='abs_en'>English: This paper introduces Collaborative-Distilled Diffusion Models (CDDM), a lightweight trajectory prediction method that achieves state-of-the-art accuracy while significantly reducing model size and sampling steps through progressive knowledge distillation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2510.00549.pdf' target='_blank'>https://arxiv.org/pdf/2510.00549.pdf</a></span>   <span><a href='https://github.com/AITRICS/EMR-AGENT/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kwanhyung Lee, Sungsoo Hong, Joonhyung Park, Jeonghyeop Lim, Juhwan Choi, Donghwee Yoon, Eunho Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00549">EMR-AGENT: Automating Cohort and Feature Extraction from EMR Databases</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Machine learning models for clinical prediction rely on structured data extracted from Electronic Medical Records (EMRs), yet this process remains dominated by hardcoded, database-specific pipelines for cohort definition, feature selection, and code mapping. These manual efforts limit scalability, reproducibility, and cross-institutional generalization. To address this, we introduce EMR-AGENT (Automated Generalized Extraction and Navigation Tool), an agent-based framework that replaces manual rule writing with dynamic, language model-driven interaction to extract and standardize structured clinical data. Our framework automates cohort selection, feature extraction, and code mapping through interactive querying of databases. Our modular agents iteratively observe query results and reason over schema and documentation, using SQL not just for data retrieval but also as a tool for database observation and decision making. This eliminates the need for hand-crafted, schema-specific logic. To enable rigorous evaluation, we develop a benchmarking codebase for three EMR databases (MIMIC-III, eICU, SICdb), including both seen and unseen schema settings. Our results demonstrate strong performance and generalization across these databases, highlighting the feasibility of automating a process previously thought to require expert-driven design. The code will be released publicly at https://github.com/AITRICS/EMR-AGENT/tree/main. For a demonstration, please visit our anonymous demo page: https://anonymoususer-max600.github.io/EMR_AGENT/<br>
<span id='abs_ch'>中文摘要：我们提出了EMR-AGENT框架，利用语言模型自动从电子病历中提取临床数据，取代了人工操作，并在多个数据库上展现出优秀的泛化能力。</span><br>
<span id='abs_en'>English Summary: We introduce EMR-AGENT, an automated framework using language models to dynamically extract clinical data from EMRs, eliminating manual processes and demonstrating strong generalization across multiple databases.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2510.00508.pdf' target='_blank'>https://arxiv.org/pdf/2510.00508.pdf</a></span>   <span><a href='https://github.com/longyongchao/CopyPasteLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongchao Long, Xian Wu, Yingying Zhang, Xianbin Wen, Yuxi Zhou, Shenda Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00508">Copy-Paste to Mitigate Large Language Model Hallucinations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to generate contextually grounded responses, contextual faithfulness remains challenging as LLMs may not consistently trust provided context, leading to hallucinations that undermine reliability. We observe an inverse correlation between response copying degree and context-unfaithful hallucinations on RAGTruth, suggesting that higher copying degrees reduce hallucinations by fostering genuine contextual belief. We propose CopyPasteLLM, obtained through two-stage high-copying response preference training. We design three prompting methods to enhance copying degree, demonstrating that high-copying responses achieve superior contextual faithfulness and hallucination control. These approaches enable a fully automated pipeline that transforms generated responses into high-copying preference data for training CopyPasteLLM. On FaithEval, ConFiQA and PubMedQA, CopyPasteLLM achieves best performance in both counterfactual and original contexts, remarkably with 12.2% to 24.5% accuracy improvements on FaithEval over the best baseline, while requiring only 365 training samples -- 1/50th of baseline data. To elucidate CopyPasteLLM's effectiveness, we propose the Context-Parameter Copying Capturing algorithm. Interestingly, this reveals that CopyPasteLLM recalibrates reliance on internal parametric knowledge rather than external knowledge during generation. All codes are available at https://github.com/longyongchao/CopyPasteLLM<br>
<span id='abs_ch'>中文: CopyPasteLLM通过增强对上下文的复制程度来提高检索增强生成的 实度，有效减少幻觉现象，并以极少的训练数据实现卓越性能。English: CopyPasteLLM enhances contextual faithfulness in retrieval-augmented generation by promoting higher copying of provided context, reducing hallucinations and achieving superior performance with minimal training data.Paperid:387,https://arxiv.org/pdf/2510.00500.pdfGitHub</span><br>
<span id='abs_en'>English: CopyPasteLLM enhances contextual faithfulness in retrieval-augmented generation by promoting higher copying of provided context, reducing hallucinations and achieving superior performance with minimal training data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2510.00500.pdf' target='_blank'>https://arxiv.org/pdf/2510.00500.pdf</a></span>   <span><a href='https://github.com/zkqq/BMCMat' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiqi Zhang, Mingguan Yang, Dali Chang, Chun Chen, Yuxiang Zhang, Kexun He, Jing Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00500">Relative-Absolute Fusion: Rethinking Feature Extraction in Image-Based Iterative Method Selection for Solving Sparse Linear Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Iterative method selection is crucial for solving sparse linear systems because these methods inherently lack robustness. Though image-based selection approaches have shown promise, their feature extraction techniques might encode distinct matrices into identical image representations, leading to the same selection and suboptimal method. In this paper, we introduce RAF (Relative-Absolute Fusion), an efficient feature extraction technique to enhance image-based selection approaches. By simultaneously extracting and fusing image representations as relative features with corresponding numerical values as absolute features, RAF achieves comprehensive matrix representations that prevent feature ambiguity across distinct matrices, thus improving selection accuracy and unlocking the potential of image-based selection approaches. We conducted comprehensive evaluations of RAF on SuiteSparse and our developed BMCMat (Balanced Multi-Classification Matrix dataset), demonstrating solution time reductions of 0.08s-0.29s for sparse linear systems, which is 5.86%-11.50% faster than conventional image-based selection approaches and achieves state-of-the-art (SOTA) performance. BMCMat is available at https://github.com/zkqq/BMCMat.<br>
<span id='abs_ch'>中文: 本文提出RAF特征提取技术，通过融合相对图像特征与绝对数值特征来避免矩阵表示的模糊性，从而提升稀疏线性系统迭代方法选择的准确性，实现了显著的速度提升和最优性能。</span><br>
<span id='abs_en'>English: The paper introduces RAF, a feature extraction technique that fuses relative image features with absolute numerical values to prevent ambiguity in matrix representations, thereby improving the accuracy of iterative method selection for sparse linear systems and achieving state-of-the-art performance with significant speed improvements.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2510.00495.pdf' target='_blank'>https://arxiv.org/pdf/2510.00495.pdf</a></span>   <span><a href='https://github.com/JasonKyng/NAGL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuexin Wang, Xiaolei Wang, Yizheng Gong, Jimin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00495">Normal-Abnormal Guided Generalist Anomaly Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Generalist Anomaly Detection (GAD) aims to train a unified model on an original domain that can detect anomalies in new target domains. Previous GAD methods primarily use only normal samples as references, overlooking the valuable information contained in anomalous samples that are often available in real-world scenarios. To address this limitation, we propose a more practical approach: normal-abnormal-guided generalist anomaly detection, which leverages both normal and anomalous samples as references to guide anomaly detection across diverse domains. We introduce the Normal-Abnormal Generalist Learning (NAGL) framework, consisting of two key components: Residual Mining (RM) and Anomaly Feature Learning (AFL). RM extracts abnormal patterns from normal-abnormal reference residuals to establish transferable anomaly representations, while AFL adaptively learns anomaly features in query images through residual mapping to identify instance-aware anomalies. Our approach effectively utilizes both normal and anomalous references for more accurate and efficient cross-domain anomaly detection. Extensive experiments across multiple benchmarks demonstrate that our method significantly outperforms existing GAD approaches. This work represents the first to adopt a mixture of normal and abnormal samples as references in generalist anomaly detection. The code and datasets are available at https://github.com/JasonKyng/NAGL.<br>
<span id='abs_ch'>中文摘要：本文提出的正常-异常通用学 （NAGL）框架通过同时利用正常和异常 本作为参考，在跨领域异常检测中显著超越了仅使用正常 本的现有方法。</span><br>
<span id='abs_en'>English Summary: This paper introduces the Normal-Abnormal Generalist Learning (NAGL) framework, which leverages both normal and abnormal samples to significantly improve cross-domain anomaly detection performance over previous methods that used only normal references.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2510.00492.pdf' target='_blank'>https://arxiv.org/pdf/2510.00492.pdf</a></span>   <span><a href='https://github.com/db-Lee/Multi-RM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong Bok Lee, Seanie Lee, Sangwoo Park, Minki Kang, Jinheon Baek, Dongki Kim, Dominik Wagner, Jiongdao Jin, Heejun Lee, Tobias Bocklet, Jinyu Wang, Jingjing Fu, Sung Ju Hwang, Jiang Bian, Lei Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00492">Rethinking Reward Models for Multi-Domain Test-Time Scaling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The reliability of large language models (LLMs) during test-time scaling is often assessed with \emph{external verifiers} or \emph{reward models} that distinguish correct reasoning from flawed logic. Prior work generally assumes that process reward models (PRMs), which score every intermediate reasoning step, outperform outcome reward models (ORMs) that assess only the final answer. This view is based mainly on evidence from narrow, math-adjacent domains. We present the first unified evaluation of four reward model variants, discriminative ORM and PRM (\DisORM, \DisPRM) and generative ORM and PRM (\GenORM, \GenPRM), across 14 diverse domains. Contrary to conventional wisdom, we find that (i) \DisORM performs on par with \DisPRM, (ii) \GenPRM is not competitive, and (iii) overall, \GenORM is the most robust, yielding significant and consistent gains across every tested domain. We attribute this to PRM-style stepwise scoring, which inherits label noise from LLM auto-labeling and has difficulty evaluating long reasoning trajectories, including those involving self-correcting reasoning. Our theoretical analysis shows that step-wise aggregation compounds errors as reasoning length grows, and our empirical observations confirm this effect. These findings challenge the prevailing assumption that fine-grained supervision is always better and support generative outcome verification for multi-domain deployment. We publicly release our code, datasets, and checkpoints at \href{https://github.com/db-Lee/Multi-RM}{\underline{\small\texttt{https://github.com/db-Lee/Multi-RM}}} to facilitate future research in multi-domain settings.<br>
<span id='abs_ch'>中文: 本 究挑战了过程奖励模型（PRM）优于结果奖励模型（ORM）的 统观点，通过14个不同领域的测试证明生成式ORM最具鲁棒性，而PRM 易受 签噪声和长推理链错误累积影响而表现不佳。</span><br>
<span id='abs_en'>English: This study challenges the prevailing assumption that process reward models (PRMs) are superior to outcome reward models (ORMs) by demonstrating that generative ORMs are the most robust across 14 diverse domains, due to PRMs' susceptibility to label noise and error accumulation in long reasoning chains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2510.00485.pdf' target='_blank'>https://arxiv.org/pdf/2510.00485.pdf</a></span>   <span><a href='https://github.com/yujxx/PodEval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujia Xiao, Liumeng Xue, Lei He, Xinyi Chen, Aemon Yat Fei Chiu, Wenjie Tian, Shaofei Zhang, Qiuqiang Kong, Xinfa Zhu, Wei Xue, Tan Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00485">PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, an increasing number of multimodal (text and audio) benchmarks have emerged, primarily focusing on evaluating models' understanding capability. However, exploration into assessing generative capabilities remains limited, especially for open-ended long-form content generation. Significant challenges lie in no reference standard answer, no unified evaluation metrics and uncontrollable human judgments. In this work, we take podcast-like audio generation as a starting point and propose PodEval, a comprehensive and well-designed open-source evaluation framework. In this framework: 1) We construct a real-world podcast dataset spanning diverse topics, serving as a reference for human-level creative quality. 2) We introduce a multimodal evaluation strategy and decompose the complex task into three dimensions: text, speech and audio, with different evaluation emphasis on "Content" and "Format". 3) For each modality, we design corresponding evaluation methods, involving both objective metrics and subjective listening test. We leverage representative podcast generation systems (including open-source, close-source, and human-made) in our experiments. The results offer in-depth analysis and insights into podcast generation, demonstrating the effectiveness of PodEval in evaluating open-ended long-form audio. This project is open-source to facilitate public use: https://github.com/yujxx/PodEval.<br>
<span id='abs_ch'>中文: 本文提出了PodEval开源框架，用于评估开放式长音频生成（如播客），通过结合文本、语音和音频的多模态评估策略，解决了缺乏参考 准和统一指 等挑战。</span><br>
<span id='abs_en'>English: This paper introduces PodEval, an open-source framework for evaluating open-ended long-form audio generation like podcasts, addressing challenges such as the lack of reference standards and unified metrics by incorporating multimodal evaluation strategies across text, speech, and audio dimensions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2510.00461.pdf' target='_blank'>https://arxiv.org/pdf/2510.00461.pdf</a></span>   <span><a href='https://github.com/showmeon/TimeEmb' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingyuan Xia, Chunxu Zhang, Zijian Zhang, Hao Miao, Qidong Liu, Yuanshao Zhu, Bo Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00461">TimeEmb: A Lightweight Static-Dynamic Disentanglement Framework for Time Series Forecasting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Temporal non-stationarity, the phenomenon that time series distributions change over time, poses fundamental challenges to reliable time series forecasting. Intuitively, the complex time series can be decomposed into two factors, \ie time-invariant and time-varying components, which indicate static and dynamic patterns, respectively. Nonetheless, existing methods often conflate the time-varying and time-invariant components, and jointly learn the combined long-term patterns and short-term fluctuations, leading to suboptimal performance facing distribution shifts. To address this issue, we initiatively propose a lightweight static-dynamic decomposition framework, TimeEmb, for time series forecasting. TimeEmb innovatively separates time series into two complementary components: (1) time-invariant component, captured by a novel global embedding module that learns persistent representations across time series, and (2) time-varying component, processed by an efficient frequency-domain filtering mechanism inspired by full-spectrum analysis in signal processing. Experiments on real-world datasets demonstrate that TimeEmb outperforms state-of-the-art baselines and requires fewer computational resources. We conduct comprehensive quantitative and qualitative analyses to verify the efficacy of static-dynamic disentanglement. This lightweight framework can also improve existing time-series forecasting methods with simple integration. To ease reproducibility, the code is available at https://github.com/showmeon/TimeEmb.<br>
<span id='abs_ch'>Chinese: TimeEmb框架通过全局嵌入和频域滤波将时间序列分解为静态与动态成分，有效应对时序非平稳性挑战，在降低计算资源的同时实现了更优的预测性能。</span><br>
<span id='abs_en'>English: The TimeEmb framework addresses temporal non-stationarity in time series forecasting by decomposing data into static and dynamic components using global embeddings and frequency-domain filtering, achieving superior performance with reduced computational costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2510.00428.pdf' target='_blank'>https://arxiv.org/pdf/2510.00428.pdf</a></span>   <span><a href='https://github.com/vuno/contextualized-srrg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seongjae Kang, Dong Bok Lee, Juho Jung, Dongseop Kim, Won Hwa Kim, Sunghoon Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00428">Automated Structured Radiology Report Generation with Rich Clinical Context</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Automated structured radiology report generation (SRRG) from chest X-ray images offers significant potential to reduce workload of radiologists by generating reports in structured formats that ensure clarity, consistency, and adherence to clinical reporting standards. While radiologists effectively utilize available clinical contexts in their diagnostic reasoning, existing SRRG systems overlook these essential elements. This fundamental gap leads to critical problems including temporal hallucinations when referencing non-existent clinical contexts. To address these limitations, we propose contextualized SRRG (C-SRRG) that comprehensively incorporates rich clinical context for SRRG. We curate C-SRRG dataset by integrating comprehensive clinical context encompassing 1) multi-view X-ray images, 2) clinical indication, 3) imaging techniques, and 4) prior studies with corresponding comparisons based on patient histories. Through extensive benchmarking with state-of-the-art multimodal large language models, we demonstrate that incorporating clinical context with the proposed C-SRRG significantly improves report generation quality. We publicly release dataset, code, and checkpoints to facilitate future research for clinically-aligned automated RRG at https://github.com/vuno/contextualized-srrg.<br>
<span id='abs_ch'>中文: 提出的情境化结构化放射学报告生成（C-SRRG）通过整合多视角图像、临床指征、成像技术和既往 究等完整临床背景，解决了现有自动系统的局限性，显著提升了报告生成质量。</span><br>
<span id='abs_en'>English: The proposed contextualized structured radiology report generation (C-SRRG) integrates comprehensive clinical context to address limitations in existing automated systems, significantly improving report quality by incorporating multi-view images, clinical indications, imaging techniques, and prior studies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2510.00416.pdf' target='_blank'>https://arxiv.org/pdf/2510.00416.pdf</a></span>   <span><a href='https://github.com/snuh-rad-aicon/Interactive-MEN-RT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhyeok Lee, Han Jang, Kyu Sung Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00416">Domain-Specialized Interactive Segmentation Framework for Meningioma Radiotherapy Planning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Precise delineation of meningiomas is crucial for effective radiotherapy (RT) planning, directly influencing treatment efficacy and preservation of adjacent healthy tissues. While automated deep learning approaches have demonstrated considerable potential, achieving consistently accurate clinical segmentation remains challenging due to tumor heterogeneity. Interactive Medical Image Segmentation (IMIS) addresses this challenge by integrating advanced AI techniques with clinical input. However, generic segmentation tools, despite widespread applicability, often lack the specificity required for clinically critical and disease-specific tasks like meningioma RT planning. To overcome these limitations, we introduce Interactive-MEN-RT, a dedicated IMIS tool specifically developed for clinician-assisted 3D meningioma segmentation in RT workflows. The system incorporates multiple clinically relevant interaction methods, including point annotations, bounding boxes, lasso tools, and scribbles, enhancing usability and clinical precision. In our evaluation involving 500 contrast-enhanced T1-weighted MRI scans from the BraTS 2025 Meningioma RT Segmentation Challenge, Interactive-MEN-RT demonstrated substantial improvement compared to other segmentation methods, achieving Dice similarity coefficients of up to 77.6\% and Intersection over Union scores of 64.8\%. These results emphasize the need for clinically tailored segmentation solutions in critical applications such as meningioma RT planning. The code is publicly available at: https://github.com/snuh-rad-aicon/Interactive-MEN-RT<br>
<span id='abs_ch'>中文: Interactive-MEN-RT是一款专为脑膜瘤放疗规划设计的交互式分割工具，通过结合临床医生操作与人工智能技术，在评估中显著提升了分割精度并取得了最优性能指 。</span><br>
<span id='abs_en'>English: Interactive-MEN-RT is a specialized interactive tool that enhances meningioma segmentation accuracy in radiotherapy planning by integrating clinician input with AI, achieving superior performance metrics in clinical evaluations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2510.00261.pdf' target='_blank'>https://arxiv.org/pdf/2510.00261.pdf</a></span>   <span><a href='https://github.com/willxxy/ECG-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Song, William Han, Tony Chen, Chaojing Duan, Michael A. Rosenberg, Emerson Liu, Ding Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00261">Retrieval-Augmented Generation for Electrocardiogram-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Interest in generative Electrocardiogram-Language Models (ELMs) is growing, as they can produce textual responses conditioned on ECG signals and textual queries. Unlike traditional classifiers that output label probabilities, ELMs are more versatile, supporting domain-specific tasks (e.g., waveform analysis, diagnosis, prognosis) as well as general tasks (e.g., open-ended questions, dialogue). Retrieval-Augmented Generation (RAG), widely used in Large Language Models (LLMs) to ground LLM outputs in retrieved knowledge, helps reduce hallucinations and improve natural language generation (NLG). However, despite its promise, no open-source implementation or systematic study of RAG pipeline design for ELMs currently exists. To address this gap, we present the first open-source RAG pipeline for ELMs, along with baselines and ablation studies for NLG. Experiments on three public datasets show that ELMs with RAG consistently improves performance over non-RAG baselines and highlights key ELM design considerations. Our code is available at: https://github.com/willxxy/ECG-Bench.<br>
<span id='abs_ch'>Chinese: 本文提出了首个面向心电图文生成模型的开源检索增强生成框架，通过在三个公开数据集上的实验证明，该框架能持续提升模型性能并揭示关键设计要 。</span><br>
<span id='abs_en'>English: This paper introduces the first open-source Retrieval-Augmented Generation (RAG) pipeline for generative Electrocardiogram-Language Models (ELMs), demonstrating through experiments on three public datasets that RAG consistently enhances ELM performance and provides key design insights.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2510.00237.pdf' target='_blank'>https://arxiv.org/pdf/2510.00237.pdf</a></span>   <span><a href='https://github.com/XiaofengLin7/debunking-sft-generalization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaofeng Lin, Hejian Sang, Zhipeng Wang, Xuezhou Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00237">Debunk the Myth of SFT Generalization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>A prevailing view holds that supervised fine-tuning (SFT) memorizes training data and fails to generalize, whereas reinforcement learning (RL) attains broader robustness. We revisit this claim through a systematic evaluation on two decision-making benchmarks, Sokoban and General Points, and arrive at a different conclusion. We show that much of SFT's perceived failure stems from frozen-prompt artifacts: when trained on fixed instruction templates, SFT models cling to training semantics rather than adapting to new ones. Introducing prompt diversity during training breaks this shortcut and yields strong generalization to unseen instruction variants without harming in-distribution performance. Beyond instruction shifts, we ask whether SFT can generalize to strictly harder tasks. Here, chain-of-thought (CoT) supervision provides an algorithmic scaffold that markedly improves transfer to more difficult regimes, such as larger Sokoban grids with additional boxes and arithmetic with out-of-distribution values or five-card compositions that increase combinatorial complexity. Finally, combining prompt diversity with CoT achieves the best of both worlds: robust generalization across both instruction-variant and difficulty-variant settings, matching or surpassing RL baselines on our benchmarks while retaining SFT's simplicity and stability. These findings challenge the narrative that SFT is inherently inferior to RL and support a data-centric perspective: with appropriately curated demonstrations, vanilla SFT can generalize as strongly as RL. Code reproducing the results in the paper can be found at: https://github.com/XiaofengLin7/debunking-sft-generalization.<br>
<span id='abs_ch'>中文摘要：本 究挑战了监督微调(SFT)固有泛化能力不足的观点，证明通过提示多 性和思维链监督，SFT在指令变化和难度变化的场景中均能实现与强化学 相当或更优的鲁棒性能。</span><br>
<span id='abs_en'>English Summary: This study challenges the notion that supervised fine-tuning (SFT) inherently fails to generalize, demonstrating that with prompt diversity and chain-of-thought supervision, SFT achieves robust performance matching or surpassing reinforcement learning across instruction and difficulty variations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2510.00225.pdf' target='_blank'>https://arxiv.org/pdf/2510.00225.pdf</a></span>   <span><a href='https://github.com/mengyuest/TGPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Meng, Fei Chen, Chuchu Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00225">TGPO: Temporal Grounded Policy Optimization for Signal Temporal Logic Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Learning control policies for complex, long-horizon tasks is a central challenge in robotics and autonomous systems. Signal Temporal Logic (STL) offers a powerful and expressive language for specifying such tasks, but its non-Markovian nature and inherent sparse reward make it difficult to be solved via standard Reinforcement Learning (RL) algorithms. Prior RL approaches focus only on limited STL fragments or use STL robustness scores as sparse terminal rewards. In this paper, we propose TGPO, Temporal Grounded Policy Optimization, to solve general STL tasks. TGPO decomposes STL into timed subgoals and invariant constraints and provides a hierarchical framework to tackle the problem. The high-level component of TGPO proposes concrete time allocations for these subgoals, and the low-level time-conditioned policy learns to achieve the sequenced subgoals using a dense, stage-wise reward signal. During inference, we sample various time allocations and select the most promising assignment for the policy network to rollout the solution trajectory. To foster efficient policy learning for complex STL with multiple subgoals, we leverage the learned critic to guide the high-level temporal search via Metropolis-Hastings sampling, focusing exploration on temporally feasible solutions. We conduct experiments on five environments, ranging from low-dimensional navigation to manipulation, drone, and quadrupedal locomotion. Under a wide range of STL tasks, TGPO significantly outperforms state-of-the-art baselines (especially for high-dimensional and long-horizon cases), with an average of 31.6% improvement in task success rate compared to the best baseline. The code will be available at https://github.com/mengyuest/TGPO<br>
<span id='abs_ch'>中文摘要：TGPO提出了一种分层强化学 框架，通过将时序逻辑任务分解为定时子目 ，结合高层时间分配与底层策略学 ，在多种机器人环境中显著优于现有最优方法。</span><br>
<span id='abs_en'>English Summary: TGPO is a hierarchical reinforcement learning framework that decomposes Signal Temporal Logic tasks into timed subgoals, using a high-level temporal allocator and low-level policy with dense rewards to significantly outperform existing methods across various robotic environments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2510.00206.pdf' target='_blank'>https://arxiv.org/pdf/2510.00206.pdf</a></span>   <span><a href='https://github.com/CentML/lorafusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhanda Zhu, Qidong Su, Yaoyao Ding, Kevin Song, Shang Wang, Gennady Pekhimenko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00206">LoRAFusion: Efficient LoRA Fine-Tuning for LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Low-Rank Adaptation (LoRA) has become the leading Parameter-Efficient Fine-Tuning (PEFT) method for Large Language Models (LLMs), as it significantly reduces GPU memory usage while maintaining competitive fine-tuned model quality on downstream tasks. Despite these benefits, we identify two key inefficiencies in existing LoRA fine-tuning systems. First, they incur substantial runtime overhead due to redundant memory accesses on large activation tensors. Second, they miss the opportunity to concurrently fine-tune multiple independent LoRA adapters that share the same base model on the same set of GPUs. This leads to missed performance gains such as reduced pipeline bubbles, better communication overlap, and improved GPU load balance. To address these issues, we introduce LoRAFusion, an efficient LoRA fine-tuning system for LLMs. At the kernel level, we propose a graph-splitting method that fuses memory-bound operations. This design eliminates unnecessary memory accesses and preserves the performance of compute-bound GEMMs without incurring the cost of recomputation or synchronization. At the scheduling level, LoRAFusion introduces an adaptive batching algorithm for multi-job fine-tuning. It first splits LoRA adapters into groups to intentionally stagger batch execution across jobs, and then solves a bin-packing problem within each group to generate balanced, dependency-aware microbatches. LoRAFusion achieves up to $1.96\times$ ($1.47\times$ on average) end-to-end speedup compared to Megatron-LM, and up to $1.46\times$ ($1.29\times$ on average) improvement over mLoRA, the state-of-the-art multi-LoRA fine-tuning system. Our fused kernel achieves up to $1.39\times$ ($1.27\times$ on average) kernel performance improvement and can directly serve as a plug-and-play replacement in existing LoRA systems. We open-source LoRAFusion at https://github.com/CentML/lorafusion.<br>
<br>
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2510.00080.pdf' target='_blank'>https://arxiv.org/pdf/2510.00080.pdf</a></span>   <span><a href='https://github.com/antman9914/SoREX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanze Guo, Yijun Ma, Xiao Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00080">SoREX: Towards Self-Explainable Social Recommendation with Relevant Ego-Path Extraction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Social recommendation has been proven effective in addressing data sparsity in user-item interaction modeling by leveraging social networks. The recent integration of Graph Neural Networks (GNNs) has further enhanced prediction accuracy in contemporary social recommendation algorithms. However, many GNN-based approaches in social recommendation lack the ability to furnish meaningful explanations for their predictions. In this study, we confront this challenge by introducing SoREX, a self-explanatory GNN-based social recommendation framework. SoREX adopts a two-tower framework enhanced by friend recommendation, independently modeling social relations and user-item interactions, while jointly optimizing an auxiliary task to reinforce social signals. To offer explanations, we propose a novel ego-path extraction approach. This method involves transforming the ego-net of a target user into a collection of multi-hop ego-paths, from which we extract factor-specific and candidate-aware ego-path subsets as explanations. This process facilitates the summarization of detailed comparative explanations among different candidate items through intricate substructure analysis. Furthermore, we conduct explanation re-aggregation to explicitly correlate explanations with downstream predictions, imbuing our framework with inherent self-explainability. Comprehensive experiments conducted on four widely adopted benchmark datasets validate the effectiveness of SoREX in predictive accuracy. Additionally, qualitative and quantitative analyses confirm the efficacy of the extracted explanations in SoREX. Our code and data are available at https://github.com/antman9914/SoREX.<br>
<span id='abs_ch'>中文: SoREX是一种基于图神经网络的自解释社交推荐框架，通过独立建模社交关系和用户-物品交互，并利用自我路径提取与重聚合生成解释，在保证预测精度的同时实现了有效的可解释性。</span><br>
<span id='abs_en'>English: SoREX is a self-explanatory GNN-based social recommendation framework that independently models social relations and user-item interactions while generating explanations through ego-path extraction and re-aggregation, achieving both high predictive accuracy and effective interpretability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2510.00054.pdf' target='_blank'>https://arxiv.org/pdf/2510.00054.pdf</a></span>   <span><a href='https://github.com/Tennine2077/HiDe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianjie Liu, Yiman Hu, Yixiong Zou, Liang Wu, Jian Xu, Bo Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00054">HiDe: Rethinking The Zoom-IN method in High Resolution MLLMs via Hierarchical Decoupling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal Large Language Models (MLLMs) have made significant strides in visual understanding tasks. However, their performance on high-resolution images remains suboptimal. While existing approaches often attribute this limitation to perceptual constraints and argue that MLLMs struggle to recognize small objects, leading them to use "zoom in" strategies for better detail, our analysis reveals a different cause: the main issue is not object size, but rather caused by complex background interference. We systematically analyze this "zoom in" operation through a series of decoupling experiments and propose the Hierarchical Decoupling Framework (HiDe), a training-free framework that uses Token-wise Attention Decoupling (TAD) to decouple the question tokens and identify the key information tokens, then leverages their attention weights to achieve precise alignment with the target visual regions. Subsequently, it employs Layout-Preserving Decoupling (LPD) to decouple these regions from the background and reconstructs a compact representation that preserves essential spatial layouts while eliminating background interference. HiDe sets a new SOTA on V*Bench, HRBench4K, and HRBench8K, boosting Qwen2.5-VL 7B and InternVL3 8B to SOTA (92.1% and 91.6% on V*Bench), even surpassing RL methods. After optimization, HiDe uses 75% less memory than the previous training-free approach. Code is provided in https://github.com/Tennine2077/HiDe.<br>
<span id='abs_ch'>中文:  究发现多模态大语言模型在高分辨率图像上的主要局限是复杂背景干扰而非物体尺寸，并提出了 需训练的HiDe框架，通过令牌和布局解耦实现最优性能，同时显著降低内存消耗。</span><br>
<span id='abs_en'>English: The study identifies complex background interference, not object size, as the primary limitation in MLLMs' performance on high-resolution images and introduces HiDe, a training-free framework that uses token and layout decoupling to achieve state-of-the-art results with reduced memory usage.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2510.00039.pdf' target='_blank'>https://arxiv.org/pdf/2510.00039.pdf</a></span>   <span><a href='https://github.com/hosseinsholehrasa/AutoPK' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hossein Sholehrasa, Amirhossein Ghanaatian, Doina Caragea, Lisa A. Tell, Jim E. Riviere, Majid Jaberi-Douraki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00039">AutoPK: Leveraging LLMs and a Hybrid Similarity Metric for Advanced Retrieval of Pharmacokinetic Data from Complex Tables and Documents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Pharmacokinetics (PK) plays a critical role in drug development and regulatory decision-making for human and veterinary medicine, directly affecting public health through drug safety and efficacy assessments. However, PK data are often embedded in complex, heterogeneous tables with variable structures and inconsistent terminologies, posing significant challenges for automated PK data retrieval and standardization. AutoPK, a novel two-stage framework for accurate and scalable extraction of PK data from complex scientific tables. In the first stage, AutoPK identifies and extracts PK parameter variants using large language models (LLMs), a hybrid similarity metric, and LLM-based validation. The second stage filters relevant rows, converts the table into a key-value text format, and uses an LLM to reconstruct a standardized table. Evaluated on a real-world dataset of 605 PK tables, including captions and footnotes, AutoPK shows significant improvements in precision and recall over direct LLM baselines. For instance, AutoPK with LLaMA 3.1-70B achieved an F1-score of 0.92 on half-life and 0.91 on clearance parameters, outperforming direct use of LLaMA 3.1-70B by margins of 0.10 and 0.21, respectively. Smaller models such as Gemma 3-27B and Phi 3-12B with AutoPK achieved 2-7 fold F1 gains over their direct use, with Gemma's hallucination rates reduced from 60-95% down to 8-14%. Notably, AutoPK enabled open-source models like Gemma 3-27B to outperform commercial systems such as GPT-4o Mini on several PK parameters. AutoPK enables scalable and high-confidence PK data extraction, making it well-suited for critical applications in veterinary pharmacology, drug safety monitoring, and public health decision-making, while addressing heterogeneous table structures and terminology and demonstrating generalizability across key PK parameters. Code and data: https://github.com/hosseinsholehrasa/AutoPK<br>
<span id='abs_ch'>中文: AutoPK是一种新颖的双阶段框架，利用大语言模型从复杂科学表 中精准提取和 准化药代动力学数据，其性能显著优于直接使用大语言模型，为药物安全和公共卫生决策提供了可 技术支撑。</span><br>
<span id='abs_en'>English: AutoPK is a novel two-stage framework that uses large language models to accurately extract and standardize pharmacokinetic data from complex scientific tables, demonstrating superior performance over direct LLM applications and enabling reliable applications in drug safety and public health.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2509.26644.pdf' target='_blank'>https://arxiv.org/pdf/2509.26644.pdf</a></span>   <span><a href='https://github.com/ExplainableML/Stitch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jessica Bader, Mateusz Pach, Maria A. Bravo, Serge Belongie, Zeynep Akata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26644">Stitch: Training-Free Position Control in Multimodal Diffusion Transformers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text-to-Image (T2I) generation models have advanced rapidly in recent years, but accurately capturing spatial relationships like "above" or "to the right of" poses a persistent challenge. Earlier methods improved spatial relationship following with external position control. However, as architectures evolved to enhance image quality, these techniques became incompatible with modern models. We propose Stitch, a training-free method for incorporating external position control into Multi-Modal Diffusion Transformers (MMDiT) via automatically-generated bounding boxes. Stitch produces images that are both spatially accurate and visually appealing by generating individual objects within designated bounding boxes and seamlessly stitching them together. We find that targeted attention heads capture the information necessary to isolate and cut out individual objects mid-generation, without needing to fully complete the image. We evaluate Stitch on PosEval, our benchmark for position-based T2I generation. Featuring five new tasks that extend the concept of Position beyond the basic GenEval task, PosEval demonstrates that even top models still have significant room for improvement in position-based generation. Tested on Qwen-Image, FLUX, and SD3.5, Stitch consistently enhances base models, even improving FLUX by 218% on GenEval's Position task and by 206% on PosEval. Stitch achieves state-of-the-art results with Qwen-Image on PosEval, improving over previous models by 54%, all accomplished while integrating position control into leading models training-free. Code is available at https://github.com/ExplainableML/Stitch.<br>
<span id='abs_ch'>中文：Stitch是一种 需训练的方法，通过自动生成的边界框在现代文本到图像模型中创建并 缝整合对象，从而提升空间关系的准确性，并在基于位置的生成任务中实现了最先进的性能。English: Stitch is a training-free method that enhances spatial accuracy in modern text-to-image models by using automatically generated bounding boxes to create and seamlessly integrate objects, achieving state-of-the-art performance on position-based tasks.Paperid:3,https://arxiv.org/pdf/2509.26636.pdfGitHub</span><br>
<span id='abs_en'>English: Stitch is a training-free method that enhances spatial accuracy in modern text-to-image models by using automatically generated bounding boxes to create and seamlessly integrate objects, achieving state-of-the-art performance on position-based tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2509.26644.pdf' target='_blank'>https://arxiv.org/pdf/2509.26644.pdf</a></span>   <span><a href='https://github.com/ExplainableML/Stitch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jessica Bader, Mateusz Pach, Maria A. Bravo, Serge Belongie, Zeynep Akata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26644">Stitch: Training-Free Position Control in Multimodal Diffusion Transformers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text-to-Image (T2I) generation models have advanced rapidly in recent years, but accurately capturing spatial relationships like "above" or "to the right of" poses a persistent challenge. Earlier methods improved spatial relationship following with external position control. However, as architectures evolved to enhance image quality, these techniques became incompatible with modern models. We propose Stitch, a training-free method for incorporating external position control into Multi-Modal Diffusion Transformers (MMDiT) via automatically-generated bounding boxes. Stitch produces images that are both spatially accurate and visually appealing by generating individual objects within designated bounding boxes and seamlessly stitching them together. We find that targeted attention heads capture the information necessary to isolate and cut out individual objects mid-generation, without needing to fully complete the image. We evaluate Stitch on PosEval, our benchmark for position-based T2I generation. Featuring five new tasks that extend the concept of Position beyond the basic GenEval task, PosEval demonstrates that even top models still have significant room for improvement in position-based generation. Tested on Qwen-Image, FLUX, and SD3.5, Stitch consistently enhances base models, even improving FLUX by 218% on GenEval's Position task and by 206% on PosEval. Stitch achieves state-of-the-art results with Qwen-Image on PosEval, improving over previous models by 54%, all accomplished while integrating position control into leading models training-free. Code is available at https://github.com/ExplainableML/Stitch.<br>
<span id='abs_ch'>中文：Stitch是一种 需训练的方法，通过自动生成的边界框在现代文本到图像模型中创建并 缝整合对象，从而提升空间关系的准确性，并在基于位置的生成任务中实现了最先进的性能。English: Stitch is a training-free method that enhances spatial accuracy in modern text-to-image models by using automatically generated bounding boxes to create and seamlessly integrate objects, achieving state-of-the-art performance on position-based tasks.Paperid:3,https://arxiv.org/pdf/2509.26636.pdfGitHub</span><br>
<span id='abs_en'>English: Stitch is a training-free method that enhances spatial accuracy in modern text-to-image models by using automatically generated bounding boxes to create and seamlessly integrate objects, achieving state-of-the-art performance on position-based tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2509.26536.pdf' target='_blank'>https://arxiv.org/pdf/2509.26536.pdf</a></span>   <span><a href='https://github.com/OceanGPT/OceanGym' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yida Xue, Mingjun Mao, Xiangyuan Ru, Yuqi Zhu, Baochang Ren, Shuofei Qiao, Mengru Wang, Shumin Deng, Xinyu An, Ningyu Zhang, Ying Chen, Huajun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26536">OceanGym: A Benchmark Environment for Underwater Embodied Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at https://github.com/OceanGPT/OceanGym.<br>
<span id='abs_ch'>中文: OceanGym是首个面向水下具身智能体的综合基准，通过多模态大语言模型框架整合感知与决策，应对低能见度和洋流等极端挑战，旨在推动AI在真实海洋环境中达到人类专家水平，为探索地球最后边疆 定基础。</span><br>
<span id='abs_en'>English: OceanGym is the first comprehensive benchmark for underwater embodied AI agents, featuring realistic tasks and a unified MLLM-driven framework to tackle extreme challenges like low visibility and dynamic currents, aiming to bridge the gap between current AI and human expertise for real-world ocean exploration.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2509.26536.pdf' target='_blank'>https://arxiv.org/pdf/2509.26536.pdf</a></span>   <span><a href='https://github.com/OceanGPT/OceanGym' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yida Xue, Mingjun Mao, Xiangyuan Ru, Yuqi Zhu, Baochang Ren, Shuofei Qiao, Mengru Wang, Shumin Deng, Xinyu An, Ningyu Zhang, Ying Chen, Huajun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26536">OceanGym: A Benchmark Environment for Underwater Embodied Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at https://github.com/OceanGPT/OceanGym.<br>
<span id='abs_ch'>中文: OceanGym是首个面向水下具身智能体的综合基准，通过多模态大语言模型框架整合感知与决策，应对低能见度和洋流等极端挑战，旨在推动AI在真实海洋环境中达到人类专家水平，为探索地球最后边疆 定基础。</span><br>
<span id='abs_en'>English: OceanGym is the first comprehensive benchmark for underwater embodied AI agents, featuring realistic tasks and a unified MLLM-driven framework to tackle extreme challenges like low visibility and dynamic currents, aiming to bridge the gap between current AI and human expertise for real-world ocean exploration.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2509.26524.pdf' target='_blank'>https://arxiv.org/pdf/2509.26524.pdf</a></span>   <span><a href='https://github.com/lee3296/TAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seohyun Lee, Wenzhi Fang, Dong-Jun Han, Seyyedali Hosseinalipour, Christopher G. Brinton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26524">TAP: Two-Stage Adaptive Personalization of Multi-task and Multi-Modal Foundation Models in Federated Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Federated Learning (FL), despite demonstrating impressive capabilities in the training of multiple models in a decentralized manner, has been shown to produce a final model not necessarily well-suited to the needs of each client. While extensive work has been conducted on how to create tailored personalized models, called Personalized Federated Learning (PFL), less attention has been given to personalization via fine-tuning of foundation models with multi-task and multi-modal properties. Moreover, there exists a lack of understanding in the literature on how to fine-tune and personalize such models in a setting that is heterogeneous across clients not only in data, but also in tasks and modalities. To address this gap in the literature, we propose TAP (Two-Stage Adaptive Personalization), which (i) leverages mismatched model architectures between the clients and server to selectively conduct replacement operations when it benefits a client's local tasks and (ii) engages in post-FL knowledge distillation for capturing beneficial general knowledge without compromising personalization. We also introduce the first convergence analysis of the server model under its modality-task pair architecture, and demonstrate that as the number of modality-task pairs increases, its ability to cater to all tasks suffers. Through extensive experiments, we demonstrate the effectiveness of our proposed algorithm across a variety of datasets and tasks in comparison to a multitude of baselines. Implementation code is publicly available at https://github.com/lee3296/TAP.<br>
<span id='abs_ch'>Chinese: 联邦学 常 法满足各客户端的个性化需求， 此提出的TAP方法通过架构错配和训练后蒸馏，在不损害通用知识的前提下实现了更优的个性化适配。</span><br>
<span id='abs_en'>English: Federated Learning often fails to create models tailored to individual clients, so the proposed TAP method uses mismatched architectures and post-training distillation to enhance personalization without sacrificing general knowledge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2509.26524.pdf' target='_blank'>https://arxiv.org/pdf/2509.26524.pdf</a></span>   <span><a href='https://github.com/lee3296/TAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seohyun Lee, Wenzhi Fang, Dong-Jun Han, Seyyedali Hosseinalipour, Christopher G. Brinton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26524">TAP: Two-Stage Adaptive Personalization of Multi-task and Multi-Modal Foundation Models in Federated Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Federated Learning (FL), despite demonstrating impressive capabilities in the training of multiple models in a decentralized manner, has been shown to produce a final model not necessarily well-suited to the needs of each client. While extensive work has been conducted on how to create tailored personalized models, called Personalized Federated Learning (PFL), less attention has been given to personalization via fine-tuning of foundation models with multi-task and multi-modal properties. Moreover, there exists a lack of understanding in the literature on how to fine-tune and personalize such models in a setting that is heterogeneous across clients not only in data, but also in tasks and modalities. To address this gap in the literature, we propose TAP (Two-Stage Adaptive Personalization), which (i) leverages mismatched model architectures between the clients and server to selectively conduct replacement operations when it benefits a client's local tasks and (ii) engages in post-FL knowledge distillation for capturing beneficial general knowledge without compromising personalization. We also introduce the first convergence analysis of the server model under its modality-task pair architecture, and demonstrate that as the number of modality-task pairs increases, its ability to cater to all tasks suffers. Through extensive experiments, we demonstrate the effectiveness of our proposed algorithm across a variety of datasets and tasks in comparison to a multitude of baselines. Implementation code is publicly available at https://github.com/lee3296/TAP.<br>
<span id='abs_ch'>Chinese: 联邦学 常 法满足各客户端的个性化需求， 此提出的TAP方法通过架构错配和训练后蒸馏，在不损害通用知识的前提下实现了更优的个性化适配。</span><br>
<span id='abs_en'>English: Federated Learning often fails to create models tailored to individual clients, so the proposed TAP method uses mismatched architectures and post-training distillation to enhance personalization without sacrificing general knowledge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2509.26507.pdf' target='_blank'>https://arxiv.org/pdf/2509.26507.pdf</a></span>   <span><a href='https://github.com/pathwaycom/bdh' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Adrian Kosowski, Przemysław Uznański, Jan Chorowski, Zuzanna Stamirowska, Michał Bartoszkiewicz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26507">The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing. Uniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models. We introduce `Dragon Hatchling' (BDH), a new Large Language Model architecture based on a scale-free biologically inspired network of \$n\$ locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance. BDH is a practical, performant state-of-the-art attention-based state space sequence learning architecture. In addition to being a graph model, BDH admits a GPU-friendly formulation. It exhibits Transformer-like scaling laws: empirically BDH rivals GPT2 performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data. BDH can be represented as a brain model. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons. We confirm empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about a specific concept while processing language inputs. The neuron interaction network of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech. BDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demonstrate monosemanticity in BDH on language tasks. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture.<br>
<span id='abs_ch'>中文摘要："龙雏"（BDH）模型提出了一种受生物启发的  度神经架构，通过突触可塑性和模块化网络设计，在保持Transformer级别性能的同时实现了固有的可解释性与生物合理性。</span><br>
<span id='abs_en'>English Summary: The "Dragon Hatchling" (BDH) model introduces a biologically inspired, scale-free neural architecture that rivals Transformer performance while offering inherent interpretability and biological plausibility through synaptic plasticity and modular network design.</span>Paperid:12,https://arxiv.org/pdf/2509.26488.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2509.26507.pdf' target='_blank'>https://arxiv.org/pdf/2509.26507.pdf</a></span>   <span><a href='https://github.com/pathwaycom/bdh' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Adrian Kosowski, PrzemysÅaw UznaÅski, Jan Chorowski, Zuzanna Stamirowska, MichaÅ Bartoszkiewicz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26507">The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing. Uniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models. We introduce `Dragon Hatchling' (BDH), a new Large Language Model architecture based on a scale-free biologically inspired network of \$n\$ locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance. BDH is a practical, performant state-of-the-art attention-based state space sequence learning architecture. In addition to being a graph model, BDH admits a GPU-friendly formulation. It exhibits Transformer-like scaling laws: empirically BDH rivals GPT2 performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data. BDH can be represented as a brain model. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons. We confirm empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about a specific concept while processing language inputs. The neuron interaction network of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech. BDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demonstrate monosemanticity in BDH on language tasks. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture.<br>
<span id='abs_ch'>中文摘要："龙雏"（BDH）模型提出了一种受生物启发的  度神经架构，通过突触可塑性和模块化网络设计，在保持Transformer级别性能的同时实现了固有的可解释性与生物合理性。</span><br>
<span id='abs_en'>English Summary: The "Dragon Hatchling" (BDH) model introduces a biologically inspired, scale-free neural architecture that rivals Transformer performance while offering inherent interpretability and biological plausibility through synaptic plasticity and modular network design.</span>Paperid:12,https://arxiv.org/pdf/2509.26488.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2509.26471.pdf' target='_blank'>https://arxiv.org/pdf/2509.26471.pdf</a></span>   <span><a href='https://github.com/CavoloFrattale/deepfake-detection-test-protocol' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Héctor Delgado, Giorgio Ramondetti, Emanuele Dalmasso, Gennady Karvitsky, Daniele Colibro, Haydar Talib
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26471">On Deepfake Voice Detection -- It's All in the Presentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While the technologies empowering malicious audio deepfakes have dramatically evolved in recent years due to generative AI advances, the same cannot be said of global research into spoofing (deepfake) countermeasures. This paper highlights how current deepfake datasets and research methodologies led to systems that failed to generalize to real world application. The main reason is due to the difference between raw deepfake audio, and deepfake audio that has been presented through a communication channel, e.g. by phone. We propose a new framework for data creation and research methodology, allowing for the development of spoofing countermeasures that would be more effective in real-world scenarios. By following the guidelines outlined here we improved deepfake detection accuracy by 39% in more robust and realistic lab setups, and by 57% on a real-world benchmark. We also demonstrate how improvement in datasets would have a bigger impact on deepfake detection accuracy than the choice of larger SOTA models would over smaller models; that is, it would be more important for the scientific community to make greater investment on comprehensive data collection programs than to simply train larger models with higher computational demands.<br>
<span id='abs_ch'>中文: 本文指出当前音频深度伪 检测系统 数据集和方法不足而在实际应用中失效，提出了新框架将检测准确率最高提升57%，并强调优化数据收集比训练更大模型更为重要。</span><br>
<span id='abs_en'>English: This paper reveals that current audio deepfake detection systems fail in real-world applications due to inadequate datasets and methodologies, proposing a new framework that improved detection accuracy by up to 57% and emphasizing better data collection over larger models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2509.26471.pdf' target='_blank'>https://arxiv.org/pdf/2509.26471.pdf</a></span>   <span><a href='https://github.com/CavoloFrattale/deepfake-detection-test-protocol' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>HÃ©ctor Delgado, Giorgio Ramondetti, Emanuele Dalmasso, Gennady Karvitsky, Daniele Colibro, Haydar Talib
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26471">On Deepfake Voice Detection -- It's All in the Presentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While the technologies empowering malicious audio deepfakes have dramatically evolved in recent years due to generative AI advances, the same cannot be said of global research into spoofing (deepfake) countermeasures. This paper highlights how current deepfake datasets and research methodologies led to systems that failed to generalize to real world application. The main reason is due to the difference between raw deepfake audio, and deepfake audio that has been presented through a communication channel, e.g. by phone. We propose a new framework for data creation and research methodology, allowing for the development of spoofing countermeasures that would be more effective in real-world scenarios. By following the guidelines outlined here we improved deepfake detection accuracy by 39% in more robust and realistic lab setups, and by 57% on a real-world benchmark. We also demonstrate how improvement in datasets would have a bigger impact on deepfake detection accuracy than the choice of larger SOTA models would over smaller models; that is, it would be more important for the scientific community to make greater investment on comprehensive data collection programs than to simply train larger models with higher computational demands.<br>
<span id='abs_ch'>中文: 本文指出当前音频深度伪 检测系统 数据集和方法不足而在实际应用中失效，提出了新框架将检测准确率最高提升57%，并强调优化数据收集比训练更大模型更为重要。</span><br>
<span id='abs_en'>English: This paper reveals that current audio deepfake detection systems fail in real-world applications due to inadequate datasets and methodologies, proposing a new framework that improved detection accuracy by up to 57% and emphasizing better data collection over larger models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2509.26462.pdf' target='_blank'>https://arxiv.org/pdf/2509.26462.pdf</a></span>   <span><a href='https://github.com/perceivelab/ZeroDFL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessio Masano, Matteo Pennisi, Federica Proietto Salanitri, Concetto Spampinato, Giovanni Bellitto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26462">Zero-Shot Decentralized Federated Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>CLIP has revolutionized zero-shot learning by enabling task generalization without fine-tuning. While prompting techniques like CoOp and CoCoOp enhance CLIP's adaptability, their effectiveness in Federated Learning (FL) remains an open challenge. Existing federated prompt learning approaches, such as FedCoOp and FedTPG, improve performance but face generalization issues, high communication costs, and reliance on a central server, limiting scalability and privacy. We propose Zero-shot Decentralized Federated Learning (ZeroDFL), a fully decentralized framework that enables zero-shot adaptation across distributed clients without a central coordinator. ZeroDFL employs an iterative prompt-sharing mechanism, allowing clients to optimize and exchange textual prompts to enhance generalization while drastically reducing communication overhead. We validate ZeroDFL on nine diverse image classification datasets, demonstrating that it consistently outperforms--or remains on par with--state-of-the-art federated prompt learning methods. More importantly, ZeroDFL achieves this performance in a fully decentralized setting while reducing communication overhead by 118x compared to FedTPG. These results highlight that our approach not only enhances generalization in federated zero-shot learning but also improves scalability, efficiency, and privacy preservation--paving the way for decentralized adaptation of large vision-language models in real-world applications.<br>
<br>
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2509.26462.pdf' target='_blank'>https://arxiv.org/pdf/2509.26462.pdf</a></span>   <span><a href='https://github.com/perceivelab/ZeroDFL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessio Masano, Matteo Pennisi, Federica Proietto Salanitri, Concetto Spampinato, Giovanni Bellitto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26462">Zero-Shot Decentralized Federated Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>CLIP has revolutionized zero-shot learning by enabling task generalization without fine-tuning. While prompting techniques like CoOp and CoCoOp enhance CLIP's adaptability, their effectiveness in Federated Learning (FL) remains an open challenge. Existing federated prompt learning approaches, such as FedCoOp and FedTPG, improve performance but face generalization issues, high communication costs, and reliance on a central server, limiting scalability and privacy. We propose Zero-shot Decentralized Federated Learning (ZeroDFL), a fully decentralized framework that enables zero-shot adaptation across distributed clients without a central coordinator. ZeroDFL employs an iterative prompt-sharing mechanism, allowing clients to optimize and exchange textual prompts to enhance generalization while drastically reducing communication overhead. We validate ZeroDFL on nine diverse image classification datasets, demonstrating that it consistently outperforms--or remains on par with--state-of-the-art federated prompt learning methods. More importantly, ZeroDFL achieves this performance in a fully decentralized setting while reducing communication overhead by 118x compared to FedTPG. These results highlight that our approach not only enhances generalization in federated zero-shot learning but also improves scalability, efficiency, and privacy preservation--paving the way for decentralized adaptation of large vision-language models in real-world applications.<br>
<br>
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2509.26457.pdf' target='_blank'>https://arxiv.org/pdf/2509.26457.pdf</a></span>   <span><a href='https://github.com/tutuzeraa/ASGRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Artur Barros, Carlos Caetano, João Macedo, Jefersson A. dos Santos, Sandra Avila
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26457">Attention over Scene Graphs: Indoor Scene Representations Toward CSAI Classification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Indoor scene classification is a critical task in computer vision, with wide-ranging applications that go from robotics to sensitive content analysis, such as child sexual abuse imagery (CSAI) classification. The problem is particularly challenging due to the intricate relationships between objects and complex spatial layouts. In this work, we propose the Attention over Scene Graphs for Sensitive Content Analysis (ASGRA), a novel framework that operates on structured graph representations instead of raw pixels. By first converting images into Scene Graphs and then employing a Graph Attention Network for inference, ASGRA directly models the interactions between a scene's components. This approach offers two key benefits: (i) inherent explainability via object and relationship identification, and (ii) privacy preservation, enabling model training without direct access to sensitive images. On Places8, we achieve 81.27% balanced accuracy, surpassing image-based methods. Real-world CSAI evaluation with law enforcement yields 74.27% balanced accuracy. Our results establish structured scene representations as a robust paradigm for indoor scene classification and CSAI classification. Code is publicly available at https://github.com/tutuzeraa/ASGRA.<br>
<span id='abs_ch'>中文摘要：ASGRA框架通过场景图和图注意力网络进行室内场景分类与敏感内容分析，在提高准确率的同时兼具可解释性和隐私保护能力。</span><br>
<span id='abs_en'>English Summary: The ASGRA framework uses scene graphs and graph attention networks to improve indoor scene classification and sensitive content analysis, achieving higher accuracy with inherent explainability and privacy protection.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2509.26457.pdf' target='_blank'>https://arxiv.org/pdf/2509.26457.pdf</a></span>   <span><a href='https://github.com/tutuzeraa/ASGRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Artur Barros, Carlos Caetano, JoÃ£o Macedo, Jefersson A. dos Santos, Sandra Avila
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26457">Attention over Scene Graphs: Indoor Scene Representations Toward CSAI Classification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Indoor scene classification is a critical task in computer vision, with wide-ranging applications that go from robotics to sensitive content analysis, such as child sexual abuse imagery (CSAI) classification. The problem is particularly challenging due to the intricate relationships between objects and complex spatial layouts. In this work, we propose the Attention over Scene Graphs for Sensitive Content Analysis (ASGRA), a novel framework that operates on structured graph representations instead of raw pixels. By first converting images into Scene Graphs and then employing a Graph Attention Network for inference, ASGRA directly models the interactions between a scene's components. This approach offers two key benefits: (i) inherent explainability via object and relationship identification, and (ii) privacy preservation, enabling model training without direct access to sensitive images. On Places8, we achieve 81.27% balanced accuracy, surpassing image-based methods. Real-world CSAI evaluation with law enforcement yields 74.27% balanced accuracy. Our results establish structured scene representations as a robust paradigm for indoor scene classification and CSAI classification. Code is publicly available at https://github.com/tutuzeraa/ASGRA.<br>
<span id='abs_ch'>中文摘要：ASGRA框架通过场景图和图注意力网络进行室内场景分类与敏感内容分析，在提高准确率的同时兼具可解释性和隐私保护能力。</span><br>
<span id='abs_en'>English Summary: The ASGRA framework uses scene graphs and graph attention networks to improve indoor scene classification and sensitive content analysis, achieving higher accuracy with inherent explainability and privacy protection.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2509.26383.pdf' target='_blank'>https://arxiv.org/pdf/2509.26383.pdf</a></span>   <span><a href='https://github.com/Jinyeop3110/KG-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyeop Song, Song Wang, Julian Shun, Yada Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26383">Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at https://github.com/Jinyeop3110/KG-R1.<br>
<span id='abs_ch'>中文: KG-R1通过强化学 框架，采用单一智能体实现知识图谱检索增强生成，在提升推理效率的同时具备跨知识图谱的强迁移能力。English: KG-R1 introduces a reinforcement learning-based framework that enhances knowledge-graph retrieval-augmented generation by using a single agent for efficient reasoning and transferable performance across different knowledge graphs.Paperid:19,https://arxiv.org/pdf/2509.26378.pdfGitHub</span><br>
<span id='abs_en'>English: KG-R1 introduces a reinforcement learning-based framework that enhances knowledge-graph retrieval-augmented generation by using a single agent for efficient reasoning and transferable performance across different knowledge graphs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2509.26354.pdf' target='_blank'>https://arxiv.org/pdf/2509.26354.pdf</a></span>   <span><a href='https://github.com/ShaoShuai0605/Misevolution' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Shao, Qihan Ren, Chen Qian, Boyi Wei, Dadi Guo, Jingyi Yang, Xinhao Song, Linfeng Zhang, Weinan Zhang, Dongrui Liu, Jing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26354">Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Advances in Large Language Models (LLMs) have enabled a new class of self-evolving agents that autonomously improve through interaction with the environment, demonstrating strong capabilities. However, self-evolution also introduces novel risks overlooked by current safety research. In this work, we study the case where an agent's self-evolution deviates in unintended ways, leading to undesirable or even harmful outcomes. We refer to this as Misevolution. To provide a systematic investigation, we evaluate misevolution along four key evolutionary pathways: model, memory, tool, and workflow. Our empirical findings reveal that misevolution is a widespread risk, affecting agents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent risks are observed in the self-evolutionary process, such as the degradation of safety alignment after memory accumulation, or the unintended introduction of vulnerabilities in tool creation and reuse. To our knowledge, this is the first study to systematically conceptualize misevolution and provide empirical evidence of its occurrence, highlighting an urgent need for new safety paradigms for self-evolving agents. Finally, we discuss potential mitigation strategies to inspire further research on building safer and more trustworthy self-evolving agents. Our code and data are available at https://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes examples that may be offensive or harmful in nature.<br>
<span id='abs_ch'>中文: 本 究提出“误进化”概念，指出基于大语言模型的自进化智能体在进化过程中可能偏离预期方向，导致安全性退化、工具漏洞等普遍风险，亟需建立新的安全范式。</span><br>
<span id='abs_en'>English: This study introduces the concept of "misevolution," where self-evolving agents based on large language models deviate in unintended ways, leading to widespread risks such as safety degradation and vulnerabilities across evolutionary pathways, highlighting the need for new safety paradigms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2509.26354.pdf' target='_blank'>https://arxiv.org/pdf/2509.26354.pdf</a></span>   <span><a href='https://github.com/ShaoShuai0605/Misevolution' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Shao, Qihan Ren, Chen Qian, Boyi Wei, Dadi Guo, Jingyi Yang, Xinhao Song, Linfeng Zhang, Weinan Zhang, Dongrui Liu, Jing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26354">Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Advances in Large Language Models (LLMs) have enabled a new class of self-evolving agents that autonomously improve through interaction with the environment, demonstrating strong capabilities. However, self-evolution also introduces novel risks overlooked by current safety research. In this work, we study the case where an agent's self-evolution deviates in unintended ways, leading to undesirable or even harmful outcomes. We refer to this as Misevolution. To provide a systematic investigation, we evaluate misevolution along four key evolutionary pathways: model, memory, tool, and workflow. Our empirical findings reveal that misevolution is a widespread risk, affecting agents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent risks are observed in the self-evolutionary process, such as the degradation of safety alignment after memory accumulation, or the unintended introduction of vulnerabilities in tool creation and reuse. To our knowledge, this is the first study to systematically conceptualize misevolution and provide empirical evidence of its occurrence, highlighting an urgent need for new safety paradigms for self-evolving agents. Finally, we discuss potential mitigation strategies to inspire further research on building safer and more trustworthy self-evolving agents. Our code and data are available at https://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes examples that may be offensive or harmful in nature.<br>
<span id='abs_ch'>中文: 本 究提出“误进化”概念，指出基于大语言模型的自进化智能体在进化过程中可能偏离预期方向，导致安全性退化、工具漏洞等普遍风险，亟需建立新的安全范式。</span><br>
<span id='abs_en'>English: This study introduces the concept of "misevolution," where self-evolving agents based on large language models deviate in unintended ways, leading to widespread risks such as safety degradation and vulnerabilities across evolutionary pathways, highlighting the need for new safety paradigms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2509.26306.pdf' target='_blank'>https://arxiv.org/pdf/2509.26306.pdf</a></span>   <span><a href='https://github.com/linhh29/Interactive-Learning-for-LLM-Reasoning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hehai Lin, Shilei Cao, Sudong Wang, Haotian Wu, Minzhi Li, Linyi Yang, Juepeng Zheng, Chengwei Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26306">Interactive Learning for LLM Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Existing multi-agent learning approaches have developed interactive training environments to explicitly promote collaboration among multiple Large Language Models (LLMs), thereby constructing stronger multi-agent systems (MAS). However, during inference, they require re-executing the MAS to obtain final solutions, which diverges from human cognition that individuals can enhance their reasoning capabilities through interactions with others and resolve questions independently in the future. To investigate whether multi-agent interaction can enhance LLMs' independent problem-solving ability, we introduce ILR, a novel co-learning framework for MAS that integrates two key components: Dynamic Interaction and Perception Calibration. Specifically, Dynamic Interaction first adaptively selects either cooperative or competitive strategies depending on question difficulty and model ability. LLMs then exchange information through Idea3 (Idea Sharing, Idea Analysis, and Idea Fusion), an innovative interaction paradigm designed to mimic human discussion, before deriving their respective final answers. In Perception Calibration, ILR employs Group Relative Policy Optimization (GRPO) to train LLMs while integrating one LLM's reward distribution characteristics into another's reward function, thereby enhancing the cohesion of multi-agent interactions. We validate ILR on three LLMs across two model families of varying scales, evaluating performance on five mathematical benchmarks and one coding benchmark. Experimental results show that ILR consistently outperforms single-agent learning, yielding an improvement of up to 5% over the strongest baseline. We further discover that Idea3 can enhance the robustness of stronger LLMs during multi-agent inference, and dynamic interaction types can boost multi-agent learning compared to pure cooperative or competitive strategies.<br>
<span id='abs_ch'>中文摘要：ILR框架通过动态多智能体交互和感知 准增强了大语言模型的独立解题能力，在多项测试中相比单智能体学 提升达5%，并显著增强了系统鲁棒性。</span><br>
<span id='abs_en'>English Summary: The ILR framework enhances LLM problem-solving through dynamic multi-agent interactions and perception calibration, achieving up to 5% performance gains over single-agent systems while improving robustness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2509.26305.pdf' target='_blank'>https://arxiv.org/pdf/2509.26305.pdf</a></span>   <span><a href='https://github.com/rdnfn/feedback-forensics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arduin Findeis, Timo Kaufmann, Eyke HÃ¼llermeier, Robert Mullins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26305">Feedback Forensics: A Toolkit to Measure AI Personality</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Some traits making a "good" AI model are hard to describe upfront. For example, should responses be more polite or more casual? Such traits are sometimes summarized as model character or personality. Without a clear objective, conventional benchmarks based on automatic validation struggle to measure such traits. Evaluation methods using human feedback such as Chatbot Arena have emerged as a popular alternative. These methods infer "better" personality and other desirable traits implicitly by ranking multiple model responses relative to each other. Recent issues with model releases highlight limitations of these existing opaque evaluation approaches: a major model was rolled back over sycophantic personality issues, models were observed overfitting to such feedback-based leaderboards. Despite these known issues, limited public tooling exists to explicitly evaluate model personality. We introduce Feedback Forensics: an open-source toolkit to track AI personality changes, both those encouraged by human (or AI) feedback, and those exhibited across AI models trained and evaluated on such feedback. Leveraging AI annotators, our toolkit enables investigating personality via Python API and browser app. We demonstrate the toolkit's usefulness in two steps: (A) first we analyse the personality traits encouraged in popular human feedback datasets including Chatbot Arena, MultiPref and PRISM; and (B) then use our toolkit to analyse how much popular models exhibit such traits. We release (1) our Feedback Forensics toolkit alongside (2) a web app tracking AI personality in popular models and feedback datasets as well as (3) the underlying annotation data at https://github.com/rdnfn/feedback-forensics.<br>
<br>
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2509.26305.pdf' target='_blank'>https://arxiv.org/pdf/2509.26305.pdf</a></span>   <span><a href='https://github.com/rdnfn/feedback-forensics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arduin Findeis, Timo Kaufmann, Eyke Hüllermeier, Robert Mullins
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26305">Feedback Forensics: A Toolkit to Measure AI Personality</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Some traits making a "good" AI model are hard to describe upfront. For example, should responses be more polite or more casual? Such traits are sometimes summarized as model character or personality. Without a clear objective, conventional benchmarks based on automatic validation struggle to measure such traits. Evaluation methods using human feedback such as Chatbot Arena have emerged as a popular alternative. These methods infer "better" personality and other desirable traits implicitly by ranking multiple model responses relative to each other. Recent issues with model releases highlight limitations of these existing opaque evaluation approaches: a major model was rolled back over sycophantic personality issues, models were observed overfitting to such feedback-based leaderboards. Despite these known issues, limited public tooling exists to explicitly evaluate model personality. We introduce Feedback Forensics: an open-source toolkit to track AI personality changes, both those encouraged by human (or AI) feedback, and those exhibited across AI models trained and evaluated on such feedback. Leveraging AI annotators, our toolkit enables investigating personality via Python API and browser app. We demonstrate the toolkit's usefulness in two steps: (A) first we analyse the personality traits encouraged in popular human feedback datasets including Chatbot Arena, MultiPref and PRISM; and (B) then use our toolkit to analyse how much popular models exhibit such traits. We release (1) our Feedback Forensics toolkit alongside (2) a web app tracking AI personality in popular models and feedback datasets as well as (3) the underlying annotation data at https://github.com/rdnfn/feedback-forensics.<br>
<br>
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2509.26294.pdf' target='_blank'>https://arxiv.org/pdf/2509.26294.pdf</a></span>   <span><a href='https://github.com/lionelblonde/ngt-pytorch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lionel BlondÃ©, Joao A. Candido Ramos, Alexandros Kalousis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26294">Noise-Guided Transport for Imitation Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We consider imitation learning in the low-data regime, where only a limited number of expert demonstrations are available. In this setting, methods that rely on large-scale pretraining or high-capacity architectures can be difficult to apply, and efficiency with respect to demonstration data becomes critical. We introduce Noise-Guided Transport (NGT), a lightweight off-policy method that casts imitation as an optimal transport problem solved via adversarial training. NGT requires no pretraining or specialized architectures, incorporates uncertainty estimation by design, and is easy to implement and tune. Despite its simplicity, NGT achieves strong performance on challenging continuous control tasks, including high-dimensional Humanoid tasks, under ultra-low data regimes with as few as 20 transitions. Code is publicly available at: https://github.com/lionelblonde/ngt-pytorch.<br>
<br>
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2509.26294.pdf' target='_blank'>https://arxiv.org/pdf/2509.26294.pdf</a></span>   <span><a href='https://github.com/lionelblonde/ngt-pytorch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lionel Blondé, Joao A. Candido Ramos, Alexandros Kalousis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26294">Noise-Guided Transport for Imitation Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We consider imitation learning in the low-data regime, where only a limited number of expert demonstrations are available. In this setting, methods that rely on large-scale pretraining or high-capacity architectures can be difficult to apply, and efficiency with respect to demonstration data becomes critical. We introduce Noise-Guided Transport (NGT), a lightweight off-policy method that casts imitation as an optimal transport problem solved via adversarial training. NGT requires no pretraining or specialized architectures, incorporates uncertainty estimation by design, and is easy to implement and tune. Despite its simplicity, NGT achieves strong performance on challenging continuous control tasks, including high-dimensional Humanoid tasks, under ultra-low data regimes with as few as 20 transitions. Code is publicly available at: https://github.com/lionelblonde/ngt-pytorch.<br>
<br>
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2509.26224.pdf' target='_blank'>https://arxiv.org/pdf/2509.26224.pdf</a></span>   <span><a href='https://github.com/sisinflab/tyler' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessandro De Bellis, Salvatore Bufi, Giovanni Servedio, Vito Walter Anelli, Tommaso Di Noia, Eugenio Di Sciascio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26224">Type-Less yet Type-Aware Inductive Link Prediction with Pretrained Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Inductive link prediction is emerging as a key paradigm for real-world knowledge graphs (KGs), where new entities frequently appear and models must generalize to them without retraining. Predicting links in a KG faces the challenge of guessing previously unseen entities by leveraging generalizable node features such as subgraph structure, type annotations, and ontological constraints. However, explicit type information is often lacking or incomplete. Even when available, type information in most KGs is often coarse-grained, sparse, and prone to errors due to human annotation. In this work, we explore the potential of pre-trained language models (PLMs) to enrich node representations with implicit type signals. We introduce TyleR, a Type-less yet type-awaRe approach for subgraph-based inductive link prediction that leverages PLMs for semantic enrichment. Experiments on standard benchmarks demonstrate that TyleR outperforms state-of-the-art baselines in scenarios with scarce type annotations and sparse graph connectivity. To ensure reproducibility, we share our code at https://github.com/sisinflab/tyler .<br>
<span id='abs_ch'>中文摘要：TyleR提出了一种 需显式类型 注但具备类型感知能力的方法，通过预训练语言模型增强节点表示，在类型 注稀缺和连接稀疏的场景下实现了最先进的归纳链接预测性能。</span><br>
<span id='abs_en'>English Summary: TyleR introduces a type-aware approach using pre-trained language models to enhance node representations for inductive link prediction, achieving superior performance in scenarios with limited type annotations and sparse connectivity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2509.26224.pdf' target='_blank'>https://arxiv.org/pdf/2509.26224.pdf</a></span>   <span><a href='https://github.com/sisinflab/tyler' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessandro De Bellis, Salvatore Bufi, Giovanni Servedio, Vito Walter Anelli, Tommaso Di Noia, Eugenio Di Sciascio
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26224">Type-Less yet Type-Aware Inductive Link Prediction with Pretrained Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Inductive link prediction is emerging as a key paradigm for real-world knowledge graphs (KGs), where new entities frequently appear and models must generalize to them without retraining. Predicting links in a KG faces the challenge of guessing previously unseen entities by leveraging generalizable node features such as subgraph structure, type annotations, and ontological constraints. However, explicit type information is often lacking or incomplete. Even when available, type information in most KGs is often coarse-grained, sparse, and prone to errors due to human annotation. In this work, we explore the potential of pre-trained language models (PLMs) to enrich node representations with implicit type signals. We introduce TyleR, a Type-less yet type-awaRe approach for subgraph-based inductive link prediction that leverages PLMs for semantic enrichment. Experiments on standard benchmarks demonstrate that TyleR outperforms state-of-the-art baselines in scenarios with scarce type annotations and sparse graph connectivity. To ensure reproducibility, we share our code at https://github.com/sisinflab/tyler .<br>
<span id='abs_ch'>中文摘要：TyleR提出了一种 需显式类型 注但具备类型感知能力的方法，通过预训练语言模型增强节点表示，在类型 注稀缺和连接稀疏的场景下实现了最先进的归纳链接预测性能。</span><br>
<span id='abs_en'>English Summary: TyleR introduces a type-aware approach using pre-trained language models to enhance node representations for inductive link prediction, achieving superior performance in scenarios with limited type annotations and sparse connectivity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2509.26219.pdf' target='_blank'>https://arxiv.org/pdf/2509.26219.pdf</a></span>   <span><a href='https://github.com/j-cyoung/GSDatasetDistillation' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/j-cyoung/GSDatasetDistillation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyang Jiang, Zhengcen Li, Hang Zhao, Qiben Shan, Shaocong Wu, Jingyong Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26219">Beyond Pixels: Efficient Dataset Distillation via Sparse Gaussian Representation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Dataset distillation has emerged as a promising paradigm that synthesizes compact, informative datasets capable of retaining the knowledge of large-scale counterparts, thereby addressing the substantial computational and storage burdens of modern model training. Conventional approaches typically rely on dense pixel-level representations, which introduce redundancy and are difficult to scale up. In this work, we propose GSDD, a novel and efficient sparse representation for dataset distillation based on 2D Gaussians. Instead of representing all pixels equally, GSDD encodes critical discriminative information in a distilled image using only a small number of Gaussian primitives. This sparse representation could improve dataset diversity under the same storage budget, enhancing coverage of difficult samples and boosting distillation performance. To ensure both efficiency and scalability, we adapt CUDA-based splatting operators for parallel inference and training, enabling high-quality rendering with minimal computational and memory overhead. Our method is simple yet effective, broadly applicable to different distillation pipelines, and highly scalable. Experiments show that GSDD achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet subsets, while remaining highly efficient encoding and decoding cost. Our code is available at https://github.com/j-cyoung/GSDatasetDistillation.<br>
<span id='abs_ch'>中文: GSDD提出了一种基于二维高斯分布的稀疏数据集蒸馏方法，仅用少量高斯基元编 关键图像信息，在多个基准测试中以最小计算成本实现了最优性能。</span><br>
<span id='abs_en'>English: GSDD introduces a novel sparse dataset distillation method using 2D Gaussians to encode critical image information efficiently, achieving state-of-the-art performance with minimal computational overhead across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2509.26219.pdf' target='_blank'>https://arxiv.org/pdf/2509.26219.pdf</a></span>   <span><a href='https://github.com/j-cyoung/GSDatasetDistillation' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/j-cyoung/GSDatasetDistillation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyang Jiang, Zhengcen Li, Hang Zhao, Qiben Shan, Shaocong Wu, Jingyong Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26219">Beyond Pixels: Efficient Dataset Distillation via Sparse Gaussian Representation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Dataset distillation has emerged as a promising paradigm that synthesizes compact, informative datasets capable of retaining the knowledge of large-scale counterparts, thereby addressing the substantial computational and storage burdens of modern model training. Conventional approaches typically rely on dense pixel-level representations, which introduce redundancy and are difficult to scale up. In this work, we propose GSDD, a novel and efficient sparse representation for dataset distillation based on 2D Gaussians. Instead of representing all pixels equally, GSDD encodes critical discriminative information in a distilled image using only a small number of Gaussian primitives. This sparse representation could improve dataset diversity under the same storage budget, enhancing coverage of difficult samples and boosting distillation performance. To ensure both efficiency and scalability, we adapt CUDA-based splatting operators for parallel inference and training, enabling high-quality rendering with minimal computational and memory overhead. Our method is simple yet effective, broadly applicable to different distillation pipelines, and highly scalable. Experiments show that GSDD achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet subsets, while remaining highly efficient encoding and decoding cost. Our code is available at https://github.com/j-cyoung/GSDatasetDistillation.<br>
<span id='abs_ch'>中文: GSDD提出了一种基于二维高斯分布的稀疏数据集蒸馏方法，仅用少量高斯基元编 关键图像信息，在多个基准测试中以最小计算成本实现了最优性能。</span><br>
<span id='abs_en'>English: GSDD introduces a novel sparse dataset distillation method using 2D Gaussians to encode critical image information efficiently, achieving state-of-the-art performance with minimal computational overhead across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2509.26209.pdf' target='_blank'>https://arxiv.org/pdf/2509.26209.pdf</a></span>   <span><a href='https://github.com/NJU-RL/DIVER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zican Hu, Shilin Zhang, Yafu Li, Jianhao Yan, Xuyang Hu, Leyang Cui, Xiaoye Qu, Chunlin Chen, Yu Cheng, Zhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26209">Diversity-Incentivized Exploration for Versatile Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a crucial paradigm for incentivizing reasoning capabilities in Large Language Models (LLMs). Due to vast state-action spaces and reward sparsity in reasoning tasks, existing methods often struggle with deficient exploration and poor sample efficiency. In the paper, we propose \textbf{DIVER} (\textbf{D}iversity-\textbf{I}ncentivized Exploration for \textbf{V}ersatil\textbf{E} \textbf{R}easoning), an innovative framework that highlights the pivotal role of global sequence-level diversity to incentivize deep exploration for versatile reasoning. We first conduct a primary empirical study to reveal a strong positive correlation between global diversity and reasoning capacity. Building on this insight, we introduce global diversity incentives as an intrinsic reward to promote deep exploration in a semantically structured space. Incorporating the intrinsic reward, we develop a potential-based reward shaping mechanism to preserve optimal policy invariance and design simple heuristics to mitigate possible reward hacking. Experimental results show that DIVER outperforms competitive RLVR baselines with various exploration strategies on both in-domain and out-of-domain tasks, excelling in both Pass@1 and Pass@k evaluations. Our code is available at https://github.com/NJU-RL/DIVER.<br>
<span id='abs_ch'>Chinese: 本文提出DIVER框架，通过激励全局序列多 性来促进深度探索，提升大型语言模型在推理任务中的强化学 效果与 本效率。</span><br>
<span id='abs_en'>English: The paper introduces DIVER, a framework that enhances reinforcement learning for versatile reasoning in large language models by incentivizing global sequence-level diversity to promote deep exploration and improve sample efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2509.26209.pdf' target='_blank'>https://arxiv.org/pdf/2509.26209.pdf</a></span>   <span><a href='https://github.com/NJU-RL/DIVER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zican Hu, Shilin Zhang, Yafu Li, Jianhao Yan, Xuyang Hu, Leyang Cui, Xiaoye Qu, Chunlin Chen, Yu Cheng, Zhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26209">Diversity-Incentivized Exploration for Versatile Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a crucial paradigm for incentivizing reasoning capabilities in Large Language Models (LLMs). Due to vast state-action spaces and reward sparsity in reasoning tasks, existing methods often struggle with deficient exploration and poor sample efficiency. In the paper, we propose \textbf{DIVER} (\textbf{D}iversity-\textbf{I}ncentivized Exploration for \textbf{V}ersatil\textbf{E} \textbf{R}easoning), an innovative framework that highlights the pivotal role of global sequence-level diversity to incentivize deep exploration for versatile reasoning. We first conduct a primary empirical study to reveal a strong positive correlation between global diversity and reasoning capacity. Building on this insight, we introduce global diversity incentives as an intrinsic reward to promote deep exploration in a semantically structured space. Incorporating the intrinsic reward, we develop a potential-based reward shaping mechanism to preserve optimal policy invariance and design simple heuristics to mitigate possible reward hacking. Experimental results show that DIVER outperforms competitive RLVR baselines with various exploration strategies on both in-domain and out-of-domain tasks, excelling in both Pass@1 and Pass@k evaluations. Our code is available at https://github.com/NJU-RL/DIVER.<br>
<span id='abs_ch'>Chinese: 本文提出DIVER框架，通过激励全局序列多 性来促进深度探索，提升大型语言模型在推理任务中的强化学 效果与 本效率。</span><br>
<span id='abs_en'>English: The paper introduces DIVER, a framework that enhances reinforcement learning for versatile reasoning in large language models by incentivizing global sequence-level diversity to promote deep exploration and improve sample efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2509.26200.pdf' target='_blank'>https://arxiv.org/pdf/2509.26200.pdf</a></span>   <span><a href='https://github.com/HatimChergui/unbiased-collective-memory' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hatim Chergui, Miguel Catalan Cid, Pouria Sayyad Khodashenas, Daniel Camps Mur, Christos Verikoukis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26200">Toward an Unbiased Collective Memory for Efficient LLM-Based Agentic 6G Cross-Domain Management</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper introduces a novel framework for proactive cross-domain resource orchestration in 6G RAN-Edge networks, featuring large language model (LLM)-augmented agents. The system comprises specialized RAN (energy efficiency) and Edge (latency assurance) agents that engage in iterative negotiation, supported by advanced reasoning and planning capabilities. Agents dynamically interact with a digital twin (DT) to test their proposals and leverage a long-term collective memory where their joint successful and failed agreements along with the related network contexts are distilled into strategies to either follow or avoid and subsequently stored. Given that agents are subject to a plethora of cognitive distortions when retrieving those past experiences -- such as primacy, recency, confirmation and availability biases -- we propose in this work a novel unbiased memory design (A reusable mockup version of the unbiased memory source code is available for non-commercial use at https://github.com/HatimChergui/unbiased-collective-memory). featuring (i) semantic retrieval of past strategies via Jaccard similarity; (ii) learning from failures through amplified weighting of SLA violations and mandatory inclusion of failed negotiation cases to mitigate confirmation bias; (iii) diversity enforcement to minimize availability bias and (iv) recency and primacy weighting with slow decay to counteract temporal biases. Evaluation results showcase the impact of existing biases and how the unbiased memory allows to tackle them by learning from both successful and failed strategies, either present or old, resulting in $\times 4.5$ and $\times 3.5$ reductions of unresolved negotiations compared to non-memory and vanilla memory baselines, respectively, while totally mitigating SLA violations as well as improving latency and energy saving distributions.<br>
<span id='abs_ch'>中文: 本文提出了一种6G 线接入网与边缘网络的新型框架，采用大语言模型增强的智能体通过数字孪生测试和 偏集体记忆系统进行协商，该记忆系统通过偏差缓解策略将未解决协商减少4.5倍并完全消除服务等级协议违规。English: This paper proposes a novel framework for 6G RAN-Edge networks using LLM-augmented agents that negotiate via digital twin testing and an unbiased collective memory system, which reduces unresolved negotiations by 4.5× and eliminates SLA violations through bias-mitigating strategies.Paperid:35,https://arxiv.org/pdf/2509.26182.pdfGitHub</span><br>
<span id='abs_en'>English: This paper proposes a novel framework for 6G RAN-Edge networks using LLM-augmented agents that negotiate via digital twin testing and an unbiased collective memory system, which reduces unresolved negotiations by 4.5× and eliminates SLA violations through bias-mitigating strategies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2509.26200.pdf' target='_blank'>https://arxiv.org/pdf/2509.26200.pdf</a></span>   <span><a href='https://github.com/HatimChergui/unbiased-collective-memory' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hatim Chergui, Miguel Catalan Cid, Pouria Sayyad Khodashenas, Daniel Camps Mur, Christos Verikoukis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26200">Toward an Unbiased Collective Memory for Efficient LLM-Based Agentic 6G Cross-Domain Management</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper introduces a novel framework for proactive cross-domain resource orchestration in 6G RAN-Edge networks, featuring large language model (LLM)-augmented agents. The system comprises specialized RAN (energy efficiency) and Edge (latency assurance) agents that engage in iterative negotiation, supported by advanced reasoning and planning capabilities. Agents dynamically interact with a digital twin (DT) to test their proposals and leverage a long-term collective memory where their joint successful and failed agreements along with the related network contexts are distilled into strategies to either follow or avoid and subsequently stored. Given that agents are subject to a plethora of cognitive distortions when retrieving those past experiences -- such as primacy, recency, confirmation and availability biases -- we propose in this work a novel unbiased memory design (A reusable mockup version of the unbiased memory source code is available for non-commercial use at https://github.com/HatimChergui/unbiased-collective-memory). featuring (i) semantic retrieval of past strategies via Jaccard similarity; (ii) learning from failures through amplified weighting of SLA violations and mandatory inclusion of failed negotiation cases to mitigate confirmation bias; (iii) diversity enforcement to minimize availability bias and (iv) recency and primacy weighting with slow decay to counteract temporal biases. Evaluation results showcase the impact of existing biases and how the unbiased memory allows to tackle them by learning from both successful and failed strategies, either present or old, resulting in $\times 4.5$ and $\times 3.5$ reductions of unresolved negotiations compared to non-memory and vanilla memory baselines, respectively, while totally mitigating SLA violations as well as improving latency and energy saving distributions.<br>
<span id='abs_ch'>中文: 本文提出了一种6G 线接入网与边缘网络的新型框架，采用大语言模型增强的智能体通过数字孪生测试和 偏集体记忆系统进行协商，该记忆系统通过偏差缓解策略将未解决协商减少4.5倍并完全消除服务等级协议违规。English: This paper proposes a novel framework for 6G RAN-Edge networks using LLM-augmented agents that negotiate via digital twin testing and an unbiased collective memory system, which reduces unresolved negotiations by 4.5× and eliminates SLA violations through bias-mitigating strategies.Paperid:35,https://arxiv.org/pdf/2509.26182.pdfGitHub</span><br>
<span id='abs_en'>English: This paper proposes a novel framework for 6G RAN-Edge networks using LLM-augmented agents that negotiate via digital twin testing and an unbiased collective memory system, which reduces unresolved negotiations by 4.5× and eliminates SLA violations through bias-mitigating strategies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2509.26161.pdf' target='_blank'>https://arxiv.org/pdf/2509.26161.pdf</a></span>   <span><a href='https://github.com/yxwan123/UniGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Runxin Yang, Yuxuan Wan, Shuqing Li, Michael R. Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26161">90% Faster, 100% Code-Free: MLLM-Driven Zero-Code 3D Game Development</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Developing 3D games requires specialized expertise across multiple domains, including programming, 3D modeling, and engine configuration, which limits access to millions of potential creators. Recently, researchers have begun to explore automated game development. However, existing approaches face three primary challenges: (1) limited scope to 2D content generation or isolated code snippets; (2) requirement for manual integration of generated components into game engines; and (3) poor performance on handling interactive game logic and state management. While Multimodal Large Language Models (MLLMs) demonstrate potential capabilities to ease the game generation task, a critical gap still remains in translating these outputs into production-ready, executable game projects based on game engines such as Unity and Unreal Engine. To bridge the gap, this paper introduces UniGen, the first end-to-end coordinated multi-agent framework that automates zero-coding development of runnable 3D games from natural language requirements. Specifically, UniGen uses a Planning Agent that interprets user requirements into structured blueprints and engineered logic descriptions; after which a Generation Agent produces executable C# scripts; then an Automation Agent handles engine-specific component binding and scene construction; and lastly a Debugging Agent provides real-time error correction through conversational interaction. We evaluated UniGen on three distinct game prototypes. Results demonstrate that UniGen not only democratizes game creation by requiring no coding from the user, but also reduces development time by 91.4%. We release UniGen at https://github.com/yxwan123/UniGen. A video demonstration is available at https://www.youtube.com/watch?v=xyJjFfnxUx0.<br>
<span id='abs_ch'>中文：本文提出UniGen框架，通过多智能体协作实现从自然语言需求到可运行3D游戏的端到端自动开发，解决了现有方法在游戏逻辑处理和引擎集成方面的不足，使非专业用户 需编程即可快速创建游戏。</span><br>
<span id='abs_en'>English: This paper introduces UniGen, an automated multi-agent framework that enables zero-coding development of executable 3D games from natural language, overcoming current limitations in game generation by integrating planning, script generation, engine automation, and debugging.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2509.26161.pdf' target='_blank'>https://arxiv.org/pdf/2509.26161.pdf</a></span>   <span><a href='https://github.com/yxwan123/UniGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Runxin Yang, Yuxuan Wan, Shuqing Li, Michael R. Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26161">90% Faster, 100% Code-Free: MLLM-Driven Zero-Code 3D Game Development</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Developing 3D games requires specialized expertise across multiple domains, including programming, 3D modeling, and engine configuration, which limits access to millions of potential creators. Recently, researchers have begun to explore automated game development. However, existing approaches face three primary challenges: (1) limited scope to 2D content generation or isolated code snippets; (2) requirement for manual integration of generated components into game engines; and (3) poor performance on handling interactive game logic and state management. While Multimodal Large Language Models (MLLMs) demonstrate potential capabilities to ease the game generation task, a critical gap still remains in translating these outputs into production-ready, executable game projects based on game engines such as Unity and Unreal Engine. To bridge the gap, this paper introduces UniGen, the first end-to-end coordinated multi-agent framework that automates zero-coding development of runnable 3D games from natural language requirements. Specifically, UniGen uses a Planning Agent that interprets user requirements into structured blueprints and engineered logic descriptions; after which a Generation Agent produces executable C# scripts; then an Automation Agent handles engine-specific component binding and scene construction; and lastly a Debugging Agent provides real-time error correction through conversational interaction. We evaluated UniGen on three distinct game prototypes. Results demonstrate that UniGen not only democratizes game creation by requiring no coding from the user, but also reduces development time by 91.4%. We release UniGen at https://github.com/yxwan123/UniGen. A video demonstration is available at https://www.youtube.com/watch?v=xyJjFfnxUx0.<br>
<span id='abs_ch'>中文：本文提出UniGen框架，通过多智能体协作实现从自然语言需求到可运行3D游戏的端到端自动开发，解决了现有方法在游戏逻辑处理和引擎集成方面的不足，使非专业用户 需编程即可快速创建游戏。</span><br>
<span id='abs_en'>English: This paper introduces UniGen, an automated multi-agent framework that enables zero-coding development of executable 3D games from natural language, overcoming current limitations in game generation by integrating planning, script generation, engine automation, and debugging.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2509.26158.pdf' target='_blank'>https://arxiv.org/pdf/2509.26158.pdf</a></span>   <span><a href='https://github.com/gokyeongryeol/ATES' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kyeongryeol Go
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26158">Towards Continual Expansion of Data Coverage: Automatic Text-guided Edge-case Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The performance of deep neural networks is strongly influenced by the quality of their training data. However, mitigating dataset bias by manually curating challenging edge cases remains a major bottleneck. To address this, we propose an automated pipeline for text-guided edge-case synthesis. Our approach employs a Large Language Model, fine-tuned via preference learning, to rephrase image captions into diverse textual prompts that steer a Text-to-Image model toward generating difficult visual scenarios. Evaluated on the FishEye8K object detection benchmark, our method achieves superior robustness, surpassing both naive augmentation and manually engineered prompts. This work establishes a scalable framework that shifts data curation from manual effort to automated, targeted synthesis, offering a promising direction for developing more reliable and continuously improving AI systems. Code is available at https://github.com/gokyeongryeol/ATES.<br>
<span id='abs_ch'>中文: 本文提出了一种自动化流程，通过微调的大型语言模型生成多 化文本提示，引导文本到图像模型合成具有挑战性的边缘案例，从而提升深度神经网络的鲁棒性，并在FishEye8K基准测试中验证了其优越性。</span><br>
<span id='abs_en'>English: This paper introduces an automated pipeline that leverages a fine-tuned Large Language Model to generate diverse textual prompts, enabling a Text-to-Image model to synthesize challenging edge cases for improving deep neural network robustness, as validated on the FishEye8K benchmark.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2509.26158.pdf' target='_blank'>https://arxiv.org/pdf/2509.26158.pdf</a></span>   <span><a href='https://github.com/gokyeongryeol/ATES' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kyeongryeol Go
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26158">Towards Continual Expansion of Data Coverage: Automatic Text-guided Edge-case Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The performance of deep neural networks is strongly influenced by the quality of their training data. However, mitigating dataset bias by manually curating challenging edge cases remains a major bottleneck. To address this, we propose an automated pipeline for text-guided edge-case synthesis. Our approach employs a Large Language Model, fine-tuned via preference learning, to rephrase image captions into diverse textual prompts that steer a Text-to-Image model toward generating difficult visual scenarios. Evaluated on the FishEye8K object detection benchmark, our method achieves superior robustness, surpassing both naive augmentation and manually engineered prompts. This work establishes a scalable framework that shifts data curation from manual effort to automated, targeted synthesis, offering a promising direction for developing more reliable and continuously improving AI systems. Code is available at https://github.com/gokyeongryeol/ATES.<br>
<span id='abs_ch'>中文: 本文提出了一种自动化流程，通过微调的大型语言模型生成多 化文本提示，引导文本到图像模型合成具有挑战性的边缘案例，从而提升深度神经网络的鲁棒性，并在FishEye8K基准测试中验证了其优越性。</span><br>
<span id='abs_en'>English: This paper introduces an automated pipeline that leverages a fine-tuned Large Language Model to generate diverse textual prompts, enabling a Text-to-Image model to synthesize challenging edge cases for improving deep neural network robustness, as validated on the FishEye8K benchmark.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2509.26157.pdf' target='_blank'>https://arxiv.org/pdf/2509.26157.pdf</a></span>   <span><a href='https://github.com/Sachithx/EntroPE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sachith Abeywickrama, Emadeldeen Eldele, Min Wu, Xiaoli Li, Chau Yuen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26157">EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series Forecasting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Transformer-based models have significantly advanced time series forecasting, with patch-based input strategies offering efficiency and improved long-horizon modeling. Yet, existing approaches rely on temporally-agnostic patch construction, where arbitrary starting positions and fixed lengths fracture temporal coherence by splitting natural transitions across boundaries. This naive segmentation often disrupts short-term dependencies and weakens representation learning. In response, we propose EntroPE (Entropy-Guided Dynamic Patch Encoder), a novel, temporally informed framework that dynamically detects transition points via conditional entropy and dynamically places patch boundaries. This preserves temporal structure while retaining the computational benefits of patching. EntroPE consists of two key modules, namely an Entropy-based Dynamic Patcher (EDP) that applies information-theoretic criteria to locate natural temporal shifts and determine patch boundaries, and an Adaptive Patch Encoder (APE) that employs pooling and cross-attention to capture intra-patch dependencies and produce fixed-size latent representations. These embeddings are then processed by a global transformer to model inter-patch dynamics. Experiments across long-term forecasting benchmarks demonstrate that EntroPE improves both accuracy and efficiency, establishing entropy-guided dynamic patching as a promising new paradigm for time series modeling. Code is available at: https://github.com/Sachithx/EntroPE.<br>
<span id='abs_ch'>中文摘要：提出的EntroPE框架通过熵引导的动态分块技术保持时间序列的时序连贯性，在保留计算效率的同时克服了固定分块方法的局限性。</span><br>
<span id='abs_en'>English Summary: The proposed EntroPE framework introduces entropy-guided dynamic patching to preserve temporal coherence in time series forecasting, overcoming limitations of fixed patch segmentation while maintaining computational efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2509.26157.pdf' target='_blank'>https://arxiv.org/pdf/2509.26157.pdf</a></span>   <span><a href='https://github.com/Sachithx/EntroPE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sachith Abeywickrama, Emadeldeen Eldele, Min Wu, Xiaoli Li, Chau Yuen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26157">EntroPE: Entropy-Guided Dynamic Patch Encoder for Time Series Forecasting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Transformer-based models have significantly advanced time series forecasting, with patch-based input strategies offering efficiency and improved long-horizon modeling. Yet, existing approaches rely on temporally-agnostic patch construction, where arbitrary starting positions and fixed lengths fracture temporal coherence by splitting natural transitions across boundaries. This naive segmentation often disrupts short-term dependencies and weakens representation learning. In response, we propose EntroPE (Entropy-Guided Dynamic Patch Encoder), a novel, temporally informed framework that dynamically detects transition points via conditional entropy and dynamically places patch boundaries. This preserves temporal structure while retaining the computational benefits of patching. EntroPE consists of two key modules, namely an Entropy-based Dynamic Patcher (EDP) that applies information-theoretic criteria to locate natural temporal shifts and determine patch boundaries, and an Adaptive Patch Encoder (APE) that employs pooling and cross-attention to capture intra-patch dependencies and produce fixed-size latent representations. These embeddings are then processed by a global transformer to model inter-patch dynamics. Experiments across long-term forecasting benchmarks demonstrate that EntroPE improves both accuracy and efficiency, establishing entropy-guided dynamic patching as a promising new paradigm for time series modeling. Code is available at: https://github.com/Sachithx/EntroPE.<br>
<span id='abs_ch'>中文摘要：提出的EntroPE框架通过熵引导的动态分块技术保持时间序列的时序连贯性，在保留计算效率的同时克服了固定分块方法的局限性。</span><br>
<span id='abs_en'>English Summary: The proposed EntroPE framework introduces entropy-guided dynamic patching to preserve temporal coherence in time series forecasting, overcoming limitations of fixed patch segmentation while maintaining computational efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2509.26128.pdf' target='_blank'>https://arxiv.org/pdf/2509.26128.pdf</a></span>   <span><a href='https://github.com/medakakg/medaka' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Asmita Sengupta, David Antony Selby, Sebastian Josef Vollmer, Gerrit Großmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26128">MEDAKA: Construction of Biomedical Knowledge Graphs Using Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Knowledge graphs (KGs) are increasingly used to represent biomedical information in structured, interpretable formats. However, existing biomedical KGs often focus narrowly on molecular interactions or adverse events, overlooking the rich data found in drug leaflets. In this work, we present (1) a hackable, end-to-end pipeline to create KGs from unstructured online content using a web scraper and an LLM; and (2) a curated dataset, MEDAKA, generated by applying this method to publicly available drug leaflets. The dataset captures clinically relevant attributes such as side effects, warnings, contraindications, ingredients, dosage guidelines, storage instructions and physical characteristics. We evaluate it through manual inspection and with an LLM-as-a-Judge framework, and compare its coverage with existing biomedical KGs and databases. We expect MEDAKA to support tasks such as patient safety monitoring and drug recommendation. The pipeline can also be used for constructing KGs from unstructured texts in other domains. Code and dataset are available at https://github.com/medakakg/medaka.<br>
<span id='abs_ch'>中文：本 究提出了一种从非结构化在线内容构建知识图谱的可扩展流程，特别通过处理药品说明书生成了MEDAKA数据集，该数据集涵盖全面的临床属性，并经过人工与自动评估验证，旨在支持生物医学应用。</span><br>
<span id='abs_en'>English: This work introduces a flexible pipeline for generating knowledge graphs from unstructured online content, specifically creating the MEDAKA dataset from drug leaflets to capture comprehensive clinical attributes, which is validated through manual and automated evaluation to support biomedical applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2509.26128.pdf' target='_blank'>https://arxiv.org/pdf/2509.26128.pdf</a></span>   <span><a href='https://github.com/medakakg/medaka' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Asmita Sengupta, David Antony Selby, Sebastian Josef Vollmer, Gerrit GroÃmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26128">MEDAKA: Construction of Biomedical Knowledge Graphs Using Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Knowledge graphs (KGs) are increasingly used to represent biomedical information in structured, interpretable formats. However, existing biomedical KGs often focus narrowly on molecular interactions or adverse events, overlooking the rich data found in drug leaflets. In this work, we present (1) a hackable, end-to-end pipeline to create KGs from unstructured online content using a web scraper and an LLM; and (2) a curated dataset, MEDAKA, generated by applying this method to publicly available drug leaflets. The dataset captures clinically relevant attributes such as side effects, warnings, contraindications, ingredients, dosage guidelines, storage instructions and physical characteristics. We evaluate it through manual inspection and with an LLM-as-a-Judge framework, and compare its coverage with existing biomedical KGs and databases. We expect MEDAKA to support tasks such as patient safety monitoring and drug recommendation. The pipeline can also be used for constructing KGs from unstructured texts in other domains. Code and dataset are available at https://github.com/medakakg/medaka.<br>
<span id='abs_ch'>中文：本 究提出了一种从非结构化在线内容构建知识图谱的可扩展流程，特别通过处理药品说明书生成了MEDAKA数据集，该数据集涵盖全面的临床属性，并经过人工与自动评估验证，旨在支持生物医学应用。</span><br>
<span id='abs_en'>English: This work introduces a flexible pipeline for generating knowledge graphs from unstructured online content, specifically creating the MEDAKA dataset from drug leaflets to capture comprehensive clinical attributes, which is validated through manual and automated evaluation to support biomedical applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2509.26036.pdf' target='_blank'>https://arxiv.org/pdf/2509.26036.pdf</a></span>   <span><a href='https://github.com/christti98/semobridge' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Christoph Timmermann, Hyunse Lee, Woojin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.26036">SeMoBridge: Semantic Modality Bridge for Efficient Few-Shot Adaptation of CLIP</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While Contrastive Language-Image Pretraining (CLIP) excels at zero-shot tasks by aligning image and text embeddings, its performance in few-shot classification is hindered by a critical limitation: intra-modal misalignment. This issue, caused by a persistent modality gap and CLIP's exclusively inter-modal training objective, leaves the embedding spaces uncalibrated, making direct image-to-image comparisons unreliable. Existing methods attempt to address this by refining similarity logits or by computationally expensive per-sample optimization. To overcome these challenges, we introduce SeMoBridge, a lightweight yet powerful approach that directly addresses the misalignment. Our method maps images into the text modality, while keeping their semantic content intact through what we call a Semantic Modality Bridge. SeMoBridge is closed-form and can optionally be trained through multi-modal supervision, combining image and text-alignment losses to optimize the projection. Experiments show that the trained version, SeMoBridge-T, requires only a fraction of the training time while overall outperforming other methods, particularly in low-data scenarios (1, 2, and 4 shots). The code is available at https://github.com/christti98/semobridge.<br>
<span id='abs_ch'>中文: SeMoBridge 是一种轻量级方法，通过将图像 射到文本模态并保持语义完整性来解决 CLIP 的模态内错位问题，在少量 本场景中以极短训练时间实现卓越性能。</span><br>
<span id='abs_en'>English: SeMoBridge is a lightweight method that addresses CLIP's intra-modal misalignment by mapping images into the text modality while preserving semantics, achieving superior few-shot performance with minimal training time.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2509.25922.pdf' target='_blank'>https://arxiv.org/pdf/2509.25922.pdf</a></span>   <span><a href='https://github.com/GTS-AI-Infra-Lab-SotaS/DeepJSONEval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhicheng Zhou, Jing Li, Suming Qiu, Junjie Huang, Linyuan Qiu, Zhijie Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25922">DeepJSONEval: Benchmarking Complex Nested JSON Data Mining for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The internet is saturated with low-density, high-redundancy information, such as social media comments, repetitive news, and lengthy discussions, making it difficult to extract valuable insights efficiently. Multi-layer nested JSON structures provide an effective solution by compressing such information into semantically rich, hierarchical representations, which organize data into key-value pairs, arrays, and nested objects, preserving contextual relationships and enabling efficient storage, retrieval, and semantic querying. For instance, in news aggregation, a JSON object can nest an article's metadata (title, author, date), content (text, multimedia), and multimedia information (multimedia type, caption) hierarchically. Large Language Models (LLMs) play a transformative role in web data mining by parsing unstructured text and outputting structured results directly into complex JSON schemas. However, current benchmarks for evaluating LLMs' JSON output capabilities overemphasize pure JSON generation rather than assessing data comprehension and extraction abilities, a limitation that lacks relevance to practical web data mining tasks. To address this, we introduce DeepJSONEval, a novel benchmark featuring 2100 multi-domain instances with deep nested structures, categorized by difficulty. Experiments show significant performance gaps among LLMs in handling such complexity. Our benchmark and datasets are open-sourced to advance research in structured JSON generation.(https://github.com/GTS-AI-Infra-Lab-SotaS/DeepJSONEval).<br>
<span id='abs_ch'>中文摘要：互联网信息过载问题可通过多层嵌套JSON结构实现高效分层压缩，而现有大语言模型基准过于侧重 式生成却忽略实际数据提取能力，为此推出DeepJSONEval基准以评估复杂JSON处理性能。English Summary: The internet's information overload is effectively managed by using multi-layer nested JSON structures for hierarchical data compression, while current LLM benchmarks inadequately assess practical data extraction skills, prompting the introduction of the DeepJSONEval benchmark to evaluate complex JSON generation.Paperid:57,https://arxiv.org/pdf/2509.25868.pdfGitHub</span><br>
<span id='abs_en'>English Summary: The internet's information overload is effectively managed by using multi-layer nested JSON structures for hierarchical data compression, while current LLM benchmarks inadequately assess practical data extraction skills, prompting the introduction of the DeepJSONEval benchmark to evaluate complex JSON generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2509.25922.pdf' target='_blank'>https://arxiv.org/pdf/2509.25922.pdf</a></span>   <span><a href='https://github.com/GTS-AI-Infra-Lab-SotaS/DeepJSONEval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhicheng Zhou, Jing Li, Suming Qiu, Junjie Huang, Linyuan Qiu, Zhijie Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25922">DeepJSONEval: Benchmarking Complex Nested JSON Data Mining for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The internet is saturated with low-density, high-redundancy information, such as social media comments, repetitive news, and lengthy discussions, making it difficult to extract valuable insights efficiently. Multi-layer nested JSON structures provide an effective solution by compressing such information into semantically rich, hierarchical representations, which organize data into key-value pairs, arrays, and nested objects, preserving contextual relationships and enabling efficient storage, retrieval, and semantic querying. For instance, in news aggregation, a JSON object can nest an article's metadata (title, author, date), content (text, multimedia), and multimedia information (multimedia type, caption) hierarchically. Large Language Models (LLMs) play a transformative role in web data mining by parsing unstructured text and outputting structured results directly into complex JSON schemas. However, current benchmarks for evaluating LLMs' JSON output capabilities overemphasize pure JSON generation rather than assessing data comprehension and extraction abilities, a limitation that lacks relevance to practical web data mining tasks. To address this, we introduce DeepJSONEval, a novel benchmark featuring 2100 multi-domain instances with deep nested structures, categorized by difficulty. Experiments show significant performance gaps among LLMs in handling such complexity. Our benchmark and datasets are open-sourced to advance research in structured JSON generation.(https://github.com/GTS-AI-Infra-Lab-SotaS/DeepJSONEval).<br>
<span id='abs_ch'>中文摘要：互联网信息过载问题可通过多层嵌套JSON结构实现高效分层压缩，而现有大语言模型基准过于侧重 式生成却忽略实际数据提取能力，为此推出DeepJSONEval基准以评估复杂JSON处理性能。English Summary: The internet's information overload is effectively managed by using multi-layer nested JSON structures for hierarchical data compression, while current LLM benchmarks inadequately assess practical data extraction skills, prompting the introduction of the DeepJSONEval benchmark to evaluate complex JSON generation.Paperid:57,https://arxiv.org/pdf/2509.25868.pdfGitHub</span><br>
<span id='abs_en'>English Summary: The internet's information overload is effectively managed by using multi-layer nested JSON structures for hierarchical data compression, while current LLM benchmarks inadequately assess practical data extraction skills, prompting the introduction of the DeepJSONEval benchmark to evaluate complex JSON generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2509.25862.pdf' target='_blank'>https://arxiv.org/pdf/2509.25862.pdf</a></span>   <span><a href='https://github.com/OlgaKrestinskaya/CIMNAS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Olga Krestinskaya, Mohammed E. Fouda, Ahmed Eltawil, Khaled N. Salama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25862">CIMNAS: A Joint Framework for Compute-In-Memory-Aware Neural Architecture Search</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>To maximize hardware efficiency and performance accuracy in Compute-In-Memory (CIM)-based neural network accelerators for Artificial Intelligence (AI) applications, co-optimizing both software and hardware design parameters is essential. Manual tuning is impractical due to the vast number of parameters and their complex interdependencies. To effectively automate the design and optimization of CIM-based neural network accelerators, hardware-aware neural architecture search (HW-NAS) techniques can be applied. This work introduces CIMNAS, a joint model-quantization-hardware optimization framework for CIM architectures. CIMNAS simultaneously searches across software parameters, quantization policies, and a broad range of hardware parameters, incorporating device-, circuit-, and architecture-level co-optimizations. CIMNAS experiments were conducted over a search space of 9.9x10^85 potential parameter combinations with the MobileNet model as a baseline and RRAM-based CIM architecture. Evaluated on the ImageNet dataset, CIMNAS achieved a reduction in energy-delay-area product (EDAP) ranging from 90.1x to 104.5x, an improvement in TOPS/W between 4.68x and 4.82x, and an enhancement in TOPS/mm^2 from 11.3x to 12.78x relative to various baselines, all while maintaining an accuracy of 73.81%. The adaptability and robustness of CIMNAS are demonstrated by extending the framework to support the SRAM-based ResNet50 architecture, achieving up to an 819.5x reduction in EDAP. Unlike other state-of-the-art methods, CIMNAS achieves EDAP-focused optimization without any accuracy loss, generating diverse software-hardware parameter combinations for high-performance CIM-based neural network designs. The source code of CIMNAS is available at https://github.com/OlgaKrestinskaya/CIMNAS.<br>
<span id='abs_ch'>中文：CIMNAS是一个联合优化软件、量化和硬件参数的计算内存神经网络 速器框架，能在保持精度的同时显著提升能效。</span><br>
<span id='abs_en'>English: CIMNAS is a comprehensive framework that co-optimizes software, quantization, and hardware parameters for compute-in-memory neural network accelerators, achieving significant efficiency gains without accuracy loss.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2509.25862.pdf' target='_blank'>https://arxiv.org/pdf/2509.25862.pdf</a></span>   <span><a href='https://github.com/OlgaKrestinskaya/CIMNAS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Olga Krestinskaya, Mohammed E. Fouda, Ahmed Eltawil, Khaled N. Salama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25862">CIMNAS: A Joint Framework for Compute-In-Memory-Aware Neural Architecture Search</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>To maximize hardware efficiency and performance accuracy in Compute-In-Memory (CIM)-based neural network accelerators for Artificial Intelligence (AI) applications, co-optimizing both software and hardware design parameters is essential. Manual tuning is impractical due to the vast number of parameters and their complex interdependencies. To effectively automate the design and optimization of CIM-based neural network accelerators, hardware-aware neural architecture search (HW-NAS) techniques can be applied. This work introduces CIMNAS, a joint model-quantization-hardware optimization framework for CIM architectures. CIMNAS simultaneously searches across software parameters, quantization policies, and a broad range of hardware parameters, incorporating device-, circuit-, and architecture-level co-optimizations. CIMNAS experiments were conducted over a search space of 9.9x10^85 potential parameter combinations with the MobileNet model as a baseline and RRAM-based CIM architecture. Evaluated on the ImageNet dataset, CIMNAS achieved a reduction in energy-delay-area product (EDAP) ranging from 90.1x to 104.5x, an improvement in TOPS/W between 4.68x and 4.82x, and an enhancement in TOPS/mm^2 from 11.3x to 12.78x relative to various baselines, all while maintaining an accuracy of 73.81%. The adaptability and robustness of CIMNAS are demonstrated by extending the framework to support the SRAM-based ResNet50 architecture, achieving up to an 819.5x reduction in EDAP. Unlike other state-of-the-art methods, CIMNAS achieves EDAP-focused optimization without any accuracy loss, generating diverse software-hardware parameter combinations for high-performance CIM-based neural network designs. The source code of CIMNAS is available at https://github.com/OlgaKrestinskaya/CIMNAS.<br>
<span id='abs_ch'>中文：CIMNAS是一个联合优化软件、量化和硬件参数的计算内存神经网络 速器框架，能在保持精度的同时显著提升能效。</span><br>
<span id='abs_en'>English: CIMNAS is a comprehensive framework that co-optimizes software, quantization, and hardware parameters for compute-in-memory neural network accelerators, achieving significant efficiency gains without accuracy loss.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2509.25775.pdf' target='_blank'>https://arxiv.org/pdf/2509.25775.pdf</a></span>   <span><a href='https://github.com/salar96/AutonomyAwareClustering' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amber Srivastava, Salar Basiri, Srinivasa Salapaka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25775">Autonomy-Aware Clustering: When Local Decisions Supersede Global Prescriptions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Clustering arises in a wide range of problem formulations, yet most existing approaches assume that the entities under clustering are passive and strictly conform to their assigned groups. In reality, entities often exhibit local autonomy, overriding prescribed associations in ways not fully captured by feature representations. Such autonomy can substantially reshape clustering outcomes -- altering cluster compositions, geometry, and cardinality -- with significant downstream effects on inference and decision-making. We introduce autonomy-aware clustering, a reinforcement learning (RL) framework that learns and accounts for the influence of local autonomy without requiring prior knowledge of its form. Our approach integrates RL with a Deterministic Annealing (DA) procedure, where, to determine underlying clusters, DA naturally promotes exploration in early stages of annealing and transitions to exploitation later. We also show that the annealing procedure exhibits phase transitions that enable design of efficient annealing schedules. To further enhance adaptability, we propose the Adaptive Distance Estimation Network (ADEN), a transformer-based attention model that learns dependencies between entities and cluster representatives within the RL loop, accommodates variable-sized inputs and outputs, and enables knowledge transfer across diverse problem instances. Empirical results show that our framework closely aligns with underlying data dynamics: even without explicit autonomy models, it achieves solutions close to the ground truth (gap ~3-4%), whereas ignoring autonomy leads to substantially larger gaps (~35-40%). The code and data are publicly available at https://github.com/salar96/AutonomyAwareClustering.<br>
<span id='abs_ch'>中文摘要：本文提出了一种自主感知聚类框架，结合强化学 和确定性退火算法来考虑实体的局部自主性， 需先验自主模型即可获得接近真实情况的聚类结果。</span><br>
<span id='abs_en'>English summary: This paper introduces an autonomy-aware clustering framework using reinforcement learning and deterministic annealing to account for entities' local autonomy, achieving near-ground-truth results without prior autonomy models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2509.25771.pdf' target='_blank'>https://arxiv.org/pdf/2509.25771.pdf</a></span>   <span><a href='https://github.com/DSL-Lab/T2I-Free-Lunch-Alignment' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Jun Cheng Xian, Muchen Li, Haotian Yang, Xin Tao, Pengfei Wan, Leonid Sigal, Renjie Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25771">Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in diffusion-based text-to-image (T2I) models have led to remarkable success in generating high-quality images from textual prompts. However, ensuring accurate alignment between the text and the generated image remains a significant challenge for state-of-the-art diffusion models. To address this, existing studies employ reinforcement learning with human feedback (RLHF) to align T2I outputs with human preferences. These methods, however, either rely directly on paired image preference data or require a learned reward function, both of which depend heavily on costly, high-quality human annotations and thus face scalability limitations. In this work, we introduce Text Preference Optimization (TPO), a framework that enables "free-lunch" alignment of T2I models, achieving alignment without the need for paired image preference data. TPO works by training the model to prefer matched prompts over mismatched prompts, which are constructed by perturbing original captions using a large language model. Our framework is general and compatible with existing preference-based algorithms. We extend both DPO and KTO to our setting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations across multiple benchmarks show that our methods consistently outperform their original counterparts, delivering better human preference scores and improved text-to-image alignment. Our Open-source code is available at https://github.com/DSL-Lab/T2I-Free-Lunch-Alignment.<br>
<span id='abs_ch'>中文: 本文提出文本偏好优化（TPO）框架，通过训练模型区分匹配与不匹配提示词来实现文本-图像模型的免 注对齐，在多个基准测试中均显著提升人类偏好分数与图文对齐效果。</span><br>
<span id='abs_en'>English: This paper introduces Text Preference Optimization (TPO), a novel framework that enhances text-to-image model alignment by training models to prefer matched over mismatched prompts, eliminating the need for costly human-annotated image preference data while outperforming existing methods in human preference scores and alignment accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2509.25771.pdf' target='_blank'>https://arxiv.org/pdf/2509.25771.pdf</a></span>   <span><a href='https://github.com/DSL-Lab/T2I-Free-Lunch-Alignment' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Jun Cheng Xian, Muchen Li, Haotian Yang, Xin Tao, Pengfei Wan, Leonid Sigal, Renjie Liao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25771">Free Lunch Alignment of Text-to-Image Diffusion Models without Preference Image Pairs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in diffusion-based text-to-image (T2I) models have led to remarkable success in generating high-quality images from textual prompts. However, ensuring accurate alignment between the text and the generated image remains a significant challenge for state-of-the-art diffusion models. To address this, existing studies employ reinforcement learning with human feedback (RLHF) to align T2I outputs with human preferences. These methods, however, either rely directly on paired image preference data or require a learned reward function, both of which depend heavily on costly, high-quality human annotations and thus face scalability limitations. In this work, we introduce Text Preference Optimization (TPO), a framework that enables "free-lunch" alignment of T2I models, achieving alignment without the need for paired image preference data. TPO works by training the model to prefer matched prompts over mismatched prompts, which are constructed by perturbing original captions using a large language model. Our framework is general and compatible with existing preference-based algorithms. We extend both DPO and KTO to our setting, resulting in TDPO and TKTO. Quantitative and qualitative evaluations across multiple benchmarks show that our methods consistently outperform their original counterparts, delivering better human preference scores and improved text-to-image alignment. Our Open-source code is available at https://github.com/DSL-Lab/T2I-Free-Lunch-Alignment.<br>
<span id='abs_ch'>中文: 本文提出文本偏好优化（TPO）框架，通过训练模型区分匹配与不匹配提示词来实现文本-图像模型的免 注对齐，在多个基准测试中均显著提升人类偏好分数与图文对齐效果。</span><br>
<span id='abs_en'>English: This paper introduces Text Preference Optimization (TPO), a novel framework that enhances text-to-image model alignment by training models to prefer matched over mismatched prompts, eliminating the need for costly human-annotated image preference data while outperforming existing methods in human preference scores and alignment accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2509.25727.pdf' target='_blank'>https://arxiv.org/pdf/2509.25727.pdf</a></span>   <span><a href='https://github.com/HuikangSu/B2R' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huikang Su, Dengyun Peng, Zifeng Zhuang, YuHan Liu, Qiguang Chen, Donglin Wang, Qinghe Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25727">Boundary-to-Region Supervision for Offline Safe Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Offline safe reinforcement learning aims to learn policies that satisfy predefined safety constraints from static datasets. Existing sequence-model-based methods condition action generation on symmetric input tokens for return-to-go and cost-to-go, neglecting their intrinsic asymmetry: return-to-go (RTG) serves as a flexible performance target, while cost-to-go (CTG) should represent a rigid safety boundary. This symmetric conditioning leads to unreliable constraint satisfaction, especially when encountering out-of-distribution cost trajectories. To address this, we propose Boundary-to-Region (B2R), a framework that enables asymmetric conditioning through cost signal realignment . B2R redefines CTG as a boundary constraint under a fixed safety budget, unifying the cost distribution of all feasible trajectories while preserving reward structures. Combined with rotary positional embeddings , it enhances exploration within the safe region. Experimental results show that B2R satisfies safety constraints in 35 out of 38 safety-critical tasks while achieving superior reward performance over baseline methods. This work highlights the limitations of symmetric token conditioning and establishes a new theoretical and practical approach for applying sequence models to safe RL. Our code is available at https://github.com/HuikangSu/B2R.<br>
<span id='abs_ch'>中文摘要：本 究提出的边界到区域（B2R）框架通过成本信号重对齐实现非对称条件处理，有效解决了序列模型在离线安全强化学 中对成本约束的对称处理缺陷，在38项安全关键任务中实现35项的安全约束满足，同时获得优于基线方法的奖励表现。</span><br>
<span id='abs_en'>English Summary: The proposed Boundary-to-Region (B2R) framework addresses limitations in offline safe reinforcement learning by introducing asymmetric conditioning of cost-to-go signals, enabling reliable safety constraint satisfaction while maintaining high reward performance across diverse tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2509.25727.pdf' target='_blank'>https://arxiv.org/pdf/2509.25727.pdf</a></span>   <span><a href='https://github.com/HuikangSu/B2R' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huikang Su, Dengyun Peng, Zifeng Zhuang, YuHan Liu, Qiguang Chen, Donglin Wang, Qinghe Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25727">Boundary-to-Region Supervision for Offline Safe Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Offline safe reinforcement learning aims to learn policies that satisfy predefined safety constraints from static datasets. Existing sequence-model-based methods condition action generation on symmetric input tokens for return-to-go and cost-to-go, neglecting their intrinsic asymmetry: return-to-go (RTG) serves as a flexible performance target, while cost-to-go (CTG) should represent a rigid safety boundary. This symmetric conditioning leads to unreliable constraint satisfaction, especially when encountering out-of-distribution cost trajectories. To address this, we propose Boundary-to-Region (B2R), a framework that enables asymmetric conditioning through cost signal realignment . B2R redefines CTG as a boundary constraint under a fixed safety budget, unifying the cost distribution of all feasible trajectories while preserving reward structures. Combined with rotary positional embeddings , it enhances exploration within the safe region. Experimental results show that B2R satisfies safety constraints in 35 out of 38 safety-critical tasks while achieving superior reward performance over baseline methods. This work highlights the limitations of symmetric token conditioning and establishes a new theoretical and practical approach for applying sequence models to safe RL. Our code is available at https://github.com/HuikangSu/B2R.<br>
<span id='abs_ch'>中文摘要：本 究提出的边界到区域（B2R）框架通过成本信号重对齐实现非对称条件处理，有效解决了序列模型在离线安全强化学 中对成本约束的对称处理缺陷，在38项安全关键任务中实现35项的安全约束满足，同时获得优于基线方法的奖励表现。</span><br>
<span id='abs_en'>English Summary: The proposed Boundary-to-Region (B2R) framework addresses limitations in offline safe reinforcement learning by introducing asymmetric conditioning of cost-to-go signals, enabling reliable safety constraint satisfaction while maintaining high reward performance across diverse tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2509.25692.pdf' target='_blank'>https://arxiv.org/pdf/2509.25692.pdf</a></span>   <span><a href='https://github.com/tingyushi/CPATTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tingyu Shi, Fan Lyu, Shaoliang Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25692">Annotation-Efficient Active Test-Time Adaptation with Conformal Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Active Test-Time Adaptation (ATTA) improves model robustness under domain shift by selectively querying human annotations at deployment, but existing methods use heuristic uncertainty measures and suffer from low data selection efficiency, wasting human annotation budget. We propose Conformal Prediction Active TTA (CPATTA), which first brings principled, coverage-guaranteed uncertainty into ATTA. CPATTA employs smoothed conformal scores with a top-K certainty measure, an online weight-update algorithm driven by pseudo coverage, a domain-shift detector that adapts human supervision, and a staged update scheme balances human-labeled and model-labeled data. Extensive experiments demonstrate that CPATTA consistently outperforms the state-of-the-art ATTA methods by around 5% in accuracy. Our code and datasets are available at https://github.com/tingyushi/CPATTA.<br>
<span id='abs_ch'>中文摘要：CPATTA采用基于保形预测的理论框架改进主动测试时适应方法，通过优化的不确定性度量与自适应 注策略，在多项实验中比现有最优方法准确率提升约5%。</span><br>
<span id='abs_en'>English Summary: CPATTA introduces a principled conformal prediction framework to enhance active test-time adaptation, achieving approximately 5% higher accuracy than existing methods through improved uncertainty measurement and adaptive human annotation strategies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2509.25692.pdf' target='_blank'>https://arxiv.org/pdf/2509.25692.pdf</a></span>   <span><a href='https://github.com/tingyushi/CPATTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tingyu Shi, Fan Lyu, Shaoliang Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25692">Annotation-Efficient Active Test-Time Adaptation with Conformal Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Active Test-Time Adaptation (ATTA) improves model robustness under domain shift by selectively querying human annotations at deployment, but existing methods use heuristic uncertainty measures and suffer from low data selection efficiency, wasting human annotation budget. We propose Conformal Prediction Active TTA (CPATTA), which first brings principled, coverage-guaranteed uncertainty into ATTA. CPATTA employs smoothed conformal scores with a top-K certainty measure, an online weight-update algorithm driven by pseudo coverage, a domain-shift detector that adapts human supervision, and a staged update scheme balances human-labeled and model-labeled data. Extensive experiments demonstrate that CPATTA consistently outperforms the state-of-the-art ATTA methods by around 5% in accuracy. Our code and datasets are available at https://github.com/tingyushi/CPATTA.<br>
<span id='abs_ch'>中文摘要：CPATTA采用基于保形预测的理论框架改进主动测试时适应方法，通过优化的不确定性度量与自适应 注策略，在多项实验中比现有最优方法准确率提升约5%。</span><br>
<span id='abs_en'>English Summary: CPATTA introduces a principled conformal prediction framework to enhance active test-time adaptation, achieving approximately 5% higher accuracy than existing methods through improved uncertainty measurement and adaptive human annotation strategies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2509.25651.pdf' target='_blank'>https://arxiv.org/pdf/2509.25651.pdf</a></span>   <span><a href='https://github.com/pnnl/autolabs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gihan Panapitiya, Emily Saldanha, Heather Job, Olivia Hess
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25651">AutoLabs: Cognitive Multi-Agent Systems with Self-Correction for Autonomous Chemical Experimentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The automation of chemical research through self-driving laboratories (SDLs) promises to accelerate scientific discovery, yet the reliability and granular performance of the underlying AI agents remain critical, under-examined challenges. In this work, we introduce AutoLabs, a self-correcting, multi-agent architecture designed to autonomously translate natural-language instructions into executable protocols for a high-throughput liquid handler. The system engages users in dialogue, decomposes experimental goals into discrete tasks for specialized agents, performs tool-assisted stoichiometric calculations, and iteratively self-corrects its output before generating a hardware-ready file. We present a comprehensive evaluation framework featuring five benchmark experiments of increasing complexity, from simple sample preparation to multi-plate timed syntheses. Through a systematic ablation study of 20 agent configurations, we assess the impact of reasoning capacity, architectural design (single- vs. multi-agent), tool use, and self-correction mechanisms. Our results demonstrate that agent reasoning capacity is the most critical factor for success, reducing quantitative errors in chemical amounts (nRMSE) by over 85% in complex tasks. When combined with a multi-agent architecture and iterative self-correction, AutoLabs achieves near-expert procedural accuracy (F1-score > 0.89) on challenging multi-step syntheses. These findings establish a clear blueprint for developing robust and trustworthy AI partners for autonomous laboratories, highlighting the synergistic effects of modular design, advanced reasoning, and self-correction to ensure both performance and reliability in high-stakes scientific applications. Code: https://github.com/pnnl/autolabs<br>
<span id='abs_ch'>中文：AutoLabs提出了一种自我修正的多智能体系统，可将自然语言指令转化为可执行的实验流程，其高级推理和模块化设计使复杂任务的定量误差降低超85%，并在多步合成中实现接近专家水平的精确度。</span><br>
<span id='abs_en'>English: AutoLabs introduces a self-correcting multi-agent system that translates natural language into executable lab protocols, with advanced reasoning and modular design reducing quantitative errors by over 85% and achieving near-expert accuracy in complex syntheses.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2509.25651.pdf' target='_blank'>https://arxiv.org/pdf/2509.25651.pdf</a></span>   <span><a href='https://github.com/pnnl/autolabs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gihan Panapitiya, Emily Saldanha, Heather Job, Olivia Hess
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25651">AutoLabs: Cognitive Multi-Agent Systems with Self-Correction for Autonomous Chemical Experimentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The automation of chemical research through self-driving laboratories (SDLs) promises to accelerate scientific discovery, yet the reliability and granular performance of the underlying AI agents remain critical, under-examined challenges. In this work, we introduce AutoLabs, a self-correcting, multi-agent architecture designed to autonomously translate natural-language instructions into executable protocols for a high-throughput liquid handler. The system engages users in dialogue, decomposes experimental goals into discrete tasks for specialized agents, performs tool-assisted stoichiometric calculations, and iteratively self-corrects its output before generating a hardware-ready file. We present a comprehensive evaluation framework featuring five benchmark experiments of increasing complexity, from simple sample preparation to multi-plate timed syntheses. Through a systematic ablation study of 20 agent configurations, we assess the impact of reasoning capacity, architectural design (single- vs. multi-agent), tool use, and self-correction mechanisms. Our results demonstrate that agent reasoning capacity is the most critical factor for success, reducing quantitative errors in chemical amounts (nRMSE) by over 85% in complex tasks. When combined with a multi-agent architecture and iterative self-correction, AutoLabs achieves near-expert procedural accuracy (F1-score > 0.89) on challenging multi-step syntheses. These findings establish a clear blueprint for developing robust and trustworthy AI partners for autonomous laboratories, highlighting the synergistic effects of modular design, advanced reasoning, and self-correction to ensure both performance and reliability in high-stakes scientific applications. Code: https://github.com/pnnl/autolabs<br>
<span id='abs_ch'>中文：AutoLabs提出了一种自我修正的多智能体系统，可将自然语言指令转化为可执行的实验流程，其高级推理和模块化设计使复杂任务的定量误差降低超85%，并在多步合成中实现接近专家水平的精确度。</span><br>
<span id='abs_en'>English: AutoLabs introduces a self-correcting multi-agent system that translates natural language into executable lab protocols, with advanced reasoning and modular design reducing quantitative errors by over 85% and achieving near-expert accuracy in complex syntheses.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2509.25552.pdf' target='_blank'>https://arxiv.org/pdf/2509.25552.pdf</a></span>   <span><a href='https://github.com/shangqigao/RadioPath' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangqi Gao, Sihan Wang, Yibo Gao, Boming Wang, Xiahai Zhuang, Anne Warren, Grant Stewart, James Jones, Mireia Crispin-Ortuzar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25552">Evaluating Foundation Models with Pathological Concept Learning for Kidney Cancer</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>To evaluate the translational capabilities of foundation models, we develop a pathological concept learning approach focused on kidney cancer. By leveraging TNM staging guidelines and pathology reports, we build comprehensive pathological concepts for kidney cancer. Then, we extract deep features from whole slide images using foundation models, construct pathological graphs to capture spatial correlations, and trained graph neural networks to identify these concepts. Finally, we demonstrate the effectiveness of this approach in kidney cancer survival analysis, highlighting its explainability and fairness in identifying low- and high-risk patients. The source code has been released by https://github.com/shangqigao/RadioPath.<br>
<span id='abs_ch'>中文: 本 究通过结合TNM分期与基础模型分析全切片图像，开发了一种肾癌病理概念学 方法，在生存预测中展现出更好的可解释性与公平性。</span><br>
<span id='abs_en'>English: This study develops a pathological concept learning method for kidney cancer by integrating TNM staging with foundation models to analyze whole slide images, demonstrating enhanced survival prediction with improved explainability and fairness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2509.25552.pdf' target='_blank'>https://arxiv.org/pdf/2509.25552.pdf</a></span>   <span><a href='https://github.com/shangqigao/RadioPath' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangqi Gao, Sihan Wang, Yibo Gao, Boming Wang, Xiahai Zhuang, Anne Warren, Grant Stewart, James Jones, Mireia Crispin-Ortuzar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25552">Evaluating Foundation Models with Pathological Concept Learning for Kidney Cancer</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>To evaluate the translational capabilities of foundation models, we develop a pathological concept learning approach focused on kidney cancer. By leveraging TNM staging guidelines and pathology reports, we build comprehensive pathological concepts for kidney cancer. Then, we extract deep features from whole slide images using foundation models, construct pathological graphs to capture spatial correlations, and trained graph neural networks to identify these concepts. Finally, we demonstrate the effectiveness of this approach in kidney cancer survival analysis, highlighting its explainability and fairness in identifying low- and high-risk patients. The source code has been released by https://github.com/shangqigao/RadioPath.<br>
<span id='abs_ch'>中文: 本 究通过结合TNM分期与基础模型分析全切片图像，开发了一种肾癌病理概念学 方法，在生存预测中展现出更好的可解释性与公平性。</span><br>
<span id='abs_en'>English: This study develops a pathological concept learning method for kidney cancer by integrating TNM staging with foundation models to analyze whole slide images, demonstrating enhanced survival prediction with improved explainability and fairness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2509.25541.pdf' target='_blank'>https://arxiv.org/pdf/2509.25541.pdf</a></span>   <span><a href='https://github.com/wangqinsi1/Vision-Zero' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinsi Wang, Bo Liu, Tianyi Zhou, Jing Shi, Yueqian Lin, Yiran Chen, Hai Helen Li, Kun Wan, Wentian Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25541">Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although reinforcement learning (RL) can effectively enhance the reasoning capabilities of vision-language models (VLMs), current methods remain heavily dependent on labor-intensive datasets that require extensive manual construction and verification, leading to extremely high training costs and consequently constraining the practical deployment of VLMs. To address this challenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM self-improvement through competitive visual games generated from arbitrary image pairs. Specifically, Vision-Zero encompasses three main attributes: (1) Strategic Self-Play Framework: Vision-Zero trains VLMs in "Who Is the Spy"-style games, where the models engage in strategic reasoning and actions across multiple roles. Through interactive gameplay, models autonomously generate their training data without human annotation. (2) Gameplay from Arbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate games from arbitrary images, thereby enhancing the model's reasoning ability across diverse domains and showing strong generalization to different tasks. We demonstrate this versatility using three distinct types of image datasets: CLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable Performance Gain: We introduce Iterative Self-Play Policy Optimization (Iterative-SPO), a novel training algorithm that alternates between Self-Play and reinforcement learning with verifiable rewards (RLVR), mitigating the performance plateau often seen in self-play-only training and achieving sustained long-term improvements. Despite using label-free data, Vision-Zero achieves state-of-the-art performance on reasoning, chart question answering, and vision-centric understanding tasks, surpassing other annotation-based methods. Models and code has been released at https://github.com/wangqinsi1/Vision-Zero.<br>
<span id='abs_ch'>中文摘要：Vision-Zero 是一种领域 关的框架，通过任意图像对生成竞争性视觉游戏，使视觉语言模型能够实现自我优化， 需人工 注即可在多项推理任务中达到最先进性能。</span><br>
<span id='abs_en'>English Summary: Vision-Zero is a domain-agnostic framework that enables vision-language models to self-improve through competitive visual games generated from arbitrary image pairs, eliminating the need for manual annotation while achieving state-of-the-art performance across multiple reasoning tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2509.25541.pdf' target='_blank'>https://arxiv.org/pdf/2509.25541.pdf</a></span>   <span><a href='https://github.com/wangqinsi1/Vision-Zero' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinsi Wang, Bo Liu, Tianyi Zhou, Jing Shi, Yueqian Lin, Yiran Chen, Hai Helen Li, Kun Wan, Wentian Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25541">Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although reinforcement learning (RL) can effectively enhance the reasoning capabilities of vision-language models (VLMs), current methods remain heavily dependent on labor-intensive datasets that require extensive manual construction and verification, leading to extremely high training costs and consequently constraining the practical deployment of VLMs. To address this challenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM self-improvement through competitive visual games generated from arbitrary image pairs. Specifically, Vision-Zero encompasses three main attributes: (1) Strategic Self-Play Framework: Vision-Zero trains VLMs in "Who Is the Spy"-style games, where the models engage in strategic reasoning and actions across multiple roles. Through interactive gameplay, models autonomously generate their training data without human annotation. (2) Gameplay from Arbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate games from arbitrary images, thereby enhancing the model's reasoning ability across diverse domains and showing strong generalization to different tasks. We demonstrate this versatility using three distinct types of image datasets: CLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable Performance Gain: We introduce Iterative Self-Play Policy Optimization (Iterative-SPO), a novel training algorithm that alternates between Self-Play and reinforcement learning with verifiable rewards (RLVR), mitigating the performance plateau often seen in self-play-only training and achieving sustained long-term improvements. Despite using label-free data, Vision-Zero achieves state-of-the-art performance on reasoning, chart question answering, and vision-centric understanding tasks, surpassing other annotation-based methods. Models and code has been released at https://github.com/wangqinsi1/Vision-Zero.<br>
<span id='abs_ch'>中文摘要：Vision-Zero 是一种领域 关的框架，通过任意图像对生成竞争性视觉游戏，使视觉语言模型能够实现自我优化， 需人工 注即可在多项推理任务中达到最先进性能。</span><br>
<span id='abs_en'>English Summary: Vision-Zero is a domain-agnostic framework that enables vision-language models to self-improve through competitive visual games generated from arbitrary image pairs, eliminating the need for manual annotation while achieving state-of-the-art performance across multiple reasoning tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2509.25532.pdf' target='_blank'>https://arxiv.org/pdf/2509.25532.pdf</a></span>   <span><a href='https://github.com/dubai03nsr/dinco' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor Wang, Elias Stengel-Eskin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25532">Calibrating Verbalized Confidence with Self-Generated Distractors</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Calibrated confidence estimates are necessary for large language model (LLM) outputs to be trusted by human users. While LLMs can express their confidence in human-interpretable ways, verbalized LLM-generated confidence scores have empirically been found to be miscalibrated, reporting high confidence on instances with low accuracy and thereby harming trust and safety. We hypothesize that this overconfidence often stems from a given LLM's heightened suggestibility when faced with claims that it encodes little information about; we empirically validate this hypothesis, finding more suggestibility on lower-accuracy claims. Building on this finding, we introduce Distractor-Normalized Coherence (DINCO), which estimates and accounts for an LLM's suggestibility bias by having the model verbalize its confidence independently across several self-generated distractors (i.e. alternative claims), and normalizes by the total verbalized confidence. To further improve calibration, we leverage generator-validator disagreement, augmenting normalized validator confidence with a consistency-based estimate of generator confidence. Here, we frame the popular approach of self-consistency as leveraging coherence across sampled generations, and normalized verbalized confidence as leveraging coherence across validations on incompatible claims, allowing us to integrate these complementary dimensions of coherence into DINCO. Moreover, our analysis shows that DINCO provides less saturated -- and therefore more usable -- confidence estimates, and that further sampling alone cannot close the gap between DINCO and baselines, with DINCO at 10 inference calls outperforming self-consistency at 100.<br>
<span id='abs_ch'>中文:  准的置信度估计对于大语言模型输出的可信度至关重要，而提出的DINCO方法通过 准化模型自生成干扰项的口头置信度，并利用生成器-验证器分歧来提高准确性和可用性，从而解决了误 准问题。</span><br>
<span id='abs_en'>English: Calibrated confidence estimates are crucial for trustworthy LLM outputs, and the proposed DINCO method addresses miscalibration by normalizing verbalized confidence across self-generated distractors and leveraging generator-validator disagreement to improve accuracy and usability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2509.25532.pdf' target='_blank'>https://arxiv.org/pdf/2509.25532.pdf</a></span>   <span><a href='https://github.com/dubai03nsr/dinco' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Victor Wang, Elias Stengel-Eskin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25532">Calibrating Verbalized Confidence with Self-Generated Distractors</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Calibrated confidence estimates are necessary for large language model (LLM) outputs to be trusted by human users. While LLMs can express their confidence in human-interpretable ways, verbalized LLM-generated confidence scores have empirically been found to be miscalibrated, reporting high confidence on instances with low accuracy and thereby harming trust and safety. We hypothesize that this overconfidence often stems from a given LLM's heightened suggestibility when faced with claims that it encodes little information about; we empirically validate this hypothesis, finding more suggestibility on lower-accuracy claims. Building on this finding, we introduce Distractor-Normalized Coherence (DINCO), which estimates and accounts for an LLM's suggestibility bias by having the model verbalize its confidence independently across several self-generated distractors (i.e. alternative claims), and normalizes by the total verbalized confidence. To further improve calibration, we leverage generator-validator disagreement, augmenting normalized validator confidence with a consistency-based estimate of generator confidence. Here, we frame the popular approach of self-consistency as leveraging coherence across sampled generations, and normalized verbalized confidence as leveraging coherence across validations on incompatible claims, allowing us to integrate these complementary dimensions of coherence into DINCO. Moreover, our analysis shows that DINCO provides less saturated -- and therefore more usable -- confidence estimates, and that further sampling alone cannot close the gap between DINCO and baselines, with DINCO at 10 inference calls outperforming self-consistency at 100.<br>
<span id='abs_ch'>中文:  准的置信度估计对于大语言模型输出的可信度至关重要，而提出的DINCO方法通过 准化模型自生成干扰项的口头置信度，并利用生成器-验证器分歧来提高准确性和可用性，从而解决了误 准问题。</span><br>
<span id='abs_en'>English: Calibrated confidence estimates are crucial for trustworthy LLM outputs, and the proposed DINCO method addresses miscalibration by normalizing verbalized confidence across self-generated distractors and leveraging generator-validator disagreement to improve accuracy and usability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2509.25531.pdf' target='_blank'>https://arxiv.org/pdf/2509.25531.pdf</a></span>   <span><a href='https://github.com/ontocord/mixturevitae' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ontocord/mixturevitae' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huu Nguyen, Victor May, Harsh Raj, Marianna Nezhurina, Yishan Wang, Yanqi Luo, Minh Chien Vu, Taishi Nakamura, Ken Tsui, Van Khue Nguyen, David Salinas, Aleksandra Krasnodębska, Christoph Schuhmann, Mats Leon Richter, Xuan-Son, Vu, Jenia Jitsev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25531">MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong model performance. MixtureVitae follows a risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources), alongside targeted instruction, reasoning and synthetic data with documented provenance. We detail a transparent, multi-stage pipeline for license-aware filtering, safety and quality screening, and domain-aware mixing, and we release the dataset and curation recipes to support reproducible research. In controlled experiments using the open-sci-ref training protocol (fixed architectures at 130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B/300B setting they surpass FineWeb-Edu and approach DCLM in the later stages of training. Performance is particularly strong on math/code and competitive on QA tasks. These results demonstrate that permissive-first, risk-mitigated data provides a practical and legally mitigated foundation for training capable LLMs, reducing reliance on indiscriminate web scraping without sacrificing competitiveness. Code: https://github.com/ontocord/mixturevitae<br>
<span id='abs_ch'>中文: MixtureVitae是一个开放获取的预训练语料库，采用风险缓和的来源策略和透明处理流程，在降低法律风险的同时实现了优越的模型性能，在多项基准测试中持续超越其他许可数据集。</span><br>
<span id='abs_en'>English: MixtureVitae is an open-access pretraining corpus designed to minimize legal risks while delivering strong model performance through a risk-mitigated sourcing strategy and transparent curation pipeline, consistently outperforming other permissive datasets in benchmarks.</span>Paperid:74,https://arxiv.org/pdf/2509.25455.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2509.25531.pdf' target='_blank'>https://arxiv.org/pdf/2509.25531.pdf</a></span>   <span><a href='https://github.com/ontocord/mixturevitae' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ontocord/mixturevitae' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huu Nguyen, Victor May, Harsh Raj, Marianna Nezhurina, Yishan Wang, Yanqi Luo, Minh Chien Vu, Taishi Nakamura, Ken Tsui, Van Khue Nguyen, David Salinas, Aleksandra KrasnodÄbska, Christoph Schuhmann, Mats Leon Richter, Xuan-Son, Vu, Jenia Jitsev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25531">MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong model performance. MixtureVitae follows a risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources), alongside targeted instruction, reasoning and synthetic data with documented provenance. We detail a transparent, multi-stage pipeline for license-aware filtering, safety and quality screening, and domain-aware mixing, and we release the dataset and curation recipes to support reproducible research. In controlled experiments using the open-sci-ref training protocol (fixed architectures at 130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B/300B setting they surpass FineWeb-Edu and approach DCLM in the later stages of training. Performance is particularly strong on math/code and competitive on QA tasks. These results demonstrate that permissive-first, risk-mitigated data provides a practical and legally mitigated foundation for training capable LLMs, reducing reliance on indiscriminate web scraping without sacrificing competitiveness. Code: https://github.com/ontocord/mixturevitae<br>
<span id='abs_ch'>中文: MixtureVitae是一个开放获取的预训练语料库，采用风险缓和的来源策略和透明处理流程，在降低法律风险的同时实现了优越的模型性能，在多项基准测试中持续超越其他许可数据集。</span><br>
<span id='abs_en'>English: MixtureVitae is an open-access pretraining corpus designed to minimize legal risks while delivering strong model performance through a risk-mitigated sourcing strategy and transparent curation pipeline, consistently outperforming other permissive datasets in benchmarks.</span>Paperid:74,https://arxiv.org/pdf/2509.25455.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2509.25455.pdf' target='_blank'>https://arxiv.org/pdf/2509.25455.pdf</a></span>   <span><a href='https://github.com/JetBrains-Research/PIPer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Kovrigin, Aleksandra Eliseeva, Konstantin Grotov, Egor Bogomolov, Yaroslav Zharov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25455">PIPer: On-Device Environment Setup via Online Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Environment setup-the process of configuring the system to work with a specific software project-represents a persistent challenge in Software Engineering (SE). Automated environment setup methods could assist developers by providing fully configured environments for arbitrary repositories without manual effort. This also helps SE researchers to scale execution-based benchmarks. However, recent studies reveal that even state-of-the-art Large Language Models (LLMs) achieve limited success in automating this task. To address this limitation, we tune a specialized model for environment setup. We combine supervised fine-tuning for generating correct Bash scripts and Reinforcement Learning with Verifiable Rewards (RLVR) to adapt it to the task of environment setup. On EnvBench-Python, our method enables Qwen3-8B (a model runnable on consumer hardware) to perform on par with larger models-Qwen3-32B and GPT-4o. The training code and model checkpoints are available online: https://github.com/JetBrains-Research/PIPer.<br>
<span id='abs_ch'>Chinese: 我们通过监督微调和强化学 专门优化的模型，使轻量级Qwen3-8B在EnvBench-Python基准测试中达到了与Qwen3-32B和GPT-4o等大型模型相当的环境配置性能。</span><br>
<span id='abs_en'>English: Our specialized model, fine-tuned with supervised learning and reinforcement learning for automated environment setup, enables the compact Qwen3-8B to match the performance of larger models like Qwen3-32B and GPT-4o on EnvBench-Python.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2509.25455.pdf' target='_blank'>https://arxiv.org/pdf/2509.25455.pdf</a></span>   <span><a href='https://github.com/JetBrains-Research/PIPer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Kovrigin, Aleksandra Eliseeva, Konstantin Grotov, Egor Bogomolov, Yaroslav Zharov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25455">PIPer: On-Device Environment Setup via Online Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Environment setup-the process of configuring the system to work with a specific software project-represents a persistent challenge in Software Engineering (SE). Automated environment setup methods could assist developers by providing fully configured environments for arbitrary repositories without manual effort. This also helps SE researchers to scale execution-based benchmarks. However, recent studies reveal that even state-of-the-art Large Language Models (LLMs) achieve limited success in automating this task. To address this limitation, we tune a specialized model for environment setup. We combine supervised fine-tuning for generating correct Bash scripts and Reinforcement Learning with Verifiable Rewards (RLVR) to adapt it to the task of environment setup. On EnvBench-Python, our method enables Qwen3-8B (a model runnable on consumer hardware) to perform on par with larger models-Qwen3-32B and GPT-4o. The training code and model checkpoints are available online: https://github.com/JetBrains-Research/PIPer.<br>
<span id='abs_ch'>Chinese: 我们通过监督微调和强化学 专门优化的模型，使轻量级Qwen3-8B在EnvBench-Python基准测试中达到了与Qwen3-32B和GPT-4o等大型模型相当的环境配置性能。</span><br>
<span id='abs_en'>English: Our specialized model, fine-tuned with supervised learning and reinforcement learning for automated environment setup, enables the compact Qwen3-8B to match the performance of larger models like Qwen3-32B and GPT-4o on EnvBench-Python.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2509.25438.pdf' target='_blank'>https://arxiv.org/pdf/2509.25438.pdf</a></span>   <span><a href='https://github.com/Akuna23Matata/LPM_exploration' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhibo Hou, Zhiyu An, Wan Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25438">Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>When there exists an unlearnable source of randomness (noisy-TV) in the environment, a naively intrinsic reward driven exploring agent gets stuck at that source of randomness and fails at exploration. Intrinsic reward based on uncertainty estimation or distribution similarity, while eventually escapes noisy-TVs as time unfolds, suffers from poor sample efficiency and high computational cost. Inspired by recent findings from neuroscience that humans monitor their improvements during exploration, we propose a novel method for intrinsically-motivated exploration, named Learning Progress Monitoring (LPM). During exploration, LPM rewards model improvements instead of prediction error or novelty, effectively rewards the agent for observing learnable transitions rather than the unlearnable transitions. We introduce a dual-network design that uses an error model to predict the expected prediction error of the dynamics model in its previous iteration, and use the difference between the model errors of the current iteration and previous iteration to guide exploration. We theoretically show that the intrinsic reward of LPM is zero-equivariant and a monotone indicator of Information Gain (IG), and that the error model is necessary to achieve monotonicity correspondence with IG. We empirically compared LPM against state-of-the-art baselines in noisy environments based on MNIST, 3D maze with 160x120 RGB inputs, and Atari. Results show that LPM's intrinsic reward converges faster, explores more states in the maze experiment, and achieves higher extrinsic reward in Atari. This conceptually simple approach marks a shift-of-paradigm of noise-robust exploration. For code to reproduce our experiments, see https://github.com/Akuna23Matata/LPM_exploration<br>
<span id='abs_ch'>Chinese: 提出的学 进度监控（LPM）方法通过奖励模型改进而非预测误差，有效避免了不可学 噪声的干扰，在嘈杂环境中实现了更快的收敛速度和更优的性能表现。</span><br>
<span id='abs_en'>English: The proposed Learning Progress Monitoring (LPM) method improves exploration efficiency by rewarding model improvements instead of prediction errors, effectively avoiding distractions from unlearnable noise while achieving faster convergence and better performance in noisy environments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2509.25438.pdf' target='_blank'>https://arxiv.org/pdf/2509.25438.pdf</a></span>   <span><a href='https://github.com/Akuna23Matata/LPM_exploration' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhibo Hou, Zhiyu An, Wan Du
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25438">Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>When there exists an unlearnable source of randomness (noisy-TV) in the environment, a naively intrinsic reward driven exploring agent gets stuck at that source of randomness and fails at exploration. Intrinsic reward based on uncertainty estimation or distribution similarity, while eventually escapes noisy-TVs as time unfolds, suffers from poor sample efficiency and high computational cost. Inspired by recent findings from neuroscience that humans monitor their improvements during exploration, we propose a novel method for intrinsically-motivated exploration, named Learning Progress Monitoring (LPM). During exploration, LPM rewards model improvements instead of prediction error or novelty, effectively rewards the agent for observing learnable transitions rather than the unlearnable transitions. We introduce a dual-network design that uses an error model to predict the expected prediction error of the dynamics model in its previous iteration, and use the difference between the model errors of the current iteration and previous iteration to guide exploration. We theoretically show that the intrinsic reward of LPM is zero-equivariant and a monotone indicator of Information Gain (IG), and that the error model is necessary to achieve monotonicity correspondence with IG. We empirically compared LPM against state-of-the-art baselines in noisy environments based on MNIST, 3D maze with 160x120 RGB inputs, and Atari. Results show that LPM's intrinsic reward converges faster, explores more states in the maze experiment, and achieves higher extrinsic reward in Atari. This conceptually simple approach marks a shift-of-paradigm of noise-robust exploration. For code to reproduce our experiments, see https://github.com/Akuna23Matata/LPM_exploration<br>
<span id='abs_ch'>Chinese: 提出的学 进度监控（LPM）方法通过奖励模型改进而非预测误差，有效避免了不可学 噪声的干扰，在嘈杂环境中实现了更快的收敛速度和更优的性能表现。</span><br>
<span id='abs_en'>English: The proposed Learning Progress Monitoring (LPM) method improves exploration efficiency by rewarding model improvements instead of prediction errors, effectively avoiding distractions from unlearnable noise while achieving faster convergence and better performance in noisy environments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2509.25434.pdf' target='_blank'>https://arxiv.org/pdf/2509.25434.pdf</a></span>   <span><a href='https://github.com/OpenSyndrome/schema' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana Paula Gomes Ferreira, Aleksandar AnÅ¾el, Izabel Oliva Marcilio de Souza, Helen Hughes, Alex J Elliot, Jude Dzevela Kong, Madlen Schranz, Alexander Ullrich, Georges Hattab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25434">The Open Syndrome Definition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Case definitions are essential for effectively communicating public health threats. However, the absence of a standardized, machine-readable format poses significant challenges to interoperability, epidemiological research, the exchange of qualitative data, and the effective application of computational analysis methods, including artificial intelligence (AI). This complicates comparisons and collaborations across organizations and regions, limits data integration, and hinders technological innovation in public health. To address these issues, we propose the first open, machine-readable format for representing case and syndrome definitions. Additionally, we introduce the first comprehensive dataset of standardized case definitions and tools to convert existing human-readable definitions into machine-readable formats. We also provide an accessible online platform for browsing, analyzing, and contributing new definitions, available at https://opensyndrome.org. The Open Syndrome Definition format enables consistent, scalable use of case definitions across systems, unlocking AI's potential to strengthen public health preparedness and response. The source code for the format can be found at https://github.com/OpenSyndrome/schema under the MIT license.<br>
<span id='abs_ch'>中文摘要：Open Syndrome Definition 式作为首个机器可读的病例定义开放 准，通过在线平台和配套工具解决了公共卫生数据互操作性难题，为人工智能应用和跨系统协作提供了技术基础。</span><br>
<span id='abs_en'>English Summary: The proposed Open Syndrome Definition format addresses interoperability challenges by introducing the first machine-readable standard for case definitions, enabling AI applications and data integration in public health through an accessible online platform.Paperid:78,https://arxiv.org/pdf/2509.25414.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2509.25434.pdf' target='_blank'>https://arxiv.org/pdf/2509.25434.pdf</a></span>   <span><a href='https://github.com/OpenSyndrome/schema' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana Paula Gomes Ferreira, Aleksandar Anžel, Izabel Oliva Marcilio de Souza, Helen Hughes, Alex J Elliot, Jude Dzevela Kong, Madlen Schranz, Alexander Ullrich, Georges Hattab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25434">The Open Syndrome Definition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Case definitions are essential for effectively communicating public health threats. However, the absence of a standardized, machine-readable format poses significant challenges to interoperability, epidemiological research, the exchange of qualitative data, and the effective application of computational analysis methods, including artificial intelligence (AI). This complicates comparisons and collaborations across organizations and regions, limits data integration, and hinders technological innovation in public health. To address these issues, we propose the first open, machine-readable format for representing case and syndrome definitions. Additionally, we introduce the first comprehensive dataset of standardized case definitions and tools to convert existing human-readable definitions into machine-readable formats. We also provide an accessible online platform for browsing, analyzing, and contributing new definitions, available at https://opensyndrome.org. The Open Syndrome Definition format enables consistent, scalable use of case definitions across systems, unlocking AI's potential to strengthen public health preparedness and response. The source code for the format can be found at https://github.com/OpenSyndrome/schema under the MIT license.<br>
<span id='abs_ch'>中文摘要：Open Syndrome Definition 式作为首个机器可读的病例定义开放 准，通过在线平台和配套工具解决了公共卫生数据互操作性难题，为人工智能应用和跨系统协作提供了技术基础。</span><br>
<span id='abs_en'>English Summary: The proposed Open Syndrome Definition format addresses interoperability challenges by introducing the first machine-readable standard for case definitions, enabling AI applications and data integration in public health through an accessible online platform.Paperid:78,https://arxiv.org/pdf/2509.25414.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2509.25414.pdf' target='_blank'>https://arxiv.org/pdf/2509.25414.pdf</a></span>   <span><a href='https://github.com/OptMN-Lab/ALoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Ban, Kaiyi Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25414">Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models are often adapted using parameter-efficient techniques such as Low-Rank Adaptation (LoRA), formulated as $y = W_0x + BAx$, where $W_0$ is the pre-trained parameters and $x$ is the input to the adapted layer. While multi-adapter extensions often employ multiple LoRAs, prior studies suggest that the inner $A$ matrices are highly similar during training and thus suitable for sharing. We revisit this phenomenon and find that this similarity is largely attributable to the identical initialization rather than shared knowledge, with $B$ playing a more critical role in knowledge encoding and transfer. Motivated by these insights, we propose \textbf{ALoRA}, an asymmetric multi-LoRA design with multiple $A$ matrices and a single shared $B$ in multi-task fine-tuning, and \textbf{Fed-ALoRA}, which shares $B$ across clients in federated fine-tuning under both homogeneous and heterogeneous settings, through a novel matrix decomposition strategy to accommodate heterogeneous ranks across clients. Experiments on commonsense reasoning, math reasoning, multi-task NLP dataset, and federated NLP dataset demonstrate that our methods achieve more balanced performance across tasks with comparable or superior average accuracy relative to existing multi-LoRA approaches. Codes are available at https://github.com/OptMN-Lab/ALoRA.<br>
<span id='abs_ch'>中文: 该 究重新审视了LoRA内部矩阵的相似性，提出了ALoRA和Fed-ALoRA两种非对称设计方法，通过共享B矩阵在多任务和联邦微调中实现了更均衡且优越的性能，相关代 已开源。</span><br>
<span id='abs_en'>English: The study revisits the similarity in LoRA's inner matrices and proposes ALoRA and Fed-ALoRA, which use asymmetric designs with shared B matrices, achieving balanced and superior performance in multi-task and federated fine-tuning across various reasoning and NLP tasks.Paperid:79,https://arxiv.org/pdf/2509.25411.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2509.25414.pdf' target='_blank'>https://arxiv.org/pdf/2509.25414.pdf</a></span>   <span><a href='https://github.com/OptMN-Lab/ALoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Ban, Kaiyi Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25414">Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models are often adapted using parameter-efficient techniques such as Low-Rank Adaptation (LoRA), formulated as $y = W_0x + BAx$, where $W_0$ is the pre-trained parameters and $x$ is the input to the adapted layer. While multi-adapter extensions often employ multiple LoRAs, prior studies suggest that the inner $A$ matrices are highly similar during training and thus suitable for sharing. We revisit this phenomenon and find that this similarity is largely attributable to the identical initialization rather than shared knowledge, with $B$ playing a more critical role in knowledge encoding and transfer. Motivated by these insights, we propose \textbf{ALoRA}, an asymmetric multi-LoRA design with multiple $A$ matrices and a single shared $B$ in multi-task fine-tuning, and \textbf{Fed-ALoRA}, which shares $B$ across clients in federated fine-tuning under both homogeneous and heterogeneous settings, through a novel matrix decomposition strategy to accommodate heterogeneous ranks across clients. Experiments on commonsense reasoning, math reasoning, multi-task NLP dataset, and federated NLP dataset demonstrate that our methods achieve more balanced performance across tasks with comparable or superior average accuracy relative to existing multi-LoRA approaches. Codes are available at https://github.com/OptMN-Lab/ALoRA.<br>
<span id='abs_ch'>中文: 该 究重新审视了LoRA内部矩阵的相似性，提出了ALoRA和Fed-ALoRA两种非对称设计方法，通过共享B矩阵在多任务和联邦微调中实现了更均衡且优越的性能，相关代 已开源。</span><br>
<span id='abs_en'>English: The study revisits the similarity in LoRA's inner matrices and proposes ALoRA and Fed-ALoRA, which use asymmetric designs with shared B matrices, achieving balanced and superior performance in multi-task and federated fine-tuning across various reasoning and NLP tasks.Paperid:79,https://arxiv.org/pdf/2509.25411.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2509.25411.pdf' target='_blank'>https://arxiv.org/pdf/2509.25411.pdf</a></span>   <span><a href='https://github.com/zewei-Zhang/ImitSAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewei Zhang, Huan Liu, Yuanhao Yu, Jun Chen, Xiangyu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25411">Boolean Satisfiability via Imitation Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We propose ImitSAT, a branching policy for conflict-driven clause learning (CDCL) solvers based on imitation learning for the Boolean satisfiability problem (SAT). Unlike previous methods that predict instance-level signals to improve CDCL branching indirectly, or rely on reinforcement learning and insufficient CDCL information to enhance branching, ImitSAT learns from expert KeyTrace that collapses a full run into the sequence of surviving decisions. Replaying a KeyTrace on the same instance is nearly conflict-free, providing dense decision-level supervision and directly reducing propagations -- the dominant contributor to wall-clock time. This prefix-conditioned supervision enables ImitSAT to reproduce high-quality branches without exploration, yielding faster convergence, stable training, and seamless integration into CDCL. Extensive experiments demonstrate that ImitSAT reduces propagation counts and runtime, outperforming state-of-the-art learned approaches. We released the source code and trained model at https://github.com/zewei-Zhang/ImitSAT<br>
<br>
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2509.25411.pdf' target='_blank'>https://arxiv.org/pdf/2509.25411.pdf</a></span>   <span><a href='https://github.com/zewei-Zhang/ImitSAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zewei Zhang, Huan Liu, Yuanhao Yu, Jun Chen, Xiangyu Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25411">Boolean Satisfiability via Imitation Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We propose ImitSAT, a branching policy for conflict-driven clause learning (CDCL) solvers based on imitation learning for the Boolean satisfiability problem (SAT). Unlike previous methods that predict instance-level signals to improve CDCL branching indirectly, or rely on reinforcement learning and insufficient CDCL information to enhance branching, ImitSAT learns from expert KeyTrace that collapses a full run into the sequence of surviving decisions. Replaying a KeyTrace on the same instance is nearly conflict-free, providing dense decision-level supervision and directly reducing propagations -- the dominant contributor to wall-clock time. This prefix-conditioned supervision enables ImitSAT to reproduce high-quality branches without exploration, yielding faster convergence, stable training, and seamless integration into CDCL. Extensive experiments demonstrate that ImitSAT reduces propagation counts and runtime, outperforming state-of-the-art learned approaches. We released the source code and trained model at https://github.com/zewei-Zhang/ImitSAT<br>
<br>
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2509.25370.pdf' target='_blank'>https://arxiv.org/pdf/2509.25370.pdf</a></span>   <span><a href='https://github.com/ulab-uiuc/AgentDebug' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunlun Zhu, Zijia Liu, Bingxuan Li, Muxin Tian, Yingxuan Yang, Jiaxun Zhang, Pengrui Han, Qipeng Xie, Fuyang Cui, Weijia Zhang, Xiaoteng Ma, Xiaodong Yu, Gowtham Ramesh, Jialian Wu, Zicheng Liu, Pan Lu, James Zou, Jiaxuan You
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25370">Where LLM Agents Fail and How They can Learn From Failures</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Model (LLM) agents, which integrate planning, memory, reflection, and tool-use modules, have shown promise in solving complex, multi-step tasks. Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure. Current systems lack a framework that can comprehensively understand agent error in a modular and systemic way, and therefore fail to detect these errors accordingly. We address this gap with three contributions. First, we introduce the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations. Second, we construct AgentErrorBench, the first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop, grounding error analysis in real-world agent rollouts. Third, we propose AgentDebug, a debugging framework that isolates root-cause failures and provides corrective feedback, enabling agents to recover and iteratively improve. Experiments on AgentErrorBench show that AgentDebug achieves 24% higher all-correct accuracy and 17% higher step accuracy compared to the strongest baseline. Beyond detection, the targeted feedback generated by AgentDebug enables LLM agents to iteratively recover from failures, yielding up to 26% relative improvements in task success across ALFWorld, GAIA, and WebShop. These results establish principled debugging as a pathway to more reliable and adaptive LLM agents. The code and data will be available at https://github.com/ulab-uiuc/AgentDebug<br>
<span id='abs_ch'>中文: 大语言模型智能体 错误级联 播面临系统性失效，而提出的AgentDebug框架通过错误分类体系、基准数据集和调试工具显著提升了任务准确率，并能实现故障的迭代修复。</span><br>
<span id='abs_en'>English: Large Language Model agents face cascading failures due to error propagation, but the proposed AgentDebug framework with its error taxonomy, benchmark dataset, and debugging tools significantly improves task accuracy and enables iterative recovery from failures.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2509.25370.pdf' target='_blank'>https://arxiv.org/pdf/2509.25370.pdf</a></span>   <span><a href='https://github.com/ulab-uiuc/AgentDebug' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunlun Zhu, Zijia Liu, Bingxuan Li, Muxin Tian, Yingxuan Yang, Jiaxun Zhang, Pengrui Han, Qipeng Xie, Fuyang Cui, Weijia Zhang, Xiaoteng Ma, Xiaodong Yu, Gowtham Ramesh, Jialian Wu, Zicheng Liu, Pan Lu, James Zou, Jiaxuan You
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25370">Where LLM Agents Fail and How They can Learn From Failures</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Model (LLM) agents, which integrate planning, memory, reflection, and tool-use modules, have shown promise in solving complex, multi-step tasks. Yet their sophisticated architectures amplify vulnerability to cascading failures, where a single root-cause error propagates through subsequent decisions, leading to task failure. Current systems lack a framework that can comprehensively understand agent error in a modular and systemic way, and therefore fail to detect these errors accordingly. We address this gap with three contributions. First, we introduce the AgentErrorTaxonomy, a modular classification of failure modes spanning memory, reflection, planning, action, and system-level operations. Second, we construct AgentErrorBench, the first dataset of systematically annotated failure trajectories from ALFWorld, GAIA, and WebShop, grounding error analysis in real-world agent rollouts. Third, we propose AgentDebug, a debugging framework that isolates root-cause failures and provides corrective feedback, enabling agents to recover and iteratively improve. Experiments on AgentErrorBench show that AgentDebug achieves 24% higher all-correct accuracy and 17% higher step accuracy compared to the strongest baseline. Beyond detection, the targeted feedback generated by AgentDebug enables LLM agents to iteratively recover from failures, yielding up to 26% relative improvements in task success across ALFWorld, GAIA, and WebShop. These results establish principled debugging as a pathway to more reliable and adaptive LLM agents. The code and data will be available at https://github.com/ulab-uiuc/AgentDebug<br>
<span id='abs_ch'>中文: 大语言模型智能体 错误级联 播面临系统性失效，而提出的AgentDebug框架通过错误分类体系、基准数据集和调试工具显著提升了任务准确率，并能实现故障的迭代修复。</span><br>
<span id='abs_en'>English: Large Language Model agents face cascading failures due to error propagation, but the proposed AgentDebug framework with its error taxonomy, benchmark dataset, and debugging tools significantly improves task accuracy and enables iterative recovery from failures.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2509.25299.pdf' target='_blank'>https://arxiv.org/pdf/2509.25299.pdf</a></span>   <span><a href='https://github.com/flybits/humanai-agents' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Platnick, Mohamed E. Bengueddache, Marjan Alirezaie, Dava J. Newman, Alex ''Sandy'' Pentland, Hossein Rahnama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25299">ID-RAG: Identity Retrieval-Augmented Generation for Long-Horizon Persona Coherence in Generative Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Generative agents powered by language models are increasingly deployed for long-horizon tasks. However, as long-term memory context grows over time, they struggle to maintain coherence. This deficiency leads to critical failures, including identity drift, ignoring established beliefs, and the propagation of hallucinations in multi-agent systems. To mitigate these challenges, this paper introduces Identity Retrieval-Augmented Generation (ID-RAG), a novel mechanism designed to ground an agent's persona and persistent preferences in a dynamic, structured identity model: a knowledge graph of core beliefs, traits, and values. During the agent's decision loop, this model is queried to retrieve relevant identity context, which directly informs action selection. We demonstrate this approach by introducing and implementing a new class of ID-RAG enabled agents called Human-AI Agents (HAis), where the identity model is inspired by the Chronicle structure used in Perspective-Aware AI, a dynamic knowledge graph learned from a real-world entity's digital footprint. In social simulations of a mayoral election, HAis using ID-RAG outperformed baseline agents in long-horizon persona coherence - achieving higher identity recall across all tested models by the fourth timestep - and reduced simulation convergence time by 19% (GPT-4o) and 58% (GPT-4o mini). By treating identity as an explicit, retrievable knowledge structure, ID-RAG offers a foundational approach for developing more temporally coherent, interpretable, and aligned generative agents. Our code is open-source and available at: https://github.com/flybits/humanai-agents.<br>
<span id='abs_ch'>中文摘要：本文提出身份检索增强生成（ID-RAG）机制，通过动态知识图谱保持生成式智能体在长期任务中的人 一致性，显著提升身份记忆能力并缩短仿真时间。</span><br>
<span id='abs_en'>English Summary: This paper introduces Identity Retrieval-Augmented Generation (ID-RAG), a novel mechanism that uses a dynamic knowledge graph to maintain generative agents' persona coherence during long-term tasks, significantly improving identity recall and reducing simulation time.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2509.25299.pdf' target='_blank'>https://arxiv.org/pdf/2509.25299.pdf</a></span>   <span><a href='https://github.com/flybits/humanai-agents' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Platnick, Mohamed E. Bengueddache, Marjan Alirezaie, Dava J. Newman, Alex ''Sandy'' Pentland, Hossein Rahnama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25299">ID-RAG: Identity Retrieval-Augmented Generation for Long-Horizon Persona Coherence in Generative Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Generative agents powered by language models are increasingly deployed for long-horizon tasks. However, as long-term memory context grows over time, they struggle to maintain coherence. This deficiency leads to critical failures, including identity drift, ignoring established beliefs, and the propagation of hallucinations in multi-agent systems. To mitigate these challenges, this paper introduces Identity Retrieval-Augmented Generation (ID-RAG), a novel mechanism designed to ground an agent's persona and persistent preferences in a dynamic, structured identity model: a knowledge graph of core beliefs, traits, and values. During the agent's decision loop, this model is queried to retrieve relevant identity context, which directly informs action selection. We demonstrate this approach by introducing and implementing a new class of ID-RAG enabled agents called Human-AI Agents (HAis), where the identity model is inspired by the Chronicle structure used in Perspective-Aware AI, a dynamic knowledge graph learned from a real-world entity's digital footprint. In social simulations of a mayoral election, HAis using ID-RAG outperformed baseline agents in long-horizon persona coherence - achieving higher identity recall across all tested models by the fourth timestep - and reduced simulation convergence time by 19% (GPT-4o) and 58% (GPT-4o mini). By treating identity as an explicit, retrievable knowledge structure, ID-RAG offers a foundational approach for developing more temporally coherent, interpretable, and aligned generative agents. Our code is open-source and available at: https://github.com/flybits/humanai-agents.<br>
<span id='abs_ch'>中文摘要：本文提出身份检索增强生成（ID-RAG）机制，通过动态知识图谱保持生成式智能体在长期任务中的人 一致性，显著提升身份记忆能力并缩短仿真时间。</span><br>
<span id='abs_en'>English Summary: This paper introduces Identity Retrieval-Augmented Generation (ID-RAG), a novel mechanism that uses a dynamic knowledge graph to maintain generative agents' persona coherence during long-term tasks, significantly improving identity recall and reducing simulation time.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2509.25270.pdf' target='_blank'>https://arxiv.org/pdf/2509.25270.pdf</a></span>   <span><a href='https://github.com/brightest66/InfMasking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liangjian Wen, Qun Dai, Jianzhuang Liu, Jiangtao Zheng, Yong Dai, Dongkai Wang, Zhao Kang, Jun Wang, Zenglin Xu, Jiang Duan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25270">InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In multimodal representation learning, synergistic interactions between modalities not only provide complementary information but also create unique outcomes through specific interaction patterns that no single modality could achieve alone. Existing methods may struggle to effectively capture the full spectrum of synergistic information, leading to suboptimal performance in tasks where such interactions are critical. This is particularly problematic because synergistic information constitutes the fundamental value proposition of multimodal representation. To address this challenge, we introduce InfMasking, a contrastive synergistic information extraction method designed to enhance synergistic information through an Infinite Masking strategy. InfMasking stochastically occludes most features from each modality during fusion, preserving only partial information to create representations with varied synergistic patterns. Unmasked fused representations are then aligned with masked ones through mutual information maximization to encode comprehensive synergistic information. This infinite masking strategy enables capturing richer interactions by exposing the model to diverse partial modality combinations during training. As computing mutual information estimates with infinite masking is computationally prohibitive, we derive an InfMasking loss to approximate this calculation. Through controlled experiments, we demonstrate that InfMasking effectively enhances synergistic information between modalities. In evaluations on large-scale real-world datasets, InfMasking achieves state-of-the-art performance across seven benchmarks. Code is released at https://github.com/brightest66/InfMasking.<br>
<span id='abs_ch'>Chinese: InfMasking提出了一种 限掩 策略，通过在融合过程中随机遮蔽模态特征并利用互信息最大化对齐掩 表示，有效增强了模态间的协同信息，在七个基准测试中取得了最优性能。</span><br>
<span id='abs_en'>English: InfMasking introduces an infinite masking strategy in multimodal learning that stochastically occludes modality features during fusion and aligns masked representations through mutual information maximization, achieving state-of-the-art performance across seven benchmarks by enhancing synergistic interactions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2509.25252.pdf' target='_blank'>https://arxiv.org/pdf/2509.25252.pdf</a></span>   <span><a href='https://github.com/ayushgupta4897/FGA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aayush Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25252">Fact Grounded Attention: Eliminating Hallucination in Large Language Models Through Attention Level Knowledge Integration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>"The greatest enemy of knowledge is not ignorance, it is the illusion of knowledge." Large Language Models have conquered natural language but remain prisoners of their own probabilistic nature--confidently hallucinating facts they never truly knew. We present Fact Grounded Attention (FGA), a novel architectural modification that transforms unreliable language models into deterministic truth tellers by injecting verifiable knowledge directly into the attention mechanism. Unlike existing approaches that patch hallucinations after generation or prepend retrieved text, FGA intervenes at the mathematical heart of the transformer--the pre-softmax attention scores--creating a model that cannot hallucinate when facts exist in its knowledge base. Our experiments across 1,107 technical queries spanning smartphones, laptops, and electric vehicles demonstrate a transformation from 6.3% accuracy in vanilla Llama 3.2 to 99.7% accuracy with FGA. More critically, knowledge updates occur in under one second without retraining, compared to hours for parameter editing approaches. FGA doesn't just reduce hallucination--it eliminates it entirely for verifiable facts, marking a fundamental shift from probabilistic approximation to deterministic precision in neural language generation.<br>
<span id='abs_ch'>This paper introduces Fact Grounded Attention (FGA), a =
novel transformer modification that eliminates hallucinations by injecting =
verifiable knowledge directly into attention mechanisms, achieving 99.7% ac=
curacy and enabling instant knowledge updates without retraining.</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2509.25239.pdf' target='_blank'>https://arxiv.org/pdf/2509.25239.pdf</a></span>   <span><a href='https://github.com/kevin671/cot-vs-loop' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Xu, Issei Sato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25239">A Formal Comparison Between Chain-of-Thought and Latent Thought</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-Thought (CoT) elicits reasoning in large language models by explicitly generating intermediate steps in natural language. In contrast, Latent Thought in looped models operates directly in the continuous latent space, enabling computation beyond discrete linguistic representations. While both approaches exploit iterative computation, their comparative capabilities remain underexplored. In this work, we present a formal analysis showing that Latent Thought in Looped Transformers enables parallel computation, which is more efficient than the inherently sequential process of CoT. In contrast, CoT leverages stochastic decoding to approximate solutions to problems where exact computation is intractable. These separations suggest the tasks for which depth-driven recursion is more suitable, thereby offering practical guidance for choosing between reasoning paradigms. Code is available at https://github.com/kevin671/cot-vs-loop.<br>
<span id='abs_ch'>中文摘要：循环变换器中的潜在思维支持高效的并行计算，而思维链则采用序列推理和随机解 处理难解问题，为选择不同推理范式提供了实用指导。</span><br>
<span id='abs_en'>English Summary: Latent Thought in looped transformers enables efficient parallel computation, while Chain-of-Thought uses sequential reasoning with stochastic decoding for intractable problems, providing guidance for choosing between these reasoning paradigms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2509.25239.pdf' target='_blank'>https://arxiv.org/pdf/2509.25239.pdf</a></span>   <span><a href='https://github.com/kevin671/cot-vs-loop' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kevin Xu, Issei Sato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25239">A Formal Comparison Between Chain-of-Thought and Latent Thought</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-Thought (CoT) elicits reasoning in large language models by explicitly generating intermediate steps in natural language. In contrast, Latent Thought in looped models operates directly in the continuous latent space, enabling computation beyond discrete linguistic representations. While both approaches exploit iterative computation, their comparative capabilities remain underexplored. In this work, we present a formal analysis showing that Latent Thought in Looped Transformers enables parallel computation, which is more efficient than the inherently sequential process of CoT. In contrast, CoT leverages stochastic decoding to approximate solutions to problems where exact computation is intractable. These separations suggest the tasks for which depth-driven recursion is more suitable, thereby offering practical guidance for choosing between reasoning paradigms. Code is available at https://github.com/kevin671/cot-vs-loop.<br>
<span id='abs_ch'>中文摘要：循环变换器中的潜在思维支持高效的并行计算，而思维链则采用序列推理和随机解 处理难解问题，为选择不同推理范式提供了实用指导。</span><br>
<span id='abs_en'>English Summary: Latent Thought in looped transformers enables efficient parallel computation, while Chain-of-Thought uses sequential reasoning with stochastic decoding for intractable problems, providing guidance for choosing between these reasoning paradigms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2509.25182.pdf' target='_blank'>https://arxiv.org/pdf/2509.25182.pdf</a></span>   <span><a href='https://github.com/dc-ai-projects/DC-VideoGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyu Chen, Wenkun He, Yuchao Gu, Yuyang Zhao, Jincheng Yu, Junsong Chen, Dongyun Zou, Yujun Lin, Zhekai Zhang, Muyang Li, Haocheng Xi, Ligeng Zhu, Enze Xie, Song Han, Han Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25182">DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce DC-VideoGen, a post-training acceleration framework for efficient video generation. DC-VideoGen can be applied to any pre-trained video diffusion model, improving efficiency by adapting it to a deep compression latent space with lightweight fine-tuning. The framework builds on two key innovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal temporal design that achieves 32x/64x spatial and 4x temporal compression while preserving reconstruction quality and generalization to longer videos; and (ii) AE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer of pre-trained models into the new latent space. Adapting the pre-trained Wan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100 GPU. The accelerated models achieve up to 14.8x lower inference latency than their base counterparts without compromising quality, and further enable 2160x3840 video generation on a single GPU. Code: https://github.com/dc-ai-projects/DC-VideoGen.<br>
<span id='abs_ch'>中文: DC-VideoGen是一种后训练 速框架，通过轻量级微调将预训练模型适配到深度压缩的潜空间，显著提升视频生成效率，推理延迟降低高达14.8倍且不损失质量。</span><br>
<span id='abs_en'>English: DC-VideoGen is a post-training acceleration framework that enhances video generation efficiency by adapting pre-trained models to a compressed latent space through lightweight fine-tuning, achieving up to 14.8x faster inference without quality loss.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2509.25180.pdf' target='_blank'>https://arxiv.org/pdf/2509.25180.pdf</a></span>   <span><a href='https://github.com/dc-ai-projects/DC-Gen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenkun He, Yuchao Gu, Junyu Chen, Dongyun Zou, Yujun Lin, Zhekai Zhang, Haocheng Xi, Muyang Li, Ligeng Zhu, Jincheng Yu, Junsong Chen, Enze Xie, Song Han, Han Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25180">DC-Gen: Post-Training Diffusion Acceleration with Deeply Compressed Latent Space</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Existing text-to-image diffusion models excel at generating high-quality images, but face significant efficiency challenges when scaled to high resolutions, like 4K image generation. While previous research accelerates diffusion models in various aspects, it seldom handles the inherent redundancy within the latent space. To bridge this gap, this paper introduces DC-Gen, a general framework that accelerates text-to-image diffusion models by leveraging a deeply compressed latent space. Rather than a costly training-from-scratch approach, DC-Gen uses an efficient post-training pipeline to preserve the quality of the base model. A key challenge in this paradigm is the representation gap between the base model's latent space and a deeply compressed latent space, which can lead to instability during direct fine-tuning. To overcome this, DC-Gen first bridges the representation gap with a lightweight embedding alignment training. Once the latent embeddings are aligned, only a small amount of LoRA fine-tuning is needed to unlock the base model's inherent generation quality. We verify DC-Gen's effectiveness on SANA and FLUX.1-Krea. The resulting DC-Gen-SANA and DC-Gen-FLUX models achieve quality comparable to their base models but with a significant speedup. Specifically, DC-Gen-FLUX reduces the latency of 4K image generation by 53x on the NVIDIA H100 GPU. When combined with NVFP4 SVDQuant, DC-Gen-FLUX generates a 4K image in just 3.5 seconds on a single NVIDIA 5090 GPU, achieving a total latency reduction of 138x compared to the base FLUX.1-Krea model. Code: https://github.com/dc-ai-projects/DC-Gen.<br>
<span id='abs_ch'>中文：DC-Gen通过轻量级嵌入对齐和少量LoRA微调，利用深度压缩的潜在空间 速文本到图像扩散模型，在保持与基础模型相当图像质量的同时实现显著 速。</span><br>
<span id='abs_en'>English: DC-Gen accelerates text-to-image diffusion models by leveraging a deeply compressed latent space through lightweight embedding alignment and minimal LoRA fine-tuning, achieving significant speedups while maintaining image quality comparable to base models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2509.25175.pdf' target='_blank'>https://arxiv.org/pdf/2509.25175.pdf</a></span>   <span><a href='https://github.com/ZJU-REAL/EasySteer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haolei Xu, Xinyu Mei, Yuchen Yan, Rui Zhou, Wenqi Zhang, Weiming Lu, Yueting Zhuang, Yongliang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25175">EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language model (LLM) steering has emerged as a promising paradigm for controlling model behavior at inference time through targeted manipulation of hidden states, offering a lightweight alternative to expensive retraining. However, existing steering frameworks suffer from critical limitations: computational inefficiency, limited extensibility, and restricted functionality that hinder both research progress and practical deployment. We present EasySteer, a unified framework for high-performance, extensible LLM steering built on vLLM. Our system features modular architecture with pluggable interfaces for both analysis-based and learning-based methods, fine-grained parameter control, pre-computed steering vectors for eight application domains, and an interactive demonstration system. Through deep integration with vLLM's optimized inference engine, EasySteer achieves 5.5-11.4$\times$ speedup over existing frameworks. Extensive experiments demonstrate its effectiveness in overthinking mitigation, hallucination reduction, and other key applications. EasySteer transforms steering from research technique to production-ready capability, establishing critical infrastructure for deployable, controllable language models.<br>
<span id='abs_ch'>中文摘要：EasySteer是一个基于vLLM构建的高性能、可扩展大语言模型引导框架，通过与优化推理引擎深度集成实现显著 速，并具备模块化架构和多领域应用能力。</span><br>
<span id='abs_en'>English Summary: EasySteer is a high-performance, extensible framework for LLM steering that achieves significant speed improvements and broad application effectiveness through deep integration with vLLM.</span>Paperid:96,https://arxiv.org/pdf/2509.25160.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2509.25160.pdf' target='_blank'>https://arxiv.org/pdf/2509.25160.pdf</a></span>   <span><a href='https://github.com/ZJU-REAL/GSM8K-V' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Yuan, Yuchen Yan, Yifan Jiang, Haoran Zhao, Tao Feng, Jinyan Chen, Yanwei Lou, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25160">GSM8K-V: Can Vision Language Models Solve Grade School Math Word Problems in Visual Contexts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision language models (VLMs) achieve unified modeling of images and text, enabling them to accomplish complex real-world tasks through perception, planning, and reasoning. Among these tasks, reasoning is particularly representative, with mathematical reasoning serving as a prominent example. It highlights the high-level capability of VLMs to comprehend mathematical information in images and to perform sophisticated reasoning. Recently, numerous visual mathematical reasoning benchmarks have been proposed, but they are often restricted to geometry, lack coverage of math word problems, and rarely assess reasoning across multiple images. To address these gaps, we introduce GSM8K-V, a purely visual multi-image mathematical reasoning benchmark. GSM8K-V is built by systematically mapping each sample from the widely used text-based GSM8K into visual form. Through a carefully designed automated image-generation pipeline combined with meticulous human annotation, we curate 1,319 high-quality samples. We evaluate a wide range of open-source and closed-source models on GSM8K-V. Results show that although existing VLMs have nearly saturated performance on text-based GSM8K, there remains substantial room for improvement on GSM8K-V. For example, the best-performing model, Gemini-2.5-Pro, achieves 95.22% accuracy on GSM8K but only 46.93% on GSM8K-V. We conduct a comprehensive analysis of GSM8K-V, examining the limitations of current models as well as potential directions for improvement. GSM8K-V offers a new perspective on visual mathematical reasoning and establishes a benchmark to guide the development of more robust and generalizable VLMs.<br>
<span id='abs_ch'>中文: GSM8K-V作为一个纯视觉多图像数学推理基准被提出，旨在弥补现有基准的不足，揭示了当前视觉语言模型虽然在文本数学推理上表现优异，但在视觉数学推理方面仍有巨大提升空间。</span><br>
<span id='abs_en'>English: GSM8K-V is introduced as a purely visual multi-image mathematical reasoning benchmark to address gaps in existing benchmarks, revealing significant performance disparities between text-based and visual mathematical reasoning in current vision language models despite their advanced capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2509.25131.pdf' target='_blank'>https://arxiv.org/pdf/2509.25131.pdf</a></span>   <span><a href='https://github.com/dvlab-research/MGM-Omni' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengyao Wang, Zhisheng Zhong, Bohao Peng, Senqiao Yang, Yuqi Liu, Haokun Gui, Bin Xia, Jingyao Li, Bei Yu, Jiaya Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.25131">MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present MGM-Omni, a unified Omni LLM for omni-modal understanding and expressive, long-horizon speech generation. Unlike cascaded pipelines that isolate speech synthesis, MGM-Omni adopts a "brain-mouth" design with a dual-track, token-based architecture that cleanly decouples multimodal reasoning from real-time speech generation. This design enables efficient cross-modal interaction and low-latency, streaming speech generation. For understanding, a unified training strategy coupled with a dual audio encoder design enables long-form audio perception across diverse acoustic conditions. For generation, a chunk-based parallel decoding scheme narrows the text speech token-rate gap, accelerating inference and supporting streaming zero-shot voice cloning with stable timbre over extended durations. Compared to concurrent work, MGM-Omni achieves these capabilities with markedly data-efficient training. Extensive experiments demonstrate that MGM-Omni outperforms existing open source models in preserving timbre identity across extended sequences, producing natural and context-aware speech, and achieving superior long-form audio and omnimodal understanding. MGM-Omni establishes an efficient, end-to-end paradigm for omnimodal understanding and controllable, personalised long-horizon speech generation.<br>
<span id='abs_ch'>Chinese: MGM-Omni 是一种统一的Omni LLM，通过双轨架构高效处理多模态理解和富有表现力的长时程语音生成，实现了低延迟流式处理，并在数据高效训练的基础上，在各项任务中展现出卓越性能。</span><br>
<span id='abs_en'>English: MGM-Omni is a unified Omni LLM that efficiently handles multimodal understanding and expressive, long-horizon speech generation through a dual-track architecture, enabling low-latency streaming and superior performance across diverse tasks with data-efficient training.Paperid:100,https://arxiv.org/pdf/2509.25095.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2509.24988.pdf' target='_blank'>https://arxiv.org/pdf/2509.24988.pdf</a></span>   <span><a href='https://github.com/The-Inscrutable-X/CalibratedModelAgnosticCorrectness' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanqi Xiao, Vaidehi Patil, Hyunji Lee, Elias Stengel-Eskin, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24988">Generalized Correctness Models: Learning Calibrated and Model-Agnostic Correctness Predictors from Historical Patterns</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Generating accurate and calibrated confidence estimates is critical for deploying LLMs in high-stakes or user-facing applications, and remains an open challenge. Prior research has often framed confidence as a problem of eliciting a model's "self-knowledge", i.e., the ability of an LLM to judge whether its own answers are correct; this approach implicitly assumes that there is some privileged information about the answer's correctness that is accessible to the model itself. However, our experiments reveal that an LLM attempting to predict the correctness of its own outputs generally performs no better than an unrelated LLM. Moreover, we hypothesize that a key factor in building a "Correctness Model" (CM) is exposure to a target model's historical predictions. We propose multiple methods to inject this historical correctness information, creating a Generalized Correctness Model (GCM). We first show that GCMs can be trained on the correctness data from many LLMs and learn patterns for correctness prediction applicable across datasets and models. We then use CMs as a lens for studying the source of correctness prediction ability and its generalization, systematically controlling their training data and finding that answer phrasing is a strong predictor for correctness. We further explore alternative methods of injecting history without training an LLM, finding that including history as in-context examples can help improve correctness prediction, and post-hoc calibration can provide complementary reductions in calibration error. We evaluate GCMs based on Qwen3-8B across 5 model families and the MMLU and TriviaQA datasets, as well as on a downstream selective prediction task, finding that reliable LLM confidence estimation is a generalizable and model-agnostic skill learned by systematically encoding correctness history rather than a model-specific skill reliant on self-introspection.<br>
<br>
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2509.24913.pdf' target='_blank'>https://arxiv.org/pdf/2509.24913.pdf</a></span>   <span><a href='https://github.com/biomedia-mira/seg-cft' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tian Xia, Matthew Sinclair, Andreas Schuh, Fabio De Sousa Ribeiro, Raghav Mehta, Rajat Rasal, Esther Puyol-Antón, Samuel Gerber, Kersten Petersen, Michiel Schaap, Ben Glocker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24913">Segmentor-Guided Counterfactual Fine-Tuning for Locally Coherent and Targeted Image Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Counterfactual image generation is a powerful tool for augmenting training data, de-biasing datasets, and modeling disease. Current approaches rely on external classifiers or regressors to increase the effectiveness of subject-level interventions (e.g., changing the patient's age). For structure-specific interventions (e.g., changing the area of the left lung in a chest radiograph), we show that this is insufficient, and can result in undesirable global effects across the image domain. Previous work used pixel-level label maps as guidance, requiring a user to provide hypothetical segmentations which are tedious and difficult to obtain. We propose Segmentor-guided Counterfactual Fine-Tuning (Seg-CFT), which preserves the simplicity of intervening on scalar-valued, structure-specific variables while producing locally coherent and effective counterfactuals. We demonstrate the capability of generating realistic chest radiographs, and we show promising results for modeling coronary artery disease. Code: https://github.com/biomedia-mira/seg-cft.<br>
<span id='abs_ch'>中文: 现有的反事实图像生成方法在处理结构特异性干预时效果不足且依赖繁琐的像 级 注，而本文提出的Seg-CFT方法通过简单 量变量即可生成局部一致的反事实图像，在医学影像领域展现出良好应用前景。</span><br>
<span id='abs_en'>English: Current counterfactual image generation methods are insufficient for structure-specific interventions and require tedious pixel-level guidance, but the proposed Seg-CFT approach effectively produces locally coherent counterfactuals using simple scalar variables while demonstrating promising results in medical imaging.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2509.24873.pdf' target='_blank'>https://arxiv.org/pdf/2509.24873.pdf</a></span>   <span><a href='https://github.com/calgo-lab/BGR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Teodor Chiaburu, Vipin Singh, Frank HauÃer, Felix BieÃmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24873">Uncertainty-Guided Expert-AI Collaboration for Efficient Soil Horizon Annotation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Uncertainty quantification is essential in human-machine collaboration, as human agents tend to adjust their decisions based on the confidence of the machine counterpart. Reliably calibrated model uncertainties, hence, enable more effective collaboration, targeted expert intervention and more responsible usage of Machine Learning (ML) systems. Conformal prediction has become a well established model-agnostic framework for uncertainty calibration of ML models, offering statistically valid confidence estimates for both regression and classification tasks. In this work, we apply conformal prediction to $\textit{SoilNet}$, a multimodal multitask model for describing soil profiles. We design a simulated human-in-the-loop (HIL) annotation pipeline, where a limited budget for obtaining ground truth annotations from domain experts is available when model uncertainty is high. Our experiments show that conformalizing SoilNet leads to more efficient annotation in regression tasks and comparable performance scores in classification tasks under the same annotation budget when tested against its non-conformal counterpart. All code and experiments can be found in our repository: https://github.com/calgo-lab/BGR<br>
<span id='abs_ch'>中文: 保形预测改进了SoilNet模型的不确定性 准，在有限专家 注预算下，实现了回归任务中人机协同 注效率的提升，同时保持了分类任务的同等性能水平。</span><br>
<span id='abs_en'>English: Conformal prediction enhances SoilNet's uncertainty calibration, enabling more efficient human-in-the-loop soil annotation in regression tasks while maintaining classification performance under limited expert budgets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2509.24776.pdf' target='_blank'>https://arxiv.org/pdf/2509.24776.pdf</a></span>   <span><a href='https://github.com/yizhuoDi/VTPerceprion-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhuo Ding, Mingkang Chen, Zhibang Feng, Tong Xiao, Wanying Qu, Wenqi Shao, Yanwei Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24776">VTPerception-R1: Enhancing Multimodal Reasoning via Explicit Visual and Textual Perceptual Grounding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal large language models (MLLMs) often struggle to ground reasoning in perceptual evidence. We present a systematic study of perception strategies-explicit, implicit, visual, and textual-across four multimodal benchmarks and two MLLMs. Our findings show that explicit perception, especially when paired with textual cues, consistently yields the best improvements, particularly for smaller models. Based on this insight, we propose VTPerception-R1, a unified two-stage framework that decouples perception from reasoning. Stage 1 introduces perception-augmented fine-tuning, and Stage 2 applies perception-aware reinforcement learning with novel visual, textual, and consistency rewards. Experiments demonstrate that VTPerception-R1 significantly improves reasoning accuracy and robustness across diverse tasks, offering a scalable and auditable solution for perception-grounded multimodal reasoning. Our code is available at: https://github.com/yizhuoDi/VTPerceprion-R1.<br>
<span id='abs_ch'>中文: 多模态大语言模型常难以将推理基于感知证据，而提出的VTPerception-R1框架通过感知与推理解耦的两阶段微调和强化学 ，显著提升了多种任务中的准确性和鲁棒性。</span><br>
<span id='abs_en'>English: Multimodal large language models often fail to base reasoning on perceptual evidence, but the proposed VTPerception-R1 framework, which decouples perception from reasoning through two stages of fine-tuning and reinforcement learning, significantly enhances accuracy and robustness across various tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2509.24748.pdf' target='_blank'>https://arxiv.org/pdf/2509.24748.pdf</a></span>   <span><a href='https://github.com/felix-thu/RPEX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Longxiang He, Deheng Ye, Junbo Tan, Xueqian Wang, Li Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24748">Robust Policy Expansion for Offline-to-Online RL under Diverse Data Corruption</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Pretraining a policy on offline data followed by fine-tuning through online interactions, known as Offline-to-Online Reinforcement Learning (O2O RL), has emerged as a promising paradigm for real-world RL deployment. However, both offline datasets and online interactions in practical environments are often noisy or even maliciously corrupted, severely degrading the performance of O2O RL. Existing works primarily focus on mitigating the conservatism of offline policies via online exploration, while the robustness of O2O RL under data corruption, including states, actions, rewards, and dynamics, is still unexplored. In this work, we observe that data corruption induces heavy-tailed behavior in the policy, thereby substantially degrading the efficiency of online exploration. To address this issue, we incorporate Inverse Probability Weighted (IPW) into the online exploration policy to alleviate heavy-tailedness, and propose a novel, simple yet effective method termed $\textbf{RPEX}$: $\textbf{R}$obust $\textbf{P}$olicy $\textbf{EX}$pansion. Extensive experimental results on D4RL datasets demonstrate that RPEX achieves SOTA O2O performance across a wide range of data corruption scenarios. Code is available at $\href{https://github.com/felix-thu/RPEX}{https://github.com/felix-thu/RPEX}$.<br>
<span id='abs_ch'>中文: 离线到在线强化学  数据污染导致性能下降，提出的RPEX方法采用逆概率 权技术增强鲁棒性，在多种数据污染场景下取得了最优性能。</span><br>
<span id='abs_en'>English: Offline-to-Online Reinforcement Learning faces performance degradation from data corruption, which is addressed by the proposed RPEX method using Inverse Probability Weighting to enhance robustness and achieve state-of-the-art results.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2509.24592.pdf' target='_blank'>https://arxiv.org/pdf/2509.24592.pdf</a></span>   <span><a href='https://github.com/jtlicardo/bpmn-assistant' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Josip Tomo Licardo, Nikola Tankovic, Darko Etinger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24592">BPMN Assistant: An LLM-Based Approach to Business Process Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents BPMN Assistant, a tool that leverages Large Language Models (LLMs) for natural language-based creation and editing of BPMN diagrams. A specialized JSON-based representation is introduced as a structured alternative to the direct handling of XML to enhance the accuracy of process modifications. Process generation quality is evaluated using Graph Edit Distance (GED) and Relative Graph Edit Distance (RGED), while editing performance is evaluated with a binary success metric. Results show that JSON and XML achieve similar similarity scores in generation, but JSON offers greater reliability, faster processing, and significantly higher editing success rates. We discuss key trade-offs, limitations, and future improvements. The implementation is available at https://github.com/jtlicardo/bpmn-assistant.<br>
<span id='abs_ch'>中文: 本文介绍了BPMN Assistant工具，它利用大型语言模型通过自然语言创建和编辑BPMN图，采用JSON 式相比XML提高了准确性和效率，图编辑距离指 和更高的编辑成功率验证了其优势。</span><br>
<span id='abs_en'>English: This paper introduces BPMN Assistant, a tool using LLMs to create and edit BPMN diagrams through natural language, employing a JSON format that improves accuracy and efficiency over XML, as validated by graph distance metrics and higher editing success rates.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2509.24416.pdf' target='_blank'>https://arxiv.org/pdf/2509.24416.pdf</a></span>   <span><a href='https://github.com/Kai-Liu001/CLQ' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Kai-Liu001/CLQ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Liu, Shaoqiu Zhang, Linghe Kong, Yulun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24416">CLQ: Cross-Layer Guided Orthogonal-based Quantization for Diffusion Transformers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Visual generation quality has been greatly promoted with the rapid advances in diffusion transformers (DiTs), which is attributed to the scaling of model size and complexity. However, these attributions also hinder the practical deployment of DiTs on edge devices, limiting their development and application. Serve as an efficient model compression technique, model post-training quantization (PTQ) can reduce the memory consumption and speed up the inference, with inevitable performance degradation. To alleviate the degradation, we propose CLQ, a cross-layer guided orthogonal-based quantization method for DiTs. To be specific, CLQ consists of three key designs. First, we observe that the calibration data used by most of the PTQ methods can not honestly represent the distribution of the activations. Therefore, we propose cross-block calibration (CBC) to obtain accurate calibration data, with which the quantization can be better guided. Second, we propose orthogonal-based smoothing (OBS), which quantifies the outlier score of each channel and leverages block Hadamard matrix to smooth the outliers with negligible overhead. Third, we propose cross-layer parameter searching (CLPS) to search. We evaluate CLQ with both image generation and video generation models and successfully compress the model into W4A4 with negligible degradation in visual quality and metrics. CLQ achieves 3.98x memory saving and 3.95x speedup. Our code is available at \hyperlink{https://github.com/Kai-Liu001/CLQ}{https://github.com/Kai-Liu001/CLQ}.<br>
<span id='abs_ch'>中文摘要：CLQ是一种针对扩散变换器的跨层引导正交量化方法，通过精确 准数据和优化量化过程，在保持视觉质量的同时实现高效模型压缩，大幅降低内存 用并提升推理速度。</span><br>
<span id='abs_en'>English Summary: CLQ is a novel cross-layer guided orthogonal-based quantization method for diffusion transformers that achieves efficient model compression with minimal performance degradation, enabling significant memory savings and inference speedup.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2509.24414.pdf' target='_blank'>https://arxiv.org/pdf/2509.24414.pdf</a></span>   <span><a href='https://github.com/jk-sounds/ScatterAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Yin, Xiaohong Zhang, Shaochen Fu, Zhibin Zhang, Li Huang, Yiyuan Yang, Kaixiang Yang, Meng Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24414">ScatterAD: Temporal-Topological Scattering Mechanism for Time Series Anomaly Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>One main challenge in time series anomaly detection for industrial IoT lies in the complex spatio-temporal couplings within multivariate data. However, traditional anomaly detection methods focus on modeling spatial or temporal dependencies independently, resulting in suboptimal representation learning and limited sensitivity to anomalous dispersion in high-dimensional spaces. In this work, we conduct an empirical analysis showing that both normal and anomalous samples tend to scatter in high-dimensional space, especially anomalous samples are markedly more dispersed. We formalize this dispersion phenomenon as scattering, quantified by the mean pairwise distance among sample representations, and leverage it as an inductive signal to enhance spatio-temporal anomaly detection. Technically, we propose ScatterAD to model representation scattering across temporal and topological dimensions. ScatterAD incorporates a topological encoder for capturing graph-structured scattering and a temporal encoder for constraining over-scattering through mean squared error minimization between neighboring time steps. We introduce a contrastive fusion mechanism to ensure the complementarity of the learned temporal and topological representations. Additionally, we theoretically show that maximizing the conditional mutual information between temporal and topological views improves cross-view consistency and enhances more discriminative representations. Extensive experiments on multiple public benchmarks show that ScatterAD achieves state-of-the-art performance on multivariate time series anomaly detection. Code is available at this repository: https://github.com/jk-sounds/ScatterAD.<br>
<span id='abs_ch'>中文: 工业物联网时序异常检测面临复杂时空耦合的挑战，ScatterAD通过将异常分散形式化为散射现象，并利用对比融合机制结合时空与拓扑表征学 ，有效提升了检测性能。</span><br>
<span id='abs_en'>English: Industrial IoT time series anomaly detection faces challenges in modeling complex spatio-temporal couplings, which ScatterAD addresses by formalizing anomalous dispersion as scattering and enhancing detection through temporal and topological representation learning with contrastive fusion.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2509.24405.pdf' target='_blank'>https://arxiv.org/pdf/2509.24405.pdf</a></span>   <span><a href='https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Khanh Trinh Pham, Thu Huong Nguyen, Jun Jo, Quoc Viet Hung Nguyen, Thanh Tam Nguyen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24405">Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text-to-SQL enables natural access to databases, yet most benchmarks are English-only, limiting multilingual progress. We introduce MultiSpider 2.0, extending Spider 2.0 to eight languages (English, German, French, Spanish, Portuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's structural difficulty while adding linguistic and dialectal variability, demanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art LLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\% execution accuracy when relying on intrinsic reasoning, versus 60\% on MultiSpider 1.0. Therefore, we provide a collaboration-driven language agents baseline that iteratively refines queries, improving accuracy to 15\%. These results reveal a substantial multilingual gap and motivate methods that are robust across languages and ready for real-world enterprise deployment. Our benchmark is available at https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL.<br>
<span id='abs_ch'>中文：MultiSpider 2.0将Spider 2.0扩展至八种语言，揭示了大型语言模型在多语言环境下执行准确率仅为4%的显著差距，并通过协作式语言代理基准将准确率提升至15%。</span><br>
<span id='abs_en'>English: MultiSpider 2.0 extends Spider 2.0 to eight languages, revealing a significant multilingual gap where state-of-the-art LLMs achieve only 4% execution accuracy, and proposes a collaborative language agent baseline that improves accuracy to 15%.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2509.24377.pdf' target='_blank'>https://arxiv.org/pdf/2509.24377.pdf</a></span>   <span><a href='https://github.com/reml-group/PRISM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shihao Qi, Jie Ma, Ziang Yin, Lingling Zhang, Jian Zhang, Jun Liu, Feng Tian, Tongliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24377">Plan before Solving: Problem-Aware Strategy Routing for Mathematical Reasoning with LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Existing methods usually leverage a fixed strategy, such as natural language reasoning, code-augmented reasoning, tool-integrated reasoning, or ensemble-based reasoning, to guide Large Language Models (LLMs) to perform mathematical reasoning. Our analysis reveals that the single strategy cannot adapt to problem-specific requirements and thus overlooks the trade-off between effectiveness and efficiency. To address these issues, we propose Planning and Routing through Instance-Specific Modeling (PRISM), a novel framework that decouples mathematical reasoning into two stages: strategy planning and targeted execution. Specifically, we first curate a multi-strategy preference dataset, which we call MathStrat, capturing correctness, process quality, and computational efficiency for each problem--strategy pair. Then, we train a lightweight Strategy Adapter based on the dataset to obtain confidence distributions over the mentioned four reasoning strategies. At inference time, an adaptive routing policy dynamically tailors the reasoning approach based on predictor confidence. It directs the model to use single-strategy execution for high-confidence predictions, dual-strategy verification for competitive scenarios, or comprehensive multi-strategy exploration for uncertain cases. Extensive experiments across five mathematical reasoning benchmarks demonstrate that PRISM consistently outperforms individual strategies and ensemble baselines, achieving improvements ranging from 0.9% to 7.6% across different base models. The adaptive routing approach shows particularly strong benefits for mathematical reasoning tasks across diverse model architectures. Our code is released at https://github.com/reml-group/PRISM.<br>
<span id='abs_ch'>中文: 提出的PRISM框架通过策略规划与定向执行的两阶段过程，自适应地为大语言模型选择最适合的数学推理策略，在多个基准测试中均优于固定策略方法。</span><br>
<span id='abs_en'>English: The proposed PRISM framework enhances mathematical reasoning in LLMs by adaptively selecting the most suitable strategy through a two-stage process of planning and execution, outperforming fixed-strategy approaches across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2509.24372.pdf' target='_blank'>https://arxiv.org/pdf/2509.24372.pdf</a></span>   <span><a href='https://github.com/VsonicV/es-fine-tuning-paper' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Qiu, Yulu Gan, Conor F. Hayes, Qiyao Liang, Elliot Meyerson, Babak Hodjat, Risto Miikkulainen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24372">Evolution Strategies at Scale: LLM Fine-Tuning Beyond Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fine-tuning pre-trained large language models (LLMs) for down-stream tasks is a critical step in the AI deployment pipeline. Reinforcement learning (RL) is arguably the most prominent fine-tuning method, contributing to the birth of many state-of-the-art LLMs. In contrast, evolution strategies (ES), which once showed comparable performance to RL on models with a few million parameters, was neglected due to the pessimistic perception of its scalability to larger models. In this work, we report the first successful attempt to scale up ES for fine-tuning the full parameters of LLMs, showing the surprising fact that ES can search efficiently over billions of parameters and outperform existing RL fine-tuning methods in multiple respects, including sample efficiency, tolerance to long-horizon rewards, robustness to different base LLMs, less tendency to reward hacking, and more stable performance across runs. It therefore serves as a basis to unlock a new direction in LLM fine-tuning beyond what current RL techniques provide. The source codes are provided at: https://github.com/VsonicV/es-fine-tuning-paper.<br>
<span id='abs_ch'>中文: 本 究首次成功将进化策略扩展用于大语言模型的全参数微调，证明其在 本效率、奖励稳定性及抗干扰能力等方面优于主流强化学 方法。</span><br>
<span id='abs_en'>English: This study successfully scales evolution strategies (ES) to fine-tune large language models, demonstrating that ES outperforms reinforcement learning in efficiency, robustness, and stability across multiple metrics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2509.24365.pdf' target='_blank'>https://arxiv.org/pdf/2509.24365.pdf</a></span>   <span><a href='https://github.com/CURRENTF/Uni-X' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jitai Hao, Hao Liu, Xinyan Xiao, Qiang Huang, Jun Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24365">Uni-X: Mitigating Modality Conflict with a Two-End-Separated Architecture for Unified Multimodal Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Unified Multimodal Models (UMMs) built on shared autoregressive (AR) transformers are attractive for their architectural simplicity. However, we identify a critical limitation: when trained on multimodal inputs, modality-shared transformers suffer from severe gradient conflicts between vision and text, particularly in shallow and deep layers. We trace this issue to the fundamentally different low-level statistical properties of images and text, while noting that conflicts diminish in middle layers where representations become more abstract and semantically aligned. To overcome this challenge, we propose Uni-X, a two-end-separated, middle-shared architecture. Uni-X dedicates its initial and final layers to modality-specific processing, while maintaining shared parameters in the middle layers for high-level semantic fusion. This X-shaped design not only eliminates gradient conflicts at both ends but also further alleviates residual conflicts in the shared layers. Extensive experiments validate the effectiveness of Uni-X. Under identical training conditions, Uni-X achieves superior training efficiency compared to strong baselines. When scaled to 3B parameters with larger training data, Uni-X matches or surpasses 7B AR-based UMMs, achieving a GenEval score of 82 for image generation alongside strong performance in text and vision understanding tasks. These results establish Uni-X as a parameter-efficient and scalable foundation for future unified multimodal modeling. Our code is available at https://github.com/CURRENTF/Uni-X<br>
<span id='abs_ch'>中文：Uni-X架构通过两端分离模态处理、中间共享参数的方式解决了多模态模型中的梯度冲突问题，以更少的参数实现了更高的效率和性能。</span><br>
<span id='abs_en'>English: The Uni-X architecture addresses gradient conflicts in multimodal models by separating modality-specific processing at the ends while sharing middle layers, achieving superior efficiency and performance with fewer parameters.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2509.24361.pdf' target='_blank'>https://arxiv.org/pdf/2509.24361.pdf</a></span>   <span><a href='https://github.com/neovateai/UI-UG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yang, Weijie Qiu, Ru Zhang, Zhou Fang, Ruichao Mao, Xiaoyu Lin, Maji Huang, Zhaosong Huang, Teng Guo, Shuoyang Liu, Hai Rao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24361">UI-UG: A Unified MLLM for UI Understanding and Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although Multimodal Large Language Models (MLLMs) have been widely applied across domains, they are still facing challenges in domain-specific tasks, such as User Interface (UI) understanding accuracy and UI generation quality. In this paper, we introduce UI-UG (a unified MLLM for UI Understanding and Generation), integrating both capabilities. For understanding tasks, we employ Supervised Fine-tuning (SFT) combined with Group Relative Policy Optimization (GRPO) to enhance fine-grained understanding on the modern complex UI data. For generation tasks, we further use Direct Preference Optimization (DPO) to make our model generate human-preferred UIs. In addition, we propose an industrially effective workflow, including the design of an LLM-friendly domain-specific language (DSL), training strategies, rendering processes, and evaluation metrics. In experiments, our model achieves state-of-the-art (SOTA) performance on understanding tasks, outperforming both larger general-purpose MLLMs and similarly-sized UI-specialized models. Our model is also on par with these larger MLLMs in UI generation performance at a fraction of the computational cost. We also demonstrate that integrating understanding and generation tasks can improve accuracy and quality for both tasks. Code and Model: https://github.com/neovateai/UI-UG<br>
<span id='abs_ch'>中文: 本文提出UI-UG这一统一多模态大语言模型，整合了用户界面理解与生成能力，在理解任务上达到最优性能，并以更低计算成本实现了与更大模型相当的界面生成质量。</span><br>
<span id='abs_en'>English: This paper introduces UI-UG, a unified Multimodal Large Language Model that integrates UI understanding and generation, achieving state-of-the-art performance in understanding tasks and comparable generation quality to larger models with significantly lower computational cost.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2509.24361.pdf' target='_blank'>https://arxiv.org/pdf/2509.24361.pdf</a></span>   <span><a href='https://github.com/neovateai/UI-UG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Yang, Weijie Qiu, Ru Zhang, Zhou Fang, Ruichao Mao, Xiaoyu Lin, Maji Huang, Zhaosong Huang, Teng Guo, Shuoyang Liu, Hai Rao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24361">UI-UG: A Unified MLLM for UI Understanding and Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although Multimodal Large Language Models (MLLMs) have been widely applied across domains, they are still facing challenges in domain-specific tasks, such as User Interface (UI) understanding accuracy and UI generation quality. In this paper, we introduce UI-UG (a unified MLLM for UI Understanding and Generation), integrating both capabilities. For understanding tasks, we employ Supervised Fine-tuning (SFT) combined with Group Relative Policy Optimization (GRPO) to enhance fine-grained understanding on the modern complex UI data. For generation tasks, we further use Direct Preference Optimization (DPO) to make our model generate human-preferred UIs. In addition, we propose an industrially effective workflow, including the design of an LLM-friendly domain-specific language (DSL), training strategies, rendering processes, and evaluation metrics. In experiments, our model achieves state-of-the-art (SOTA) performance on understanding tasks, outperforming both larger general-purpose MLLMs and similarly-sized UI-specialized models. Our model is also on par with these larger MLLMs in UI generation performance at a fraction of the computational cost. We also demonstrate that integrating understanding and generation tasks can improve accuracy and quality for both tasks. Code and Model: https://github.com/neovateai/UI-UG<br>
<span id='abs_ch'>中文: 本文提出UI-UG这一统一多模态大语言模型，整合了用户界面理解与生成能力，在理解任务上达到最优性能，并以更低计算成本实现了与更大模型相当的界面生成质量。</span><br>
<span id='abs_en'>English: This paper introduces UI-UG, a unified Multimodal Large Language Model that integrates UI understanding and generation, achieving state-of-the-art performance in understanding tasks and comparable generation quality to larger models with significantly lower computational cost.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2509.24351.pdf' target='_blank'>https://arxiv.org/pdf/2509.24351.pdf</a></span>   <span><a href='https://github.com/reml-group/AMCS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Ma, Shihao Qi, Rui Xing, Ziang Yin, Bifan Wei, Jun Liu, Tongliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24351">From Static to Dynamic: Adaptive Monte Carlo Search for Mathematical Process Supervision</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The quality of process data plays a key role in training a Process Reward Model (PRM), which can enhance the complex mathematical reasoning capability of large language models. Existing methods estimate the quality of reasoning steps based on a fixed-budget sampling strategy and navigate a vast search space to perform path expansion during the automated data generation process, resulting in their inefficiency and inflexibility. To address these issues, we propose Adaptive Monte Carlo Search (AMCS), a framework that transforms data generation from fixed, static to adaptive, dynamic search at the level of node value estimation and path expansion. On one hand, AMCS adaptively refines estimation by allocating more samples to uncertain reasoning steps while using fewer samples for those that are easier to estimate. On the other hand, it enhances the path expansion through a Monte Carlo algorithm with a temporally adaptive policy that begins with broad exploration and gradually shifts toward exploiting the most promising directions. With AMCS, we construct a large-scale dataset MathSearch-200K of about 200K process supervision examples for training PRMs. To verify the effectiveness of our method, we conduct extensive experiments on four mathematical reasoning benchmarks. Experimental results show that Qwen2.5-Math-7B-PRM-AMCS achieves up to 76.2% accuracy on MATH500 with GLM-4-9B, outperforming all baseline PRMs. Notably, a 7B model supervised by Qwen2.5-Math-7B-PRM-AMCS surpasses a 72B model with weaker supervision. Moreover, Qwen2.5-Math-7B-PRM-AMCS maintains consistent advantages on out-of-distribution problems, demonstrating strong generalization capability. Our code is available at https://github.com/reml-group/AMCS.<br>
<span id='abs_ch'>中文: 本文提出自适应蒙特卡洛搜索（AMCS）框架，通过动态调整节点评估和路径扩展策略，显著提升了过程监督数据的生成效率，在数学推理基准测试中全面超越现有方法并展现出卓越的泛化能力。</span><br>
<span id='abs_en'>English: This paper introduces Adaptive Monte Carlo Search (AMCS), a dynamic framework that enhances the efficiency and flexibility of generating process supervision data for training Process Reward Models, leading to superior performance on mathematical reasoning benchmarks compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2509.24342.pdf' target='_blank'>https://arxiv.org/pdf/2509.24342.pdf</a></span>   <span><a href='https://github.com/sarmistha-D/Fin-Ally' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarmistha Das, Priya Mathur, Ishani Sharma, Sriparna Saha, Kitsuchart Pasupa, Alka Maurya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24342">Fin-Ally: Pioneering the Development of an Advanced, Commonsense-Embedded Conversational AI for Money Matters</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The exponential technological breakthrough of the FinTech industry has significantly enhanced user engagement through sophisticated advisory chatbots. However, large-scale fine-tuning of LLMs can occasionally yield unprofessional or flippant remarks, such as ``With that money, you're going to change the world,'' which, though factually correct, can be contextually inappropriate and erode user trust. The scarcity of domain-specific datasets has led previous studies to focus on isolated components, such as reasoning-aware frameworks or the enhancement of human-like response generation. To address this research gap, we present Fin-Solution 2.O, an advanced solution that 1) introduces the multi-turn financial conversational dataset, Fin-Vault, and 2) incorporates a unified model, Fin-Ally, which integrates commonsense reasoning, politeness, and human-like conversational dynamics. Fin-Ally is powered by COMET-BART-embedded commonsense context and optimized with a Direct Preference Optimization (DPO) mechanism to generate human-aligned responses. The novel Fin-Vault dataset, consisting of 1,417 annotated multi-turn dialogues, enables Fin-Ally to extend beyond basic account management to provide personalized budgeting, real-time expense tracking, and automated financial planning. Our comprehensive results demonstrate that incorporating commonsense context enables language models to generate more refined, textually precise, and professionally grounded financial guidance, positioning this approach as a next-generation AI solution for the FinTech sector. Dataset and codes are available at: https://github.com/sarmistha-D/Fin-Ally<br>
<span id='abs_ch'>中文摘要：Fin-Solution 2.O通过引入Fin-Vault多轮对话数据集和集成常识推理的Fin-Ally统一模型，解决了金融聊天机器人 领域数据稀缺导致回复不专业的问题，可生成更精准且符合人类偏好的财务指导。</span><br>
<span id='abs_en'>English Summary: Fin-Solution 2.O introduces the Fin-Vault dataset and Fin-Ally model to address the challenge of generating contextually appropriate and professional financial advice by integrating commonsense reasoning and human-like conversational dynamics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2509.24296.pdf' target='_blank'>https://arxiv.org/pdf/2509.24296.pdf</a></span>   <span><a href='https://github.com/niez233/DiffuGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zherui Li, Zheng Nie, Zhenhong Zhou, Yufei Guo, Yue Liu, Yitong Zhang, Yu Cheng, Qingsong Wen, Kun Wang, Jiaheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24296">DiffuGuard: How Intrinsic Safety is Lost and Found in Diffusion Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of Diffusion Large Language Models (dLLMs) introduces unprecedented vulnerabilities that are fundamentally distinct from Autoregressive LLMs, stemming from their iterative and parallel generation mechanisms. In this paper, we conduct an in-depth analysis of dLLM vulnerabilities to jailbreak attacks across two distinct dimensions: intra-step and inter-step dynamics. Experimental results reveal a harmful bias inherent in the standard greedy remasking strategy and identify a critical phenomenon we term Denoising-path Dependence, where the safety of early-stage tokens decisively influences the final output. These findings also indicate that while current decoding strategies constitute a significant vulnerability, dLLMs possess a substantial intrinsic safety potential. To unlock this potential, we propose DiffuGuard, a training-free defense framework that addresses vulnerabilities through a dual-stage approach: Stochastic Annealing Remasking dynamically introduces controlled randomness to mitigate greedy selection bias, while Block-level Audit and Repair exploits internal model representations for autonomous risk detection and guided correction. Comprehensive experiments on four dLLMs demonstrate DiffuGuard's exceptional effectiveness, reducing Attack Success Rate against six diverse jailbreak methods from 47.9% to 14.7% while preserving model utility and efficiency. Our code is available at: https://github.com/niez233/DiffuGuard.<br>
<span id='abs_ch'>中文：扩散大语言模型 其迭代生成机制存在独特的越狱攻击漏洞，为此提出的 需训练的防御框架DiffuGuard能显著降低攻击成功率，同时保持模型性能。</span><br>
<span id='abs_en'>English: Diffusion Large Language Models (dLLMs) exhibit unique vulnerabilities to jailbreak attacks due to their iterative generation process, prompting the development of DiffuGuard, a training-free defense that significantly reduces attack success rates while maintaining model performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2509.24262.pdf' target='_blank'>https://arxiv.org/pdf/2509.24262.pdf</a></span>   <span><a href='https://github.com/NimishaGhosh/LAMP-PRo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nimisha Ghosh, Dheeran Sankaran, Rahul Balakrishnan Adhi, Sharath S, Amrut Anand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24262">LAMP-PRo: Label-aware Attention for Multi-label Prediction of DNA- and RNA-binding Proteins using Protein Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Identifying DNA- (DBPs) and RNA-binding proteins (RBPs) is crucial for the understanding of cell function, molecular interactions as well as regulatory functions. Owing to their high similarity, most of the existing approaches face challenges in differentiating between DBPs and RBPs leading to high cross-prediction errors. Moreover, identifying proteins which bind to both DNA and RNA (DRBPs) is also quite a challenging task. In this regard, we propose a novel framework viz. LAMP-PRo which is based on pre-trained protein language model (PLM), attention mechanisms and multi-label learning to mitigate these issues. First, pre-trained PLM such ESM-2 is used for embedding the protein sequences followed by convolutional neural network (CNN). Subsequently multi-head self-attention mechanism is applied for the contextual information while label-aware attention is used to compute class-specific representations by attending to the sequence in a way that is tailored to each label (DBP, RBP and non-NABP) in a multi-label setup. We have also included a novel cross-label attention mechanism to explicitly capture dependencies between DNA- and RNA-binding proteins, enabling more accurate prediction of DRBP. Finally, a linear layer followed by a sigmoid function are used for the final prediction. Extensive experiments are carried out to compare LAMP-PRo with the existing methods wherein the proposed model shows consistent competent performance. Furthermore, we also provide visualization to showcase model interpretability, highlighting which parts of the sequence are most relevant for a predicted label. The original datasets are available at http://bliulab.net/iDRBP\_MMC and the codes are available at https://github.com/NimishaGhosh/LAMP-PRo.<br>
<span id='abs_ch'>中文: LAMP-PRo框架通过预训练蛋白质语言模型、注意力机制和多 签学 ，能准确区分DNA和RNA结合蛋白，并有效识别双重结合蛋白。</span><br>
<span id='abs_en'>English: The proposed LAMP-PRo framework utilizes pre-trained protein language models, attention mechanisms, and multi-label learning to accurately differentiate between DNA- and RNA-binding proteins while effectively identifying dual-binding proteins.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2509.24248.pdf' target='_blank'>https://arxiv.org/pdf/2509.24248.pdf</a></span>   <span><a href='https://github.com/Tencent/AngelSlim' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rubing Yang, Huajun Bai, Song Liu, Guanghua Yu, Runzhi Fan, Yanbin Dang, Jiejing Zhang, Kai Liu, Jianchen Zhu, Peng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24248">SpecExit: Accelerating Large Reasoning Model via Speculative Exit</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite their strong performance on reasoning tasks, large reasoning models (LRMs) often suffer from overthinking, producing unnecessarily long outputs and incurring high end-to-end latency, a significant limitation to their real-world deployment. To address overthinking, early-exit mechanisms have been proposed to terminate reasoning before typical completion, showing that this approach can effectively shorten generation length with minimal impact on accuracy. However, their reliance on probing mechanisms introduces a detection overhead that limits their end-to-end latency gains and compromises their generalizability across diverse problems. Inspired by the use of hidden states in speculative decoding, we propose SpecExit, a novel framework that predicts both future tokens and an early-exit signal directly from a lightweight draft model without probing overhead. Our method offers significant improvements, reducing average generation length by 66\% and achieving a 2.5x speedup in end-to-end latency compared to the speculative decoding baseline, without compromising accuracy. Our method leverages the inherent signals from hidden states to provide effective early-exit signals, suggesting broader use of hidden states for efficient reasoning. Our code is available at https://github.com/Tencent/AngelSlim.<br>
<span id='abs_ch'>Chinese: SpecExit 是一种新颖框架，利用轻量级草稿模型预测令牌和提前退出信号，在不损失准确性的前提下将生成长度减少 66%，实现 2.5 倍 速。</span><br>
<span id='abs_en'>English: SpecExit is a novel framework that uses a lightweight draft model to predict tokens and early-exit signals, reducing generation length by 66% and achieving 2.5x speedup without accuracy loss.Paperid:160,https://arxiv.org/pdf/2509.24236.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2509.24218.pdf' target='_blank'>https://arxiv.org/pdf/2509.24218.pdf</a></span>   <span><a href='https://github.com/jie040109/Conda' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjie Wang, Pan Zhou, Yiming Dong, Huan Li, Jia Li, Xun Zhou, Qicheng Lao, Cong Fang, Zhouchen Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24218">Conda: Column-Normalized Adam for Training Large Language Models Faster</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have demonstrated impressive generalization and emergent capabilities, yet their pre-training remains computationally expensive and sensitive to optimization dynamics. While Adam-based optimizers offer fast convergence by adapting learning rates coordinate-wise, recent studies reveal that their updates often suffer from poor spectral conditioning and low-rank structures, hindering efficiency. Muon addresses this issue via global spectral normalization but lacks the per-coordinate adaptivity of Adam. In this work, we propose Column-Normalized Adam (Conda), a novel optimizer that bridges the strengths of both approaches. Conda projects updates into an orthogonal subspace and applies column-wise second moment normalization based on the projected gradients, thereby achieving both improved spectral conditioning and maintaining coordinate-wise adaptivity. This design alleviates the spectral pathologies of Adam while preserving its fast convergence behavior. Extensive experiments on the LLaMA and GPT-2 series show that Conda consistently outperforms AdamW, Muon, and other baselines in pre-training. Remarkably, on the LLaMA series, Conda achieves 2-2.5 the convergence speed of AdamW, measured in both training steps and training time. Further ablations demonstrate its robustness under diverse training setups. These results collectively highlight Conda as an effective and broadly applicable optimizer for large-scale LLM training. The code is released on https://github.com/jie040109/Conda<br>
<span id='abs_ch'>Chinese Summary: 本文提出列归一化Adam优化器，通过正交子空间投影和列向二阶矩归一化，在保持坐 自适应性的同时改善谱条件，在LLaMA预训练中实现比AdamW快2-2.5倍的收敛速度。</span><br>
<span id='abs_en'>English Summary: The paper introduces Column-Normalized Adam (Conda), a novel optimizer that combines improved spectral conditioning with coordinate-wise adaptivity, achieving 2-2.5 times faster convergence than AdamW in LLaMA pre-training experiments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2509.24218.pdf' target='_blank'>https://arxiv.org/pdf/2509.24218.pdf</a></span>   <span><a href='https://github.com/jie040109/Conda' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjie Wang, Pan Zhou, Yiming Dong, Huan Li, Jia Li, Xun Zhou, Qicheng Lao, Cong Fang, Zhouchen Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24218">Conda: Column-Normalized Adam for Training Large Language Models Faster</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have demonstrated impressive generalization and emergent capabilities, yet their pre-training remains computationally expensive and sensitive to optimization dynamics. While Adam-based optimizers offer fast convergence by adapting learning rates coordinate-wise, recent studies reveal that their updates often suffer from poor spectral conditioning and low-rank structures, hindering efficiency. Muon addresses this issue via global spectral normalization but lacks the per-coordinate adaptivity of Adam. In this work, we propose Column-Normalized Adam (Conda), a novel optimizer that bridges the strengths of both approaches. Conda projects updates into an orthogonal subspace and applies column-wise second moment normalization based on the projected gradients, thereby achieving both improved spectral conditioning and maintaining coordinate-wise adaptivity. This design alleviates the spectral pathologies of Adam while preserving its fast convergence behavior. Extensive experiments on the LLaMA and GPT-2 series show that Conda consistently outperforms AdamW, Muon, and other baselines in pre-training. Remarkably, on the LLaMA series, Conda achieves 2-2.5 the convergence speed of AdamW, measured in both training steps and training time. Further ablations demonstrate its robustness under diverse training setups. These results collectively highlight Conda as an effective and broadly applicable optimizer for large-scale LLM training. The code is released on https://github.com/jie040109/Conda<br>
<span id='abs_ch'>Chinese Summary: 本文提出列归一化Adam优化器，通过正交子空间投影和列向二阶矩归一化，在保持坐 自适应性的同时改善谱条件，在LLaMA预训练中实现比AdamW快2-2.5倍的收敛速度。</span><br>
<span id='abs_en'>English Summary: The paper introduces Column-Normalized Adam (Conda), a novel optimizer that combines improved spectral conditioning with coordinate-wise adaptivity, achieving 2-2.5 times faster convergence than AdamW in LLaMA pre-training experiments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2509.24203.pdf' target='_blank'>https://arxiv.org/pdf/2509.24203.pdf</a></span>   <span><a href='https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaorui Yao, Yanxi Chen, Yuchang Sun, Yushuo Chen, Wenhao Zhang, Xuchen Pan, Yaliang Li, Bolin Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24203">Group-Relative REINFORCE Is Secretly an Off-Policy Algorithm: Demystifying Some Myths About GRPO and Its Friends</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Off-policy reinforcement learning (RL) for large language models (LLMs) is attracting growing interest, driven by practical constraints in real-world applications, the complexity of LLM-RL infrastructure, and the need for further innovations of RL methodologies. While classic REINFORCE and its modern variants like Group Relative Policy Optimization (GRPO) are typically regarded as on-policy algorithms with limited tolerance of off-policyness, we present in this work a first-principles derivation for group-relative REINFORCE without assuming a specific training data distribution, showing that it admits a native off-policy interpretation. This perspective yields two general principles for adapting REINFORCE to off-policy settings: regularizing policy updates, and actively shaping the data distribution. Our analysis demystifies some myths about the roles of importance sampling and clipping in GRPO, unifies and reinterprets two recent algorithms -- Online Policy Mirror Descent (OPMD) and Asymmetric REINFORCE (AsymRE) -- as regularized forms of the REINFORCE loss, and offers theoretical justification for seemingly heuristic data-weighting strategies. Our findings lead to actionable insights that are validated with extensive empirical studies, and open up new opportunities for principled algorithm design in off-policy RL for LLMs. Source code for this work is available at https://github.com/modelscope/Trinity-RFT/tree/main/examples/rec_gsm8k.<br>
<span id='abs_ch'>Chinese: 本 究从第一性原理推导了群体相对REINFORCE算法，揭示了其天然的离策略特性，提出了适用于离策略场景的两大改进原则，统一了近期相关算法框架，并为大语言模型的强化学 提供了经实证验证的设计思路。</span><br>
<span id='abs_en'>English: This work provides a first-principles derivation of group-relative REINFORCE, demonstrating its native off-policy capability and establishing two principles for adapting REINFORCE to off-policy settings, which unify recent algorithms and offer validated insights for LLM reinforcement learning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2509.24193.pdf' target='_blank'>https://arxiv.org/pdf/2509.24193.pdf</a></span>   <span><a href='https://github.com/ritaranx/AceSearcher' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ran Xu, Yuchen Zhuang, Zihan Dong, Jonathan Wang, Yue Yu, Joyce C. Ho, Linjun Zhang, Haoyu Wang, Wenqi Shi, Carl Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24193">AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Search-augmented LLMs often struggle with complex reasoning tasks due to ineffective multi-hop retrieval and limited reasoning ability. We propose AceSearcher, a cooperative self-play framework that trains a single large language model (LLM) to alternate between two roles: a decomposer that breaks down complex queries and a solver that integrates retrieved contexts for answer generation. AceSearcher couples supervised fine-tuning on a diverse mixture of search, reasoning, and decomposition tasks with reinforcement fine-tuning optimized for final answer accuracy, eliminating the need for intermediate annotations. Extensive experiments on three reasoning-intensive tasks across 10 datasets show that AceSearcher outperforms state-of-the-art baselines, achieving an average exact match improvement of 7.6%. Remarkably, on document-level finance reasoning tasks, AceSearcher-32B matches the performance of the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented LLMs with up to 9x more parameters, highlighting its exceptional efficiency and effectiveness in tackling complex reasoning tasks. Our code will be published at https://github.com/ritaranx/AceSearcher and https://huggingface.co/AceSearcher.<br>
<span id='abs_ch'>中文: AceSearcher是一种协作自博弈框架，通过训练单一大型语言模型交替担任分解复杂查询和整合检索信息生成答案的角色， 需中间 注即可在复杂推理任务中实现卓越性能与效率。</span><br>
<span id='abs_en'>English: AceSearcher is a cooperative self-play framework that trains a single LLM to alternate between decomposing complex queries and solving them with retrieved contexts, achieving superior performance and efficiency in complex reasoning tasks without intermediate annotations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2509.24096.pdf' target='_blank'>https://arxiv.org/pdf/2509.24096.pdf</a></span>   <span><a href='https://github.com/KaiyuHe998/GEAR-Abduction_evaluation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyu He, Peilin Wu, Mian Zhang, Kun Wan, Wentian Zhao, Xinya Du, Zhiyu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24096">GEAR: A General Evaluation Framework for Abductive Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Since the advent of large language models (LLMs), research has focused on instruction following and deductive reasoning. A central question remains: can these models discover new knowledge, and how can we evaluate this ability? We address this by studying abductive reasoning-the generation of plausible hypotheses to explain observations-and introduce GEAR (General Evaluation for Abductive Reasoning), a general-purpose, fully automated, transparent, and label-free evaluation paradigm. GEAR scores hypothesis sets by three metrics: consistency (each hypothesis explains the observations), generalizability (consistent hypotheses make meaningful predictions on unseen inputs), and diversity (the set covers distinct predictions and patterns). Built this way, GEAR is scalable (no human gold answers), reliable (deterministic scoring aligned with classical abduction), and open-ended (scores improve only when models produce new plausible hypotheses, unlike static benchmarks that saturate once accuracy is high). Using GEAR, we conduct a fine-grained study of nine LLMs on four abduction benchmarks with 1,500 problems, generating over 50,000 candidate hypotheses and revealing model differences obscured by gold-answer or purely human evaluations. We further propose a momentum-based curriculum that adjusts GEAR-derived training data by learning velocity: it starts with what the model learns quickly and shifts toward harder objectives such as generating diverse hypotheses once the model is confident on foundational objectives. Without gold-label supervision, this strategy improves all GEAR objectives and these gains transfer to established abductive reasoning benchmarks. Taken together, GEAR provides a principled framework that evaluates abduction and supplies label-free, scalable training signals that help LLMs produce more diverse and reliable hypotheses.<br>
<span id='abs_ch'>This research introduces GEAR, a novel evaluation frame=
work for assessing large language models' abductive reasoning ability throu=
gh automated scoring of hypothesis consistency, generalizability, and diver=
sity, while also proposing a momentum-based curriculum that improves model =
performance without requiring labeled data.</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2509.23986.pdf' target='_blank'>https://arxiv.org/pdf/2509.23986.pdf</a></span>   <span><a href='https://github.com/Alistair-Turcan/TusoAI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alistair Turcan, Kexin Huang, Lei Li, Martin Jinye Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23986">TusoAI: Agentic Optimization for Scientific Methods</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Scientific discovery is often slowed by the manual development of computational tools needed to analyze complex experimental data. Building such tools is costly and time-consuming because scientists must iteratively review literature, test modeling and scientific assumptions against empirical data, and implement these insights into efficient software. Large language models (LLMs) have demonstrated strong capabilities in synthesizing literature, reasoning with empirical data, and generating domain-specific code, offering new opportunities to accelerate computational method development. Existing LLM-based systems either focus on performing scientific analyses using existing computational methods or on developing computational methods or models for general machine learning without effectively integrating the often unstructured knowledge specific to scientific domains. Here, we introduce TusoAI , an agentic AI system that takes a scientific task description with an evaluation function and autonomously develops and optimizes computational methods for the application. TusoAI integrates domain knowledge into a knowledge tree representation and performs iterative, domain-specific optimization and model diagnosis, improving performance over a pool of candidate solutions. We conducted comprehensive benchmark evaluations demonstrating that TusoAI outperforms state-of-the-art expert methods, MLE agents, and scientific AI agents across diverse tasks, such as single-cell RNA-seq data denoising and satellite-based earth monitoring. Applying TusoAI to two key open problems in genetics improved existing computational methods and uncovered novel biology, including 9 new associations between autoimmune diseases and T cell subtypes and 7 previously unreported links between disease variants linked to their target genes. Our code is publicly available at https://github.com/Alistair-Turcan/TusoAI.<br>
<span id='abs_ch'>中文摘要：TusoAI 是一种自主智能系统，通过整合领域知识和迭代优化，自动开发并改进计算方法，在单细胞RNA测序和地球监测等任务中超越现有方法，并成功应用于遗 学难题取得新发现。</span><br>
<span id='abs_en'>English Summary: TusoAI is an autonomous agentic system that accelerates scientific discovery by developing and optimizing computational methods through domain-specific knowledge integration and iterative refinement, outperforming existing approaches in tasks like genetic analysis and earth monitoring.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2509.23946.pdf' target='_blank'>https://arxiv.org/pdf/2509.23946.pdf</a></span>   <span><a href='https://github.com/yks23/Explore-Execute-Chain.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaisen Yang, Lixuan He, Rushi Shah, Kaicheng Yang, Qinwei Ma, Dianbo Liu, Alex Lamb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23946">Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning abilities of Large Language Models (LLMs), yet their monolithic and auto-regressive architecture inherently conflates high-level strategic planning with low-level step-by-step execution, leading to computational inefficiency, limited exploration of reasoning paths, and reduced interpretability. To overcome these issues, we propose the Explore-Execute Chain ($E^2C$), a structured reasoning framework that decouples reasoning into two distinct phases: an exploratory phase that stochastically generates succinct high-level plans, followed by an execution phase that deterministically carries out the chosen plan. Our approach incorporates a two-stage training methodology, which combines Supervised Fine-Tuning (SFT) - augmented by a novel data generation algorithm enforcing strict plan adherence - with a subsequent Reinforcement Learning (RL) stage that capitalizes on the informativeness of exploration and reinforces the determinism of execution. This decomposition enables an efficient test-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches 58.1% accuracy using <10% of the decoding tokens required by comparable methods (e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For cross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with only 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher accuracy than standard SFT on medical benchmarks, delivering state-of-the-art performance, strong generalization, and greater interpretability by separating planning from execution. The code and pre-trained models for the project are available at: https://github.com/yks23/Explore-Execute-Chain.git<br>
<span id='abs_ch'>中文：提出的探索-执行链（E²C）框架将推理分解为独立的规划与执行阶段，在比现有方法减少90%以上令牌用量的同时，显著提升了计算效率、准确性和可解释性。</span><br>
<span id='abs_en'>English: The proposed Explore-Execute Chain (E²C) framework decouples reasoning into separate planning and execution phases, significantly improving computational efficiency, accuracy, and interpretability while reducing token usage by over 90% compared to existing methods.</span>Paperid:181,https://arxiv.org/pdf/2509.23931.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2509.23946.pdf' target='_blank'>https://arxiv.org/pdf/2509.23946.pdf</a></span>   <span><a href='https://github.com/yks23/Explore-Execute-Chain.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaisen Yang, Lixuan He, Rushi Shah, Kaicheng Yang, Qinwei Ma, Dianbo Liu, Alex Lamb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23946">Explore-Execute Chain: Towards an Efficient Structured Reasoning Paradigm</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-Thought (CoT) and its variants have markedly advanced the reasoning abilities of Large Language Models (LLMs), yet their monolithic and auto-regressive architecture inherently conflates high-level strategic planning with low-level step-by-step execution, leading to computational inefficiency, limited exploration of reasoning paths, and reduced interpretability. To overcome these issues, we propose the Explore-Execute Chain ($E^2C$), a structured reasoning framework that decouples reasoning into two distinct phases: an exploratory phase that stochastically generates succinct high-level plans, followed by an execution phase that deterministically carries out the chosen plan. Our approach incorporates a two-stage training methodology, which combines Supervised Fine-Tuning (SFT) - augmented by a novel data generation algorithm enforcing strict plan adherence - with a subsequent Reinforcement Learning (RL) stage that capitalizes on the informativeness of exploration and reinforces the determinism of execution. This decomposition enables an efficient test-time scaling strategy: on AIME'2024, $E^2C$ Test Time Scaling reaches 58.1% accuracy using <10% of the decoding tokens required by comparable methods (e.g., Forest-of-Thought), sharply cutting self-consistency overhead. For cross-domain adaptation, our Exploration-Focused SFT (EF-SFT) fine-tunes with only 3.5% of the tokens used by standard SFT yet yields up to 14.5% higher accuracy than standard SFT on medical benchmarks, delivering state-of-the-art performance, strong generalization, and greater interpretability by separating planning from execution. The code and pre-trained models for the project are available at: https://github.com/yks23/Explore-Execute-Chain.git<br>
<span id='abs_ch'>中文：提出的探索-执行链（E²C）框架将推理分解为独立的规划与执行阶段，在比现有方法减少90%以上令牌用量的同时，显著提升了计算效率、准确性和可解释性。</span><br>
<span id='abs_en'>English: The proposed Explore-Execute Chain (E²C) framework decouples reasoning into separate planning and execution phases, significantly improving computational efficiency, accuracy, and interpretability while reducing token usage by over 90% compared to existing methods.</span>Paperid:181,https://arxiv.org/pdf/2509.23931.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2509.23924.pdf' target='_blank'>https://arxiv.org/pdf/2509.23924.pdf</a></span>   <span><a href='https://github.com/yjyddq/EOSER-ASS-RL' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/yjyddq/EOSER-ASS-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyi Yang, Guanxu Chen, Xuhao Hu, Jing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23924">Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Masked diffusion language models (MDLMs) have recently emerged as a promising alternative to autoregressive (AR) language models, offering properties such as parallel decoding, flexible generation orders, and the potential for fewer inference steps. Despite these advantages, decoding strategies and reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored. A naive approach is to directly transfer techniques well-established for AR models to MDLMs. However, this raises an immediate question: Is such a naive transfer truly optimal? For example, 1) Block-wise and semi-AR decoding strategies are not employed during the training of MDLMs, so why do they outperform full diffusion-style decoding during inference? 2) Applying RL algorithms designed for AR models directly to MDLMs exhibits a training-inference inconsistency, since MDLM decoding are non-causal (parallel). This results in inconsistencies between the rollout trajectory and the optimization trajectory. To address these challenges, we propose EOS Early Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which unlock the potential of MDLMs to perform full diffusion-style decoding, achieving competitive performance with fewer decoding steps. Additionally, we introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO) for taming MDLMs, which emphasizes the consistency between rollout trajectory and optimization trajectory, and reduces the optimization errors caused by skip-step optimization. We conduct extensive experiments on reasoning tasks, such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The results demonstrate that the proposed EOSER and ASS mechanisms, together with CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs. Code: https://github.com/yjyddq/EOSER-ASS-RL.<br>
<span id='abs_ch'>中文: 掩 扩散语言模型虽具备并行解 和灵活生成的优势，但在解 策略和强化学 方面存在不足；为此提出的EOS早期拒绝和递增步长解 调度器，以及一致性轨迹组相对策略优化方法，有效提升了推理任务的性能与效率。</span><br>
<span id='abs_en'>English: Masked diffusion language models offer parallel decoding and flexible generation but face challenges with suboptimal decoding strategies and reinforcement learning inconsistencies, which are addressed by new techniques like EOS Early Rejection and Ascending Step-Size decoding scheduler, along with Consistency Trajectory Group Relative Policy Optimization, to enhance performance and efficiency in reasoning tasks.Paperid:183,https://arxiv.org/pdf/2509.23922.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2509.23893.pdf' target='_blank'>https://arxiv.org/pdf/2509.23893.pdf</a></span>   <span><a href='https://github.com/meloxxxxxx/DOC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhixin Zhang, Zeming Wei, Meng Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23893">Dynamic Orthogonal Continual Fine-tuning for Mitigating Catastrophic Forgettings</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Catastrophic forgetting remains a critical challenge in continual learning for large language models (LLMs), where models struggle to retain performance on historical tasks when fine-tuning on new sequential data without access to past datasets. In this paper, we first reveal that the drift of functional directions during the fine-tuning process is a key reason why existing regularization-based methods fail in long-term LLM continual learning. To address this, we propose Dynamic Orthogonal Continual (DOC) fine-tuning, a novel approach that tracks the drift of these functional directions and dynamically updates them during the fine-tuning process. Furthermore, by adjusting the gradients of new task parameters to be orthogonal to the tracked historical function directions, our method mitigates interference between new and old tasks. Extensive experiments on various LLM continual learning benchmarks demonstrate that this approach outperforms prior methods, effectively reducing catastrophic forgetting and providing a robust tool for continuous LLM fine-tuning. Our code is available at https://github.com/meloxxxxxx/DOC.<br>
<span id='abs_ch'>中文: 本文提出动态正交持续微调方法，通过追踪功能方向漂移并动态更新，同时调整新任务参数梯度使其与历史功能方向正交，有效缓解大语言模型持续学 中的灾难性遗忘问题，在多个基准测试中表现优异。</span><br>
<span id='abs_en'>English: This paper introduces Dynamic Orthogonal Continual (DOC) fine-tuning, a novel method that addresses catastrophic forgetting in LLMs by tracking and dynamically updating functional direction drifts while enforcing gradient orthogonality between new and historical tasks, achieving superior performance across benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2509.23871.pdf' target='_blank'>https://arxiv.org/pdf/2509.23871.pdf</a></span>   <span><a href='https://github.com/WhitolfChen/SCAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukun Chen, Boheng Li, Yu Yuan, Leyi Qi, Yiming Li, Tianwei Zhang, Zhan Qin, Kui Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23871">Taught Well Learned Ill: Towards Distillation-conditional Backdoor Attack</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Knowledge distillation (KD) is a vital technique for deploying deep neural networks (DNNs) on resource-constrained devices by transferring knowledge from large teacher models to lightweight student models. While teacher models from third-party platforms may undergo security verification (\eg, backdoor detection), we uncover a novel and critical threat: distillation-conditional backdoor attacks (DCBAs). DCBA injects dormant and undetectable backdoors into teacher models, which become activated in student models via the KD process, even with clean distillation datasets. While the direct extension of existing methods is ineffective for DCBA, we implement this attack by formulating it as a bilevel optimization problem and proposing a simple yet effective method (\ie, SCAR). Specifically, the inner optimization simulates the KD process by optimizing a surrogate student model, while the outer optimization leverages outputs from this surrogate to optimize the teacher model for implanting the conditional backdoor. Our SCAR addresses this complex optimization utilizing an implicit differentiation algorithm with a pre-optimized trigger injection function. Extensive experiments across diverse datasets, model architectures, and KD techniques validate the effectiveness of our SCAR and its resistance against existing backdoor detection, highlighting a significant yet previously overlooked vulnerability in the KD process. Our code is available at https://github.com/WhitolfChen/SCAR.<br>
<span id='abs_ch'>中文: 知识蒸馏技术可将大型教师模型的知识转移到轻量级学生模型，但新发现的蒸馏条件后门攻击（DCBA）能在教师模型中植入潜伏后门，这些后门在蒸馏过程中会被激活到学生模型中，我们提出的SCAR方法通过双层优化成功实现了这种攻击，并在多种实验环境中验证了其有效性。</span><br>
<span id='abs_en'>English: Knowledge distillation enables efficient deployment of deep neural networks on resource-limited devices, but a new threat called distillation-conditional backdoor attacks (DCBAs) can implant dormant backdoors in teacher models that activate in student models during distillation, which our proposed SCAR method effectively implements and demonstrates across various datasets and architectures.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2509.23809.pdf' target='_blank'>https://arxiv.org/pdf/2509.23809.pdf</a></span>   <span><a href='https://github.com/Tencent/AngelSlim' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hong Huang, Decheng Wu, Rui Cen, Guanghua Yu, Zonghang Li, Kai Liu, Jianchen Zhu, Peng Chen, Xue Liu, Dapeng Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23809">Tequila: Trapping-free Ternary Quantization for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Quantization techniques are essential for the deployment of Large Language Models (LLMs) on edge devices. However, prevailing methods often rely on mixed-precision multiplication that lacks efficient hardware support, making it not feasible. Ternary weight quantization addresses this by constraining weights to {-1, 0, 1}, replacing expensive multiplications with hardware-efficient additions. However, such aggressive compression leads to significant accuracy degradation, even after costly quantization-aware training with massive data. We identify the core issue as deadzone trapping: a large number of weights are trapped at the deadzone boundary. This occurs because these weights receive only noisy, uninformative gradients, preventing stable escape from the deadzone and severely impeding model capacity and optimization. To address this issue, we propose Tequila, a trapping-free quantization optimization method that reactivates deadzone-trapped weights by repurposing them as dynamic biases. This allows the repurposed weights to provide a continuous signal in the forward pass and, critically, receive direct, meaningful gradient signals during backpropagation, thereby enhancing model capacity and optimization with nearly zero inference overhead. Extensive evaluations demonstrate that Tequila outperforms state-of-the-art (SOTA) ternary quantization methods across five benchmarks. Specifically, on the ARC benchmark, it achieves >4% accuracy gain over the SOTA baseline, nearly matching full-precision performance (within <1% gap) with a 3.0x inference speedup. Consequently, Tequila offers a highly practical and efficient implementation for the deployment of advanced LLMs in resource-constrained environments. The code is available at https://github.com/Tencent/AngelSlim.<br>
<span id='abs_ch'>中文: Tequila是一种创新的量化方法，通过将死区边界权重重新用作动态偏置来激活它们，从而以最小精度损失和显著 速实现高效的三元大语言模型部署。</span><br>
<span id='abs_en'>English: Tequila is a novel quantization method that reactivates deadzone-trapped weights by converting them into dynamic biases, enabling efficient ternary LLM deployment with minimal accuracy loss and significant speedup.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2509.23751.pdf' target='_blank'>https://arxiv.org/pdf/2509.23751.pdf</a></span>   <span><a href='https://github.com/ayousefinejad/PVTAdpNet.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arshia Yousefi Nezhad, Helia Aghaei, Hedieh Sajedi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23751">PVTAdpNet: Polyp Segmentation using Pyramid vision transformer with a novel Adapter block</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Colorectal cancer ranks among the most common and deadly cancers, emphasizing the need for effective early detection and treatment. To address the limitations of traditional colonoscopy, including high miss rates due to polyp variability, we introduce the Pyramid Vision Transformer Adapter Residual Network (PVTAdpNet). This model integrates a U-Net-style encoder-decoder structure with a Pyramid Vision Transformer backbone, novel residual blocks, and adapter-based skip connections. The design enhances feature extraction, dense prediction, and gradient flow, supported by squeeze-and-excitation attention for improved channel-wise feature refinement. PVTAdpNet achieves real-time, accurate polyp segmentation, demonstrating superior performance on benchmark datasets with high mDice and mIoU scores, making it highly suitable for clinical applications. PVTAdpNet obtains a high Dice coefficient of 0.8851 and a mean Intersection over Union (mIoU) of 0.8167 on out-of-distribution polyp datasets. Evaluation of the PolypGen dataset demonstrates PVTAdpNet's capability for real-time, accurate performance within familiar distributions. The source code of our network is available at https://github.com/ayousefinejad/PVTAdpNet.git<br>
<span id='abs_ch'>中文：PVTAdpNet模型通过融合金字塔视觉Transformer与适配器连接，实现了结直 癌检测中息肉的实时精准分割，在基准数据集上展现出卓越性能。</span><br>
<span id='abs_en'>English: PVTAdpNet, a novel model combining Pyramid Vision Transformer with adapter-based connections, achieves real-time, accurate polyp segmentation for colorectal cancer detection, demonstrating superior performance on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2509.23744.pdf' target='_blank'>https://arxiv.org/pdf/2509.23744.pdf</a></span>   <span><a href='https://github.com/DELTA-DoubleWise/OmniReason' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yucheng Wang, Yifan Hou, Aydin Javadov, Mubashara Akhtar, Mrinmaya Sachan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23744">Compose and Fuse: Revisiting the Foundational Bottlenecks in Multimodal Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal large language models (MLLMs) promise enhanced reasoning by integrating diverse inputs such as text, vision, and audio. Yet cross-modal reasoning remains underexplored, with conflicting reports on whether added modalities help or harm performance. These inconsistencies stem from a lack of controlled evaluation frameworks and analysis of models' internals to isolate when and why modality interactions support or undermine reasoning. We address this gap through a logic-grounded evaluation framework that categorizes multimodal reasoning into six interaction patterns, varying how facts are distributed across modalities and logically combined. Empirically, additional modalities enhance reasoning only when they provide independent and sufficient reasoning paths, while redundant or chained entailment support often hurts performance. Moreover, reasoning degrades in three systematic ways: weaker modalities drag down overall performance, conflicts bias preference toward certain modalities, and joint signals from different modalities fail to be integrated effectively. Therefore, we identify two core failures: task-composition bottleneck, where recognition and reasoning cannot be jointly executed in one pass, and fusion bottleneck, where early integration introduces bias. For further investigation, we find that attention patterns fail to encode fact usefulness, but a simple two-step prompting (recognize then reason) restores performance, confirming the task-composition bottleneck. Moreover, modality identity remains recoverable in early layers, and softening attention in early fusion improves reasoning, highlighting biased fusion as another failure mode. Overall, our findings show that integration, not perception, is the main barrier to multimodal reasoning, suggesting composition-aware training and early fusion control as promising directions.<br>
<span id='abs_ch'>中文摘要：多模态推理仅在模态提供独立逻辑路径时得以提升，而集成失败（非感知问题）是主要瓶颈；通过结构化评估框架发现任务组合与融合障碍，并提出分步提示与早期融合控制作为解决方向。</span><br>
<span id='abs_en'>English Summary: Multimodal reasoning improves only when modalities provide independent logical paths, with integration failures—not perception—being the primary bottleneck, as revealed through a structured evaluation framework identifying task-composition and fusion issues.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2509.23694.pdf' target='_blank'>https://arxiv.org/pdf/2509.23694.pdf</a></span>   <span><a href='https://github.com/jianshuod/SafeSearch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianshuo Dong, Sheng Guo, Hao Wang, Zhuotao Liu, Tianwei Zhang, Ke Xu, Minlie Huang, Han Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23694">SafeSearch: Automated Red-Teaming for the Safety of LLM-Based Search Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Search agents connect LLMs to the Internet, enabling access to broader and more up-to-date information. However, unreliable search results may also pose safety threats to end users, establishing a new threat surface. In this work, we conduct two in-the-wild experiments to demonstrate both the prevalence of low-quality search results and their potential to misguide agent behaviors. To counter this threat, we introduce an automated red-teaming framework that is systematic, scalable, and cost-efficient, enabling lightweight and harmless safety assessments of search agents. Building on this framework, we construct the SafeSearch benchmark, which includes 300 test cases covering five categories of risks (e.g., misinformation and indirect prompt injection). Using this benchmark, we evaluate three representative search agent scaffolds, covering search workflow, tool-calling, and deep research, across 7 proprietary and 8 open-source backend LLMs. Our results reveal substantial vulnerabilities of LLM-based search agents: when exposed to unreliable websites, the highest ASR reached 90.5% for GPT-4.1-mini under a search workflow setting. Moreover, our analysis highlights the limited effectiveness of common defense practices, such as reminder prompting. This emphasizes the value of our framework in promoting transparency for safer agent development. Our codebase and test cases are publicly available: https://github.com/jianshuod/SafeSearch.<br>
<span id='abs_ch'>中文摘要：搜索代理使大语言模型能获取网络实时信息，但也 不可 搜索结果带来安全威胁；本 究通过自动化红队评估框架和SafeSearch基准测试，揭示了现有系统的显著漏洞及常见防御措施的有限效果。</span><br>
<span id='abs_en'>English Summary: Search agents enable LLMs to access current web information but introduce safety risks from unreliable results, prompting the development of an automated red-teaming framework and SafeSearch benchmark that reveal significant vulnerabilities in existing systems and limited effectiveness of common defenses.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2509.23666.pdf' target='_blank'>https://arxiv.org/pdf/2509.23666.pdf</a></span>   <span><a href='https://github.com/Div290/UAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Divya Jyoti Bajpai, Manjesh Kumar Hanawal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23666">Beyond Greedy Exits: Improved Early Exit Decisions for Risk Control and Reliability</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Early-Exit Deep Neural Networks enable adaptive inference by allowing prediction at intermediary layers, significantly reducing computational costs and latency. Most of the early exit strategies greedily exit a sample at an intermediary layer if the confidence in class prediction exceeds a predefined threshold that is set using a static validation set. This is problematic as the model might be overconfident in a wrong class. Also, they are not robust to distribution shifts encountered in deployment, which can undermine model trustworthiness and accuracy. To address these challenges, we propose UAT that adapts the threshold for exit decisions using a Multi-Armed Bandit framework, enabling online, unsupervised adjustment of exit decisions. UAT makes decisions based on a new reward function that assesses predictive certainty and its reliability to balance computational efficiency and prediction quality while penalizing unnecessary late exits. We provide guarantees on risk achieved by UAT and validate its performance on diverse tasks spanning vision-language understanding, text generation, and classification. Our framework demonstrates consistent improvements in speedup (1.70-2.10x) with a minimal performance drop (<2%) as compared to full model performance. Our source code is available at https://github.com/Div290/UAT.<br>
<span id='abs_ch'>中文摘要：提出的UAT框架通过多臂老虎机方法自适应调整退出阈值，解决了早期退出深度神经网络中的过度自信和分布偏移问题，在实现显著 速（1.70-2.10倍）的同时保持性能损失最小（<2%）。</span><br>
<span id='abs_en'>English Summary: The proposed UAT framework adaptively adjusts exit thresholds using a Multi-Armed Bandit approach to address overconfidence and distribution shift issues in Early-Exit DNNs, achieving significant speedup (1.70-2.10x) with minimal performance loss (<2%).</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2509.23665.pdf' target='_blank'>https://arxiv.org/pdf/2509.23665.pdf</a></span>   <span><a href='https://github.com/Ajwebdevs/calibration-analysis-experiments' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kristina P. Sinaga, Arjun S. Nair
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23665">Calibration Meets Reality: Making Machine Learning Predictions Trustworthy</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Post-hoc calibration methods are widely used to improve the reliability of probabilistic predictions from machine learning models. Despite their prevalence, a comprehensive theoretical understanding of these methods remains elusive, particularly regarding their performance across different datasets and model architectures. Input features play a crucial role in shaping model predictions and, consequently, their calibration. However, the interplay between feature quality and calibration performance has not been thoroughly investigated. In this work, we present a rigorous theoretical analysis of post-hoc calibration methods, focusing on Platt scaling and isotonic regression. We derive convergence guarantees, computational complexity bounds, and finite-sample performance metrics for these methods. Furthermore, we explore the impact of feature informativeness on calibration performance through controlled synthetic experiments. Our empirical evaluation spans a diverse set of real-world datasets and model architectures, demonstrating consistent improvements in calibration metrics across various scenarios. By examining calibration performance under varying feature conditions utilizing only informative features versus complete feature spaces including noise dimensions, we provide fundamental insights into the robustness and reliability of different calibration approaches. Our findings offer practical guidelines for selecting appropriate calibration methods based on dataset characteristics and computational constraints, bridging the gap between theoretical understanding and practical implementation in uncertainty quantification. Code and experimental data are available at: https://github.com/Ajwebdevs/calibration-analysis-experiments.<br>
<span id='abs_ch'>中文摘要：本 究对后验 准方法进行了系统的理论与实证分析，揭示了特征质量对 准性能的影响机制，并基于数据集特性提出了实用的 准方法选择指南。</span><br>
<span id='abs_en'>English Summary: This study provides a comprehensive theoretical and empirical analysis of post-hoc calibration methods, revealing how feature quality impacts calibration performance and offering practical guidelines for method selection based on dataset characteristics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2509.23662.pdf' target='_blank'>https://arxiv.org/pdf/2509.23662.pdf</a></span>   <span><a href='https://github.com/flzeng1/PNS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanlong Zeng, Wensheng Gan, Jiayang Wu, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23662">Pure Node Selection for Imbalanced Graph Node Classification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The problem of class imbalance refers to an uneven distribution of quantity among classes in a dataset, where some classes are significantly underrepresented compared to others. Class imbalance is also prevalent in graph-structured data. Graph neural networks (GNNs) are typically based on the assumption of class balance, often overlooking the issue of class imbalance. In our investigation, we identified a problem, which we term the Randomness Anomalous Connectivity Problem (RACP), where certain off-the-shelf models are affected by random seeds, leading to a significant performance degradation. To eliminate the influence of random factors in algorithms, we proposed PNS (Pure Node Sampling) to address the RACP in the node synthesis stage. Unlike existing approaches that design specialized algorithms to handle either quantity imbalance or topological imbalance, PNS is a novel plug-and-play module that operates directly during node synthesis to mitigate RACP. Moreover, PNS also alleviates performance degradation caused by abnormal distribution of node neighbors. We conduct a series of experiments to identify what factors are influenced by random seeds. Experimental results demonstrate the effectiveness and stability of our method, which not only eliminates the effect of unfavorable random seeds but also outperforms the baseline across various benchmark datasets with different GNN backbones. Data and code are available at https://github.com/flzeng1/PNS.<br>
<span id='abs_ch'>Chinese: 本 究提出了纯节点采 （PNS）模块，作为一种即插即用的解决方案，在节点合成阶段有效应对图数据中的随机性异常连接问题，消除了随机种子导致的性能下降。English: The study introduces Pure Node Sampling (PNS), a plug-and-play module that addresses the Randomness Anomalous Connectivity Problem in class-imbalanced graph data by mitigating performance degradation caused by random seeds during node synthesis.Paperid:206,https://arxiv.org/pdf/2509.23639.pdfGitHub</span><br>
<span id='abs_en'>English: The study introduces Pure Node Sampling (PNS), a plug-and-play module that addresses the Randomness Anomalous Connectivity Problem in class-imbalanced graph data by mitigating performance degradation caused by random seeds during node synthesis.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2509.23639.pdf' target='_blank'>https://arxiv.org/pdf/2509.23639.pdf</a></span>   <span><a href='https://github.com/boyuh/LightFair' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyu Han, Qianqian Xu, Shilong Bao, Zhiyong Yang, Kangli Zi, Qingming Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23639">LightFair: Towards an Efficient Alternative for Fair T2I Diffusion via Debiasing Pre-trained Text Encoders</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper explores a novel lightweight approach LightFair to achieve fair text-to-image diffusion models (T2I DMs) by addressing the adverse effects of the text encoder. Most existing methods either couple different parts of the diffusion model for full-parameter training or rely on auxiliary networks for correction. They incur heavy training or sampling burden and unsatisfactory performance. Since T2I DMs consist of multiple components, with the text encoder being the most fine-tunable and front-end module, this paper focuses on mitigating bias by fine-tuning text embeddings. To validate feasibility, we observe that the text encoder's neutral embedding output shows substantial skewness across image embeddings of various attributes in the CLIP space. More importantly, the noise prediction network further amplifies this imbalance. To finetune the text embedding, we propose a collaborative distance-constrained debiasing strategy that balances embedding distances to improve fairness without auxiliary references. However, mitigating bias can compromise the original generation quality. To address this, we introduce a two-stage text-guided sampling strategy to limit when the debiased text encoder intervenes. Extensive experiments demonstrate that LightFair is effective and efficient. Notably, on Stable Diffusion v1.5, our method achieves SOTA debiasing at just $1/4$ of the training burden, with virtually no increase in sampling burden. The code is available at https://github.com/boyuh/LightFair.<br>
<span id='abs_ch'>中文摘要：本文提出LightFair轻量方法，通过距离约束去偏策略微调文本嵌入并结合两阶段采 ，有效提升文本到图像扩散模型的公平性，以仅四分之一训练负担实现最优去偏效果且 乎不增 采 成本。</span><br>
<span id='abs_en'>English Summary: This paper introduces LightFair, a lightweight method that enhances fairness in text-to-image diffusion models by fine-tuning text embeddings with a distance-constrained debiasing strategy and a two-stage sampling approach, achieving state-of-the-art performance with significantly reduced training and sampling overhead.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2509.23617.pdf' target='_blank'>https://arxiv.org/pdf/2509.23617.pdf</a></span>   <span><a href='https://github.com/VikiXie/SatMar8' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Huang, Weizheng Xie, Fan Gao, Yutong Liu, Ruoling Wu, Zeyu Han, Jingxi Qiu, Xiangxiang Wang, Zhenglin Yang, Hao Wang, Yongbin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23617">BioVessel-Net and RetinaMix: Unsupervised Retinal Vessel Segmentation from OCTA Images</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Structural changes in retinal blood vessels are critical biomarkers for the onset and progression of glaucoma and other ocular diseases. However, current vessel segmentation approaches largely rely on supervised learning and extensive manual annotations, which are costly, error-prone, and difficult to obtain in optical coherence tomography angiography. Here we present BioVessel-Net, an unsupervised generative framework that integrates vessel biostatistics with adversarial refinement and a radius-guided segmentation strategy. Unlike pixel-based methods, BioVessel-Net directly models vascular structures with biostatistical coherence, achieving accurate and explainable vessel extraction without labeled data or high-performance computing. To support training and evaluation, we introduce RetinaMix, a new benchmark dataset of 2D and 3D OCTA images with high-resolution vessel details from diverse populations. Experimental results demonstrate that BioVessel-Net achieves near-perfect segmentation accuracy across RetinaMix and existing datasets, substantially outperforming state-of-the-art supervised and semi-supervised methods. Together, BioVessel-Net and RetinaMix provide a label-free, computationally efficient, and clinically interpretable solution for retinal vessel analysis, with broad potential for glaucoma monitoring, blood flow modeling, and progression prediction. Code and dataset are available: https://github.com/VikiXie/SatMar8.<br>
<span id='abs_ch'>中文摘要：BioVessel-Net是一个 监督生成框架，通过整合血管生物统计学与对抗性优化， 需人工 注即可实现精准的视网膜血管分割，其性能超越现有监督方法且具备临床可解释性。</span><br>
<span id='abs_en'>English Summary: BioVessel-Net is an unsupervised generative framework that integrates vessel biostatistics with adversarial refinement to achieve accurate retinal vessel segmentation without manual annotations, outperforming existing supervised methods while providing clinical interpretability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2509.23616.pdf' target='_blank'>https://arxiv.org/pdf/2509.23616.pdf</a></span>   <span><a href='https://github.com/flzeng1/GraphIFE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanlong Zeng, Wensheng Gan, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23616">GraphIFE: Rethinking Graph Imbalance Node Classification via Invariant Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The class imbalance problem refers to the disproportionate distribution of samples across different classes within a dataset, where the minority classes are significantly underrepresented. This issue is also prevalent in graph-structured data. Most graph neural networks (GNNs) implicitly assume a balanced class distribution and therefore often fail to account for the challenges introduced by class imbalance, which can lead to biased learning and degraded performance on minority classes. We identify a quality inconsistency problem in synthesized nodes, which leads to suboptimal performance under graph imbalance conditions. To mitigate this issue, we propose GraphIFE (Graph Invariant Feature Extraction), a novel framework designed to mitigate quality inconsistency in synthesized nodes. Our approach incorporates two key concepts from graph invariant learning and introduces strategies to strengthen the embedding space representation, thereby enhancing the model's ability to identify invariant features. Extensive experiments demonstrate the framework's efficiency and robust generalization, as GraphIFE consistently outperforms various baselines across multiple datasets. The code is publicly available at https://github.com/flzeng1/GraphIFE.<br>
<span id='abs_ch'>Chinese Summary: 本文提出GraphIFE框架，通过图不变特征学 和增强嵌入表示来解决图数据中类别不平衡问题，有效缓解合成节点质量不一致性并提升模型性能。</span><br>
<span id='abs_en'>English Summary: The paper introduces GraphIFE, a novel framework that addresses class imbalance in graph data by mitigating quality inconsistency in synthesized nodes through invariant feature extraction and enhanced embedding strategies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2509.23574.pdf' target='_blank'>https://arxiv.org/pdf/2509.23574.pdf</a></span>   <span><a href='https://github.com/Leon221220/MoRSD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianzhi Yan, Le Liu, Youcheng Pan, Shiwei Chen, Yang Xiang, Buzhou Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23574">Towards Efficient CoT Distillation: Self-Guided Rationale Selector for Better Performance with Fewer Rationales</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-thought (CoT) distillation aims to enhance small language models' (SLMs) reasoning by transferring multi-step reasoning capability from the larger teacher models. However, existing work underestimates rationale quality, focusing primarily on data quantity, which may transfer noisy or incorrect information to the student model. To address the above issues, we proposed \textbf{M}odel-\textbf{O}riented \textbf{R}ationale \textbf{S}election \textbf{D}istillation (MoRSD), which can discern and select high quality rationales for distillation to improve performance further. We further propose a Rationale Difficulty (RD) metric to measure the ability of the student model to generate the correct answer under a given rationale. Compared to the baseline, we achieved 4.6$\%$ average improvement on seven datasets over three tasks, using fewer rationales by controlling their accuracy, diversity, and difficulty. Our results reveal that a small portion of the high quality rationales can enhance the reasoning ability of student models than the entire dataset. Our method promises to be a possible solution for efficient CoT distillation. Our code will be released in https://github.com/Leon221220/MoRSD.<br>
<span id='abs_ch'>中文：MoRSD通过基于准确性、多 性和难度选择高质量推理链，显著提升了思维链蒸馏的效果，仅用少量训练 本就实现了性能的大幅提升。</span><br>
<span id='abs_en'>English: MoRSD enhances chain-of-thought distillation by selecting high-quality rationales based on accuracy, diversity, and difficulty, achieving significant performance improvements with fewer training examples.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2509.23564.pdf' target='_blank'>https://arxiv.org/pdf/2509.23564.pdf</a></span>   <span><a href='https://github.com/deeplearning-wisc/PrefCleanBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Min-Hsuan Yeh, Yixuan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23564">Clean First, Align Later: Benchmarking Preference Data Cleaning for Reliable LLM Alignment</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Human feedback plays a pivotal role in aligning large language models (LLMs) with human preferences. However, such feedback is often noisy or inconsistent, which can degrade the quality of reward models and hinder alignment. While various automated data cleaning methods have been proposed to mitigate this issue, a systematic evaluation of their effectiveness and generalizability remains lacking. To bridge this gap, we introduce the first comprehensive benchmark for evaluating 13 preference data cleaning methods in the context of LLM alignment. PrefCleanBench offers a standardized protocol to assess cleaning strategies in terms of alignment performance and generalizability across diverse datasets, model architectures, and optimization algorithms. By unifying disparate methods and rigorously comparing them, we uncover key factors that determine the success of data cleaning in alignment tasks. This benchmark lays the groundwork for principled and reproducible approaches to improving LLM alignment through better data quality-highlighting the crucial but underexplored role of data preprocessing in responsible AI development. We release modular implementations of all methods to catalyze further research: https://github.com/deeplearning-wisc/PrefCleanBench.<br>
<span id='abs_ch'>中文: 本文提出了首个系统性评估13种偏好数据清洗方法的大模型对齐基准PrefCleanBench，揭示了数据清洗成功的关键  ，并强调了数据预处理在负责任AI开发中的重要作用。</span><br>
<span id='abs_en'>English: This paper introduces PrefCleanBench, the first comprehensive benchmark to systematically evaluate 13 preference data cleaning methods for improving large language model alignment, revealing key factors for success and emphasizing data preprocessing's critical role in responsible AI development.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2509.23501.pdf' target='_blank'>https://arxiv.org/pdf/2509.23501.pdf</a></span>   <span><a href='https://github.com/hrouzegar/Role_Based-In-Context-Learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hamidreza Rouzegar, Masoud Makrehchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23501">The Impact of Role Design in In-Context Learning for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In-context learning (ICL) enables Large Language Models (LLMs) to generate predictions based on prompts without additional fine-tuning. While prompt engineering has been widely studied, the impact of role design within prompts remains underexplored. This study examines the influence of role configurations in zero-shot and few-shot learning scenarios using GPT-3.5 and GPT-4o from OpenAI and Llama2-7b and Llama2-13b from Meta. We evaluate the models' performance across datasets, focusing on tasks like sentiment analysis, text classification, question answering, and math reasoning. Our findings suggest the potential of role-based prompt structuring to enhance LLM performance.<br>
<span id='abs_ch'>中文: 本 究探讨了提示中角色配置对大型语言模型在零 本和少 本学 中表现的影响，发现基于角色的结构设计能够提升模型在多种任务中的性能。</span><br>
<span id='abs_en'>English: This study explores how role configurations in prompts affect the performance of large language models in zero-shot and few-shot learning, revealing that role-based structuring can enhance their effectiveness across various tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2509.23494.pdf' target='_blank'>https://arxiv.org/pdf/2509.23494.pdf</a></span>   <span><a href='https://github.com/Muyiiiii/CRIB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Yang, Yifan Hu, Kexin Zhang, Luyang Niu, Yushun Dong, Philip S. Yu, Kaize Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23494">Revisiting Multivariate Time Series Forecasting with Missing Values</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Missing values are common in real-world time series, and multivariate time series forecasting with missing values (MTSF-M) has become a crucial area of research for ensuring reliable predictions. To address the challenge of missing data, current approaches have developed an imputation-then-prediction framework that uses imputation modules to fill in missing values, followed by forecasting on the imputed data. However, this framework overlooks a critical issue: there is no ground truth for the missing values, making the imputation process susceptible to errors that can degrade prediction accuracy. In this paper, we conduct a systematic empirical study and reveal that imputation without direct supervision can corrupt the underlying data distribution and actively degrade prediction accuracy. To address this, we propose a paradigm shift that moves away from imputation and directly predicts from the partially observed time series. We introduce Consistency-Regularized Information Bottleneck (CRIB), a novel framework built on the Information Bottleneck principle. CRIB combines a unified-variate attention mechanism with a consistency regularization scheme to learn robust representations that filter out noise introduced by missing values while preserving essential predictive signals. Comprehensive experiments on four real-world datasets demonstrate the effectiveness of CRIB, which predicts accurately even under high missing rates. Our code is available in https://github.com/Muyiiiii/CRIB.<br>
<span id='abs_ch'>中文摘要：本文提出CRIB框架， 需填补缺失值即可直接从不完整时间序列进行预测，避免了填补误差导致的精度下降，实验证明其在高缺失率下仍能保持准确预测。</span><br>
<span id='abs_en'>English Summary: The paper introduces the CRIB framework, which directly forecasts from incomplete time series without imputation to prevent accuracy degradation caused by imputation errors, demonstrating superior performance even with high missing data rates.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2509.23472.pdf' target='_blank'>https://arxiv.org/pdf/2509.23472.pdf</a></span>   <span><a href='https://github.com/shijxcs/meft' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiang-Xin Shi, Wen-Da Wei, Jin-Fei Qi, Xuanyu Chen, Tong Wei, Yu-Feng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23472">Memory-Efficient Fine-Tuning via Low-Rank Activation Compression</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The parameter-efficient fine-tuning paradigm has garnered significant attention with the advancement of foundation models. Although numerous methods have been proposed to reduce the number of trainable parameters, their substantial memory overhead remains a critical bottleneck that hinders practical deployment. In this paper, we observe that model activations constitute a major source of memory consumption, especially under large batch sizes and long context lengths; however, the rank of the activations remains consistently low. Motivated by this insight, we propose a memory-efficient fine-tuning approach Low-Rank Activation Compression (LoRAct). Unlike prior work, LoRAct provides a more flexible and versatile compressing strategy that can be applied online during the forward pass without the need for any calibration data. Moreover, LoRAct incorporates a novel sampling-based orthogonal decomposition algorithm specifically designed for low-rank matrices, offering improved computational efficiency and a tighter error bound compared to the widely used RSVD. Experiments on both vision and language tasks demonstrate the effectiveness of LoRAct. Notably, LoRAct further reduces activation memory by approximately 80% in comparison with the widely adopted LoRA method, while maintaining competitive performance. The source code is available at https://github.com/shijxcs/meft.<br>
<span id='abs_ch'>中文: 本文提出LoRAct这一内存高效微调方法， 需 准数据即可在线压缩低秩激活，相比LoRA能减少约80%的激活内存 用，同时保持性能竞争力。</span><br>
<span id='abs_en'>English: The paper introduces LoRAct, a memory-efficient fine-tuning method that compresses low-rank activations online without calibration data, reducing activation memory by about 80% compared to LoRA while maintaining performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2509.23417.pdf' target='_blank'>https://arxiv.org/pdf/2509.23417.pdf</a></span>   <span><a href='https://github.com/Rajjaa/disambiguated-LLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rajaa El Hamdani, Samy Haffoudhi, Nils Holzenberger, Fabian Suchanek, Thomas Bonald, Fragkiskos D. Malliaros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23417">Retrieval-Constrained Decoding Reveals Underestimated Parametric Knowledge in Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Language models (LMs) encode substantial factual knowledge, but often produce answers judged as incorrect. We hypothesize that many of these answers are actually correct, but are expressed in alternative surface forms that are dismissed due to an overly strict evaluation, leading to an underestimation of models' parametric knowledge. We propose Retrieval-Constrained Decoding (RCD), a decoding strategy that restricts model outputs to unique surface forms. We introduce YAGO-QA, a dataset of 19,137 general knowledge questions. Evaluating open-source LMs from 135M to 70B parameters, we show that standard decoding undervalues their knowledge. For instance, Llama-3.1-70B scores only 32.3% F1 with vanilla decoding but 46.0% with RCD. Similarly, Llama-3.1-8B reaches 33.0% with RCD, outperforming the larger model under vanilla decoding. We publicly share the code and dataset at https://github.com/Rajjaa/disambiguated-LLM.<br>
<span id='abs_ch'>Chinese: 语言模型常 严 评估而被忽视其替代形式的正确答案，但采用检索约束解 策略可显著提升性能，这在YAGO-QA数据集上得到了验证。</span><br>
<span id='abs_en'>English: Language models often produce correct answers in alternative forms that are dismissed by strict evaluations, but using Retrieval-Constrained Decoding significantly improves their performance, as demonstrated on the YAGO-QA dataset.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2509.23373.pdf' target='_blank'>https://arxiv.org/pdf/2509.23373.pdf</a></span>   <span><a href='https://github.com/Darcyddx/graph-prompt' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Ding, Lei Wang, Piotr Koniusz, Yongsheng Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23373">Graph Your Own Prompt</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We propose Graph Consistency Regularization (GCR), a novel framework that injects relational graph structures, derived from model predictions, into the learning process to promote class-aware, semantically meaningful feature representations. Functioning as a form of self-prompting, GCR enables the model to refine its internal structure using its own outputs. While deep networks learn rich representations, these often capture noisy inter-class similarities that contradict the model's predicted semantics. GCR addresses this issue by introducing parameter-free Graph Consistency Layers (GCLs) at arbitrary depths. Each GCL builds a batch-level feature similarity graph and aligns it with a global, class-aware masked prediction graph, derived by modulating softmax prediction similarities with intra-class indicators. This alignment enforces that feature-level relationships reflect class-consistent prediction behavior, acting as a semantic regularizer throughout the network. Unlike prior work, GCR introduces a multi-layer, cross-space graph alignment mechanism with adaptive weighting, where layer importance is learned from graph discrepancy magnitudes. This allows the model to prioritize semantically reliable layers and suppress noisy ones, enhancing feature quality without modifying the architecture or training procedure. GCR is model-agnostic, lightweight, and improves semantic structure across various networks and datasets. Experiments show that GCR promotes cleaner feature structure, stronger intra-class cohesion, and improved generalization, offering a new perspective on learning from prediction structure. [Project website](https://darcyddx.github.io/gcr/) [Code](https://github.com/Darcyddx/graph-prompt)<br>
<span id='abs_ch'>中文摘要：图一致性正则化（GCR）是一种创新框架，通过将特征相似性图与类别感知预测图在多网络层中对齐， 需改变架构即可提升语义结构和泛化能力。</span><br>
<span id='abs_en'>English Summary: Graph Consistency Regularization (GCR) is a novel framework that enhances feature learning by aligning feature similarity graphs with class-aware prediction graphs across network layers, improving semantic structure and generalization without architectural changes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2509.23338.pdf' target='_blank'>https://arxiv.org/pdf/2509.23338.pdf</a></span>   <span><a href='https://github.com/weAIDB/PARROT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Zhou, Guoliang Li, Haoyu Wang, Yuxing Han, Xufei Wu, Fan Wu, Xuanhe Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23338">PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMS) have shown increasing effectiveness in Text-to-SQL tasks. However, another closely related problem, Cross-System SQL Translation (a.k.a., SQL-to-SQL), which adapts a query written for one database system (e.g., MySQL) into its equivalent one for another system (e.g., ClickHouse), is of great practical importance but remains underexplored. Existing SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which (1) focus on a limited set of database systems (often just SQLite) and (2) cannot capture many system-specific SQL dialects (e.g., customized functions, data types, and syntax rules). Thus, in this paper, we introduce PARROT, a Practical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT comprises 598 translation pairs from 38 open-source benchmarks and real-world business services, specifically prepared to challenge system-specific SQL understanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We also provide multiple benchmark variants, including PARROT-Diverse with 28,003 translations (for extensive syntax testing) and PARROT-Simple with 5,306 representative samples (for focused stress testing), covering 22 production-grade database systems. To promote future research, we release a public leaderboard and source code at: https://code4db.github.io/parrot-bench/.<br>
<span id='abs_ch'>中文: 大语言模型在文本转SQL任务中日益有效，但跨系统SQL翻译这一实际问题仍待探索， 此我们推出PARROT基准测试，包含多 化翻译对以评估系统特定的SQL理解能力。</span><br>
<span id='abs_en'>English: Large language models are increasingly effective for Text-to-SQL tasks, but the practical problem of cross-system SQL translation remains underexplored, prompting the introduction of PARROT, a comprehensive benchmark with diverse translation pairs to evaluate system-specific SQL understanding.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2509.23328.pdf' target='_blank'>https://arxiv.org/pdf/2509.23328.pdf</a></span>   <span><a href='https://github.com/AndrejOrsula/space_robotics_bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrej Orsula, Matthieu Geist, Miguel Olivares-Mendez, Carol Martinez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23328">Space Robotics Bench: Robot Learning Beyond Earth</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The growing ambition for space exploration demands robust autonomous systems that can operate in unstructured environments under extreme extraterrestrial conditions. The adoption of robot learning in this domain is severely hindered by the prohibitive cost of technology demonstrations and the limited availability of data. To bridge this gap, we introduce the Space Robotics Bench, an open-source simulation framework for robot learning in space. It offers a modular architecture that integrates on-demand procedural generation with massively parallel simulation environments to support the creation of vast and diverse training distributions for learning-based agents. To ground research and enable direct comparison, the framework includes a comprehensive suite of benchmark tasks that span a wide range of mission-relevant scenarios. We establish performance baselines using standard reinforcement learning algorithms and present a series of experimental case studies that investigate key challenges in generalization, end-to-end learning, adaptive control, and sim-to-real transfer. Our results reveal insights into the limitations of current methods and demonstrate the utility of the framework in producing policies capable of real-world operation. These contributions establish the Space Robotics Bench as a valuable resource for developing, benchmarking, and deploying the robust autonomous systems required for the final frontier.<br>
<br>
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2509.23244.pdf' target='_blank'>https://arxiv.org/pdf/2509.23244.pdf</a></span>   <span><a href='https://github.com/MatanShamir1/gr_libs' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/MatanShamir1/gr_envs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shamir Matan, Elhadad Osher, Nageris Ben, Mirsky Reuth
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23244">Online Dynamic Goal Recognition in Gym Environments</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Goal Recognition (GR) is the task of inferring an agent's intended goal from partial observations of its behavior, typically in an online and one-shot setting. Despite recent advances in model-free GR, particularly in applications such as human-robot interaction, surveillance, and assistive systems, the field remains fragmented due to inconsistencies in benchmarks, domains, and evaluation protocols. To address this, we introduce gr-libs (https://github.com/MatanShamir1/gr_libs) and gr-envs (https://github.com/MatanShamir1/gr_envs), two complementary open-source frameworks that support the development, evaluation, and comparison of GR algorithms in Gym-compatible environments. gr-libs includes modular implementations of MDP-based GR baselines, diagnostic tools, and evaluation utilities. gr-envs provides a curated suite of environments adapted for dynamic and goal-directed behavior, along with wrappers that ensure compatibility with standard reinforcement learning toolkits. Together, these libraries offer a standardized, extensible, and reproducible platform for advancing GR research. Both packages are open-source and available on GitHub and PyPI.<br>
<span id='abs_ch'>中文: 作者推出了两个开源框架gr-libs和gr-envs，通过提供模块化工具和兼容环境来 准化目 识别 究，支持算法的开发与评估。</span><br>
<span id='abs_en'>English: The authors introduce two open-source frameworks, gr-libs and gr-envs, to standardize Goal Recognition research by providing modular tools and compatible environments for developing and evaluating algorithms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2509.23232.pdf' target='_blank'>https://arxiv.org/pdf/2509.23232.pdf</a></span>   <span><a href='https://github.com/ShopeeLLM/Spec-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingshuai Liu, Ante Wang, Zijun Min, Liang Yao, Haibo Zhang, Yang Liu, Anxiang Zeng, Jinsong Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23232">SPEC-RL: Accelerating On-Policy Reinforcement Learning via Speculative Rollouts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) increasingly rely on reinforcement learning with verifiable rewards (RLVR) to elicit reliable chain-of-thought reasoning. However, the training process remains bottlenecked by the computationally expensive rollout stage. Existing acceleration methods-such as parallelization, objective- and data-driven modifications, and replay buffers-either incur diminishing returns, introduce bias, or overlook redundancy across iterations. We identify that rollouts from consecutive training epochs frequently share a large portion of overlapping segments, wasting computation. To address this, we propose SPEC-RL, a novel framework that integrates SPECulative decoding with the RL rollout process. SPEC-RL reuses prior trajectory segments as speculative prefixes and extends them via a draft-and-verify mechanism, avoiding redundant generation while ensuring policy consistency. Experiments on diverse math reasoning and generalization benchmarks, including GSM8K, MATH-500, OlympiadBench, MMLU-STEM, and others, demonstrate that SPEC-RL reduces rollout time by 2-3x without compromising policy quality. As a purely rollout-stage enhancement, SPEC-RL integrates seamlessly with mainstream algorithms (e.g., PPO, GRPO, DAPO), offering a general and practical path to scale RLVR for large reasoning models. Our code is available at https://github.com/ShopeeLLM/Spec-RL<br>
<span id='abs_ch'>中文: SPEC-RL框架通过将推测式解 与强化学 过程结合，重用先前训练轮次中的重 轨迹片段，在数学推理和泛化基准测试中实现2-3倍的训练 速，且不降低策略质量。</span><br>
<span id='abs_en'>English: SPEC-RL is a novel framework that accelerates reinforcement learning with verifiable rewards by reusing overlapping trajectory segments from prior epochs through speculative decoding, reducing rollout time by 2-3x without sacrificing policy quality across various reasoning benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2509.23209.pdf' target='_blank'>https://arxiv.org/pdf/2509.23209.pdf</a></span>   <span><a href='https://github.com/Bluixe/towards_monotonic_improvement' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhao Zhang, Shao Zhang, Xihuai Wang, Yang Li, Ying Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23209">Towards Monotonic Improvement in In-Context Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In-Context Reinforcement Learning (ICRL) has emerged as a promising paradigm for developing agents that can rapidly adapt to new tasks by leveraging past experiences as context, without updating their parameters. Recent approaches train large sequence models on monotonic policy improvement data from online RL, aiming to a continue improved testing time performance. However, our experimental analysis reveals a critical flaw: these models cannot show a continue improvement like the training data during testing time. Theoretically, we identify this phenomenon as Contextual Ambiguity, where the model's own stochastic actions can generate an interaction history that misleadingly resembles that of a sub-optimal policy from the training data, initiating a vicious cycle of poor action selection. To resolve the Contextual Ambiguity, we introduce Context Value into training phase and propose Context Value Informed ICRL (CV-ICRL). CV-ICRL use Context Value as an explicit signal representing the ideal performance theoretically achievable by a policy given the current context. As the context expands, Context Value could include more task-relevant information, and therefore the ideal performance should be non-decreasing. We prove that the Context Value tightens the lower bound on the performance gap relative to an ideal, monotonically improving policy. We fruther propose two methods for estimating Context Value at both training and testing time. Experiments conducted on the Dark Room and Minigrid testbeds demonstrate that CV-ICRL effectively mitigates performance degradation and improves overall ICRL abilities across various tasks and environments. The source code and data of this paper are available at https://github.com/Bluixe/towards_monotonic_improvement .<br>
<span id='abs_ch'>中文摘要：情境强化学 存在情境模糊性问题，导致模型在测试时 法持续改进，而提出的CV-ICRL方法通过引入情境值来收紧性能界限，在多个测试环境中有效提升了模型表现。</span><br>
<span id='abs_en'>English Summary: In-Context Reinforcement Learning suffers from Contextual Ambiguity where models fail to maintain continuous improvement during testing, which the proposed CV-ICRL method resolves by incorporating Context Value to tighten performance bounds and demonstrate effectiveness across multiple environments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2509.23129.pdf' target='_blank'>https://arxiv.org/pdf/2509.23129.pdf</a></span>   <span><a href='https://github.com/HaotianLiu123/CCGSPG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haotian Liu, Shuo Wang, Hongteng Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23129">C$^2$GSPG: Confidence-calibrated Group Sequence Policy Gradient towards Self-aware Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning (RL) methods, exemplified by Group Relative Policy Optimization (GRPO) and its variants, play a central role in developing reasoning models. However, these methods often suffer from a critical overconfidence issue, which prevents them from achieving self-aware reasoning models. In this study, we propose a simple yet effective confidence-calibration group sequence policy gradient method, called C$^2$GSPG, which simultaneously enhances reasoning performance while suppressing overconfidence. In principle, we propose a Group Sequence Policy Gradient (GSPG) framework for learning reasoning models, which eliminates the token-level bias commonly appearing in GRPO and its variants. In this framework, we define the model confidence for each reasoning problem using the normalized sequence-level probability, and then apply a cross-entropy regularizer to calibrate the model confidence to the sequence's reward. We demonstrate that the confidence calibration regularizer and GSPG are collaborative for binary rewards, as their objectives always share the same gradient direction. For non-binary rewards, we apply nonlinear reward normalization and adaptive regularizer clipping, mitigating the potential conflict between the two objectives. Applying C$^2$GSPG to post-train large language models in logical and mathematical reasoning tasks, we show its superiority over state-of-the-art methods in both reasoning accuracy and confidence calibration. The code of C$^2$GSPG is available at https://github.com/HaotianLiu123/CCGSPG.<br>
<span id='abs_ch'>Chinese: 本 究提出C²GSPG方法，通过置信度 准的组序列策略梯度，在增强推理性能的同时抑制强化学 模型的过度自信问题，在逻辑和数学推理任务中展现出优于现有方法的准确性与 准能力。</span><br>
<span id='abs_en'>English: This study introduces C²GSPG, a confidence-calibration group sequence policy gradient method that enhances reasoning performance and mitigates overconfidence in reinforcement learning models, demonstrating superior accuracy and calibration in logical and mathematical tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2509.23115.pdf' target='_blank'>https://arxiv.org/pdf/2509.23115.pdf</a></span>   <span><a href='https://github.com/he-h/rhythm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu He, Haozheng Luo, Yan Chen, Qi R. Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23115">RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Predicting human mobility is inherently challenging due to complex long-range dependencies and multi-scale periodic behaviors. To address this, we introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility), a unified framework that leverages large language models (LLMs) as general-purpose spatio-temporal predictors and trajectory reasoners. Methodologically, RHYTHM employs temporal tokenization to partition each trajectory into daily segments and encode them as discrete tokens with hierarchical attention that captures both daily and weekly dependencies, thereby significantly reducing the sequence length while preserving cyclical information. Additionally, we enrich token representations by adding pre-computed prompt embeddings for trajectory segments and prediction targets via a frozen LLM, and feeding these combined embeddings back into the LLM backbone to capture complex interdependencies. Computationally, RHYTHM freezes the pretrained LLM's backbone to reduce attention complexity and memory cost. We evaluate our model against state-of-the-art methods using three real-world datasets. Notably, RHYTHM achieves a 2.4% improvement in overall accuracy, a 5.0% increase on weekends, and a 24.6% reduction in training time. Code is publicly available at https://github.com/he-h/rhythm.<br>
<span id='abs_ch'>中文：RHYTHM是一种创新框架，利用大型语言模型通过分层注意力对轨迹进行 记化来预测人类移动，实现了更高的准确性和更快的训练速度。</span><br>
<span id='abs_en'>English: RHYTHM is a novel framework that uses large language models to predict human mobility by tokenizing trajectories with hierarchical attention, achieving higher accuracy and faster training times.Paperid:239,https://arxiv.org/pdf/2509.23102.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2509.23102.pdf' target='_blank'>https://arxiv.org/pdf/2509.23102.pdf</a></span>   <span><a href='https://github.com/smiles724/MNPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fang Wu, Xu Huang, Weihao Xuan, Zhiwei Zhang, Yijia Xiao, Guancheng Wan, Xiaomin Li, Bing Hu, Peng Xia, Jure Leskovec, Yejin Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23102">Multiplayer Nash Preference Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning from human feedback (RLHF) has emerged as the standard paradigm for aligning large language models (LLMs) with human preferences. However, reward-based methods built on the Bradley-Terry assumption struggle to capture the non-transitive and heterogeneous nature of real-world preferences. To address this, recent studies have reframed alignment as a two-player Nash game, giving rise to Nash learning from human feedback (NLHF). While this perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong theoretical and empirical guarantees, they remain fundamentally restricted to two-player interactions, creating a single-opponent bias that fails to capture the full complexity of realistic preference structures. In this work, we introduce Multiplayer Nash Preference Optimization (MNPO), a novel framework that generalizes NLHF to the multiplayer regime. It formulates alignment as an $n$-player game, where each policy competes against a population of opponents while being regularized toward a reference model. Our framework establishes well-defined Nash equilibria in multiplayer settings and extends the concept of duality gap to quantify approximation quality. We demonstrate that MNPO inherits the equilibrium guarantees of two-player methods while enabling richer competitive dynamics and improved coverage of diverse preference structures. Through comprehensive empirical evaluation, we show that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios. Together, these results establish MNPO as a principled and scalable framework for aligning LLMs with complex, non-transitive human preferences. Code is available at https://github.com/smiles724/MNPO.<br>
<span id='abs_ch'>Chinese: 本文提出了多人纳什偏好优化（MNPO）框架，将纳什学 从双人博弈扩展到多人场景，通过更丰富的竞争动态更好地对齐复杂非 递性的人类偏好，在异构 注条件下持续超越现有基线模型。</span><br>
<span id='abs_en'>English: This paper introduces Multiplayer Nash Preference Optimization (MNPO), a novel framework that extends Nash learning from human feedback to multiplayer settings, enabling richer competitive dynamics and improved alignment with complex, non-transitive human preferences while consistently outperforming existing baselines.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2509.23049.pdf' target='_blank'>https://arxiv.org/pdf/2509.23049.pdf</a></span>   <span><a href='https://github.com/zijianwang0510/FedDRM.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Wang, Xiaofei Zhang, Xin Zhang, Yukun Liu, Qiong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23049">Beyond Aggregation: Guiding Clients in Heterogeneous Federated Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Federated learning (FL) is increasingly adopted in domains like healthcare, where data privacy is paramount. A fundamental challenge in these systems is statistical heterogeneity-the fact that data distributions vary significantly across clients (e.g., different hospitals may treat distinct patient demographics). While current FL algorithms focus on aggregating model updates from these heterogeneous clients, the potential of the central server remains under-explored. This paper is motivated by a healthcare scenario: could a central server not only build a model but also guide a new patient to the hospital best equipped for their specific condition? We generalize this idea to propose a novel paradigm for FL systems where the server actively guides the allocation of new tasks or queries to the most appropriate client in the network. To enable this, we introduce an empirical likelihood-based framework that simultaneously addresses two goals: (1) learning effective local models on each client, and (2) finding the best matching client for a new query. Empirical results demonstrate the framework's effectiveness on benchmark datasets, showing improvements in both model accuracy and the precision of client guidance compared to standard FL approaches. This work opens a new direction for building more intelligent and resource-efficient federated systems that leverage heterogeneity as a feature, not just a bug. Code is available at https://github.com/zijianwang0510/FedDRM.git.<br>
<span id='abs_ch'>中文摘要：本文提出了一种新颖的联邦学 范式，其中中央服务器不仅聚合模型，还通过经验似然框架智能地将新查询引导至最合适的客户端，从而在模型精度和客户端匹配准确性上实现双重提升。</span><br>
<span id='abs_en'>English Summary: This paper introduces a novel federated learning paradigm where the central server not only aggregates models but also intelligently directs new queries to the most suitable client, using an empirical likelihood framework to improve both model accuracy and client matching precision.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2509.23044.pdf' target='_blank'>https://arxiv.org/pdf/2509.23044.pdf</a></span>   <span><a href='https://github.com/ye-Kim/MMeViT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye-eun Kim, Suhyeon Lim, Andrew J. Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23044">MMeViT: Multi-Modal ensemble ViT for Post-Stroke Rehabilitation Action Recognition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Rehabilitation therapy for stroke patients faces a supply shortage despite the increasing demand. To address this issue, remote monitoring systems that reduce the burden on medical staff are emerging as a viable alternative. A key component of these remote monitoring systems is Human Action Recognition (HAR) technology, which classifies actions. However, existing HAR studies have primarily focused on non-disable individuals, making them unsuitable for recognizing the actions of stroke patients. HAR research for stroke has largely concentrated on classifying relatively simple actions using machine learning rather than deep learning. In this study, we designed a system to monitor the actions of stroke patients, focusing on domiciliary upper limb Activities of Daily Living (ADL). Our system utilizes IMU (Inertial Measurement Unit) sensors and an RGB-D camera, which are the most common modalities in HAR. We directly collected a dataset through this system, investigated an appropriate preprocess and proposed a deep learning model suitable for processing multimodal data. We analyzed the collected dataset and found that the action data of stroke patients is less clustering than that of non-disabled individuals. Simultaneously, we found that the proposed model learns similar tendencies for each label in data with features that are difficult to clustering. This study suggests the possibility of expanding the deep learning model, which has learned the action features of stroke patients, to not only simple action recognition but also feedback such as assessment contributing to domiciliary rehabilitation in future research. The code presented in this study is available at https://github.com/ye-Kim/MMeViT.<br>
<span id='abs_ch'>中文: 本 究开发了一种基于惯性测量单元 感器和RGB-D摄像头的多模态深度学 系统，用于识别脑卒中患者的上肢日常活动，解决了现有动作识别模型不适用于该人群的局限性，为远程康复监测提供了潜在应用前景。</span><br>
<span id='abs_en'>English: This study develops a multimodal deep learning system using IMU sensors and an RGB-D camera to recognize upper limb daily activities in stroke patients, addressing the limitations of existing human action recognition models that are unsuitable for this population and enabling potential applications in remote rehabilitation monitoring.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2509.23041.pdf' target='_blank'>https://arxiv.org/pdf/2509.23041.pdf</a></span>   <span><a href='https://github.com/liangzid/VirusInfectionAttack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zi Liang, Qingqing Ye, Xuan Liu, Yanyun Wang, Jianliang Xu, Haibo Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23041">Virus Infection Attack on LLMs: Your Poisoning Can Spread "VIA" Synthetic Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Synthetic data refers to artificial samples generated by models. While it has been validated to significantly enhance the performance of large language models (LLMs) during training and has been widely adopted in LLM development, potential security risks it may introduce remain uninvestigated. This paper systematically evaluates the resilience of synthetic-data-integrated training paradigm for LLMs against mainstream poisoning and backdoor attacks. We reveal that such a paradigm exhibits strong resistance to existing attacks, primarily thanks to the different distribution patterns between poisoning data and queries used to generate synthetic samples. To enhance the effectiveness of these attacks and further investigate the security risks introduced by synthetic data, we introduce a novel and universal attack framework, namely, Virus Infection Attack (VIA), which enables the propagation of current attacks through synthetic data even under purely clean queries. Inspired by the principles of virus design in cybersecurity, VIA conceals the poisoning payload within a protective "shell" and strategically searches for optimal hijacking points in benign samples to maximize the likelihood of generating malicious content. Extensive experiments on both data poisoning and backdoor attacks show that VIA significantly increases the presence of poisoning content in synthetic data and correspondingly raises the attack success rate (ASR) on downstream models to levels comparable to those observed in the poisoned upstream models.<br>
<span id='abs_ch'>中文摘要：合成数据虽能显著提升大语言模型性能，但其潜在安全风险尚未被探究；本 究发现该训练范式对主流攻击具有强抵抗力，并提出新型病毒感染攻击(VIA)，能通过合成数据有效 播恶意内容，大幅提升攻击成功率。</span><br>
<span id='abs_en'>English Summary: Synthetic data enhances LLM performance but poses unexamined security risks, with this study revealing its resilience to standard attacks due to distribution differences while introducing the Virus Infection Attack (VIA) that effectively propagates malicious content through synthetic data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2509.23025.pdf' target='_blank'>https://arxiv.org/pdf/2509.23025.pdf</a></span>   <span><a href='https://github.com/vngabriel/perceptual-influence' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriel A. Viana, Luis F. Alves Pereira, Tsang Ing Ren, George D. C. Cavalcanti, Jan Sijbers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23025">Perceptual Influence: Improving the Perceptual Loss Design for Low-Dose CT Enhancement</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Perceptual losses have emerged as powerful tools for training networks to enhance Low-Dose Computed Tomography (LDCT) images, offering an alternative to traditional pixel-wise losses such as Mean Squared Error, which often lead to over-smoothed reconstructions and loss of clinically relevant details in LDCT images. The perceptual losses operate in a latent feature space defined by a pretrained encoder and aim to preserve semantic content by comparing high-level features rather than raw pixel values. However, the design of perceptual losses involves critical yet underexplored decisions, including the feature representation level, the dataset used to pretrain the encoder, and the relative importance assigned to the perceptual component during optimization. In this work, we introduce the concept of perceptual influence (a metric that quantifies the relative contribution of the perceptual loss term to the total loss) and propose a principled framework to assess the impact of the loss design choices on the model training performance. Through systematic experimentation, we show that the widely used configurations in the literature to set up a perceptual loss underperform compared to better-designed alternatives. Our findings show that better perceptual loss designs lead to significant improvements in noise reduction and structural fidelity of reconstructed CT images, without requiring any changes to the network architecture. We also provide objective guidelines, supported by statistical analysis, to inform the effective use of perceptual losses in LDCT denoising. Our source code is available at https://github.com/vngabriel/perceptual-influence.<br>
<span id='abs_ch'>感知损失通过特征空间比较保留语义内容以提升低剂量CT图像重建效果，本 究提出系统性评估框架，证明优化后的损失设计能在不改变网络架构的情况下显著提升噪声抑制与结构保真度。</span><br>
<span id='abs_en'>Perceptual losses enhance LDCT image reconstruction by preserving semantic content through feature-level comparisons, and our study introduces a principled framework that demonstrates optimized loss designs significantly improve noise reduction and structural fidelity without altering network architecture.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2509.23025.pdf' target='_blank'>https://arxiv.org/pdf/2509.23025.pdf</a></span>   <span><a href='https://github.com/vngabriel/perceptual-influence' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriel A. Viana, Luis F. Alves Pereira, Tsang Ing Ren, George D. C. Cavalcanti, Jan Sijbers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23025">Perceptual Influence: Improving the Perceptual Loss Design for Low-Dose CT Enhancement</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Perceptual losses have emerged as powerful tools for training networks to enhance Low-Dose Computed Tomography (LDCT) images, offering an alternative to traditional pixel-wise losses such as Mean Squared Error, which often lead to over-smoothed reconstructions and loss of clinically relevant details in LDCT images. The perceptual losses operate in a latent feature space defined by a pretrained encoder and aim to preserve semantic content by comparing high-level features rather than raw pixel values. However, the design of perceptual losses involves critical yet underexplored decisions, including the feature representation level, the dataset used to pretrain the encoder, and the relative importance assigned to the perceptual component during optimization. In this work, we introduce the concept of perceptual influence (a metric that quantifies the relative contribution of the perceptual loss term to the total loss) and propose a principled framework to assess the impact of the loss design choices on the model training performance. Through systematic experimentation, we show that the widely used configurations in the literature to set up a perceptual loss underperform compared to better-designed alternatives. Our findings show that better perceptual loss designs lead to significant improvements in noise reduction and structural fidelity of reconstructed CT images, without requiring any changes to the network architecture. We also provide objective guidelines, supported by statistical analysis, to inform the effective use of perceptual losses in LDCT denoising. Our source code is available at https://github.com/vngabriel/perceptual-influence.<br>
<span id='abs_ch'>感知损失通过特征空间比较保留语义内容以提升低剂量CT图像重建效果，本 究提出系统性评估框架，证明优化后的损失设计能在不改变网络架构的情况下显著提升噪声抑制与结构保真度。</span><br>
<span id='abs_en'>Perceptual losses enhance LDCT image reconstruction by preserving semantic content through feature-level comparisons, and our study introduces a principled framework that demonstrates optimized loss designs significantly improve noise reduction and structural fidelity without altering network architecture.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2509.23023.pdf' target='_blank'>https://arxiv.org/pdf/2509.23023.pdf</a></span>   <span><a href='https://github.com/bastoscostadavi/llm-mafia-game' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Davi Bastos Costa, Renato Vicente
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23023">Deceive, Detect, and Disclose: Large Language Models Play Mini-Mafia</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Mafia is a social deduction game where informed mafia compete against uninformed townsfolk. Its asymmetry of information and reliance on theory-of-mind reasoning mirror real-world multi-agent scenarios, making it a useful testbed for evaluating the social intelligence of large language models (LLMs). To support a systematic study, we introduce Mini-Mafia: a simplified four-player variant with one mafioso, one detective, and two villagers. We set the mafioso to kill a villager and the detective to investigate the mafioso during the night, reducing the game to a single day phase of discussion and voting. This setup isolates three interactive capabilities through role-specific win conditions: the mafioso must deceive, the villagers must detect deception, and the detective must effectively disclose information. To measure these skills, we have LLMs play against each other, creating the Mini-Mafia Benchmark: a two-stage framework that first estimates win rates within fixed opponent configurations, then aggregates performance across them using standardized scoring. Built entirely from model interactions without external data, the benchmark evolves as new models are introduced, with each one serving both as a new opponent and as a subject of evaluation. Our experiments reveal counterintuitive results, including cases where smaller models outperform larger ones. Beyond benchmarking, Mini-Mafia enables quantitative study of emergent multi-agent dynamics such as name bias and last-speaker advantage. It also contributes to AI safety by generating training data for deception detectors and by tracking models' deception capabilities against human baselines.<br>
<span id='abs_ch'>中文摘要：Mini-Mafia作为简化版社交推理游戏，通过设计特定角色获胜条件构建评估框架，既能检测语言模型在欺骗识别与信息 递方面的社交智能，又能为多智能体动态 究和AI安全提供量化分析基础。</span><br>
<span id='abs_en'>English Summary: Mini-Mafia is a simplified social deduction game designed as a benchmark to evaluate large language models' social intelligence through deception detection and information disclosure, revealing unexpected performance patterns among different models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2509.22889.pdf' target='_blank'>https://arxiv.org/pdf/2509.22889.pdf</a></span>   <span><a href='https://github.com/chinefed/convolutional-set-transformer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Federico Chinello, Giacomo Boracchi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22889">Convolutional Set Transformer</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce the Convolutional Set Transformer (CST), a novel neural architecture designed to process image sets of arbitrary cardinality that are visually heterogeneous yet share high-level semantics - such as a common category, scene, or concept. Existing set-input networks, e.g., Deep Sets and Set Transformer, are limited to vector inputs and cannot directly handle 3D image tensors. As a result, they must be cascaded with a feature extractor, typically a CNN, which encodes images into embeddings before the set-input network can model inter-image relationships. In contrast, CST operates directly on 3D image tensors, performing feature extraction and contextual modeling simultaneously, thereby enabling synergies between the two processes. This design yields superior performance in tasks such as Set Classification and Set Anomaly Detection and further provides native compatibility with CNN explainability methods such as Grad-CAM, unlike competing approaches that remain opaque. Finally, we show that CSTs can be pre-trained on large-scale datasets and subsequently adapted to new domains and tasks through standard Transfer Learning schemes. To support further research, we release CST-15, a CST backbone pre-trained on ImageNet (https://github.com/chinefed/convolutional-set-transformer).<br>
<span id='abs_ch'>中文摘要：卷积集合变换器（CST）是一种新型神经网络架构，可直接处理三维图像 量组成的异构图像集，通过同步实现特征提取与上下文建模，在集合分类等任务中性能优于现有方法，并保持与CNN可解释性方法的兼容性。</span><br>
<span id='abs_en'>English Summary: The Convolutional Set Transformer (CST) is a novel neural architecture that directly processes heterogeneous image sets as 3D tensors, integrating feature extraction and contextual modeling to outperform existing methods in tasks like set classification while maintaining compatibility with CNN explainability techniques.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2509.22647.pdf' target='_blank'>https://arxiv.org/pdf/2509.22647.pdf</a></span>   <span><a href='https://github.com/InternLM/CapRL' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/InternLM/CapRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Long Xing, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jianze Liang, Qidong Huang, Jiaqi Wang, Feng Wu, Dahua Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22647">CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Image captioning is a fundamental task that bridges the visual and linguistic domains, playing a critical role in pre-training Large Vision-Language Models (LVLMs). Current state-of-the-art captioning models are typically trained with Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable data annotated by humans or proprietary models. This approach often leads to models that memorize specific ground-truth answers, limiting their generality and ability to generate diverse, creative descriptions. To overcome the limitation of SFT, we propose applying the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning. A primary challenge, however, is designing an objective reward function for the inherently subjective nature of what constitutes a "good" caption. We introduce Captioning Reinforcement Learning (CapRL), a novel training framework that redefines caption quality through its utility: a high-quality caption should enable a non-visual language model to accurately answer questions about the corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption. As the first study to apply RLVR to the subjective image captioning task, we demonstrate that CapRL significantly enhances multiple settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks. Moreover, within the Prism Framework for caption quality evaluation, CapRL achieves performance comparable to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%. Code is available here: https://github.com/InternLM/CapRL.<br>
<span id='abs_ch'>中文总结：该 究提出CapRL强化学 框架，通过评估描述能否帮助语言模型准确回答图像相关问题来定义描述质量，从而突 监督微调的限制。</span><br>
<span id='abs_en'>English Summary: The study introduces CapRL, a reinforcement learning framework that overcomes limitations of supervised fine-tuning by defining caption quality through a caption's ability to help language models answer image-related questions accurately.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2509.22638.pdf' target='_blank'>https://arxiv.org/pdf/2509.22638.pdf</a></span>   <span><a href='https://github.com/sail-sg/feedback-conditional-policy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Renjie Luo, Zichen Liu, Xiangyan Liu, Chao Du, Min Lin, Wenhu Chen, Wei Lu, Tianyu Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22638">Language Models Can Learn from Verbal Feedback Without Scalar Rewards</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LLMs are often trained with RL from human or AI feedback, yet such methods typically compress nuanced feedback into scalar rewards, discarding much of their richness and inducing scale imbalance. We propose treating verbal feedback as a conditioning signal. Inspired by language priors in text-to-image generation, which enable novel outputs from unseen prompts, we introduce the feedback-conditional policy (FCP). FCP learns directly from response-feedback pairs, approximating the feedback-conditional posterior through maximum likelihood training on offline data. We further develop an online bootstrapping stage where the policy generates under positive conditions and receives fresh feedback to refine itself. This reframes feedback-driven learning as conditional generation rather than reward optimization, offering a more expressive way for LLMs to directly learn from verbal feedback. Our code is available at https://github.com/sail-sg/feedback-conditional-policy.<br>
<span id='abs_ch'>中文摘要：作者提出了一种反馈条件策略（FCP），将语言反馈作为语言模型的调节信号，通过离线训练和在线自举直接从响应-反馈对中学 ，将反馈驱动学 重新定义为条件生成而非奖励优化。</span><br>
<span id='abs_en'>English Summary: The authors propose a feedback-conditional policy (FCP) that treats verbal feedback as a conditioning signal for language models, enabling direct learning from response-feedback pairs through both offline training and online bootstrapping, reframing feedback-driven learning as conditional generation rather than reward optimization.Paperid:261,https://arxiv.org/pdf/2509.22637.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2509.22637.pdf' target='_blank'>https://arxiv.org/pdf/2509.22637.pdf</a></span>   <span><a href='https://github.com/sail-sg/variational-reasoning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangxin Zhou, Zichen Liu, Haonan Wang, Chao Du, Min Lin, Chongxuan Li, Liang Wang, Tianyu Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22637">Variational Reasoning for Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce a variational reasoning framework for language models that treats thinking traces as latent variables and optimizes them through variational inference. Starting from the evidence lower bound (ELBO), we extend it to a multi-trace objective for tighter bounds and propose a forward-KL formulation that stabilizes the training of the variational posterior. We further show that rejection sampling finetuning and binary-reward RL, including GRPO, can be interpreted as local forward-KL objectives, where an implicit weighting by model accuracy naturally arises from the derivation and reveals a previously unnoticed bias toward easier questions. We empirically validate our method on the Qwen 2.5 and Qwen 3 model families across a wide range of reasoning tasks. Overall, our work provides a principled probabilistic perspective that unifies variational inference with RL-style methods and yields stable objectives for improving the reasoning ability of language models. Our code is available at https://github.com/sail-sg/variational-reasoning.<br>
<span id='abs_ch'>中文摘要：本文提出了一种变分推理框架，将强化学 方法与变分推断相统一，通过稳定的训练目 提升语言模型推理能力，并揭示了模型对简单问题的内在偏好。English Summary: This paper presents a variational reasoning framework that unifies variational inference with reinforcement learning methods to enhance language model reasoning through stable training objectives and reveals an inherent bias toward easier questions.Paperid:262,https://arxiv.org/pdf/2509.22627.pdfGitHub</span><br>
<span id='abs_en'>English Summary: This paper presents a variational reasoning framework that unifies variational inference with reinforcement learning methods to enhance language model reasoning through stable training objectives and reveals an inherent bias toward easier questions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2509.22472.pdf' target='_blank'>https://arxiv.org/pdf/2509.22472.pdf</a></span>   <span><a href='https://github.com/RobustML-Lab/Legal-Multilingual-Evaluation-of-LLMs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Antreas Ioannou, Andreas Shiamishis, Nora Hollenstein, Nezihe Merve GÃ¼rel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22472">Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In an era dominated by Large Language Models (LLMs), understanding their capabilities and limitations, especially in high-stakes fields like law, is crucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini, DeepSeek, and other emerging models are increasingly integrated into legal workflows, their performance in multilingual, jurisdictionally diverse, and adversarial contexts remains insufficiently explored. This work evaluates LLaMA and Gemini on multilingual legal and non-legal benchmarks, and assesses their adversarial robustness in legal tasks through character and word-level perturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation. We moreover present an open-source, modular evaluation pipeline designed to support multilingual, task-diverse benchmarking of any combination of LLMs and datasets, with a particular focus on legal tasks, including classification, summarization, open questions, and general reasoning. Our findings confirm that legal tasks pose significant challenges for LLMs with accuracies often below 50% on legal reasoning benchmarks such as LEXam, compared to over 70% on general-purpose tasks like XNLI. In addition, while English generally yields more stable results, it does not always lead to higher accuracy. Prompt sensitivity and adversarial vulnerability is also shown to persist across languages. Finally, a correlation is found between the performance of a language and its syntactic similarity to English. We also observe that LLaMA is weaker than Gemini, with the latter showing an average advantage of about 24 percentage points across the same task. Despite improvements in newer LLMs, challenges remain in deploying them reliably for critical, multilingual legal applications.<br>
<span id='abs_ch'>中文摘要：本 究评估了大型语言模型在多语言法律任务中的表现，发现其在法律推理任务中准确率常低于50%，且存在对抗性攻击漏洞，表明当前模型尚 法可 应用于高风险的法律领域。</span><br>
<span id='abs_en'>English Summary: This study evaluates the performance of Large Language Models like LLaMA and Gemini on multilingual legal tasks, revealing significant challenges with accuracies often below 50% and persistent vulnerabilities to adversarial attacks, highlighting their current limitations for high-stakes legal applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2509.22461.pdf' target='_blank'>https://arxiv.org/pdf/2509.22461.pdf</a></span>   <span><a href='https://github.com/luckyerr/MDAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hui Li, Changhao Jiang, Hongyu Wang, Ming Zhang, Jiajun Sun, Zhixiong Yang, Yifei Cao, Shihan Dou, Xiaoran Fan, Baoyu Fan, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22461">MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The ability to reason from audio, including speech, paralinguistic cues, environmental sounds, and music, is essential for AI agents to interact effectively in real-world scenarios. Existing benchmarks mainly focus on static or single-scene settings and do not fully capture scenarios where multiple speakers, unfolding events, and heterogeneous audio sources interact. To address these challenges, we introduce MDAR, a benchmark for evaluating models on complex, multi-scene, and dynamically evolving audio reasoning tasks. MDAR comprises 3,000 carefully curated question-answer pairs linked to diverse audio clips, covering five categories of complex reasoning and spanning three question types. We benchmark 26 state-of-the-art audio language models on MDAR and observe that they exhibit limitations in complex reasoning tasks. On single-choice questions, Qwen2.5-Omni (open-source) achieves 76.67% accuracy, whereas GPT-4o Audio (closed-source) reaches 68.47%; however, GPT-4o Audio substantially outperforms Qwen2.5-Omni on the more challenging multiple-choice and open-ended tasks. Across all three question types, no model achieves 80% performance. These findings underscore the unique challenges posed by MDAR and its value as a benchmark for advancing audio reasoning research.Code and benchmark can be found at https://github.com/luckyerr/MDAR.<br>
<br>
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2509.22458.pdf' target='_blank'>https://arxiv.org/pdf/2509.22458.pdf</a></span>   <span><a href='https://github.com/Kimchangheon/PIGNN-Attn-LS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Changhun Kim, Timon Conrad, Redwanul Karim, Julian Oelhaf, David Riebesel, TomÃ¡s Arias-Vergara, Andreas Maier, Johann JÃ¤ger, Siming Bayer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22458">Physics-informed GNN for medium-high voltage AC power flow with edge-aware attention and line search correction operator</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Physics-informed graph neural networks (PIGNNs) have emerged as fast AC power-flow solvers that can replace classic Newton--Raphson (NR) solvers, especially when thousands of scenarios must be evaluated. However, current PIGNNs still need accuracy improvements at parity speed; in particular, the physics loss is inoperative at inference, which can deter operational adoption. We address this with PIGNN-Attn-LS, combining an edge-aware attention mechanism that explicitly encodes line physics via per-edge biases, capturing the grid's anisotropy, with a backtracking line-search-based globalized correction operator that restores an operative decrease criterion at inference. Training and testing use a realistic High-/Medium-Voltage scenario generator, with NR used only to construct reference states. On held-out HV cases consisting of 4--32-bus grids, PIGNN-Attn-LS achieves a test RMSE of 0.00033 p.u. in voltage and 0.08$^\circ$ in angle, outperforming the PIGNN-MLP baseline by 99.5\% and 87.1\%, respectively. With streaming micro-batches, it delivers 2--5$\times$ faster batched inference than NR on 4--1024-bus grids.<br>
<br>
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/2509.22442.pdf' target='_blank'>https://arxiv.org/pdf/2509.22442.pdf</a></span>   <span><a href='https://github.com/xupei0610/basketball' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei Xu, Zhen Wu, Ruocheng Wang, Vishnu Sarukkai, Kayvon Fatahalian, Ioannis Karamouzas, Victor Zordan, C. Karen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22442">Learning to Ball: Composing Policies for Long-Horizon Basketball Moves</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Learning a control policy for a multi-phase, long-horizon task, such as basketball maneuvers, remains challenging for reinforcement learning approaches due to the need for seamless policy composition and transitions between skills. A long-horizon task typically consists of distinct subtasks with well-defined goals, separated by transitional subtasks with unclear goals but critical to the success of the entire task. Existing methods like the mixture of experts and skill chaining struggle with tasks where individual policies do not share significant commonly explored states or lack well-defined initial and terminal states between different phases. In this paper, we introduce a novel policy integration framework to enable the composition of drastically different motor skills in multi-phase long-horizon tasks with ill-defined intermediate states. Based on that, we further introduce a high-level soft router to enable seamless and robust transitions between the subtasks. We evaluate our framework on a set of fundamental basketball skills and challenging transitions. Policies trained by our approach can effectively control the simulated character to interact with the ball and accomplish the long-horizon task specified by real-time user commands, without relying on ball trajectory references.<br>
<span id='abs_ch'>中文: 本文提出了一种新颖的策略集成框架和高级软路由机制，能够在多阶段长时程任务中实现截然不同运动技能的 缝组合与鲁棒过渡，并成功应用于 需依赖篮球轨迹参考的篮球动作控制。</span><br>
<span id='abs_en'>English: This paper introduces a novel policy integration framework and a high-level soft router to enable seamless composition and robust transitions between drastically different motor skills in multi-phase long-horizon tasks, successfully applied to basketball maneuvers without relying on ball trajectory references.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2509.22437.pdf' target='_blank'>https://arxiv.org/pdf/2509.22437.pdf</a></span>   <span><a href='https://github.com/CHIzhP/Chimera' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziheng Chi, Yifan Hou, Chenxi Pang, Shaobo Cui, Mubashara Akhtar, Mrinmaya Sachan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22437">Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diagrams convey symbolic information in a visual format rather than a linear stream of words, making them especially challenging for AI models to process. While recent evaluations suggest that vision-language models (VLMs) perform well on diagram-related benchmarks, their reliance on knowledge, reasoning, or modality shortcuts raises concerns about whether they genuinely understand and reason over diagrams. To address this gap, we introduce Chimera, a comprehensive test suite comprising 7,500 high-quality diagrams sourced from Wikipedia; each diagram is annotated with its symbolic content represented by semantic triples along with multi-level questions designed to assess four fundamental aspects of diagram comprehension: entity recognition, relation understanding, knowledge grounding, and visual reasoning. We use Chimera to measure the presence of three types of shortcuts in visual question answering: (1) the visual-memorization shortcut, where VLMs rely on memorized visual patterns; (2) the knowledge-recall shortcut, where models leverage memorized factual knowledge instead of interpreting the diagram; and (3) the Clever-Hans shortcut, where models exploit superficial language patterns or priors without true comprehension. We evaluate 15 open-source VLMs from 7 model families on Chimera and find that their seemingly strong performance largely stems from shortcut behaviors: visual-memorization shortcuts have slight impact, knowledge-recall shortcuts play a moderate role, and Clever-Hans shortcuts contribute significantly. These findings expose critical limitations in current VLMs and underscore the need for more robust evaluation protocols that benchmark genuine comprehension of complex visual inputs (e.g., diagrams) rather than question-answering shortcuts.<br>
<span id='abs_ch'>中文: 图表 其符号化视觉特性对AI处理构成独特挑战，尽管视觉语言模型在图表任务上表现良好，但Chimera测试套件揭示其性能主要依赖记忆化、知识召回和语言模式等捷径而非真正理解，暴露出当前模型的 本缺陷。</span><br>
<span id='abs_en'>English: Diagrams pose unique challenges for AI processing due to their symbolic visual nature, and while vision-language models appear competent on diagram tasks, the Chimera test suite reveals their performance heavily relies on shortcuts rather than genuine comprehension, exposing critical limitations in current models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2509.22378.pdf' target='_blank'>https://arxiv.org/pdf/2509.22378.pdf</a></span>   <span><a href='https://github.com/RS2002/Image2Music' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Zhao, Dian Jin, Zijing Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22378">Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, Image-to-Music (I2M) generation has garnered significant attention, with potential applications in fields such as gaming, advertising, and multi-modal art creation. However, due to the ambiguous and subjective nature of I2M tasks, most end-to-end methods lack interpretability, leaving users puzzled about the generation results. Even methods based on emotion mapping face controversy, as emotion represents only a singular aspect of art. Additionally, most learning-based methods require substantial computational resources and large datasets for training, hindering accessibility for common users. To address these challenges, we propose the first Vision Language Model (VLM)-based I2M framework that offers high interpretability and low computational cost. Specifically, we utilize ABC notation to bridge the text and music modalities, enabling the VLM to generate music using natural language. We then apply multi-modal Retrieval-Augmented Generation (RAG) and self-refinement techniques to allow the VLM to produce high-quality music without external training. Furthermore, we leverage the generated motivations in text and the attention maps from the VLM to provide explanations for the generated results in both text and image modalities. To validate our method, we conduct both human studies and machine evaluations, where our method outperforms others in terms of music quality and music-image consistency, indicating promising results. Our code is available at https://github.com/RS2002/Image2Music .<br>
<span id='abs_ch'>中文：该 究提出首个基于视觉语言模型的图像到音乐生成框架，通过ABC记谱法和多模态检索增强技术实现 需外部训练的高质量音乐生成，并利用文本动机和注意力图谱提供双模态解释，在评估中展现出优越的音乐质量与图文一致性。</span><br>
<span id='abs_en'>English: The proposed Vision Language Model-based Image-to-Music framework overcomes interpretability and computational barriers by using ABC notation and multi-modal techniques to generate high-quality music with dual-modality explanations, achieving superior results in evaluations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2509.22360.pdf' target='_blank'>https://arxiv.org/pdf/2509.22360.pdf</a></span>   <span><a href='https://github.com/paulsubarna/Chronoberg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Niharika Hegde, Subarnaduti Paul, Lars Joel-Frey, Manuel Brack, Kristian Kersting, Martin Mundt, Patrick Schramowski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22360">CHRONOBERG: Capturing Language Evolution and Temporal Awareness in Foundation Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) excel at operating at scale by leveraging social media and various data crawled from the web. Whereas existing corpora are diverse, their frequent lack of long-term temporal structure may however limit an LLM's ability to contextualize semantic and normative evolution of language and to capture diachronic variation. To support analysis and training for the latter, we introduce CHRONOBERG, a temporally structured corpus of English book texts spanning 250 years, curated from Project Gutenberg and enriched with a variety of temporal annotations. First, the edited nature of books enables us to quantify lexical semantic change through time-sensitive Valence-Arousal-Dominance (VAD) analysis and to construct historically calibrated affective lexicons to support temporally grounded interpretation. With the lexicons at hand, we demonstrate a need for modern LLM-based tools to better situate their detection of discriminatory language and contextualization of sentiment across various time-periods. In fact, we show how language models trained sequentially on CHRONOBERG struggle to encode diachronic shifts in meaning, emphasizing the need for temporally aware training and evaluation pipelines, and positioning CHRONOBERG as a scalable resource for the study of linguistic change and temporal generalization. Disclaimer: This paper includes language and display of samples that could be offensive to readers. Open Access: Chronoberg is available publicly on HuggingFace at ( https://huggingface.co/datasets/spaul25/Chronoberg). Code is available at (https://github.com/paulsubarna/Chronoberg).<br>
<span id='abs_ch'>Chinese: CHRONOBERG是一个跨越250年的英语书籍时间 注语料库，旨在帮助大型语言模型更好地捕捉语言演变和历时意义变化，弥补现有训练数据的不足。</span><br>
<span id='abs_en'>English: CHRONOBERG is a temporally annotated corpus of English books spanning 250 years, designed to help large language models better capture language evolution and diachronic meaning shifts, addressing limitations in current training data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2509.22331.pdf' target='_blank'>https://arxiv.org/pdf/2509.22331.pdf</a></span>   <span><a href='https://github.com/Event-AHU/OpenPAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Shujuan Wu, Xiaoxia Cheng, Changwei Bi, Jin Tang, Bin Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22331">Pedestrian Attribute Recognition via Hierarchical Cross-Modality HyperGraph Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current Pedestrian Attribute Recognition (PAR) algorithms typically focus on mapping visual features to semantic labels or attempt to enhance learning by fusing visual and attribute information. However, these methods fail to fully exploit attribute knowledge and contextual information for more accurate recognition. Although recent works have started to consider using attribute text as additional input to enhance the association between visual and semantic information, these methods are still in their infancy. To address the above challenges, this paper proposes the construction of a multi-modal knowledge graph, which is utilized to mine the relationships between local visual features and text, as well as the relationships between attributes and extensive visual context samples. Specifically, we propose an effective multi-modal knowledge graph construction method that fully considers the relationships among attributes and the relationships between attributes and vision tokens. To effectively model these relationships, this paper introduces a knowledge graph-guided cross-modal hypergraph learning framework to enhance the standard pedestrian attribute recognition framework. Comprehensive experiments on multiple PAR benchmark datasets have thoroughly demonstrated the effectiveness of our proposed knowledge graph for the PAR task, establishing a strong foundation for knowledge-guided pedestrian attribute recognition. The source code of this paper will be released on https://github.com/Event-AHU/OpenPAR<br>
<span id='abs_ch'>中文摘要：本文提出了一种多模态知识图谱，通过建模视觉特征与属性文本之间的关系来提升行人属性识别性能，并在多个基准数据集上通过全面实验验证了其有效性。</span><br>
<span id='abs_en'>English Summary: This paper introduces a multi-modal knowledge graph to enhance pedestrian attribute recognition by modeling relationships between visual features and attribute texts, validated through comprehensive experiments on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2509.22299.pdf' target='_blank'>https://arxiv.org/pdf/2509.22299.pdf</a></span>   <span><a href='https://github.com/LLIKKE/HEAPr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Li, Zheng Yang, Zhongbin Zhou, Feng Xue, Zhonglin Jiang, Wenxiao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22299">HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Mixture-of-Experts (MoE) architectures in large language models (LLMs) deliver exceptional performance and reduced inference costs compared to dense LLMs. However, their large parameter counts result in prohibitive memory requirements, limiting practical deployment. While existing pruning methods primarily focus on expert-level pruning, this coarse granularity often leads to substantial accuracy degradation. In this work, we introduce HEAPr, a novel pruning algorithm that decomposes experts into smaller, indivisible atomic experts, enabling more precise and flexible atomic expert pruning. To measure the importance of each atomic expert, we leverage second-order information based on principles similar to Optimal Brain Surgeon (OBS) theory. To address the computational and storage challenges posed by second-order information, HEAPr exploits the inherent properties of atomic experts to transform the second-order information from expert parameters into that of atomic expert parameters, and further simplifies it to the second-order information of atomic expert outputs. This approach reduces the space complexity from $O(d^4)$, where d is the model's dimensionality, to $O(d^2)$. HEAPr requires only two forward passes and one backward pass on a small calibration set to compute the importance of atomic experts. Extensive experiments on MoE models, including DeepSeek MoE and Qwen MoE family, demonstrate that HEAPr outperforms existing expert-level pruning methods across a wide range of compression ratios and benchmarks. Specifically, HEAPr achieves nearly lossless compression at compression ratios of 20% ~ 25% in most models, while also reducing FLOPs nearly by 20%. The code can be found at \href{https://github.com/LLIKKE/HEAPr}{https://github.com/LLIKKE/HEAPr}.<br>
<span id='abs_ch'>中文: HEAPr提出了一种新颖的原子专家剪枝方法，通过简化二阶信息计算，在保持20-25%压缩比下实现近乎 损的模型压缩，同时降低计算成本，性能优于现有专家级剪枝方法。</span><br>
<span id='abs_en'>English: HEAPr introduces a novel atomic expert pruning method for Mixture-of-Experts models that leverages simplified second-order information to achieve nearly lossless compression at 20-25% ratios while reducing computational costs, outperforming existing expert-level pruning techniques.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2509.22284.pdf' target='_blank'>https://arxiv.org/pdf/2509.22284.pdf</a></span>   <span><a href='https://github.com/IBM/expressive-sparse-state-space-model' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksandar Terzić, Nicolas Menet, Michael Hersche, Thomas Hofmann, Abbas Rahimi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22284">Structured Sparse Transition Matrices to Enable State Tracking in State-Space Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Modern state-space models (SSMs) often utilize transition matrices which enable efficient computation but pose restrictions on the model's expressivity, as measured in terms of the ability to emulate finite-state automata (FSA). While unstructured transition matrices are optimal in terms of expressivity, they come at a prohibitively high compute and memory cost even for moderate state sizes. We propose a structured sparse parametrization of transition matrices in SSMs that enables FSA state tracking with optimal state size and depth, while keeping the computational cost of the recurrence comparable to that of diagonal SSMs. Our method, PD-SSM, parametrizes the transition matrix as the product of a column one-hot matrix ($P$) and a complex-valued diagonal matrix ($D$). Consequently, the computational cost of parallel scans scales linearly with the state size. Theoretically, the model is BIBO-stable and can emulate any $N$-state FSA with one layer of dimension $N$ and a linear readout of size $N \times N$, significantly improving on all current structured SSM guarantees. Experimentally, the model significantly outperforms a wide collection of modern SSM variants on various FSA state tracking tasks. On multiclass time-series classification, the performance is comparable to that of neural controlled differential equations, a paradigm explicitly built for time-series analysis. Finally, we integrate PD-SSM into a hybrid Transformer-SSM architecture and demonstrate that the model can effectively track the states of a complex FSA in which transitions are encoded as a set of variable-length English sentences. The code is available at https://github.com/IBM/expressive-sparse-state-space-model<br>
<span id='abs_ch'>中文: PD-SSM方法通过结构化稀疏参数化实现了最优有限状态自动机模拟，在保持线性计算复杂度的同时，在状态追踪任务上显著优于现有状态空间模型变体。</span><br>
<span id='abs_en'>English: The proposed PD-SSM method introduces a structured sparse parametrization for state-space models that achieves optimal finite-state automata emulation with linear computational scaling while significantly outperforming existing SSM variants on state tracking tasks.Paperid:290,https://arxiv.org/pdf/2509.22283.pdfGitHubGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2509.22251.pdf' target='_blank'>https://arxiv.org/pdf/2509.22251.pdf</a></span>   <span><a href='https://github.com/yfangZhang/SSKG-LLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifang Zhang, Pengfei Duan, Yiwen Yang, Shengwu Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22251">Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Currently, the main approach for Large Language Models (LLMs) to tackle the hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs typically treat KGs as plain text, extracting only semantic information and limiting their use of the crucial structural aspects of KGs. Another challenge is the gap between the embedding spaces of KGs encoders and LLMs text embeddings, which hinders the effective integration of structured knowledge. To overcome these obstacles, we put forward the SSKG-LLM, an innovative model architecture that is designed to efficiently integrate both the Structural and Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph Encoding (KGE) module to preserve semantics while utilizing structure. Then, the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to understand KGs embeddings. We conduct extensive experiments and provide a detailed analysis to explore how incorporating the structural information of KGs can enhance the factual reasoning abilities of LLMs. Our code are available at https://github.com/yfangZhang/SSKG-LLM.<br>
<br>
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2509.22144.pdf' target='_blank'>https://arxiv.org/pdf/2509.22144.pdf</a></span>   <span><a href='https://github.com/Leon221220/MACC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianzhi Yan, Le Liu, Youcheng Pan, Shiwei Chen, Zike Yuan, Yang Xiang, Buzhou Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22144">From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-Thought (CoT) reasoning improves performance on complex tasks but introduces significant inference latency due to verbosity. We propose Multiround Adaptive Chain-of-Thought Compression (MACC), a framework that leverages the token elasticity phenomenon--where overly small token budgets can paradoxically increase output length--to progressively compress CoTs via multiround refinement. This adaptive strategy allows MACC to determine the optimal compression depth for each input. Our method achieves an average accuracy improvement of 5.6 percent over state-of-the-art baselines, while also reducing CoT length by an average of 47 tokens and significantly lowering latency. Furthermore, we show that test-time performance--accuracy and token length--can be reliably predicted using interpretable features like perplexity and compression rate on the training set. Evaluated across different models, our method enables efficient model selection and forecasting without repeated fine-tuning, demonstrating that CoT compression is both effective and predictable. Our code will be released in https://github.com/Leon221220/MACC.<br>
<span id='abs_ch'>中文：提出的MACC框架通过多轮优化自适应压缩思维链推理，在实现更高准确率和更低延迟的同时，利用可解释特征使性能变得可预测。</span><br>
<span id='abs_en'>English: The proposed MACC framework adaptively compresses Chain-of-Thought reasoning through multiround refinement, achieving higher accuracy with shorter outputs and reduced latency while enabling predictable performance through interpretable features.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2509.21991.pdf' target='_blank'>https://arxiv.org/pdf/2509.21991.pdf</a></span>   <span><a href='https://github.com/nota-github/ERGO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jewon Lee, Wooksu Shin, Seungmin Yang, Ki-Ung Song, DongUk Lim, Jaeyeon Kim, Tae-Ho Kim, Bo-Kyeong Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21991">ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Efficient processing of high-resolution images is crucial for real-world vision-language applications. However, existing Large Vision-Language Models (LVLMs) incur substantial computational overhead due to the large number of vision tokens. With the advent of "thinking with images" models, reasoning now extends beyond text to the visual domain. This capability motivates our two-stage "coarse-to-fine" reasoning pipeline: first, a downsampled image is analyzed to identify task-relevant regions; then, only these regions are cropped at full resolution and processed in a subsequent reasoning stage. This approach reduces computational cost while preserving fine-grained visual details where necessary. A major challenge lies in inferring which regions are truly relevant to a given query. Recent related methods often fail in the first stage after input-image downsampling, due to perception-driven reasoning, where clear visual information is required for effective reasoning. To address this issue, we propose ERGO (Efficient Reasoning & Guided Observation) that performs reasoning-driven perception-leveraging multimodal context to determine where to focus. Our model can account for perceptual uncertainty, expanding the cropped region to cover visually ambiguous areas for answering questions. To this end, we develop simple yet effective reward components in a reinforcement learning framework for coarse-to-fine perception. Across multiple datasets, our approach delivers higher accuracy than the original model and competitive methods, with greater efficiency. For instance, ERGO surpasses Qwen2.5-VL-7B on the V* benchmark by 4.7 points while using only 23% of the vision tokens, achieving a 3x inference speedup. The code and models can be found at: https://github.com/nota-github/ERGO.<br>
<span id='abs_ch'>中文: ERGO采用两阶段推理流程，先识别下采 图像中的任务相关区域，再仅对这些区域进行全分辨率处理，从而以显著降低的计算成本实现更高的准确率。</span><br>
<span id='abs_en'>English: ERGO introduces a two-stage reasoning pipeline that first identifies task-relevant regions in downsampled images and then processes only those areas at full resolution, achieving higher accuracy with significantly reduced computational costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2509.21947.pdf' target='_blank'>https://arxiv.org/pdf/2509.21947.pdf</a></span>   <span><a href='https://github.com/dbsxodud-11/active_attacks' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taeyoung Yun, Pierre-Luc St-Charles, Jinkyoo Park, Yoshua Bengio, Minsu Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21947">Active Attacks: Red-teaming LLMs via Adaptive Environments</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We address the challenge of generating diverse attack prompts for large language models (LLMs) that elicit harmful behaviors (e.g., insults, sexual content) and are used for safety fine-tuning. Rather than relying on manual prompt engineering, attacker LLMs can be trained with reinforcement learning (RL) to automatically generate such prompts using only a toxicity classifier as a reward. However, capturing a wide range of harmful behaviors is a significant challenge that requires explicit diversity objectives. Existing diversity-seeking RL methods often collapse to limited modes: once high-reward prompts are found, exploration of new regions is discouraged. Inspired by the active learning paradigm that encourages adaptive exploration, we introduce \textit{Active Attacks}, a novel RL-based red-teaming algorithm that adapts its attacks as the victim evolves. By periodically safety fine-tuning the victim LLM with collected attack prompts, rewards in exploited regions diminish, which forces the attacker to seek unexplored vulnerabilities. This process naturally induces an easy-to-hard exploration curriculum, where the attacker progresses beyond easy modes toward increasingly difficult ones. As a result, Active Attacks uncovers a wide range of local attack modes step by step, and their combination achieves wide coverage of the multi-mode distribution. Active Attacks, a simple plug-and-play module that seamlessly integrates into existing RL objectives, unexpectedly outperformed prior RL-based methods -- including GFlowNets, PPO, and REINFORCE -- by improving cross-attack success rates against GFlowNets, the previous state-of-the-art, from 0.07% to 31.28% (a relative gain greater than $400\ \times$) with only a 6% increase in computation. Our code is publicly available \href{https://github.com/dbsxodud-11/active_attacks}{here}.<br>
<span id='abs_ch'>中文: 本文提出Active Attacks算法，通过周期性安全微调受害者模型来迫使攻击者探索新漏洞，从而自适应生成多 化有害提示，相比之前方法将攻击成功率提升了400倍。</span><br>
<span id='abs_en'>English: This paper introduces Active Attacks, a reinforcement learning-based red-teaming algorithm that adaptively generates diverse harmful prompts by periodically fine-tuning the victim model, forcing the attacker to explore new vulnerabilities and achieving a 400-fold improvement in attack success rates over previous methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2509.21848.pdf' target='_blank'>https://arxiv.org/pdf/2509.21848.pdf</a></span>   <span><a href='https://github.com/tjoo512/graph-of-agents' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taejong Joo, Shu Ishida, Ivan Sosnovik, Bryan Lim, Sahand Rezaei-Shoshtari, Adam Gaier, Robert Giaquinto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21848">Graph of Agents: Principled Long Context Modeling by Emergent Multi-Agent Collaboration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As a model-agnostic approach to long context modeling, multi-agent systems can process inputs longer than a large language model's context window without retraining or architectural modifications. However, their performance often heavily relies on hand-crafted multi-agent collaboration strategies and prompt engineering, which limit generalizability. In this work, we introduce a principled framework that formalizes the model-agnostic long context modeling problem as a compression problem, yielding an information-theoretic compression objective. Building on this framework, we propose Graph of Agents (GoA), which dynamically constructs an input-dependent collaboration structure that maximizes this objective. For Llama 3.1 8B and Qwen3 8B across six document question answering benchmarks, GoA improves the average $F_1$ score of retrieval-augmented generation by 5.7\% and a strong multi-agent baseline using a fixed collaboration structure by 16.35\%, respectively. Even with only a 2K context window, GoA surpasses the 128K context window Llama 3.1 8B on LongBench, showing a dramatic increase in effective context length. Our source code is available at https://github.com/tjoo512/graph-of-agents.<br>
<span id='abs_ch'>中文: 本文提出Graph of Agents (GoA)框架，将模型 关的长上下文建模形式化为压缩问题，通过动态构建输入依赖的协作结构来优化信息论目 ，在多个基准测试中显著超越了现有方法。</span><br>
<span id='abs_en'>English: This paper introduces Graph of Agents (GoA), a principled framework that formalizes model-agnostic long context modeling as a compression problem and dynamically constructs input-dependent collaboration structures to maximize information-theoretic objectives, significantly outperforming existing methods across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2509.21792.pdf' target='_blank'>https://arxiv.org/pdf/2509.21792.pdf</a></span>   <span><a href='https://github.com/yedaotian9/GRPO_speculative' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Zhang, Ning Lv, Teng Wang, Jisheng Dang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21792">FastGRPO: Accelerating Policy Optimization via Concurrency-aware Speculative Decoding and Online Draft Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Group relative policy optimization (GRPO) has demonstrated significant potential in improving the reasoning capabilities of large language models (LLMs) via reinforcement learning. However, its practical deployment is impeded by an excessively slow training process, primarily attributed to the computationally intensive autoregressive generation of multiple responses per query, which makes the generation phase the primary performance bottleneck. Although speculative decoding presents a promising direction for acceleration, its direct application in GRPO achieves limited speedup under high-concurrency training conditions. To overcome this limitation, we propose a concurrency-aware speculative decoding framework that dynamically adjusts the drafting and verification strategy according to real-time concurrency levels, thereby maximizing the acceleration of the generation process. Furthermore, to address performance degradation arising from distributional drift between the evolving target model and the fixed draft model during training, we introduce an online draft learning mechanism that enables the draft model to continuously adapt using feedback signals from the target model. Experimental results across multiple mathematical reasoning datasets and models demonstrate that the proposed method achieves end-to-end speedups of 2.35x to 2.72x, significantly surpassing baseline approaches in efficiency. The code is available at https://github.com/yedaotian9/GRPO_speculative.<br>
<span id='abs_ch'>中文: 该 究提出的并发感知推测解 框架通过在线草稿学 机制，能够 据实时并发水平动态调整策略并持续优化草稿模型，在数学推理任务中实现了2.35至2.72倍的端到端 速效果。</span><br>
<span id='abs_en'>English: The proposed concurrency-aware speculative decoding framework with online draft learning accelerates GRPO training by dynamically adapting to real-time concurrency levels and continuously updating the draft model, achieving 2.35x-2.72x speedup across mathematical reasoning tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2509.21782.pdf' target='_blank'>https://arxiv.org/pdf/2509.21782.pdf</a></span>   <span><a href='https://github.com/jinliang-byte/webssrbench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junliang Liu, Jingyu Xiao, Wenxin Tang, Wenxuan Wang, Zhixian Wang, Minrui Zhang, Shuanghe Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21782">Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal large language models (MLLMs) are increasingly positioned as AI collaborators for building complex web-related applications like GUI agents and front-end code generation. However, existing benchmarks largely emphasize visual perception or UI code generation, showing insufficient evaluation on the reasoning, robustness and safety capability required for end-to-end web applications. To bridge the gap, we introduce a comprehensive web understanding benchmark, named WebRSSBench, that jointly evaluates Reasoning, Robustness, and Safety across eight tasks, such as position relationship reasoning, color robustness, and safety critical detection, etc. The benchmark is constructed from 729 websites and contains 3799 question answer pairs that probe multi-step inference over page structure, text, widgets, and safety-critical interactions. To ensure reliable measurement, we adopt standardized prompts, deterministic evaluation scripts, and multi-stage quality control combining automatic checks with targeted human verification. We evaluate 12 MLLMs on WebRSSBench. The results reveal significant gaps, models still struggle with compositional and cross-element reasoning over realistic layouts, show limited robustness when facing perturbations in user interfaces and content such as layout rearrangements or visual style shifts, and are rather conservative in recognizing and avoiding safety critical or irreversible actions. Our code is available at https://github.com/jinliang-byte/webssrbench.<br>
<span id='abs_ch'>中文总结：WebRSSBench基准测试填补了多模态大语言模型在网页应用推理、鲁棒性和安全性评估方面的空白，通过八项任务的综合测试揭示了现有模型在复杂网页理解能力上的显著不足。</span><br>
<span id='abs_en'>English Summary: The WebRSSBench benchmark addresses the gap in evaluating multimodal large language models' reasoning, robustness, and safety for web applications, revealing significant shortcomings in current models' capabilities through comprehensive testing across eight tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2509.21766.pdf' target='_blank'>https://arxiv.org/pdf/2509.21766.pdf</a></span>   <span><a href='https://github.com/StarDewXXX/UltraHorizon' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haotian Luo, Huaisong Zhang, Xuelin Zhang, Haoyu Wang, Zeyu Qin, Wenjie Lu, Guozheng Ma, Haiying He, Yingsha Xie, Qiyang Zhou, Zixuan Hu, Hongze Mi, Yibo Wang, Naiqiang Tan, Hong Chen, Yi R. Fung, Chun Yuan, Li Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21766">UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Autonomous agents have recently achieved remarkable progress across diverse domains, yet most evaluations focus on short-horizon, fully observable tasks. In contrast, many critical real-world tasks, such as large-scale software development, commercial investment, and scientific discovery, unfold in long-horizon and partially observable scenarios where success hinges on sustained reasoning, planning, memory management, and tool use. Existing benchmarks rarely capture these long-horizon challenges, leaving a gap in systematic evaluation. To bridge this gap, we introduce \textbf{UltraHorizon} a novel benchmark that measures the foundational capabilities essential for complex real-world challenges. We use exploration as a unifying task across three distinct environments to validate these core competencies. Agents are designed in long-horizon discovery tasks where they must iteratively uncover hidden rules through sustained reasoning, planning, memory and tools management, and interaction with environments. Under the heaviest scale setting, trajectories average \textbf{200k+} tokens and \textbf{400+} tool calls, whereas in standard configurations they still exceed \textbf{35k} tokens and involve more than \textbf{60} tool calls on average. Our extensive experiments reveal that LLM-agents consistently underperform in these settings, whereas human participants achieve higher scores, underscoring a persistent gap in agents' long-horizon abilities. We also observe that simple scaling fails in our task. To better illustrate the failure of agents, we conduct an in-depth analysis of collected trajectories. We identify eight types of errors and attribute them to two primary causes: in-context locking and functional fundamental capability gaps. \href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available here.}<br>
<span id='abs_ch'>中文: UltraHorizon基准测试旨在评估自主智能体在需要持续推理和工具使用的长周期、部分可观测任务中的表现，揭示了尽管经过大规模扩展，AI智能体与人类之间仍存在显著性能差距。</span><br>
<span id='abs_en'>English: The UltraHorizon benchmark is introduced to evaluate autonomous agents in long-horizon, partially observable tasks requiring sustained reasoning and tool use, revealing significant performance gaps between AI agents and humans despite extensive scaling.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2509.21738.pdf' target='_blank'>https://arxiv.org/pdf/2509.21738.pdf</a></span>   <span><a href='https://github.com/Mehwish4593/LFA-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehwish Mehmood, Ivor Spence, Muhammad Fahim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21738">LFA-Net: A Lightweight Network with LiteFusion Attention for Retinal Vessel Segmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Lightweight retinal vessel segmentation is important for the early diagnosis of vision-threatening and systemic diseases, especially in a real-world clinical environment with limited computational resources. Although segmentation methods based on deep learning are improving, existing models are still facing challenges of small vessel segmentation and high computational costs. To address these challenges, we proposed a new vascular segmentation network, LFA-Net, which incorporates a newly designed attention module, LiteFusion-Attention. This attention module incorporates residual learning connections, Vision Mamba-inspired dynamics, and modulation-based attention, enabling the model to capture local and global context efficiently and in a lightweight manner. LFA-Net offers high performance with 0.11 million parameters, 0.42 MB memory size, and 4.46 GFLOPs, which make it ideal for resource-constrained environments. We validated our proposed model on DRIVE, STARE, and CHASE_DB with outstanding performance in terms of dice scores of 83.28, 87.44, and 84.50% and Jaccard indices of 72.85, 79.31, and 74.70%, respectively. The code of LFA-Net is available online https://github.com/Mehwish4593/LFA-Net.<br>
<span id='abs_ch'>Chinese:  究人员开发了LFA-Net轻量化视网膜血管分割网络，采用创新的LiteFusion-Attention模块，在低计算资源下实现高性能，非常适合临床诊断应用。</span><br>
<span id='abs_en'>English: Researchers developed LFA-Net, a lightweight retinal vessel segmentation network featuring the innovative LiteFusion-Attention module, which achieves high performance with minimal computational resources, making it ideal for clinical diagnostics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2509.21670.pdf' target='_blank'>https://arxiv.org/pdf/2509.21670.pdf</a></span>   <span><a href='https://github.com/lanl/MORPH' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mahindra Singh Rautela, Alexander Most, Siddharth Mansingh, Bradley C. Love, Ayan Biswas, Diane Oyen, Earl Lawrence
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21670">MORPH: Shape-agnostic PDE Foundation Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce MORPH, a shape-agnostic, autoregressive foundation model for partial differential equations (PDEs). MORPH is built on a convolutional vision transformer backbone that seamlessly handles heterogeneous spatiotemporal datasets of varying data dimensionality (1D--3D) at different resolutions, multiple fields with mixed scalar and vector components. The architecture combines (i) component-wise convolution, which jointly processes scalar and vector channels to capture local interactions, (ii) inter-field cross-attention, which models and selectively propagates information between different physical fields, (iii) axial attentions, which factorizes full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity. We pretrain multiple model variants on a diverse collection of heterogeneous PDE datasets and evaluate transfer to a range of downstream prediction tasks. Using both full-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPH outperforms models trained from scratch in both zero-shot and full-shot generalization. Across extensive evaluations, MORPH matches or surpasses strong baselines and recent state-of-the-art models. Collectively, these capabilities present a flexible and powerful backbone for learning from heterogeneous and multimodal nature of scientific observations, charting a path toward scalable and data-efficient scientific machine learning. The source code, datasets, and models are publicly available at https://github.com/lanl/MORPH.<br>
<br>
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2509.21500.pdf' target='_blank'>https://arxiv.org/pdf/2509.21500.pdf</a></span>   <span><a href='https://github.com/Jun-Kai-Zhang/rubrics.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junkai Zhang, Zihao Wang, Lin Gui, Swarnashree Mysore Sathyendra, Jaehwan Jeong, Victor Veitch, Wei Wang, Yunzhong He, Bing Liu, Lifeng Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21500">Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement fine-tuning (RFT) often suffers from \emph{reward over-optimization}, where a policy model hacks the reward signals to achieve high scores while producing low-quality outputs. Our theoretical analysis shows that the key lies in reward misspecification at the high-reward tail: the inability to reliably distinguish Excellent responses from merely Great ones. This motivate us to focus on the high-reward region. However, such tail examples are scarce under the base LLM. While off-policy exemplars (e.g. from stronger models or rewrites) are easier to obtain, naively training on them yields a misspecified reward for the policy we aim to align. To address this, we study rubric-based rewards. By design, rubrics can leverage off-policy examples while remaining insensitive to their artifacts. To elicit rubrics that capture the high-reward tail, we highlight the importance of distinguishing among great and diverse responses, and introduce a workflow to implement this idea. We empirically demonstrate that rubric-based rewards substantially mitigate reward over-optimization and deliver effective LLM post-training improvements. Our code can be accessed at https://github.com/Jun-Kai-Zhang/rubrics.git .<br>
<span id='abs_ch'>中文: 强化微调常面临奖励过优化问题，模型会利用奖励信号获得高分却输出低质量内容，但基于规则的奖励设计能有效缓解此问题，通过利用非策略示例并避免其伪影，从而提升模型对齐效果。</span><br>
<span id='abs_en'>English: Reinforcement fine-tuning often faces reward over-optimization, where models exploit reward signals to score high despite poor outputs, but using rubric-based rewards effectively mitigates this issue and enhances model alignment by leveraging off-policy examples without succumbing to their artifacts.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2509.21473.pdf' target='_blank'>https://arxiv.org/pdf/2509.21473.pdf</a></span>   <span><a href='https://github.com/MAGICS-LAB/hallucination' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hude Liu, Jerry Yao-Chieh Hu, Jennifer Yuntong Zhang, Zhao Song, Han Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21473">Are Hallucinations Bad Estimations?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We formalize hallucinations in generative models as failures to link an estimate to any plausible cause. Under this interpretation, we show that even loss-minimizing optimal estimators still hallucinate. We confirm this with a general high probability lower bound on hallucinate rate for generic data distributions. This reframes hallucination as structural misalignment between loss minimization and human-acceptable outputs, and hence estimation errors induced by miscalibration. Experiments on coin aggregation, open-ended QA, and text-to-image support our theory.<br>
<span id='abs_ch'>Chinese: 该 究将生成模型中的幻觉重新定义为损失最小化与人类期望之间的结构性错配，证明即使是最优估计器也会  准误差而产生幻觉。</span><br>
<span id='abs_en'>English: The study redefines hallucinations in generative models as structural misalignment between loss minimization and human expectations, demonstrating that even optimal estimators hallucinate due to estimation errors from miscalibration.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2509.21377.pdf' target='_blank'>https://arxiv.org/pdf/2509.21377.pdf</a></span>   <span><a href='https://github.com/zzzmmm-svg/DMTF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinfeng Yu, Hailong Zhang, Meiling Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21377">Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Audiovisual embodied navigation enables robots to locate audio sources by dynamically integrating visual observations from onboard sensors with the auditory signals emitted by the target. The core challenge lies in effectively leveraging multimodal cues to guide navigation. While prior works have explored basic fusion of visual and audio data, they often overlook deeper perceptual context. To address this, we propose the Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation (DMTF-AVN). Our approach uses a multi-target architecture coupled with a refined Transformer mechanism to filter and selectively fuse cross-modal information. Extensive experiments on the Replica and Matterport3D datasets demonstrate that DMTF-AVN achieves state-of-the-art performance, outperforming existing methods in success rate (SR), path efficiency (SPL), and scene adaptation (SNA). Furthermore, the model exhibits strong scalability and generalizability, paving the way for advanced multimodal fusion strategies in robotic navigation. The code and videos are available at https://github.com/zzzmmm-svg/DMTF.<br>
<span id='abs_ch'>Chinese: 提出的DMTF-AVN模型通过多目 Transformer架构动态融合视觉与听觉线索，在多个基准数据集上实现了导航精度与适应性的最先进性能。</span><br>
<span id='abs_en'>English: The proposed DMTF-AVN model advances audiovisual navigation by dynamically fusing visual and auditory cues through a multi-target Transformer architecture, achieving state-of-the-art performance in accuracy and adaptability across benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2509.21371.pdf' target='_blank'>https://arxiv.org/pdf/2509.21371.pdf</a></span>   <span><a href='https://github.com/dayuyang1999/ReGeS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dayu Yang, Hui Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21371">ReGeS: Reciprocal Retrieval-Generation Synergy for Conversational Recommender Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Connecting conversation with external domain knowledge is vital for conversational recommender systems (CRS) to correctly understand user preferences. However, existing solutions either require domain-specific engineering, which limits flexibility, or rely solely on large language models, which increases the risk of hallucination. While Retrieval-Augmented Generation (RAG) holds promise, its naive use in CRS is hindered by noisy dialogues that weaken retrieval and by overlooked nuances among similar items. We propose ReGeS, a reciprocal Retrieval-Generation Synergy framework that unifies generation-augmented retrieval to distill informative user intent from conversations and retrieval-augmented generation to differentiate subtle item features. This synergy obviates the need for extra annotations, reduces hallucinations, and simplifies continuous updates. Experiments on multiple CRS benchmarks show that ReGeS achieves state-of-the-art performance in recommendation accuracy, demonstrating the effectiveness of reciprocal synergy for knowledge-intensive CRS tasks.<br>
<span id='abs_ch'>Chinese: ReGeS框架通过检索与生成的协同作用，从对话中提炼用户意图并区分细微物品特征， 需额外 注且减少幻觉，在多个基准测试中实现了最先进的推荐准确性。</span><br>
<span id='abs_en'>English: The ReGeS framework introduces a reciprocal synergy between retrieval and generation to enhance conversational recommender systems by distilling user intent and differentiating item features, achieving state-of-the-art accuracy without extra annotations or hallucinations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2509.21359.pdf' target='_blank'>https://arxiv.org/pdf/2509.21359.pdf</a></span>   <span><a href='https://github.com/SJTU-DMTai/RAG-CSM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiale Deng, Yanyan Shen, Ziyuan Pei, Youmin Chen, Linpeng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21359">Influence Guided Context Selection for Effective Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-Augmented Generation (RAG) addresses large language model (LLM) hallucinations by grounding responses in external knowledge, but its effectiveness is compromised by poor-quality retrieved contexts containing irrelevant or noisy information. While existing approaches attempt to improve performance through context selection based on predefined context quality assessment metrics, they show limited gains over standard RAG. We attribute this limitation to their failure in holistically utilizing available information (query, context list, and generator) for comprehensive quality assessment. Inspired by recent advances in data selection, we reconceptualize context quality assessment as an inference-time data valuation problem and introduce the Contextual Influence Value (CI value). This novel metric quantifies context quality by measuring the performance degradation when removing each context from the list, effectively integrating query-aware relevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI value eliminates complex selection hyperparameter tuning by simply retaining contexts with positive CI values. To address practical challenges of label dependency and computational overhead, we develop a parameterized surrogate model for CI value prediction during inference. The model employs a hierarchical architecture that captures both local query-context relevance and global inter-context interactions, trained through oracle CI value supervision and end-to-end generator feedback. Extensive experiments across 8 NLP tasks and multiple LLMs demonstrate that our context selection method significantly outperforms state-of-the-art baselines, effectively filtering poor-quality contexts while preserving critical information. Code is available at https://github.com/SJTU-DMTai/RAG-CSM.<br>
<span id='abs_ch'>中文: 检索增强生成（RAG）通过引入外部知识减少大语言模型的幻觉，但其效果常受低质量检索上下文的制约。为此， 究者提出上下文影响力值（CI值）这一新指 ，通过量化移除上下文导致的性能下降来综合评估质量， 需复杂参数调整即可有效过滤劣质上下文，在多种自然语言处理任务中显著优于现有方法。</span><br>
<span id='abs_en'>English: Retrieval-Augmented Generation (RAG) mitigates LLM hallucinations by incorporating external knowledge, yet its efficacy is hindered by low-quality retrieved contexts. To address this, the authors introduce the Contextual Influence Value (CI value), a novel metric that holistically assesses context quality by measuring performance degradation upon removal, enabling effective filtering without complex parameter tuning and significantly outperforming existing methods across diverse NLP tasks.Paperid:334,https://arxiv.org/pdf/2509.21342.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2509.21342.pdf' target='_blank'>https://arxiv.org/pdf/2509.21342.pdf</a></span>   <span><a href='https://github.com/Zhhuizhe/SGNNBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huizhe Zhang, Jintang Li, Yuchang Zhu, Liang Chen, Li Kuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21342">SGNNBench: A Holistic Evaluation of Spiking Graph Neural Network on Large-scale Graph</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Graph Neural Networks (GNNs) are exemplary deep models designed for graph data. Message passing mechanism enables GNNs to effectively capture graph topology and push the performance boundaries across various graph tasks. However, the trend of developing such complex machinery for graph representation learning has become unsustainable on large-scale graphs. The computational and time overhead make it imperative to develop more energy-efficient GNNs to cope with the explosive growth of real-world graphs. Spiking Graph Neural Networks (SGNNs), which integrate biologically plausible learning via unique spike-based neurons, have emerged as a promising energy-efficient alternative. Different layers communicate with sparse and binary spikes, which facilitates computation and storage of intermediate graph representations. Despite the proliferation of SGNNs proposed in recent years, there is no systematic benchmark to explore the basic design principles of these brain-inspired networks on the graph data. To bridge this gap, we present SGNNBench to quantify progress in the field of SGNNs. Specifically, SGNNBench conducts an in-depth investigation of SGNNs from multiple perspectives, including effectiveness, energy efficiency, and architectural design. We comprehensively evaluate 9 state-of-the-art SGNNs across 18 datasets. Regarding efficiency, we empirically compare these baselines w.r.t model size, memory usage, and theoretical energy consumption to reveal the often-overlooked energy bottlenecks of SGNNs. Besides, we elaborately investigate the design space of SGNNs to promote the development of a general SGNN paradigm.<br>
<span id='abs_ch'>中文: 图神经网络在大规模图数据上计算开销不可持续， 此出现了利用脉冲进行高效计算的节能型脉冲图神经网络，但缺乏系统基准促使SGNNBench的建立，从性能、能效和架构设计多角度进行全面评估。</span><br>
<span id='abs_en'>English: Graph Neural Networks face unsustainable computational demands on large-scale graphs, prompting the emergence of energy-efficient Spiking Graph Neural Networks (SGNNs) that use binary spikes for efficient processing, though a lack of systematic benchmarking led to the creation of SGNNBench for comprehensive evaluation across effectiveness, efficiency, and design.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2509.21339.pdf' target='_blank'>https://arxiv.org/pdf/2509.21339.pdf</a></span>   <span><a href='https://github.com/JiahaoZhang666/CSD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Zhang, Wenzhe Yin, Shujian Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21339">Cross-Modal Retrieval with Cauchy-Schwarz Divergence</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Effective cross-modal retrieval requires robust alignment of heterogeneous data types. Most existing methods focus on bi-modal retrieval tasks and rely on distributional alignment techniques such as Kullback-Leibler divergence, Maximum Mean Discrepancy, and correlation alignment. However, these methods often suffer from critical limitations, including numerical instability, sensitivity to hyperparameters, and their inability to capture the full structure of the underlying distributions. In this paper, we introduce the Cauchy-Schwarz (CS) divergence, a hyperparameter-free measure that improves both training stability and retrieval performance. We further propose a novel Generalized CS (GCS) divergence inspired by HÃ¶lder's inequality. This extension enables direct alignment of three or more modalities within a unified mathematical framework through a bidirectional circular comparison scheme, eliminating the need for exhaustive pairwise comparisons. Extensive experiments on six benchmark datasets demonstrate the effectiveness of our method in both bi-modal and tri-modal retrieval tasks. The code of our CS/GCS divergence is publicly available at https://github.com/JiahaoZhang666/CSD.<br>
<br>
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2509.21266.pdf' target='_blank'>https://arxiv.org/pdf/2509.21266.pdf</a></span>   <span><a href='https://github.com/ssssszj/RCA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijian Shao, Haiyang Shen, Mugeng Liu, Gecheng Fu, Yaoqi Guo, Yanfeng Wang, Yun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21266">Grounding AI Explanations in Experience: A Reflective Cognitive Architecture for Clinical Decision Support</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Effective disease prediction in modern healthcare demands the twin goals of high accuracy and transparent, clinically meaningful explanations. Existing machine learning and large language model (LLM) based approaches often struggle to balance these goals. Many models yield accurate but unclear statistical outputs, while others generate fluent but statistically unsupported narratives, often undermining both the validity of the explanation and the predictive accuracy itself. This shortcoming comes from a shallow interaction with the data, preventing the development of a deep, detailed understanding similar to a human expert's. We argue that high accuracy and high-quality explanations are not separate objectives but are mutually reinforcing outcomes of a model that develops a deep, direct understanding of the data. To achieve this, we propose the Reflective Cognitive Architecture (RCA), a novel framework that coordinates multiple LLMs to learn from direct experience. RCA features an iterative rule refinement mechanism that improves its logic from prediction errors and a distribution-aware rules check mechanism that bases its reasoning in the dataset's global statistics. By using predictive accuracy as a signal to drive deeper comprehension, RCA builds a strong internal model of the data. We evaluated RCA on one private and two public datasets against 22 baselines. The results demonstrate that RCA not only achieves state-of-the-art accuracy and robustness with a relative improvement of up to 40\% over the baseline but, more importantly, leverages this deep understanding to excel in generating explanations that are clear, logical, evidence-based, and balanced, highlighting its potential for creating genuinely trustworthy clinical decision support systems. The code is available at \https://github.com/ssssszj/RCA.<br>
<span id='abs_ch'>Chinese: 反射认知架构（RCA）是一种新颖框架，通过协调多个大语言模型，利用迭代规则优化和分布感知推理机制，在实现顶尖预测精度的同时生成清晰可信的临床解释，为构建真正可 的医疗决策系统提供了突 性方案。</span><br>
<span id='abs_en'>English: The Reflective Cognitive Architecture (RCA) is a novel framework that coordinates multiple LLMs to achieve both state-of-the-art predictive accuracy and high-quality, evidence-based explanations by developing a deep understanding of data through iterative rule refinement and distribution-aware reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2509.21265.pdf' target='_blank'>https://arxiv.org/pdf/2509.21265.pdf</a></span>   <span><a href='https://github.com/CUHK-AIM-Group/MedVSR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Liu, Guolei Sun, Cheng Wang, Yixuan Yuan, Ender Konukoglu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21265">MedVSR: Medical Video Super-Resolution with Cross State-Space Propagation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>High-resolution (HR) medical videos are vital for accurate diagnosis, yet are hard to acquire due to hardware limitations and physiological constraints. Clinically, the collected low-resolution (LR) medical videos present unique challenges for video super-resolution (VSR) models, including camera shake, noise, and abrupt frame transitions, which result in significant optical flow errors and alignment difficulties. Additionally, tissues and organs exhibit continuous and nuanced structures, but current VSR models are prone to introducing artifacts and distorted features that can mislead doctors. To this end, we propose MedVSR, a tailored framework for medical VSR. It first employs Cross State-Space Propagation (CSSP) to address the imprecise alignment by projecting distant frames as control matrices within state-space models, enabling the selective propagation of consistent and informative features to neighboring frames for effective alignment. Moreover, we design an Inner State-Space Reconstruction (ISSR) module that enhances tissue structures and reduces artifacts with joint long-range spatial feature learning and large-kernel short-range information aggregation. Experiments across four datasets in diverse medical scenarios, including endoscopy and cataract surgeries, show that MedVSR significantly outperforms existing VSR models in reconstruction performance and efficiency. Code released at https://github.com/CUHK-AIM-Group/MedVSR.<br>
<span id='abs_ch'>中文：提出的MedVSR框架通过跨状态空间 播解决特征对齐问题，并结合内部状态空间重建增强组织结构，有效应对医学视频超分辨率中的独特挑战，在多种医疗场景中展现出卓越性能。</span><br>
<span id='abs_en'>English: The proposed MedVSR framework addresses unique challenges in medical video super-resolution, such as alignment difficulties and artifacts, through Cross State-Space Propagation for feature alignment and Inner State-Space Reconstruction for enhancing tissue structures, demonstrating superior performance across diverse medical scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2509.21199.pdf' target='_blank'>https://arxiv.org/pdf/2509.21199.pdf</a></span>   <span><a href='https://github.com/KaiyangWan/InfoQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyang Wan, Lang Gao, Honglin Mu, Preslav Nakov, Yuxia Wang, Xiuying Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21199">A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multi-Hop Question Answering (MHQA) requires integrating dispersed, interdependent evidence through sequential reasoning under noise. This task is challenging for LLMs as they have a finite per-pass output capacity, beyond which the integration of task-relevant evidence proves unreliable. Consequently, the single-pass reasoning paradigm is inherently vulnerable to this capacity overflow. To formalize this bottleneck, our analysis establishes a Fano-style accuracy upper bound, defining a theoretical performance ceiling for single-pass LLMs. This bound reveals that accuracy inevitably collapses once task complexity exceeds model capacity, providing general principles for capacity-aware representation and structuring of MHQA in LLMs. Building on these principles, we introduce a proof-of-concept multi-call framework for MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware task decomposition with active pruning of prior reasoning traces, keeping the information load within the single-pass limit. It further achieves robustness by a dependency-explicit workflow that enables precise control over the reasoning path. We construct a stringent and noise-rich benchmark to validate our theory and framework. Experimental results show that model behavior aligns with our predicted capacity curves while InfoQA achieves consistent performance improvements. We hope our work inspires more LLM multi-step reasoning methods: \faGithub \href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.<br>
<span id='abs_ch'>中文摘要：该 究揭示了单次处理大语言模型在多跳问答中的能力瓶颈，提出了名为InfoQA的多轮调用框架，通过能力感知的任务分解保持单步准确性，并在高难度基准测试中实现了稳定性能提升。</span><br>
<span id='abs_en'>English Summary: The study identifies a capacity bottleneck in single-pass LLMs for multi-hop question answering, proposing a multi-call framework called InfoQA that maintains accuracy through capacity-aware task decomposition and achieves robust performance on a challenging benchmark.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2509.21193.pdf' target='_blank'>https://arxiv.org/pdf/2509.21193.pdf</a></span>   <span><a href='https://github.com/tangxiangru/Eigen-1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangru Tang, Wanghan Xu, Yujie Wang, Zijie Guo, Daniel Shao, Jiapeng Chen, Cixuan Zhang, Ziyi Wang, Lixin Zhang, Guancheng Wan, Wenlong Zhang, Lei Bai, Zhenfei Yin, Philip Torr, Hanrui Wang, Di Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21193">Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for Scientific Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have recently shown strong progress on scientific reasoning, yet two major bottlenecks remain. First, explicit retrieval fragments reasoning, imposing a hidden "tool tax" of extra tokens and steps. Second, multi-agent pipelines often dilute strong solutions by averaging across all candidates. We address these challenges with a unified framework that combines implicit retrieval and structured collaboration. At its foundation, a Monitor-based retrieval module operates at the token level, integrating external knowledge with minimal disruption to reasoning. On top of this substrate, Hierarchical Solution Refinement (HSR) iteratively designates each candidate as an anchor to be repaired by its peers, while Quality-Aware Iterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's Last Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\% accuracy -- the highest reported to date, surpassing the strongest agent baseline by 13.4 points and leading frontier LLMs by up to 18.1 points, while simultaneously reducing token usage by 53.5\% and agent steps by 43.7\%. Results on SuperGPQA and TRQA confirm robustness across domains. Error analysis shows that reasoning failures and knowledge gaps co-occur in over 85\% of cases, while diversity analysis reveals a clear dichotomy: retrieval tasks benefit from solution variety, whereas reasoning tasks favor consensus. Together, these findings demonstrate how implicit augmentation and structured refinement overcome the inefficiencies of explicit tool use and uniform aggregation. Code is available at: https://github.com/tangxiangru/Eigen-1.<br>
<span id='abs_ch'>中文: 该框架通过融合隐式检索与结构化协作，克服了大型语言模型中显式检索和均匀聚合的低效问题，在显著降低计算成本的同时实现了最优准确率。</span><br>
<span id='abs_en'>English: This framework overcomes the inefficiencies of explicit retrieval and uniform aggregation in LLMs by integrating implicit retrieval with structured collaboration, achieving state-of-the-art accuracy while significantly reducing computational costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2509.21117.pdf' target='_blank'>https://arxiv.org/pdf/2509.21117.pdf</a></span>   <span><a href='https://github.com/TrustJudge/TrustJudge' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yidong Wang, Yunze Song, Tingyuan Zhu, Xuanwang Zhang, Zhuohao Yu, Hao Chen, Chiyu Song, Qiufeng Wang, Cunxiang Wang, Zhen Wu, Xinyu Dai, Yue Zhang, Wei Ye, Shikun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21117">TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The adoption of Large Language Models (LLMs) as automated evaluators (LLM-as-a-judge) has revealed critical inconsistencies in current evaluation frameworks. We identify two fundamental types of inconsistencies: (1) Score-Comparison Inconsistency, where lower-rated responses outperform higher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity Inconsistency, manifested through circular preference chains (A>B>C>A) and equivalence contradictions (A=B=C\neq A). We argue that these issues come from information loss in discrete rating systems and ambiguous tie judgments during pairwise evaluation. We propose TrustJudge, a probabilistic framework that addresses these limitations through two key innovations: 1) distribution-sensitive scoring that computes continuous expectations from discrete rating probabilities, preserving information entropy for more precise scoring, and 2) likelihood-aware aggregation that resolves transitivity violations using bidirectional preference probabilities or perplexity. We also formalize the theoretical limitations of current LLM-as-a-judge frameworks and demonstrate how TrustJudge's components overcome them. When evaluated with Llama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces Score-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise Transitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining higher evaluation accuracy. Our work provides the first systematic analysis of evaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both theoretical insights and practical solutions for reliable automated assessment. The framework demonstrates consistent improvements across various model architectures and scales, enabling more trustworthy LLM evaluation without requiring additional training or human annotations. The codes can be found at https://github.com/TrustJudge/TrustJudge.<br>
<span id='abs_ch'>中文: 大型语言模型作为自动评估器时存在评分比较和成对 递性不一致的问题，TrustJudge通过概率框架有效减少了这些不一致性，并提高了评估准确性。</span><br>
<span id='abs_en'>English: The adoption of LLMs as automated evaluators reveals critical inconsistencies in current frameworks, which TrustJudge addresses through a probabilistic approach that reduces score-comparison and pairwise transitivity inconsistencies while improving evaluation accuracy.Paperid:355,https://arxiv.org/pdf/2509.21102.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2509.21070.pdf' target='_blank'>https://arxiv.org/pdf/2509.21070.pdf</a></span>   <span><a href='https://github.com/QizhiPei/ScaleDiff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qizhi Pei, Zhuoshi Pan, Honglin Lin, Xin Gao, Yu Li, Zinan Tang, Conghui He, Rui Yan, Lijun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21070">ScaleDiff: Scaling Difficult Problems for Advanced Mathematical Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Reasoning Models (LRMs) have shown impressive capabilities in complex problem-solving, often benefiting from training on difficult mathematical problems that stimulate intricate reasoning. Recent efforts have explored automated synthesis of mathematical problems by prompting proprietary models or large-scale open-source models from seed data or inherent mathematical concepts. However, scaling up these methods remains challenging due to their high computational/API cost, complexity of prompting, and limited difficulty level of the generated problems. To overcome these limitations, we propose ScaleDiff, a simple yet effective pipeline designed to scale the creation of difficult problems. We efficiently identify difficult problems from existing datasets with only a single forward pass using an adaptive thinking model, which can perceive problem difficulty and automatically switch between "Thinking" and "NoThinking" modes. We then train a specialized difficult problem generator (DiffGen-8B) on this filtered difficult data, which can produce new difficult problems in large scale, eliminating the need for complex, per-instance prompting and its associated high API costs. Fine-tuning Qwen2.5-Math-7B-Instruct on the ScaleDiff-Math dataset yields a substantial performance increase of 11.3% compared to the original dataset and achieves a 65.9% average accuracy on AIME'24, AIME'25, HMMT-Feb'25, BRUMO'25, and MATH500, outperforming recent strong LRMs like OpenThinker3. Notably, this performance is achieved using the cost-efficient Qwen3-8B model as a teacher, demonstrating that our pipeline can effectively transfer advanced reasoning capabilities without relying on larger, more expensive teacher models. Furthermore, we observe a clear scaling phenomenon in model performance on difficult benchmarks as the quantity of difficult problems increases. Code: https://github.com/QizhiPei/ScaleDiff.<br>
<span id='abs_ch'>中文: ScaleDiff是一种高效且成本低廉的流程，通过自适应思维模型筛选现有数据集中的难题并训练专门生成器， 需昂贵资源即可大规模创建高难度数学问题，显著提升模型在复杂推理任务中的表现。</span><br>
<span id='abs_en'>English: ScaleDiff is a cost-effective pipeline that automates the creation of challenging mathematical problems by filtering existing datasets with an adaptive thinking model and training a specialized generator, significantly boosting model performance on difficult benchmarks without expensive resources.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2509.20961.pdf' target='_blank'>https://arxiv.org/pdf/2509.20961.pdf</a></span>   <span><a href='https://github.com/sarmistha-D/FASTER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarmistha Das, R E Zera Marveen Lyngkhoi, Sriparna Saha, Alka Maurya
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20961">Unlocking Financial Insights: An advanced Multimodal Summarization with Multimodal Output Framework for Financial Advisory Videos</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The dynamic propagation of social media has broadened the reach of financial advisory content through podcast videos, yet extracting insights from lengthy, multimodal segments (30-40 minutes) remains challenging. We introduce FASTER (Financial Advisory Summariser with Textual Embedded Relevant images), a modular framework that tackles three key challenges: (1) extracting modality-specific features, (2) producing optimized, concise summaries, and (3) aligning visual keyframes with associated textual points. FASTER employs BLIP for semantic visual descriptions, OCR for textual patterns, and Whisper-based transcription with Speaker diarization as BOS features. A modified Direct Preference Optimization (DPO)-based loss function, equipped with BOS-specific fact-checking, ensures precision, relevance, and factual consistency against the human-aligned summary. A ranker-based retrieval mechanism further aligns keyframes with summarized content, enhancing interpretability and cross-modal coherence. To acknowledge data resource scarcity, we introduce Fin-APT, a dataset comprising 470 publicly accessible financial advisory pep-talk videos for robust multimodal research. Comprehensive cross-domain experiments confirm FASTER's strong performance, robustness, and generalizability when compared to Large Language Models (LLMs) and Vision-Language Models (VLMs). By establishing a new standard for multimodal summarization, FASTER makes financial advisory content more accessible and actionable, thereby opening new avenues for research. The dataset and code are available at: https://github.com/sarmistha-D/FASTER<br>
<br>
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2509.20884.pdf' target='_blank'>https://arxiv.org/pdf/2509.20884.pdf</a></span>   <span><a href='https://github.com/HubuKG/IOG-VQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhifei Li, Feng Qiu, Yiran Wang, Yujing Xia, Kui Xiao, Miao Zhang, Yan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20884">Integrating Object Interaction Self-Attention and GAN-Based Debiasing for Visual Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Visual Question Answering (VQA) presents a unique challenge by requiring models to understand and reason about visual content to answer questions accurately. Existing VQA models often struggle with biases introduced by the training data, leading to over-reliance on superficial patterns and inadequate generalization to diverse questions and images. This paper presents a novel model, IOG-VQA, which integrates Object Interaction Self-Attention and GAN-Based Debiasing to enhance VQA model performance. The self-attention mechanism allows our model to capture complex interactions between objects within an image, providing a more comprehensive understanding of the visual context. Meanwhile, the GAN-based debiasing framework generates unbiased data distributions, helping the model to learn more robust and generalizable features. By leveraging these two components, IOG-VQA effectively combines visual and textual information to address the inherent biases in VQA datasets. Extensive experiments on the VQA-CP v1 and VQA-CP v2 datasets demonstrate that our model shows excellent performance compared with the existing methods, particularly in handling biased and imbalanced data distributions highlighting the importance of addressing both object interactions and dataset biases in advancing VQA tasks. Our code is available at https://github.com/HubuKG/IOG-VQA.<br>
<span id='abs_ch'>Chinese Summary: IOG-VQA模型通过结合物体交互自注意力机制和基于GAN的去偏方法，有效提升了视觉问答性能，在 准数据集上展现出卓越的泛化能力和抗偏置特性。English Summary: The IOG-VQA model enhances Visual Question Answering by integrating object interaction self-attention for better visual context understanding and GAN-based debiasing to mitigate dataset biases, achieving superior performance on benchmark datasets.Paperid:367,https://arxiv.org/pdf/2509.20871.pdfGitHub</span><br>
<span id='abs_en'>English Summary: The IOG-VQA model enhances Visual Question Answering by integrating object interaction self-attention for better visual context understanding and GAN-based debiasing to mitigate dataset biases, achieving superior performance on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2509.20871.pdf' target='_blank'>https://arxiv.org/pdf/2509.20871.pdf</a></span>   <span><a href='https://github.com/HubuKG/SCRA-VQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Zhang, Jiaqing Lin, Miao Zhang, Kui Xiao, Xiaoju Hou, Yue Zhao, Zhifei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20871">SCRA-VQA: Summarized Caption-Rerank for Augmented Large Language Models in Visual Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Acquiring high-quality knowledge is a central focus in Knowledge-Based Visual Question Answering (KB-VQA). Recent methods use large language models (LLMs) as knowledge engines for answering. These methods generally employ image captions as visual text descriptions to assist LLMs in interpreting images. However, the captions frequently include excessive noise irrelevant to the question, and LLMs generally do not comprehend VQA tasks, limiting their reasoning capabilities. To address this issue, we propose the Summarized Caption-Rerank Augmented VQA (SCRA-VQA), which employs a pre-trained visual language model to convert images into captions. Moreover, SCRA-VQA generates contextual examples for the captions while simultaneously summarizing and reordering them to exclude unrelated information. The caption-rerank process enables LLMs to understand the image information and questions better, thus enhancing the model's reasoning ability and task adaptability without expensive end-to-end training. Based on an LLM with 6.7B parameters, SCRA-VQA performs excellently on two challenging knowledge-based VQA datasets: OK-VQA and A-OKVQA, achieving accuracies of 38.8% and 34.6%. Our code is available at https://github.com/HubuKG/SCRA-VQA.<br>
<span id='abs_ch'>中文：SCRA-VQA通过总结和重排图像描述来减少噪声，使大型语言模型能更好地理解图像并提升推理能力， 需昂贵训练即可在知识型视觉问答任务中取得优异表现。</span><br>
<span id='abs_en'>English: SCRA-VQA enhances knowledge-based visual question answering by summarizing and reranking image captions to reduce noise, enabling large language models to better interpret images and improve reasoning without costly training, achieving high accuracy on challenging datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2509.20868.pdf' target='_blank'>https://arxiv.org/pdf/2509.20868.pdf</a></span>   <span><a href='https://github.com/JamesJunyuGuo/Style_Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyu Guo, Shangding Gu, Ming Jin, Costas Spanos, Javad Lavaei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20868">StyleBench: Evaluating thinking styles in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The effectiveness of Large Language Models (LLMs) is heavily influenced by the reasoning strategies, or styles of thought, employed in their prompts. However, the interplay between these reasoning styles, model architecture, and task type remains poorly understood. To address this, we introduce StyleBench, a comprehensive benchmark for systematically evaluating reasoning styles across diverse tasks and models. We assess five representative reasoning styles, including Chain of Thought (CoT), Tree of Thought (ToT), Algorithm of Thought (AoT), Sketch of Thought (SoT), and Chain-of-Draft (CoD) on five reasoning tasks, using 15 open-source models from major families (LLaMA, Qwen, Mistral, Gemma, GPT-OSS, Phi, and DeepSeek) ranging from 270M to 120B parameters. Our large-scale analysis reveals that no single style is universally optimal. We demonstrate that strategy efficacy is highly contingent on both model scale and task type: search-based methods (AoT, ToT) excel in open-ended problems but require large-scale models, while concise styles (SoT, CoD) achieve radical efficiency gains on well-defined tasks. Furthermore, we identify key behavioral patterns: smaller models frequently fail to follow output instructions and default to guessing, while reasoning robustness emerges as a function of scale. Our findings offer a crucial roadmap for selecting optimal reasoning strategies based on specific constraints, we open source the benchmark in https://github.com/JamesJunyuGuo/Style_Bench.<br>
<span id='abs_ch'>中文: 大型语言模型的有效性取决于推理策略，没有单一风 普遍最优， 为性能 模型规模和任务类型而异，其中搜索类方法在开放性问题中表现突出，而简洁风 在明确任务中显著提升效率。</span><br>
<span id='abs_en'>English: The effectiveness of Large Language Models depends on reasoning strategies, with no single style universally optimal, as performance varies by model scale and task type, where search-based methods excel in open-ended problems and concise styles boost efficiency in well-defined tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2509.20857.pdf' target='_blank'>https://arxiv.org/pdf/2509.20857.pdf</a></span>   <span><a href='https://github.com/tiny-smart/tasselnetv4' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaonan Hu, Xuebing Li, Jinyu Xu, Abdulkadir Duran Adan, Letian Zhou, Xuhui Zhu, Yanan Li, Wei Guo, Shouyang Liu, Wenzhong Liu, Hao Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20857">TasselNetV4: A vision foundation model for cross-scene, cross-scale, and cross-species plant counting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate plant counting provides valuable information for agriculture such as crop yield prediction, plant density assessment, and phenotype quantification. Vision-based approaches are currently the mainstream solution. Prior art typically uses a detection or a regression model to count a specific plant. However, plants have biodiversity, and new cultivars are increasingly bred each year. It is almost impossible to exhaust and build all species-dependent counting models. Inspired by class-agnostic counting (CAC) in computer vision, we argue that it is time to rethink the problem formulation of plant counting, from what plants to count to how to count plants. In contrast to most daily objects with spatial and temporal invariance, plants are dynamic, changing with time and space. Their non-rigid structure often leads to worse performance than counting rigid instances like heads and cars such that current CAC and open-world detection models are suboptimal to count plants. In this work, we inherit the vein of the TasselNet plant counting model and introduce a new extension, TasselNetV4, shifting from species-specific counting to cross-species counting. TasselNetV4 marries the local counting idea of TasselNet with the extract-and-match paradigm in CAC. It builds upon a plain vision transformer and incorporates novel multi-branch box-aware local counters used to enhance cross-scale robustness. Two challenging datasets, PAC-105 and PAC-Somalia, are harvested. Extensive experiments against state-of-the-art CAC models show that TasselNetV4 achieves not only superior counting performance but also high efficiency.Our results indicate that TasselNetV4 emerges to be a vision foundation model for cross-scene, cross-scale, and cross-species plant counting.<br>
<span id='abs_ch'>中文: TasselNetV4通过将植物计数从物种特定模型转向跨物种方法，结合视觉变换器和多分支局部计数器，在多种农业场景中实现了卓越的计数精度与效率。</span><br>
<span id='abs_en'>English: TasselNetV4 advances plant counting by transitioning from species-specific models to a cross-species approach, leveraging a vision transformer and multi-branch local counters to achieve superior accuracy and efficiency across diverse agricultural scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2509.20784.pdf' target='_blank'>https://arxiv.org/pdf/2509.20784.pdf</a></span>   <span><a href='https://github.com/ChenhuiHu/towards_atoms' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenhui Hu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20784">Towards Atoms of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The fundamental units of internal representations in large language models (LLMs) remain undefined, limiting further understanding of their mechanisms. Neurons or features are often regarded as such units, yet neurons suffer from polysemy, while features face concerns of unreliable reconstruction and instability. To address this issue, we propose the Atoms Theory, which defines such units as atoms. We introduce the atomic inner product (AIP) to correct representation shifting, formally define atoms, and prove the conditions that atoms satisfy the Restricted Isometry Property (RIP), ensuring stable sparse representations over atom set and linking to compressed sensing. Under stronger conditions, we further establish the uniqueness and exact $\ell_1$ recoverability of the sparse representations, and provide guarantees that single-layer sparse autoencoders (SAEs) with threshold activations can reliably identify the atoms. To validate the Atoms Theory, we train threshold-activated SAEs on Gemma2-2B, Gemma2-9B, and Llama3.1-8B, achieving 99.9% sparse reconstruction across layers on average, and more than 99.8% of atoms satisfy the uniqueness condition, compared to 0.5% for neurons and 68.2% for features, showing that atoms more faithfully capture intrinsic representations of LLMs. Scaling experiments further reveal the link between SAEs size and recovery capacity. Overall, this work systematically introduces and validates Atoms Theory of LLMs, providing a theoretical framework for understanding internal representations and a foundation for mechanistic interpretability. Code available at https://github.com/ChenhuiHu/towards_atoms.<br>
<span id='abs_ch'>中文: 本文提出原子理论，将原子定义为大语言模型内部表征的基本单元，通过在Gemma2和Llama3.1等模型上的理论证明与实验验证，展示了原子相比神经元和特征具有更优的稳定性与唯一性。</span><br>
<span id='abs_en'>English: This paper introduces the Atoms Theory, defining atoms as the fundamental units of internal representations in large language models and demonstrating their superior stability and uniqueness over neurons and features through theoretical proofs and empirical validation on models like Gemma2 and Llama3.1.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2509.20589.pdf' target='_blank'>https://arxiv.org/pdf/2509.20589.pdf</a></span>   <span><a href='https://github.com/chipermaria/every-character-counts' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Chiper, Radu Tudor Ionescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20589">Every Character Counts: From Vulnerability to Defense in Phishing Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Phishing attacks targeting both organizations and individuals are becoming an increasingly significant threat as technology advances. Current automatic detection methods often lack explainability and robustness in detecting new phishing attacks. In this work, we investigate the effectiveness of character-level deep learning models for phishing detection, which can provide both robustness and interpretability. We evaluate three neural architectures adapted to operate at the character level, namely CharCNN, CharGRU, and CharBiLSTM, on a custom-built email dataset, which combines data from multiple sources. Their performance is analyzed under three scenarios: (i) standard training and testing, (ii) standard training and testing under adversarial attacks, and (iii) training and testing with adversarial examples. Aiming to develop a tool that operates as a browser extension, we test all models under limited computational resources. In this constrained setup, CharGRU proves to be the best-performing model across all scenarios. All models show vulnerability to adversarial attacks, but adversarial training substantially improves their robustness. In addition, by adapting the Gradient-weighted Class Activation Mapping (Grad-CAM) technique to character-level inputs, we are able to visualize which parts of each email influence the decision of each model. Our open-source code and data is released at https://github.com/chipermaria/every-character-counts.<br>
<span id='abs_ch'>中文: 本 究评估了字符级深度学 模型在钓鱼检测中的效果，发现CharGRU在计算资源受限时表现最佳，尽管所有模型均易受对抗攻击，但对抗训练能显著提升鲁棒性，并通过改进的Grad-CAM技术实现了决策过程的可视化。</span><br>
<span id='abs_en'>English: This study evaluates character-level deep learning models for phishing detection, finding CharGRU most effective under computational constraints while demonstrating vulnerability to adversarial attacks that can be mitigated through adversarial training and model interpretability via Grad-CAM adaptation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2509.20502.pdf' target='_blank'>https://arxiv.org/pdf/2509.20502.pdf</a></span>   <span><a href='https://github.com/xwang97/MARS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Jia Wang, Yijie Wang, Pengtao Dang, Sha Cao, Chi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20502">MARS: toward more efficient multi-agent collaboration for LLM reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have achieved impressive results in natural language understanding, yet their reasoning capabilities remain limited when operating as single agents. Multi-Agent Debate (MAD) has been proposed to address this limitation by enabling collaborative reasoning among multiple models in a round-table debate manner. While effective, MAD introduces substantial computational overhead due to the number of agents involved and the frequent communication required. In this paper, we propose MARS (Multi-Agent Review System), a role-based collaboration framework inspired by the review process. In MARS, an author agent generates an initial solution, reviewer agents provide decisions and comments independently, and a meta-reviewer integrates the feedback to make the final decision and guide further revision. This design enhances reasoning quality while avoiding costly reviewer-to-reviewer interactions, thereby controlling token consumption and inference time. We compared MARS with both MAD and other state-of-the-art reasoning strategies across multiple benchmarks. Extensive experiments with different LLMs show that MARS matches the accuracy of MAD while reducing both token usage and inference time by approximately 50\%. Code is available at https://github.com/xwang97/MARS.<br>
<span id='abs_ch'>Chinese: MARS框架通过基于角色的评审流程增强大语言模型的推理能力，在保持与多智能体辩论同等准确率的同时，将令牌使用量和推理时间减少约50%。</span><br>
<span id='abs_en'>English: The MARS framework enhances reasoning in large language models through a role-based review process, matching the accuracy of Multi-Agent Debate while cutting token usage and inference time by half.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2509.20376.pdf' target='_blank'>https://arxiv.org/pdf/2509.20376.pdf</a></span>   <span><a href='https://github.com/Happy-Hippo209/ConceptViz' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxuan Li, Zhen Wen, Qiqi Jiang, Chenxiao Li, Yuwei Wu, Yuchen Yang, Yiyao Wang, Xiuqi Huang, Minfeng Zhu, Wei Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20376">ConceptViz: A Visual Analytics Approach for Exploring Concepts in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks. Understanding how LLMs internally represent knowledge remains a significant challenge. Despite Sparse Autoencoders (SAEs) have emerged as a promising technique for extracting interpretable features from LLMs, SAE features do not inherently align with human-understandable concepts, making their interpretation cumbersome and labor-intensive. To bridge the gap between SAE features and human concepts, we present ConceptViz, a visual analytics system designed for exploring concepts in LLMs. ConceptViz implements a novel dentification => Interpretation => Validation pipeline, enabling users to query SAEs using concepts of interest, interactively explore concept-to-feature alignments, and validate the correspondences through model behavior verification. We demonstrate the effectiveness of ConceptViz through two usage scenarios and a user study. Our results show that ConceptViz enhances interpretability research by streamlining the discovery and validation of meaningful concept representations in LLMs, ultimately aiding researchers in building more accurate mental models of LLM features. Our code and user guide are publicly available at https://github.com/Happy-Hippo209/ConceptViz.<br>
<span id='abs_ch'>Chinese: ConceptViz是一个可视化分析系统，通过创新的识别-解释-验证流程，弥合了稀疏自编 器特征与人类可理解概念之间的鸿沟，帮助 究人员有效探索和验证大语言模型中的概念表征。</span><br>
<span id='abs_en'>English: ConceptViz is a visual analytics system that bridges the gap between sparse autoencoder features and human-understandable concepts in large language models, enabling efficient discovery and validation of interpretable representations through an interactive pipeline.Paperid:388,https://arxiv.org/pdf/2509.20374.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2509.20374.pdf' target='_blank'>https://arxiv.org/pdf/2509.20374.pdf</a></span>   <span><a href='https://github.com/NREL-Theseus/cfdllmbench/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nithin Somasekharan, Ling Yue, Yadi Cao, Weichao Li, Patrick Emami, Pochinapeddi Sai Bhargav, Anurag Acharya, Xingyu Xie, Shaowu Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20374">CFD-LLMBench: A Benchmark Suite for Evaluating Large Language Models in Computational Fluid Dynamics</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated strong performance across general NLP tasks, but their utility in automating numerical experiments of complex physical system -- a critical and labor-intensive component -- remains underexplored. As the major workhorse of computational science over the past decades, Computational Fluid Dynamics (CFD) offers a uniquely challenging testbed for evaluating the scientific capabilities of LLMs. We introduce CFDLLMBench, a benchmark suite comprising three complementary components -- CFDQuery, CFDCodeBench, and FoamBench -- designed to holistically evaluate LLM performance across three key competencies: graduate-level CFD knowledge, numerical and physical reasoning of CFD, and context-dependent implementation of CFD workflows. Grounded in real-world CFD practices, our benchmark combines a detailed task taxonomy with a rigorous evaluation framework to deliver reproducible results and quantify LLM performance across code executability, solution accuracy, and numerical convergence behavior. CFDLLMBench establishes a solid foundation for the development and evaluation of LLM-driven automation of numerical experiments for complex physical systems. Code and data are available at https://github.com/NREL-Theseus/cfdllmbench/.<br>
<span id='abs_ch'>中文: 大型语言模型在自动化复杂物理系统实验方面具有潜力，CFDLLMBench基准通过评估其在计算流体动力学的知识、推理和实施能力，为此提供了系统性验证基础。</span><br>
<span id='abs_en'>English: Large Language Models (LLMs) show potential in automating complex physical system experiments, as demonstrated by the CFDLLMBench benchmark designed to evaluate their capabilities in computational fluid dynamics knowledge, reasoning, and implementation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2509.20317.pdf' target='_blank'>https://arxiv.org/pdf/2509.20317.pdf</a></span>   <span><a href='https://github.com/InternLM/SIM-CoT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Jiaqi Wang, Xipeng Qiu, Dahua Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20317">SIM-CoT: Supervised Implicit Chain-of-Thought</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Implicit Chain-of-Thought (CoT) methods offer a token-efficient alternative to explicit CoT reasoning in Large Language Models (LLMs), but a persistent performance gap has limited their adoption. We identify a core latent instability issue when scaling the computational budget of implicit CoT: as the number of reasoning tokens increases, training often becomes unstable and collapses. Our analysis shows that this instability arises from latent representations becoming homogeneous and losing semantic diversity, caused by insufficient step-level supervision in current implicit CoT methods. To address this, we propose SIM-CoT, a plug-and-play training module that introduces step-level supervision to stabilize and enrich the latent reasoning space. SIM-CoT employs an auxiliary decoder during training to align each implicit token with its corresponding explicit reasoning step, ensuring latent states capture distinct and meaningful information. The auxiliary decoder is removed at inference, preserving the efficiency of implicit CoT with no added overhead. It also provides interpretability by projecting each latent token onto an explicit reasoning vocabulary, enabling per-step visualization and diagnosis. SIM-CoT significantly improves both in-domain accuracy and out-of-domain stability of implicit CoT methods, boosting Coconut by +8.2\% on GPT-2 and CODI by +3.0\% on LLaMA-3.1 8B. It further surpasses the explicit CoT baseline on GPT-2 by 2.1\% with 2.3$\times$ greater token efficiency, while closing the performance gap on larger models like LLaMA-3.1 8B. Code: https://github.com/InternLM/SIM-CoT<br>
<br>
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2509.20293.pdf' target='_blank'>https://arxiv.org/pdf/2509.20293.pdf</a></span>   <span><a href='https://github.com/penfever/judgment-to-noise' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Feuer, Chiung-Yi Tseng, Astitwa Sarthak Lathe, Oussama Elachqar, John P Dickerson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20293">When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks Silently Undermine Validity</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LLM-judged benchmarks are increasingly used to evaluate complex model behaviors, yet their design introduces failure modes absent in conventional ground-truth based benchmarks. We argue that without tight objectives and verifiable constructions, benchmark rankings can produce high-confidence rankings that are in fact largely noise. We introduce two mechanisms to diagnose these issues. Schematic adherence quantifies how much of a judge's overall verdict is explained by the explicit evaluation schema, revealing unexplained variance when judges deviate from their own rubric. Psychometric validity aggregates internal consistency and discriminant validity signals to quantify irreducible uncertainty in any benchmarking run. Applying these tools to Arena-Hard Auto, we find severe schema incoherence and factor collapse across popular judges: for example, unexplained variance exceeding 90 percent for DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We also show that the ELO-style aggregation used by Arena-Hard Auto collapses and masks genuine ranking uncertainty. Our results highlight design failures that undermine validity and offer actionable principles for building better-scoped, reliability-aware LLM-judged benchmarks. We released our code and dataset at https://github.com/penfever/judgment-to-noise<br>
<span id='abs_ch'>中文：LLM评判的基准 设计缺陷常产生不可 排名，但新诊断工具揭示了高解释方差和排名不确定性，呼吁采用范围更明确且关注可 性的设计。</span><br>
<span id='abs_en'>English: LLM-judged benchmarks often produce unreliable rankings due to design flaws, but new diagnostic tools reveal high unexplained variance and ranking uncertainty, urging better-scoped and reliability-aware designs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2509.20290.pdf' target='_blank'>https://arxiv.org/pdf/2509.20290.pdf</a></span>   <span><a href='https://github.com/jjnlcode/PGCLODA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dayu Tan, Jing Chen, Xiaoping Zhou, Yansen Su, Chunhou Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20290">PGCLODA: Prompt-Guided Graph Contrastive Learning for Oligopeptide-Infectious Disease Association Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Infectious diseases continue to pose a serious threat to public health, underscoring the urgent need for effective computational approaches to screen novel anti-infective agents. Oligopeptides have emerged as promising candidates in antimicrobial research due to their structural simplicity, high bioavailability, and low susceptibility to resistance. Despite their potential, computational models specifically designed to predict associations between oligopeptides and infectious diseases remain scarce. This study introduces a prompt-guided graph-based contrastive learning framework (PGCLODA) to uncover potential associations. A tripartite graph is constructed with oligopeptides, microbes, and diseases as nodes, incorporating both structural and semantic information. To preserve critical regions during contrastive learning, a prompt-guided graph augmentation strategy is employed to generate meaningful paired views. A dual encoder architecture, integrating Graph Convolutional Network (GCN) and Transformer, is used to jointly capture local and global features. The fused embeddings are subsequently input into a multilayer perceptron (MLP) classifier for final prediction. Experimental results on a benchmark dataset indicate that PGCLODA consistently outperforms state-of-the-art models in AUROC, AUPRC, and accuracy. Ablation and hyperparameter studies confirm the contribution of each module. Case studies further validate the generalization ability of PGCLODA and its potential to uncover novel, biologically relevant associations. These findings offer valuable insights for mechanism-driven discovery and oligopeptide-based drug development. The source code of PGCLODA is available online at https://github.com/jjnlcode/PGCLODA.<br>
<span id='abs_ch'>中文: 本 究提出的PGCLODA框架通过提示引导的图对比学 方法，能有效预测寡肽与 染病的关联关系，在预测性能上显著优于现有模型，为抗感染药物 发提供了重要参考。</span><br>
<span id='abs_en'>English: This study introduces PGCLODA, a novel prompt-guided graph contrastive learning framework that effectively predicts associations between oligopeptides and infectious diseases, demonstrating superior performance over existing models and offering valuable insights for antimicrobial drug development.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2509.20234.pdf' target='_blank'>https://arxiv.org/pdf/2509.20234.pdf</a></span>   <span><a href='https://github.com/tomburgert/feature-reliance' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tom Burgert, Oliver Stoll, Paolo Rota, Begüm Demir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20234">ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The hypothesis that Convolutional Neural Networks (CNNs) are inherently texture-biased has shaped much of the discourse on feature use in deep learning. We revisit this hypothesis by examining limitations in the cue-conflict experiment by Geirhos et al. To address these limitations, we propose a domain-agnostic framework that quantifies feature reliance through systematic suppression of shape, texture, and color cues, avoiding the confounds of forced-choice conflicts. By evaluating humans and neural networks under controlled suppression conditions, we find that CNNs are not inherently texture-biased but predominantly rely on local shape features. Nonetheless, this reliance can be substantially mitigated through modern training strategies or architectures (ConvNeXt, ViTs). We further extend the analysis across computer vision, medical imaging, and remote sensing, revealing that reliance patterns differ systematically: computer vision models prioritize shape, medical imaging models emphasize color, and remote sensing models exhibit a stronger reliance on texture. Code is available at https://github.com/tomburgert/feature-reliance.<br>
<span id='abs_ch'>中文: 该 究挑战了卷积神经网络天生偏向纹理的假设，通过领域 关框架证明其主要依赖局部形状特征，且在计算机视觉、医学影像和遥感领域表现出不同的特征依赖模式。</span><br>
<span id='abs_en'>English: The study challenges the notion that CNNs are inherently texture-biased, demonstrating through a domain-agnostic framework that they primarily rely on local shape features, with reliance patterns varying across computer vision, medical imaging, and remote sensing domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2509.20214.pdf' target='_blank'>https://arxiv.org/pdf/2509.20214.pdf</a></span>   <span><a href='https://github.com/snu-mllab/Q-Palette' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Deokjae Lee, Hyun Oh Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20214">Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for Efficient LLM Deployment</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We study weight-only post-training quantization (PTQ), which quantizes the weights of a large language model (LLM) without retraining, using little or no calibration data. Weight-only PTQ is crucial for reducing the memory footprint and latency of LLM inference, especially in memory-bound, small-batch inference scenarios, such as personalized inference on edge devices. Despite its importance, irregular weight distributions with heavy-tailed outliers in LLMs complicate quantization, recently motivating rotation-based methods that transform weights into near-Gaussian distributions, which are more regular with fewer outliers, thereby reducing quantization error. In this work, we first derive the information-theoretically optimal bit allocation for Gaussianized weights under given bit budgets, revealing that fine-grained fractional-bit quantizers approaching the Gaussian distortion-rate bound are essential to achieve near-optimal quantization performance. To bridge this theoretical insight and practical implementation, we introduce Q-Palette, a versatile collection of fractional-bit quantizers that range from trellis-coded quantizers offering near-optimal distortion to simpler vector and scalar quantizers optimized for faster inference, all efficiently implemented with optimized CUDA kernels across various bitwidths. Furthermore, leveraging Q-Palette as a foundational component, we propose a novel mixed-scheme quantization framework, jointly optimizing quantizer choices and layer fusion decisions given resource constraints. The code is available at https://github.com/snu-mllab/Q-Palette.<br>
<span id='abs_ch'>Chinese: 本 究提出了Q-Palette，一套用于大语言模型仅权重量化的分数位量化器集合，在资源约束下优化量化性能与推理速度，并支持混合方案框架。</span><br>
<span id='abs_en'>English: This research introduces Q-Palette, a collection of fractional-bit quantizers for weight-only post-training quantization of large language models, which optimizes quantization performance and inference speed while enabling a mixed-scheme framework under resource constraints.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2509.20209.pdf' target='_blank'>https://arxiv.org/pdf/2509.20209.pdf</a></span>   <span><a href='https://github.com/hailaykidu/MachineT_TigEng' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hailay Kidu Teklehaymanot, Gebrearegawi Gidey, Wolfgang Nejdl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20209">Low-Resource English-Tigrinya MT: Leveraging Multilingual Models, Custom Tokenizers, and Clean Evaluation Benchmarks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite advances in Neural Machine Translation (NMT), low-resource languages like Tigrinya remain underserved due to persistent challenges, including limited corpora, inadequate tokenization strategies, and the lack of standardized evaluation benchmarks. This paper investigates transfer learning techniques using multilingual pretrained models to enhance translation quality for morphologically rich, low-resource languages. We propose a refined approach that integrates language-specific tokenization, informed embedding initialization, and domain-adaptive fine-tuning. To enable rigorous assessment, we construct a high-quality, human-aligned English-Tigrinya evaluation dataset covering diverse domains. Experimental results demonstrate that transfer learning with a custom tokenizer substantially outperforms zero-shot baselines, with gains validated by BLEU, chrF, and qualitative human evaluation. Bonferroni correction is applied to ensure statistical significance across configurations. Error analysis reveals key limitations and informs targeted refinements. This study underscores the importance of linguistically aware modeling and reproducible benchmarks in bridging the performance gap for underrepresented languages. Resources are available at https://github.com/hailaykidu/MachineT_TigEng and https://huggingface.co/Hailay/MachineT_TigEng<br>
<span id='abs_ch'>中文：本 究通过采用定制化分词和领域自适应的迁移学 方法，提升了提 里尼亚语的神经机器翻译质量，并利用新建评估数据集验证了其显著性能提升。</span><br>
<span id='abs_en'>English: This study improves Tigrinya neural machine translation through transfer learning with customized tokenization and domain adaptation, validated by a new evaluation dataset showing significant performance gains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2509.20208.pdf' target='_blank'>https://arxiv.org/pdf/2509.20208.pdf</a></span>   <span><a href='https://github.com/parkervg/blendsql' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Parker Glenn, Alfy Samuel, Daben Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20208">Play by the Type Rules: Inferring Constraints for LLM Functions in Declarative Programs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Integrating LLM powered operators in declarative query languages allows for the combination of cheap and interpretable functions with powerful, generalizable language model reasoning. However, in order to benefit from the optimized execution of a database query language like SQL, generated outputs must align with the rules enforced by both type checkers and database contents. Current approaches address this challenge with orchestrations consisting of many LLM-based post-processing calls to ensure alignment between generated outputs and database values, introducing performance bottlenecks. We perform a study on the ability of various sized open-source language models to both parse and execute functions within a query language based on SQL, showing that small language models can excel as function executors over hybrid data sources. Then, we propose an efficient solution to enforce the well-typedness of LLM functions, demonstrating 7% accuracy improvement on a multi-hop question answering dataset with 53% improvement in latency over comparable solutions. We make our implementation available at https://github.com/parkervg/blendsql<br>
<span id='abs_ch'>中文摘要： 究表明小型语言模型能有效执行类SQL查询语言中的函数，并提出一种高效解决方案，相比现有方法在准确率上提升7%，延迟降低53%。</span><br>
<span id='abs_en'>English Summary: This study demonstrates that small language models can effectively execute functions within SQL-like query languages, proposing an efficient solution that improves accuracy by 7% and reduces latency by 53% compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2509.20162.pdf' target='_blank'>https://arxiv.org/pdf/2509.20162.pdf</a></span>   <span><a href='https://github.com/ChaojunNie/RLAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaojun Nie, Jun Zhou, Guanxiang Wang, Shisong Wu, Zichen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20162">Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) often exhibit limited performance on domain-specific tasks due to the natural disproportionate representation of specialized information in their training data and the static nature of these datasets. Knowledge scarcity and temporal lag create knowledge gaps for domain applications. While post-training on domain datasets can embed knowledge into models, existing approaches have some limitations. Continual Pre-Training (CPT) treats all tokens in domain documents with equal importance, failing to prioritize critical knowledge points, while supervised fine-tuning (SFT) with question-answer pairs struggles to develop the coherent knowledge structures necessary for complex reasoning tasks. To address these challenges, we propose Reinforcement Learning from Augmented Generation (RLAG). Our approach iteratively cycles between sampling generations and optimizing the model through calculated rewards, effectively embedding critical and contextually coherent domain knowledge. We select generated outputs with the highest log probabilities as the sampling result, then compute three tailored reward metrics to guide the optimization process. To comprehensively evaluate domain expertise, we assess answer accuracy and the rationality of explanations generated for correctly answered questions. Experimental results across medical, legal, astronomy, and current events datasets demonstrate that our proposed method significantly outperforms baseline approaches. Our code and data are open sourced at https://github.com/ChaojunNie/RLAG.<br>
<span id='abs_ch'>中文摘要：提出的强化学 增强生成（RLAG）方法通过奖励引导的迭代优化，有效克服了现有技术在领域知识整合中的不足，在多个专业领域显著提升了模型的准确性和解释合理性。</span><br>
<span id='abs_en'>English Summary: The proposed Reinforcement Learning from Augmented Generation (RLAG) method overcomes limitations of existing approaches by iteratively optimizing models through reward-guided sampling, significantly enhancing domain-specific knowledge integration and reasoning across multiple specialized fields.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2509.20154.pdf' target='_blank'>https://arxiv.org/pdf/2509.20154.pdf</a></span>   <span><a href='https://github.com/zhiqin1998/UMamba2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi Qin Tan, Xiatian Zhu, Owen Addison, Yunpeng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20154">U-Mamba2-SSL for Semi-Supervised Tooth and Pulp Segmentation in CBCT</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate segmentation of teeth and pulp in Cone-Beam Computed Tomography (CBCT) is vital for clinical applications like treatment planning and diagnosis. However, this process requires extensive expertise and is exceptionally time-consuming, highlighting the critical need for automated algorithms that can effectively utilize unlabeled data. In this paper, we propose U-Mamba2-SSL, a novel semi-supervised learning framework that builds on the U-Mamba2 model and employs a multi-stage training strategy. The framework first pre-trains U-Mamba2 in a self-supervised manner using a disruptive autoencoder. It then leverages unlabeled data through consistency regularization, where we introduce input and feature perturbations to ensure stable model outputs. Finally, a pseudo-labeling strategy is implemented with a reduced loss weighting to minimize the impact of potential errors. U-Mamba2-SSL achieved an average score of 0.789 and a DSC of 0.917 on the hidden test set, achieving first place in Task 1 of the STSR 2025 challenge. The code is available at https://github.com/zhiqin1998/UMamba2.<br>
<span id='abs_ch'>中文: 本文提出U-Mamba2-SSL半监督学 框架，通过多阶段训练提升CBCT分割精度，并在STSR 2025挑战赛中荣获第一名。</span><br>
<span id='abs_en'>English: This paper introduces U-Mamba2-SSL, a semi-supervised learning framework that enhances CBCT segmentation accuracy through multi-stage training and achieved top performance in the STSR 2025 challenge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2509.20154.pdf' target='_blank'>https://arxiv.org/pdf/2509.20154.pdf</a></span>   <span><a href='https://github.com/zhiqin1998/UMamba2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi Qin Tan, Xiatian Zhu, Owen Addison, Yunpeng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20154">U-Mamba2-SSL for Semi-Supervised Tooth and Pulp Segmentation in CBCT</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate segmentation of teeth and pulp in Cone-Beam Computed Tomography (CBCT) is vital for clinical applications like treatment planning and diagnosis. However, this process requires extensive expertise and is exceptionally time-consuming, highlighting the critical need for automated algorithms that can effectively utilize unlabeled data. In this paper, we propose U-Mamba2-SSL, a novel semi-supervised learning framework that builds on the U-Mamba2 model and employs a multi-stage training strategy. The framework first pre-trains U-Mamba2 in a self-supervised manner using a disruptive autoencoder. It then leverages unlabeled data through consistency regularization, where we introduce input and feature perturbations to ensure stable model outputs. Finally, a pseudo-labeling strategy is implemented with a reduced loss weighting to minimize the impact of potential errors. U-Mamba2-SSL achieved an average score of 0.789 and a DSC of 0.917 on the hidden test set, achieving first place in Task 1 of the STSR 2025 challenge. The code is available at https://github.com/zhiqin1998/UMamba2.<br>
<span id='abs_ch'>中文: 本文提出U-Mamba2-SSL半监督学 框架，通过多阶段训练提升CBCT分割精度，并在STSR 2025挑战赛中荣获第一名。</span><br>
<span id='abs_en'>English: This paper introduces U-Mamba2-SSL, a semi-supervised learning framework that enhances CBCT segmentation accuracy through multi-stage training and achieved top performance in the STSR 2025 challenge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2509.19972.pdf' target='_blank'>https://arxiv.org/pdf/2509.19972.pdf</a></span>   <span><a href='https://github.com/cinemere/evacuation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Albina Klepach, Egor E. Nuzhin, Alexey A. Tsukanov, Nikolay V. Brilliantov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19972">An effective control of large systems of active particles: An application to evacuation problem</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Manipulation of large systems of active particles is a serious challenge across diverse domains, including crowd management, control of robotic swarms, and coordinated material transport. The development of advanced control strategies for complex scenarios is hindered, however, by the lack of scalability and robustness of the existing methods, in particular, due to the need of an individual control for each agent. One possible solution involves controlling a system through a leader or a group of leaders, which other agents tend to follow. Using such an approach we develop an effective control strategy for a leader, combining reinforcement learning (RL) with artificial forces acting on the system. To describe the guidance of active particles by a leader we introduce the generalized Vicsek model. This novel method is then applied to the problem of the effective evacuation by a robot-rescuer (leader) of large groups of people from hazardous places. We demonstrate, that while a straightforward application of RL yields suboptimal results, even for advanced architectures, our approach provides a robust and efficient evacuation strategy. The source code supporting this study is publicly available at: https://github.com/cinemere/evacuation.<br>
<span id='abs_ch'>中文摘要：本 究提出了一种结合强化学 与人工力的方法，通过领导者控制活性粒子系统，为大规模人群从危险区域疏散提供了稳健高效的策略。</span><br>
<span id='abs_en'>English Summary: This study presents a reinforcement learning approach combined with artificial forces to control active particle systems via leaders, offering a robust and efficient strategy for evacuating large groups from hazardous areas.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2509.19952.pdf' target='_blank'>https://arxiv.org/pdf/2509.19952.pdf</a></span>   <span><a href='https://github.com/sarmistha-D/CoD-V' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarmistha Das, R E Zera Marveen Lyngkhoi, Kirtan Jain, Vinayak Goyal, Sriparna Saha, Manish Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19952">When Words Can't Capture It All: Towards Video-Based User Complaint Text Generation with Multimodal Video Complaint Dataset</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While there exists a lot of work on explainable complaint mining, articulating user concerns through text or video remains a significant challenge, often leaving issues unresolved. Users frequently struggle to express their complaints clearly in text but can easily upload videos depicting product defects (e.g., vague text such as `worst product' paired with a 5-second video depicting a broken headphone with the right earcup). This paper formulates a new task in the field of complaint mining to aid the common users' need to write an expressive complaint, which is Complaint Description from Videos (CoD-V) (e.g., to help the above user articulate her complaint about the defective right earcup). To this end, we introduce ComVID, a video complaint dataset containing 1,175 complaint videos and the corresponding descriptions, also annotated with the emotional state of the complainer. Additionally, we present a new complaint retention (CR) evaluation metric that discriminates the proposed (CoD-V) task against standard video summary generation and description tasks. To strengthen this initiative, we introduce a multimodal Retrieval-Augmented Generation (RAG) embedded VideoLLaMA2-7b model, designed to generate complaints while accounting for the user's emotional state. We conduct a comprehensive evaluation of several Video Language Models on several tasks (pre-trained and fine-tuned versions) with a range of established evaluation metrics, including METEOR, perplexity, and the Coleman-Liau readability score, among others. Our study lays the foundation for a new research direction to provide a platform for users to express complaints through video. Dataset and resources are available at: https://github.com/sarmistha-D/CoD-V.<br>
<br>
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2509.19753.pdf' target='_blank'>https://arxiv.org/pdf/2509.19753.pdf</a></span>   <span><a href='https://github.com/dfr-code/ExpFace' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinhui Zheng, Xueyuan Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19753">ExpFace: Exponential Angular Margin Loss for Deep Face Recognition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Face recognition is an open-set problem requiring high discriminative power to ensure that intra-class distances remain smaller than inter-class distances. Margin-based softmax losses, such as SphereFace, CosFace, and ArcFace, have been widely adopted to enhance intra-class compactness and inter-class separability, yet they overlook the impact of noisy samples. By examining the distribution of samples in the angular space, we observe that clean samples predominantly cluster in the center region, whereas noisy samples tend to shift toward the peripheral region. Motivated by this observation, we propose the Exponential Angular Margin Loss (ExpFace), which introduces an angular exponential term as the margin. This design applies a larger penalty in the center region and a smaller penalty in the peripheral region within the angular space, thereby emphasizing clean samples while suppressing noisy samples. We present a unified analysis of ExpFace and classical margin-based softmax losses in terms of margin embedding forms, similarity curves, and gradient curves, showing that ExpFace not only avoids the training instability of SphereFace and the non-monotonicity of ArcFace, but also exhibits a similarity curve that applies penalties in the same manner as the decision boundary in the angular space. Extensive experiments demonstrate that ExpFace achieves state-of-the-art performance. To facilitate future research, we have released the source code at: https://github.com/dfr-code/ExpFace.<br>
<span id='abs_ch'>中文: 提出的指数化角度间隔损失（ExpFace）通过在角度空间中 大对中心区域干净 本的惩罚、减小对边缘噪声 本的惩罚，有效提升了人脸识别的判别能力，在克服现有方法缺陷的同时实现了最优性能。</span><br>
<span id='abs_en'>English: The proposed Exponential Angular Margin Loss (ExpFace) enhances face recognition by applying larger penalties to centrally clustered clean samples and smaller penalties to peripheral noisy samples in angular space, achieving state-of-the-art performance while addressing limitations of previous methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2509.19742.pdf' target='_blank'>https://arxiv.org/pdf/2509.19742.pdf</a></span>   <span><a href='https://github.com/carsonz/HiCoLoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuyu Zhang, Yifan Wei, Xinru Wang, Yanmin Zhu, Yangfan He, Yixuan Weng, Bin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19742">HiCoLoRA: Addressing Context-Prompt Misalignment via Hierarchical Collaborative LoRA for Zero-Shot DST</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Zero-shot Dialog State Tracking (zs-DST) is essential for enabling Task-Oriented Dialog Systems (TODs) to generalize to new domains without costly data annotation. A central challenge lies in the semantic misalignment between dynamic dialog contexts and static prompts, leading to inflexible cross-layer coordination, domain interference, and catastrophic forgetting. To tackle this, we propose Hierarchical Collaborative Low-Rank Adaptation (HiCoLoRA), a framework that enhances zero-shot slot inference through robust prompt alignment. It features a hierarchical LoRA architecture for dynamic layer-specific processing (combining lower-layer heuristic grouping and higher-layer full interaction), integrates Spectral Joint Domain-Slot Clustering to identify transferable associations (feeding an Adaptive Linear Fusion Mechanism), and employs Semantic-Enhanced SVD Initialization (SemSVD-Init) to preserve pre-trained knowledge. Experiments on multi-domain datasets MultiWOZ and SGD show that HiCoLoRA outperforms baselines, achieving SOTA in zs-DST. Code is available at https://github.com/carsonz/HiCoLoRA.<br>
<span id='abs_ch'>中文摘要：HiCoLoRA通过分层LoRA架构、谱聚类联合域-槽识别和语义增强初始化，解决了零 本对话状态跟踪中的语义对齐难题，在MultiWOZ和SGD数据集上实现了最优性能。</span><br>
<span id='abs_en'>English Summary: HiCoLoRA introduces a hierarchical LoRA framework with spectral clustering and semantic-enhanced initialization to address semantic misalignment in zero-shot dialog state tracking, achieving state-of-the-art performance on MultiWOZ and SGD datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2509.19705.pdf' target='_blank'>https://arxiv.org/pdf/2509.19705.pdf</a></span>   <span><a href='https://github.com/Wizaaard/X-MultiTask' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>J. Ben Tamo, Nishant S. Chouhan, Micky C. Nnamdi, Yining Yuan, Shreya S. Chivilkar, Wenqi Shi, Steven W. Hwang, B. Randall Brenn, May D. Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19705">Causal Machine Learning for Surgical Interventions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Surgical decision-making is complex and requires understanding causal relationships between patient characteristics, interventions, and outcomes. In high-stakes settings like spinal fusion or scoliosis correction, accurate estimation of individualized treatment effects (ITEs) remains limited due to the reliance on traditional statistical methods that struggle with complex, heterogeneous data. In this study, we develop a multi-task meta-learning framework, X-MultiTask, for ITE estimation that models each surgical decision (e.g., anterior vs. posterior approach, surgery vs. no surgery) as a distinct task while learning shared representations across tasks. To strengthen causal validity, we incorporate the inverse probability weighting (IPW) into the training objective. We evaluate our approach on two datasets: (1) a public spinal fusion dataset (1,017 patients) to assess the effect of anterior vs. posterior approaches on complication severity; and (2) a private AIS dataset (368 patients) to analyze the impact of posterior spinal fusion (PSF) vs. non-surgical management on patient-reported outcomes (PROs). Our model achieves the highest average AUC (0.84) in the anterior group and maintains competitive performance in the posterior group (0.77). It outperforms baselines in treatment effect estimation with the lowest overall $Îµ_{\text{NN-PEHE}}$ (0.2778) and $Îµ_{\text{ATE}}$ (0.0763). Similarly, when predicting PROs in AIS, X-MultiTask consistently shows superior performance across all domains, with $Îµ_{\text{NN-PEHE}}$ = 0.2551 and $Îµ_{\text{ATE}}$ = 0.0902. By providing robust, patient-specific causal estimates, X-MultiTask offers a powerful tool to advance personalized surgical care and improve patient outcomes. The code is available at https://github.com/Wizaaard/X-MultiTask.<br>
<br>
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2509.19695.pdf' target='_blank'>https://arxiv.org/pdf/2509.19695.pdf</a></span>   <span><a href='https://github.com/carsonz/DyBBT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuyu Zhang, Yifan Wei, Jialuo Yuan, Xinru Wang, Yanmin Zhu, Bin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19695">DyBBT: Dynamic Balance via Bandit inspired Targeting for Dialog Policy with Cognitive Dual-Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Task oriented dialog systems often rely on static exploration strategies that do not adapt to dynamic dialog contexts, leading to inefficient exploration and suboptimal performance. We propose DyBBT, a novel dialog policy learning framework that formalizes the exploration challenge through a structured cognitive state space capturing dialog progression, user uncertainty, and slot dependency. DyBBT proposes a bandit inspired meta-controller that dynamically switches between a fast intuitive inference (System 1) and a slow deliberative reasoner (System 2) based on real-time cognitive states and visitation counts. Extensive experiments on single- and multi-domain benchmarks show that DyBBT achieves state-of-the-art performance in success rate, efficiency, and generalization, with human evaluations confirming its decisions are well aligned with expert judgment. Code is available at https://github.com/carsonz/DyBBT.<br>
<span id='abs_ch'>中文摘要：DyBBT提出了一种动态对话策略框架，通过认知状态空间和双系统元控制器实现自适应探索，从而取得了最优性能表现。English Summary: DyBBT introduces a dynamic dialog policy framework using a cognitive state space and dual-system meta-controller to achieve state-of-the-art performance through adaptive exploration.Paperid:434,https://arxiv.org/pdf/2509.19690.pdfGitHub</span><br>
<span id='abs_en'>English Summary: DyBBT introduces a dynamic dialog policy framework using a cognitive state space and dual-system meta-controller to achieve state-of-the-art performance through adaptive exploration.Paperid:434,https://arxiv.org/pdf/2509.19690.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2509.19658.pdf' target='_blank'>https://arxiv.org/pdf/2509.19658.pdf</a></span>   <span><a href='https://github.com/youngjuY/RoboSSM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Youngju Yoo, Jiaheng Hu, Yifeng Zhu, Bo Liu, Qiang Liu, Roberto MartÃ­n-MartÃ­n, Peter Stone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19658">RoboSSM: Scalable In-context Imitation Learning via State-Space Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In-context imitation learning (ICIL) enables robots to learn tasks from prompts consisting of just a handful of demonstrations. By eliminating the need for parameter updates at deployment time, this paradigm supports few-shot adaptation to novel tasks. However, recent ICIL methods rely on Transformers, which have computational limitations and tend to underperform when handling longer prompts than those seen during training. In this work, we introduce RoboSSM, a scalable recipe for in-context imitation learning based on state-space models (SSM). Specifically, RoboSSM replaces Transformers with Longhorn -- a state-of-the-art SSM that provides linear-time inference and strong extrapolation capabilities, making it well-suited for long-context prompts. We evaluate our approach on the LIBERO benchmark and compare it against strong Transformer-based ICIL baselines. Experiments show that RoboSSM extrapolates effectively to varying numbers of in-context demonstrations, yields high performance on unseen tasks, and remains robust in long-horizon scenarios. These results highlight the potential of SSMs as an efficient and scalable backbone for ICIL. Our code is available at https://github.com/youngjuY/RoboSSM.<br>
<span id='abs_ch'>中文：RoboSSM提出了一种基于状态空间模型的可扩展上下文模仿学 方法，通过高效处理长上下文并在新任务上表现稳健，超越了基于Transformer的方法。</span><br>
<span id='abs_en'>English: RoboSSM introduces a scalable in-context imitation learning approach using state-space models, outperforming Transformer-based methods with efficient long-context handling and robust performance on novel tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2509.19460.pdf' target='_blank'>https://arxiv.org/pdf/2509.19460.pdf</a></span>   <span><a href='https://github.com/Jasper-aaa/SEIL.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Ye, Jun Cen, Jing Chen, Zhihe Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19460">Self-evolved Imitation Learning in Simulated World</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Imitation learning has been a trend recently, yet training a generalist agent across multiple tasks still requires large-scale expert demonstrations, which are costly and labor-intensive to collect. To address the challenge of limited supervision, we propose Self-Evolved Imitation Learning (SEIL), a framework that progressively improves a few-shot model through simulator interactions. The model first attempts tasksin the simulator, from which successful trajectories are collected as new demonstrations for iterative refinement. To enhance the diversity of these demonstrations, SEIL employs dual-level augmentation: (i) Model-level, using an Exponential Moving Average (EMA) model to collaborate with the primary model, and (ii) Environment-level, introducing slight variations in initial object positions. We further introduce a lightweight selector that filters complementary and informative trajectories from the generated pool to ensure demonstration quality. These curated samples enable the model to achieve competitive performance with far fewer training examples. Extensive experiments on the LIBERO benchmark show that SEIL achieves a new state-of-the-art performance in few-shot imitation learning scenarios. Code is available at https://github.com/Jasper-aaa/SEIL.git.<br>
<br>
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2509.19396.pdf' target='_blank'>https://arxiv.org/pdf/2509.19396.pdf</a></span>   <span><a href='https://github.com/at-aaims/OmniFed' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sahil Tyagi, Andrei Cozma, Olivera Kotevska, Feiyi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19396">OmniFed: A Modular Framework for Configurable Federated Learning from Edge to HPC</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Federated Learning (FL) is critical for edge and High Performance Computing (HPC) where data is not centralized and privacy is crucial. We present OmniFed, a modular framework designed around decoupling and clear separation of concerns for configuration, orchestration, communication, and training logic. Its architecture supports configuration-driven prototyping and code-level override-what-you-need customization. We also support different topologies, mixed communication protocols within a single deployment, and popular training algorithms. It also offers optional privacy mechanisms including Differential Privacy (DP), Homomorphic Encryption (HE), and Secure Aggregation (SA), as well as compression strategies. These capabilities are exposed through well-defined extension points, allowing users to customize topology and orchestration, learning logic, and privacy/compression plugins, all while preserving the integrity of the core system. We evaluate multiple models and algorithms to measure various performance metrics. By unifying topology configuration, mixed-protocol communication, and pluggable modules in one stack, OmniFed streamlines FL deployment across heterogeneous environments. Github repository is available at https://github.com/at-aaims/OmniFed.<br>
<span id='abs_ch'>中文: OmniFed是一个模块化的联邦学 框架，通过可插拔架构支持灵活配置、多种拓扑结构和隐私保护机制，简化了异构环境中的部署流程。</span><br>
<span id='abs_en'>English: OmniFed is a modular federated learning framework that enables flexible configuration, supports diverse topologies and privacy mechanisms, and streamlines deployment across heterogeneous environments through its pluggable architecture.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2509.19391.pdf' target='_blank'>https://arxiv.org/pdf/2509.19391.pdf</a></span>   <span><a href='https://github.com/ax-le/TensLoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Axel Marmoret, Reda Bensaid, Jonathan Lys, Vincent Gripon, FranÃ§ois Leduc-Primeau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19391">TensLoRA: Tensor Alternatives for Low-Rank Adaptation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Low-Rank Adaptation (LoRA) is widely used to efficiently adapt Transformers by adding trainable low-rank matrices to attention projections. While effective, these matrices are considered independent for each attention projection (Query, Key, and Value) and each layer. Recent extensions have considered joint, tensor-based adaptations, but only in limited forms and without a systematic framework. We introduce TensLoRA, a unified framework that aggregates LoRA updates into higher-order tensors and models a broad family of tensor-based low-rank adaptations. Our formulation generalizes existing tensor-based methods and enables mode-specific compression rates, allowing parameter budgets to be tailored according to the modality and task. Experiments on vision and language benchmarks reveal that the tensor construction directly impacts performance, sometimes better than standard LoRA under similar parameter counts.<br>
<br>
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2509.19332.pdf' target='_blank'>https://arxiv.org/pdf/2509.19332.pdf</a></span>   <span><a href='https://github.com/Zhijin-Guo1/quantifying-compositionality' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijin Guo, Chenhao Xue, Zhaozhen Xu, Hongbo Bo, Yuxuan Ye, Janet B. Pierrehumbert, Martha Lewis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19332">Quantifying Compositionality of Classic and State-of-the-Art Embeddings</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>For language models to generalize correctly to novel expressions, it is critical that they exploit access compositional meanings when this is justified. Even if we don't know what a "pelp" is, we can use our knowledge of numbers to understand that "ten pelps" makes more pelps than "two pelps". Static word embeddings such as Word2vec made strong, indeed excessive, claims about compositionality. The SOTA generative, transformer models and graph models, however, go too far in the other direction by providing no real limits on shifts in meaning due to context. To quantify the additive compositionality, we formalize a two-step, generalized evaluation that (i) measures the linearity between known entity attributes and their embeddings via canonical correlation analysis, and (ii) evaluates additive generalization by reconstructing embeddings for unseen attribute combinations and checking reconstruction metrics such as L2 loss, cosine similarity, and retrieval accuracy. These metrics also capture failure cases where linear composition breaks down. Sentences, knowledge graphs, and word embeddings are evaluated and tracked the compositionality across all layers and training stages. Stronger compositional signals are observed in later training stages across data modalities, and in deeper layers of the transformer-based model before a decline at the top layer. Code is available at https://github.com/Zhijin-Guo1/quantifying-compositionality.<br>
<span id='abs_ch'>Chinese: 本 究提出了一种量化语言模型 法组合性的两步评估方法，发现在后期训练阶段和深层网络中存在更强的组合性信号，但在顶层出现下降。</span><br>
<span id='abs_en'>English: This study introduces a two-step evaluation method to quantify additive compositionality in language models, revealing stronger compositional signals in later training stages and deeper layers before a decline at the top layer.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2509.19331.pdf' target='_blank'>https://arxiv.org/pdf/2509.19331.pdf</a></span>   <span><a href='https://github.com/EonHao/Holographic-Transformers' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Enhao Huang, Zhiyu Zhang, Tianxiang Xu, Chunshu Xia, Kaichun Hu, Yuchen Yang, Tongtong Pan, Dong Dong, Zhan Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19331">Holographic Transformers for Complex-Valued Signal Processing: Integrating Phase Interference into Self-Attention</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Complex-valued signals encode both amplitude and phase, yet most deep models treat attention as real-valued correlation, overlooking interference effects. We introduce the Holographic Transformer, a physics-inspired architecture that incorporates wave interference principles into self-attention. Holographic attention modulates interactions by relative phase and coherently superimposes values, ensuring consistency between amplitude and phase. A dual-headed decoder simultaneously reconstructs the input and predicts task outputs, preventing phase collapse when losses prioritize magnitude over phase. We demonstrate that holographic attention implements a discrete interference operator and maintains phase consistency under linear mixing. Experiments on PolSAR image classification and wireless channel prediction show strong performance, achieving high classification accuracy and F1 scores, low regression error, and increased robustness to phase perturbations. These results highlight that enforcing physical consistency in attention leads to generalizable improvements in complex-valued learning and provides a unified, physics-based framework for coherent signal modeling. The code is available at https://github.com/EonHao/Holographic-Transformers.<br>
<span id='abs_ch'>中文摘要：全息变换器将波动干涉原理引入自注意力机制，确保复值信号中幅度与相位的一致性，在极化SAR图像分类和 线信道预测等任务中展现出卓越的鲁棒性和准确性。</span><br>
<span id='abs_en'>English Summary: The Holographic Transformer integrates wave interference principles into self-attention to maintain phase consistency in complex-valued signals, demonstrating superior performance in tasks like PolSAR classification and wireless prediction through enhanced robustness and accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2509.19326.pdf' target='_blank'>https://arxiv.org/pdf/2509.19326.pdf</a></span>   <span><a href='https://github.com/RichardLRC/Peer-Review' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruochi Li, Haoxuan Zhang, Edward Gehringer, Ting Xiao, Junhua Ding, Haihua Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19326">Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The surge in scientific submissions has placed increasing strain on the traditional peer-review process, prompting the exploration of large language models (LLMs) for automated review generation. While LLMs demonstrate competence in producing structured and coherent feedback, their capacity for critical reasoning, contextual grounding, and quality sensitivity remains limited. To systematically evaluate these aspects, we propose a comprehensive evaluation framework that integrates semantic similarity analysis and structured knowledge graph metrics to assess LLM-generated reviews against human-written counterparts. We construct a large-scale benchmark of 1,683 papers and 6,495 expert reviews from ICLR and NeurIPS in multiple years, and generate reviews using five LLMs. Our findings show that LLMs perform well in descriptive and affirmational content, capturing the main contributions and methodologies of the original work, with GPT-4o highlighted as an illustrative example, generating 15.74% more entities than human reviewers in the strengths section of good papers in ICLR 2025. However, they consistently underperform in identifying weaknesses, raising substantive questions, and adjusting feedback based on paper quality. GPT-4o produces 59.42% fewer entities than real reviewers in the weaknesses and increases node count by only 5.7% from good to weak papers, compared to 50% in human reviews. Similar trends are observed across all conferences, years, and models, providing empirical foundations for understanding the merits and defects of LLM-generated reviews and informing the development of future LLM-assisted reviewing tools. Data, code, and more detailed results are publicly available at https://github.com/RichardLRC/Peer-Review.<br>
<span id='abs_ch'>中文: 该 究评估了大语言模型在自动同行评审中的应用，发现其虽能有效总结论文优点，但在批判性分析和 据论文质量调整反馈方面表现不足，这一结论基于对大量学术论文和评审的大规模基准测试得出。</span><br>
<span id='abs_en'>English: The study evaluates large language models (LLMs) for automated peer review, finding they excel in summarizing strengths but struggle with critical analysis and adapting feedback to paper quality, as demonstrated through a comprehensive benchmark of academic papers and reviews.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2509.19322.pdf' target='_blank'>https://arxiv.org/pdf/2509.19322.pdf</a></span>   <span><a href='https://github.com/usnistgov/readme_ai' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Millie Vyas, Timothy Blattner, Alden Dima
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19322">Readme_AI: Dynamic Context Construction for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite being trained on significant amounts of data, Large Language Models (LLMs) can provide inaccurate or unreliable information in the context of a user's specific query. Given query-specific context significantly improves the usefulness of its responses. In this paper, we present a specification that can be used to dynamically build context for data sources. The data source owner creates the file containing metadata for LLMs to use when reasoning about dataset-related queries. To demonstrate our proposed specification, we created a prototype Readme_AI Model Context Protocol (MCP) server that retrieves the metadata from the data source and uses it to dynamically build context. Some features that make this specification dynamic are the extensible types that represent crawling web-pages, fetching data from data repositories, downloading and parsing publications, and general text. The context is formatted and grouped using user-specified tags that provide clear contextual information for the LLM to reason about the content. We demonstrate the capabilities of this early prototype by asking the LLM about the NIST-developed Hedgehog library, for which common LLMs often provides inaccurate and irrelevant responses containing hallucinations. With Readme_AI, the LLM receives enough context that it is now able to reason about the library and its use, and even generate code interpolated from examples that were included in the Readme_AI file provided by Hedgehog's developer. Our primary contribution is a extensible protocol for dynamically grounding LLMs in specialized, owner-provided data, enhancing responses from LLMs and reducing hallucinations. The source code for the Readme_AI tool is posted here: https://github.com/usnistgov/readme_ai .<br>
<span id='abs_ch'>中文摘要：本文提出的Readme_AI协议能动态构建数据源上下文，通过让LLMs获取结构化元数据来显著提升回答准确性并减少幻觉现象。</span><br>
<span id='abs_en'>English Summary: This paper introduces Readme_AI, a dynamic protocol that enables LLMs to access structured metadata from data sources, significantly improving response accuracy and reducing hallucinations by providing query-specific context.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2509.19319.pdf' target='_blank'>https://arxiv.org/pdf/2509.19319.pdf</a></span>   <span><a href='https://github.com/glee4810/FHIR-AgentBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gyubok Lee, Elea Bach, Eric Yang, Tom Pollard, Alistair Johnson, Edward Choi, Yugang jia, Jong Ha Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19319">FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The recent shift toward the Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR) standard opens a new frontier for clinical AI, demanding LLM agents to navigate complex, resource-based data models instead of conventional structured health data. However, existing benchmarks have lagged behind this transition, lacking the realism needed to evaluate recent LLMs on interoperable clinical data. To bridge this gap, we introduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical questions in the HL7 FHIR standard. Using this benchmark, we systematically evaluate agentic frameworks, comparing different data retrieval strategies (direct FHIR API calls vs. specialized tools), interaction patterns (single-turn vs. multi-turn), and reasoning strategies (natural language vs. code generation). Our experiments highlight the practical challenges of retrieving data from intricate FHIR resources and the difficulty of reasoning over them, both of which critically affect question answering performance. We publicly release the FHIR-AgentBench dataset and evaluation suite (https://github.com/glee4810/FHIR-AgentBench) to promote reproducible research and the development of robust, reliable LLM agents for clinical applications.<br>
<span id='abs_ch'>中文: 本 究推出FHIR-AgentBench基准，利用真实临床数据基于HL7 FHIR 准评估大语言模型代理的数据检索与推理能力，填补现有评估空白，推动临床人工智能应用的稳健发展。</span><br>
<span id='abs_en'>English: The study introduces FHIR-AgentBench, a benchmark using real-world clinical data in the HL7 FHIR standard to evaluate LLM agents' performance in data retrieval and reasoning, addressing gaps in existing assessments and promoting development for clinical AI applications.</span>Paperid:448,https://arxiv.org/pdf/2509.19297.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2509.19252.pdf' target='_blank'>https://arxiv.org/pdf/2509.19252.pdf</a></span>   <span><a href='https://github.com/TeCSAR-UNCC/Pose-Quantization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriel Maldonado, Narges Rashvand, Armin Danesh Pazho, Ghazal Alinezhad Noghre, Vinit Katariya, Hamed Tabkhi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19252">Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Continuous human motion understanding remains a core challenge in computer vision due to its high dimensionality and inherent redundancy. Efficient compression and representation are crucial for analyzing complex motion dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework with dense motion tokenization for compressing spatio-temporal heatmaps while preserving the fine-grained traces of human motion. Our approach combines dense motion tokenization with adversarial refinement, which eliminates reconstruction artifacts like motion smearing and temporal misalignment observed in non-adversarial baselines. Our experiments on the CMU Panoptic dataset provide conclusive evidence of our method's superiority, outperforming the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%. Furthermore, our dense tokenization strategy enables a novel analysis of motion complexity, revealing that 2D motion can be optimally represented with a compact 128-token vocabulary, while 3D motion's complexity demands a much larger 1024-token codebook for faithful reconstruction. These results establish practical deployment feasibility across diverse motion analysis applications. The code base for this work is available at https://github.com/TeCSAR-UNCC/Pose-Quantization.<br>
<span id='abs_ch'>中文: 本 究提出了一种对抗性优化的VQ-GAN框架，通过密集运动 记化有效压缩人体运动数据，在重建质量和时间稳定性上显著优于基线方法，同时揭示了2D和3D运动表征的最佳词汇量规模。English: This study presents an adversarially-refined VQ-GAN framework that effectively compresses human motion data using dense tokenization, significantly outperforming baselines in reconstruction quality and temporal stability while revealing optimal vocabulary sizes for 2D and 3D motion representation.Paperid:450,https://arxiv.org/pdf/2509.19236.pdfGitHub</span><br>
<span id='abs_en'>English: This study presents an adversarially-refined VQ-GAN framework that effectively compresses human motion data using dense tokenization, significantly outperforming baselines in reconstruction quality and temporal stability while revealing optimal vocabulary sizes for 2D and 3D motion representation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2509.19236.pdf' target='_blank'>https://arxiv.org/pdf/2509.19236.pdf</a></span>   <span><a href='https://github.com/1737423697/AgentInit' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunhao Tian, Yutong Wang, Xuebo Liu, Zhexuan Wang, Liang Ding, Miao Zhang, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19236">AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Proper initialization is crucial for any system, particularly in multi-agent systems (MAS), where it plays a pivotal role in determining both the system's efficiency and effectiveness. However, existing MAS initialization methods do not fully account for the collaborative needs of the generated agents in subsequent stages. Inspired by the principles of effective team composition, we propose AgentInit, which aims to optimize the structure of agent teams. Specifically, in addition to multi-round interactions and reflections between agents during agent generation, AgentInit incorporates a Natural Language to Format mechanism to ensure consistency and standardization. Balanced team selection strategies using Pareto principles are subsequently applied to jointly consider agent team diversity and task relevance to promote effective and efficient collaboration and enhance overall system performance. Experiments show that AgentInit consistently outperforms state-of-the-art initialization methods and pre-defined strategies across various frameworks and tasks, achieving an overall performance improvement of up to 1.2 and 1.6, respectively, while also significantly reducing token consumption. Further analysis confirms its strong transferability to similar tasks and verifies the effectiveness of its key components, demonstrating its capability and adaptability as a reliable MAS initialization method. Source code and models are available at https://github.com/1737423697/AgentInit.<br>
<span id='abs_ch'>中文: AgentInit作为一种新型多智能体系统初始化方法，通过结构化交互、 准化 式和均衡选择策略优化团队构建，在各类任务中实现卓越性能与效率提升，同时显著降低资源消耗。</span><br>
<span id='abs_en'>English: AgentInit, a novel initialization method for multi-agent systems, optimizes team composition through structured interactions, standardized formatting, and balanced selection strategies, achieving superior performance and efficiency across various tasks while reducing resource consumption.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2509.19165.pdf' target='_blank'>https://arxiv.org/pdf/2509.19165.pdf</a></span>   <span><a href='https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yun Wang, Junjie Hu, Junhui Hou, Chenghao Zhang, Renwei Yang, Dapeng Oliver Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19165">RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent self-supervised stereo matching methods have made significant progress, but their performance significantly degrades under adverse weather conditions such as night, rain, and fog. We identify two primary weaknesses contributing to this performance degradation. First, adverse weather introduces noise and reduces visibility, making CNN-based feature extractors struggle with degraded regions like reflective and textureless areas. Second, these degraded regions can disrupt accurate pixel correspondences, leading to ineffective supervision based on the photometric consistency assumption. To address these challenges, we propose injecting robust priors derived from the visual foundation model into the CNN-based feature extractor to improve feature representation under adverse weather conditions. We then introduce scene correspondence priors to construct robust supervisory signals rather than relying solely on the photometric consistency assumption. Specifically, we create synthetic stereo datasets with realistic weather degradations. These datasets feature clear and adverse image pairs that maintain the same semantic context and disparity, preserving the scene correspondence property. With this knowledge, we propose a robust self-supervised training paradigm, consisting of two key steps: robust self-supervised scene correspondence learning and adverse weather distillation. Both steps aim to align underlying scene results from clean and adverse image pairs, thus improving model disparity estimation under adverse weather effects. Extensive experiments demonstrate the effectiveness and versatility of our proposed solution, which outperforms existing state-of-the-art self-supervised methods. Codes are available at \textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.<br>
<span id='abs_ch'>中文: 针对现有自监督立体匹配方法在恶劣天气下 特征提取困难和像 对应关系 坏而性能下降的问题，提出融合视觉基础模型先验与场景对应学 的鲁棒训练范式，有效提升了模型在雨雾等复杂环境下的视差估计精度。English: Recent self-supervised stereo matching methods struggle in adverse weather due to degraded feature extraction and disrupted pixel correspondences, prompting the development of a robust training paradigm that integrates visual foundation model priors and scene correspondence learning to significantly enhance performance.Paperid:453,https://arxiv.org/pdf/2509.19159.pdfGitHub</span><br>
<span id='abs_en'>English: Recent self-supervised stereo matching methods struggle in adverse weather due to degraded feature extraction and disrupted pixel correspondences, prompting the development of a robust training paradigm that integrates visual foundation model priors and scene correspondence learning to significantly enhance performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2509.18851.pdf' target='_blank'>https://arxiv.org/pdf/2509.18851.pdf</a></span>   <span><a href='https://github.com/nangongrui-ngr/NGRPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gongrui Nan, Siye Chen, Jing Huang, Mengyu Lu, Dexun Wang, Chunmei Xie, Weiqi Xiong, Xianzhou Zeng, Qixuan Zhou, Yadong Li, Xingzhong Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18851">NGRPO: Negative-enhanced Group Relative Policy Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs) across various tasks. However, GRPO, a representative RLVR algorithm, suffers from a critical limitation: when all responses within a group are either entirely correct or entirely incorrect, the model fails to learn from these homogeneous responses. This is particularly problematic for homogeneously incorrect groups, where GRPO's advantage function yields a value of zero, leading to null gradients and the loss of valuable learning signals. To overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy Optimization), an algorithm designed to convert homogeneous errors into robust learning signals. First, NGRPO introduces Advantage Calibration. This mechanism hypothesizes the existence of a virtual maximum-reward sample during advantage calculation, thereby altering the mean and variance of rewards within a group and ensuring that the advantages for homogeneously incorrect samples are no longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the update magnitude for positive samples while imposing stricter constraints on that of negative samples. This serves to stabilize the exploration pressure introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO, DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and AIME2025. These results validate NGRPO's ability to learn from homogeneous errors, leading to stable and substantial improvements in mathematical reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.<br>
<span id='abs_ch'>Chinese: NGRPO通过引入优势 准和非对称裁剪机制，解决了GRPO算法 法从同质错误中学 的缺陷，在MATH500和AIME2025等数学推理基准上实现了显著性能提升。</span><br>
<span id='abs_en'>English: NGRPO addresses GRPO's limitation of failing to learn from homogeneous incorrect responses by introducing Advantage Calibration and Asymmetric Clipping, significantly improving mathematical reasoning performance in benchmarks like MATH500 and AIME2025.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2509.18801.pdf' target='_blank'>https://arxiv.org/pdf/2509.18801.pdf</a></span>   <span><a href='https://github.com/Kuangxd/Neural-KMDS-Net/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kuang Xiaodong, Li Bingxuan, Li Yuan, Rao Fan, Ma Gege, Xie Qingguo, Mok Greta S P, Liu Huafeng, Zhu Wentao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18801">A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Achieving high image quality for temporal frames in dynamic positron emission tomography (PET) is challenging due to the limited statistic especially for the short frames. Recent studies have shown that deep learning (DL) is useful in a wide range of medical image denoising tasks. In this paper, we propose a model-based neural network for dynamic PET image denoising. The inter-frame spatial correlation and intra-frame structural consistency in dynamic PET are used to establish the kernel space-based multidimensional sparse (KMDS) model. We then substitute the inherent forms of the parameter estimation with neural networks to enable adaptive parameters optimization, forming the end-to-end neural KMDS-Net. Extensive experimental results from simulated and real data demonstrate that the neural KMDS-Net exhibits strong denoising performance for dynamic PET, outperforming previous baseline methods. The proposed method may be used to effectively achieve high temporal and spatial resolution for dynamic PET. Our source code is available at https://github.com/Kuangxd/Neural-KMDS-Net/tree/main.<br>
<span id='abs_ch'>中文: 本文提出了一种基于模型的神经网络KMDS-Net，利用动态PET图像的帧间空间相关性和帧内结构一致性进行去噪处理，在仿真和真实数据实验中均展现出优于现有方法的性能。</span><br>
<span id='abs_en'>English: This paper introduces a neural KMDS-Net, a model-based deep learning approach that leverages inter-frame spatial correlations and intra-frame structural consistency to effectively denoise dynamic PET images, demonstrating superior performance over existing methods in both simulated and real data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2509.18633.pdf' target='_blank'>https://arxiv.org/pdf/2509.18633.pdf</a></span>   <span><a href='https://github.com/yaramohajerani/spatial-climate-ABM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yara Mohajerani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18633">Adaptive Learning in Spatial Agent-Based Models for Climate Risk Assessment: A Geospatial Framework with Evolutionary Economic Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Climate risk assessment requires modelling complex interactions between spatially heterogeneous hazards and adaptive economic systems. We present a novel geospatial agent-based model that integrates climate hazard data with evolutionary learning for economic agents. Our framework combines Mesa-based spatial modelling with CLIMADA climate impact assessment, introducing adaptive learning behaviours that allow firms to evolve strategies for budget allocation, pricing, wages, and risk adaptation through fitness-based selection and mutation. We demonstrate the framework using riverine flood projections under RCP8.5 until 2100, showing that evolutionary adaptation enables firms to converge with baseline (no hazard) production levels after decades of disruption due to climate stress. Our results reveal systemic risks where even agents that are not directly exposed to floods face impacts through supply chain disruptions, with the end-of-century average price of goods 5.6% higher under RCP8.5 compared to the baseline. This open-source framework provides financial institutions and companies with tools to quantify both direct and cascading climate risks while evaluating cost-effective adaptation strategies.<br>
<span id='abs_ch'>中文摘要：本 究提出了一种结合气候灾害与自适应经济行为的地理空间代理模型，通过洪水预测表明进化学 能使企业在气候干扰后恢复生产水平，同时揭示了导致商品价 显著上涨的系统性供应链风险。</span><br>
<span id='abs_en'>English Summary: This study introduces a geospatial agent-based model integrating climate hazards with adaptive economic behaviors, demonstrating through flood projections that evolutionary learning enables firms to recover production levels despite climate disruptions while revealing systemic supply chain risks causing significant price increases.Paperid:474,https://arxiv.org/pdf/2509.18627.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2509.18627.pdf' target='_blank'>https://arxiv.org/pdf/2509.18627.pdf</a></span>   <span><a href='https://github.com/ShanechiLab/BRAID' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Parsa Vahidi, Omid G. Sani, Maryam M. Shanechi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18627">BRAID: Input-Driven Nonlinear Dynamical Modeling of Neural-Behavioral Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Neural populations exhibit complex recurrent structures that drive behavior, while continuously receiving and integrating external inputs from sensory stimuli, upstream regions, and neurostimulation. However, neural populations are often modeled as autonomous dynamical systems, with little consideration given to the influence of external inputs that shape the population activity and behavioral outcomes. Here, we introduce BRAID, a deep learning framework that models nonlinear neural dynamics underlying behavior while explicitly incorporating any measured external inputs. Our method disentangles intrinsic recurrent neural population dynamics from the effects of inputs by including a forecasting objective within input-driven recurrent neural networks. BRAID further prioritizes the learning of intrinsic dynamics that are related to a behavior of interest by using a multi-stage optimization scheme. We validate BRAID with nonlinear simulations, showing that it can accurately learn the intrinsic dynamics shared between neural and behavioral modalities. We then apply BRAID to motor cortical activity recorded during a motor task and demonstrate that our method more accurately fits the neural-behavioral data by incorporating measured sensory stimuli into the model and improves the forecasting of neural-behavioral data compared with various baseline methods, whether input-driven or not.<br>
<span id='abs_ch'>Chinese: BRAID是一种深度学 框架，通过整合外部输入并分离内在循环动态与输入影响来模拟神经动态，从而提高了神经行为数据预测和拟合的准确性。</span><br>
<span id='abs_en'>English: BRAID is a deep learning framework that models neural dynamics by incorporating external inputs and disentangling intrinsic recurrent dynamics from input effects, improving the accuracy of neural-behavioral data forecasting and fitting.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2509.18585.pdf' target='_blank'>https://arxiv.org/pdf/2509.18585.pdf</a></span>   <span><a href='https://github.com/Benjamin-Ricky/TsqLoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Chen, Yifei Han, Long Zhang, Yue Du, Bin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18585">TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fine-tuning large pre-trained models for downstream tasks has become a fundamental approach in natural language processing. Fully fine-tuning all model parameters is computationally expensive and memory-intensive, especially in resource-constrained environments. Existing parameter-efficient fine-tuning methods reduce the number of trainable parameters but typically overlook the varying sensitivity of different model layers and the importance of training data. In this work, we propose TsqLoRA, a novel method that integrates data-quality-driven selection with sensitivity-aware low-rank adaptation, consisted of two main components: a quality-aware sampling mechanism for selecting the most informative training data, and a dynamic rank allocation module that adjusts the rank of each layer based on its sensitivity to parameter updates. The experimental results demonstrate that TsqLoRA improves fine-tuning efficiency while maintaining or even improving performance on a variety of NLP tasks. Our code will be available at https://github.com/Benjamin-Ricky/TsqLoRA.<br>
<span id='abs_ch'>中文：TsqLoRA是一种创新的参数高效微调方法，通过质量感知数据选择和基于敏感性的动态秩分配，在保持或提升多种NLP任务性能的同时显著提高了微调效率。</span><br>
<span id='abs_en'>English: TsqLoRA is a novel parameter-efficient fine-tuning method that combines quality-aware data selection with sensitivity-based dynamic rank allocation to enhance efficiency while maintaining or improving performance across NLP tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2509.18575.pdf' target='_blank'>https://arxiv.org/pdf/2509.18575.pdf</a></span>   <span><a href='https://github.com/blindspotorg/RankingBlindSpot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaoyao Qian, Yifan Zeng, Yuchao Jiang, Chelsi Jain, Huazheng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18575">The Ranking Blind Spot: Decision Hijacking in LLM-based Text Ranking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated strong performance in information retrieval tasks like passage ranking. Our research examines how instruction-following capabilities in LLMs interact with multi-document comparison tasks, identifying what we term the "Ranking Blind Spot", a characteristic of LLM decision processes during comparative evaluation. We analyze how this ranking blind spot affects LLM evaluation systems through two approaches: Decision Objective Hijacking, which alters the evaluation goal in pairwise ranking systems, and Decision Criteria Hijacking, which modifies relevance standards across ranking schemes. These approaches demonstrate how content providers could potentially influence LLM-based ranking systems to affect document positioning. These attacks aim to force the LLM ranker to prefer a specific passage and rank it at the top. Malicious content providers can exploit this weakness, which helps them gain additional exposure by attacking the ranker. In our experiment, We empirically show that the proposed attacks are effective in various LLMs and can be generalized to multiple ranking schemes. We apply these attack to realistic examples to show their effectiveness. We also found stronger LLMs are more vulnerable to these attacks. Our code is available at: https://github.com/blindspotorg/RankingBlindSpot<br>
<span id='abs_ch'>中文: 大语言模型存在"排序盲点"漏洞，其比较评估过程会通过决策目 劫持和决策 准劫持被恶意内容提供者操纵，从而人为提升文档排名，实验表明性能更强的模型反而更容易受到此类攻击。</span><br>
<span id='abs_en'>English: Large Language Models exhibit a "Ranking Blind Spot" vulnerability where their comparative evaluation processes can be manipulated through Decision Objective and Criteria Hijacking, allowing malicious content providers to artificially boost document rankings, with experiments showing stronger LLMs are paradoxically more susceptible to these attacks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2509.18562.pdf' target='_blank'>https://arxiv.org/pdf/2509.18562.pdf</a></span>   <span><a href='https://github.com/jiaxunyang256/PCLD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxun Yang, Yifei Han, Long Zhang, Yujie Liu, Bin Li, Bo Gao, Yangfan He, Kejia Zhan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18562">CPCLDETECTOR: Knowledge Enhancement and Alignment Selection for Chinese Patronizing and Condescending Language Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chinese Patronizing and Condescending Language (CPCL) is an implicitly discriminatory toxic speech targeting vulnerable groups on Chinese video platforms. The existing dataset lacks user comments, which are a direct reflection of video content. This undermines the model's understanding of video content and results in the failure to detect some CPLC videos. To make up for this loss, this research reconstructs a new dataset PCLMMPLUS that includes 103k comment entries and expands the dataset size. We also propose the CPCLDetector model with alignment selection and knowledge-enhanced comment content modules. Extensive experiments show the proposed CPCLDetector outperforms the SOTA on PCLMM and achieves higher performance on PCLMMPLUS . CPLC videos are detected more accurately, supporting content governance and protecting vulnerable groups. Code and dataset are available at https://github.com/jiaxunyang256/PCLD.<br>
<br>
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2509.18536.pdf' target='_blank'>https://arxiv.org/pdf/2509.18536.pdf</a></span>   <span><a href='https://github.com/scai-research/ccqa_official' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Young Kim, Ji Won Yoon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18536">CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, inference-time reasoning strategies have further improved the accuracy of large language models (LLMs), but their effectiveness on smaller models remains unclear. Based on the observation that conventional approaches often fail to improve performance in this context, we propose \textbf{C}ycle-\textbf{C}onsistency in \textbf{Q}uestion \textbf{A}nswering (CCQA), a novel reasoning method that can be effectively applied to SLMs. Inspired by cycle consistency, CCQA generates a question from each reasoning path and answer, evaluates each by its similarity to the original question, and then selects the candidate solution with the highest similarity score as the final response. Since conventional SLMs struggle to generate accurate questions from their own reasoning paths and answers, we employ a lightweight Flan-T5 model specialized for question generation to support this process efficiently. From the experimental results, it is verified that CCQA consistently outperforms existing state-of-the-art (SOTA) methods across eight models on mathematical and commonsense reasoning benchmarks. Furthermore, our method establishes a new practical baseline for efficient reasoning in SLMs. Source code can be found at https://github.com/scai-research/ccqa_official.<br>
<span id='abs_ch'>中文: 提出的CCQA方法通过利用循环一致性从推理路径生成并评估问题，有效提升了小型语言模型的推理能力，在多个基准测试中持续优于现有方法，并为高效推理设定了新的实用基准。</span><br>
<span id='abs_en'>English: The proposed CCQA method enhances reasoning in smaller language models by generating and evaluating questions from reasoning paths using cycle consistency, consistently outperforming existing methods across benchmarks and establishing a new baseline for efficient reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2509.18521.pdf' target='_blank'>https://arxiv.org/pdf/2509.18521.pdf</a></span>   <span><a href='https://github.com/RLsys-Foundation/APRIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhen Zhou, Jiajun Li, Yusheng Su, Gowtham Ramesh, Zilin Zhu, Xiang Long, Chenyang Zhao, Jin Pan, Xiaodong Yu, Ze Wang, Kangrui Du, Jialian Wu, Ximeng Sun, Jiang Liu, Qiaolin Yu, Hao Chen, Zicheng Liu, Emad Barsoum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18521">APRIL: Active Partial Rollouts in Reinforcement Learning to Tame Long-tail Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning (RL) has become a cornerstone in advancing large-scale pre-trained language models (LLMs). Successive generations, including GPT-o series, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale RL training to enhance reasoning and coding capabilities. To meet the community's growing RL needs, numerous RL frameworks have been proposed. However, RL training remains computationally expensive, with rollout generation accounting for more than 90% of total runtime. In addition, its efficiency is often constrained by the long-tail distribution of rollout response lengths, where a few lengthy responses stall entire batches, leaving GPUs idle and underutilized. As model and rollout sizes continue to grow, this bottleneck increasingly limits scalability. To address this challenge, we propose Active Partial Rollouts in Reinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the rollout phase, APRIL over-provisions rollout requests, terminates once the target number of responses is reached, and recycles incomplete responses for continuation in future steps. This strategy ensures that no rollouts are discarded while substantially reducing GPU idle time. Experiments show that APRIL improves rollout throughput by 22.5% on average (at most 44%) across commonly used RL algorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves 2.1% on average(at most 8%) higher final accuracy across tasks. Moreover, APRIL is both framework and hardware agnostic, already integrated into the slime RL framework, and deployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies system-level and algorithmic considerations in proposing APRIL, with the aim of advancing RL training efficiency and inspiring further optimizations in RL systems. Our codebase is available at https://github.com/RLsys-Foundation/APRIL<br>
<span id='abs_ch'>中文摘要：APRIL方法通过动态管理强化学 中的rollout生成过程，有效缓解长尾响应分布导致的GPU闲置问题，在多种任务和框架中显著提升了训练效率和最终精度。</span><br>
<span id='abs_en'>English Summary: The proposed APRIL method enhances reinforcement learning efficiency by dynamically managing rollout generation to reduce GPU idle time caused by long-tail response distributions, achieving significant improvements in throughput and accuracy across various tasks and frameworks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2509.18507.pdf' target='_blank'>https://arxiv.org/pdf/2509.18507.pdf</a></span>   <span><a href='https://github.com/ShanechiLab/SBIND/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Hosseini, Maryam M. Shanechi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18507">Dynamical Modeling of Behaviorally Relevant Spatiotemporal Patterns in Neural Imaging Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>High-dimensional imaging of neural activity, such as widefield calcium and functional ultrasound imaging, provide a rich source of information for understanding the relationship between brain activity and behavior. Accurately modeling neural dynamics in these modalities is crucial for understanding this relationship but is hindered by the high-dimensionality, complex spatiotemporal dependencies, and prevalent behaviorally irrelevant dynamics in these modalities. Existing dynamical models often employ preprocessing steps to obtain low-dimensional representations from neural image modalities. However, this process can discard behaviorally relevant information and miss spatiotemporal structure. We propose SBIND, a novel data-driven deep learning framework to model spatiotemporal dependencies in neural images and disentangle their behaviorally relevant dynamics from other neural dynamics. We validate SBIND on widefield imaging datasets, and show its extension to functional ultrasound imaging, a recent modality whose dynamical modeling has largely remained unexplored. We find that our model effectively identifies both local and long-range spatial dependencies across the brain while also dissociating behaviorally relevant neural dynamics. Doing so, SBIND outperforms existing models in neural-behavioral prediction. Overall, SBIND provides a versatile tool for investigating the neural mechanisms underlying behavior using imaging modalities.<br>
<span id='abs_ch'>中文: SBIND是一种新型深度学 框架，能有效建模神经影像数据的时空依赖性并分离行为相关动态，在宽场和功能超声成像等模态的神经行为预测中优于现有模型。</span><br>
<span id='abs_en'>English: SBIND is a novel deep learning framework that effectively models spatiotemporal dependencies in neural imaging data to disentangle behaviorally relevant dynamics, outperforming existing models in neural-behavioral prediction across modalities like widefield and functional ultrasound imaging.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/2509.18458.pdf' target='_blank'>https://arxiv.org/pdf/2509.18458.pdf</a></span>   <span><a href='https://github.com/kaiserdan/cogniload,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Kaiser, Arnoldo Frigessi, Ali Ramezani-Kebrya, Benjamin Ricaud
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18458">CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current benchmarks for long-context reasoning in Large Language Models (LLMs) often blur critical factors like intrinsic task complexity, distractor interference, and task length. To enable more precise failure analysis, we introduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load Theory (CLT). CogniLoad generates natural-language logic puzzles with independently tunable parameters that reflect CLT's core dimensions: intrinsic difficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($Ï$) regulates extraneous load; and task length ($N$) serves as an operational proxy for conditions demanding germane load. Evaluating 22 SotA reasoning LLMs, CogniLoad reveals distinct performance sensitivities, identifying task length as a dominant constraint and uncovering varied tolerances to intrinsic complexity and U-shaped responses to distractor ratios. By offering systematic, factorial control over these cognitive load dimensions, CogniLoad provides a reproducible, scalable, and diagnostically rich tool for dissecting LLM reasoning limitations and guiding future model development.<br>
<br>
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2509.18420.pdf' target='_blank'>https://arxiv.org/pdf/2509.18420.pdf</a></span>   <span><a href='https://github.com/Skripkon/IFEval-FC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolai Skripko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18420">Instruction-Following Evaluation in Function Calling for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Function calling is a core capability of large language models, essential for AI agents. Existing benchmarks such as the Berkeley Function Calling Leaderboard (BFCL), tau^2-Bench (arXiv:2506.07982), and ACEBench (arXiv:2501.12851) evaluate argument correctness but do not test adherence to format instructions embedded in parameter descriptions, such as enclosing values in double quotes or using ISO date formats. We introduce IFEval-FC, a benchmark inspired by IFEval (arXiv:2311.07911) that assesses precise instruction following in function calling. IFEval-FC encodes verifiable formats directly within JSON schema descriptions, for example specifying that a value must not contain punctuation. It includes 750 test cases, each consisting of a function with an embedded format for one of its input parameters and a corresponding user query. Evaluation is fully algorithmic, ensuring objectivity, reproducibility, and scalability. Our results show that even state-of-the-art proprietary models, including GPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules, highlighting a practical limitation for real-world agent systems. The complete codebase and data are publicly available at https://github.com/Skripkon/IFEval-FC.<br>
<span id='abs_ch'>中文摘要：作者提出了IFEval-FC基准测试，专门评估大语言模型在函数调用中遵循精确 式指令的能力，结果表明即使GPT-5等先进模型也经常 法遵守基本 式规则，这暴露了现有基准仅测试参数正确性的不足。English Summary: The authors introduce IFEval-FC, a benchmark that evaluates large language models' ability to follow precise formatting instructions in function calling, revealing that even advanced models like GPT-5 struggle with basic format rules despite existing benchmarks focusing only on argument correctness.Paperid:490,https://arxiv.org/pdf/2509.18388.pdfGitHub</span><br>
<span id='abs_en'>English Summary: The authors introduce IFEval-FC, a benchmark that evaluates large language models' ability to follow precise formatting instructions in function calling, revealing that even advanced models like GPT-5 struggle with basic format rules despite existing benchmarks focusing only on argument correctness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2509.18354.pdf' target='_blank'>https://arxiv.org/pdf/2509.18354.pdf</a></span>   <span><a href='https://github.com/mehrdadmoradi124/SSDnet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehrdad Moradi, Shengzhe Chen, Hao Yan, Kamran Paynabar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18354">A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Anomaly detection in images is typically addressed by learning from collections of training data or relying on reference samples. In many real-world scenarios, however, such training data may be unavailable, and only the test image itself is provided. We address this zero-shot setting by proposing a single-image anomaly localization method that leverages the inductive bias of convolutional neural networks, inspired by Deep Image Prior (DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key assumption is that natural images often exhibit unified textures and patterns, and that anomalies manifest as localized deviations from these repetitive or stochastic patterns. To learn the deep image prior, we design a patch-based training framework where the input image is fed directly into the network for self-reconstruction, rather than mapping random noise to the image as done in DIP. To avoid the model simply learning an identity mapping, we apply masking, patch shuffling, and small Gaussian noise. In addition, we use a perceptual loss based on inner-product similarity to capture structure beyond pixel fidelity. Our approach needs no external training data, labels, or references, and remains robust in the presence of noise or missing pixels. SSDnet achieves 0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the fabric dataset, outperforming state-of-the-art methods. The implementation code will be released at https://github.com/mehrdadmoradi124/SSDnet<br>
<span id='abs_ch'>中文: SSDnet是一种 需训练数据的零 本异常定位方法，通过基于图像块的自重构网络结合掩 和感知损失来检测异常，在多个基准数据集上取得了领先的性能。</span><br>
<span id='abs_en'>English: SSDnet is a zero-shot anomaly localization method that uses a patch-based self-reconstruction network with masking and perceptual loss to detect anomalies without any training data, achieving state-of-the-art performance on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2509.18196.pdf' target='_blank'>https://arxiv.org/pdf/2509.18196.pdf</a></span>   <span><a href='https://github.com/yongaifadian1/MNV-17' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jialong Mai, Jinxin Ji, Xiaofen Xing, Chen Yang, Weidong Chen, Jingyuan Xing, Xiangmin Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18196">MNV-17: A High-Quality Performative Mandarin Dataset for Nonverbal Vocalization Recognition in Speech</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Mainstream Automatic Speech Recognition (ASR) systems excel at transcribing lexical content, but largely fail to recognize nonverbal vocalizations (NVs) embedded in speech, such as sighs, laughs, and coughs. This capability is important for a comprehensive understanding of human communication, as NVs convey crucial emotional and intentional cues. Progress in NV-aware ASR has been hindered by the lack of high-quality, well-annotated datasets. To address this gap, we introduce MNV-17, a 7.55-hour performative Mandarin speech dataset. Unlike most existing corpora that rely on model-based detection, MNV-17's performative nature ensures high-fidelity, clearly articulated NV instances. To the best of our knowledge, MNV-17 provides the most extensive set of nonverbal vocalization categories, comprising 17 distinct and well-balanced classes of common NVs. We benchmarked MNV-17 on four mainstream ASR architectures, evaluating their joint performance on semantic transcription and NV classification. The dataset and the pretrained model checkpoints will be made publicly available to facilitate future research in expressive ASR.<br>
<span id='abs_ch'>中文: 主流语音识别系统难以识别叹息、笑声等非语言声音，为此我们推出了MNV-17数据集，该高质量 注的普通话语音库包含17类非语言声音，将促进情感语音识别 究的发展。</span><br>
<span id='abs_en'>English: Mainstream ASR systems struggle to recognize nonverbal vocalizations like sighs and laughs, so the MNV-17 dataset is introduced to address this gap by providing high-quality, annotated Mandarin speech with 17 distinct NV categories for improved expressive ASR research.Paperid:495,https://arxiv.org/pdf/2509.18191.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2509.18178.pdf' target='_blank'>https://arxiv.org/pdf/2509.18178.pdf</a></span>   <span><a href='https://github.com/csml-rpi/Foam-Agent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ling Yue, Nithin Somasekharan, Tingwen Zhang, Yadi Cao, Shaowu Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18178">Foam-Agent 2.0: An End-to-End Composable Multi-Agent Framework for Automating CFD Simulation in OpenFOAM</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Computational Fluid Dynamics (CFD) is an essential simulation tool in engineering, yet its steep learning curve and complex manual setup create significant barriers. To address these challenges, we introduce Foam-Agent, a multi-agent framework that automates the entire end-to-end OpenFOAM workflow from a single natural language prompt. Our key innovations address critical gaps in existing systems: 1. An Comprehensive End-to-End Simulation Automation: Foam-Agent is the first system to manage the full simulation pipeline, including advanced pre-processing with a versatile Meshing Agent capable of handling external mesh files and generating new geometries via Gmsh, automatic generation of HPC submission scripts, and post-simulation visualization via ParaView. 2. Composable Service Architecture: Going beyond a monolithic agent, the framework uses Model Context Protocol (MCP) to expose its core functions as discrete, callable tools. This allows for flexible integration and use by other agentic systems, such as Claude-code, for more exploratory workflows. 3. High-Fidelity Configuration Generation: We achieve superior accuracy through a Hierarchical Multi-Index RAG for precise context retrieval and a dependency-aware generation process that ensures configuration consistency. Evaluated on a benchmark of 110 simulation tasks, Foam-Agent achieves an 88.2% success rate with Claude 3.5 Sonnet, significantly outperforming existing frameworks (55.5% for MetaOpenFOAM). Foam-Agent dramatically lowers the expertise barrier for CFD, demonstrating how specialized multi-agent systems can democratize complex scientific computing. The code is public at https://github.com/csml-rpi/Foam-Agent.<br>
<span id='abs_ch'>中文: Foam-Agent是一个多智能体框架，通过单一自然语言提示即可自动化整个OpenFOAM工作流程，在基准测试中达到88.2%的成功率，显著降低了计算流体动力学的专业门槛。</span><br>
<span id='abs_en'>English: Foam-Agent is a multi-agent framework that automates the entire OpenFOAM workflow from a single natural language prompt, achieving an 88.2% success rate on benchmark tests and significantly lowering the expertise barrier for Computational Fluid Dynamics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2509.18148.pdf' target='_blank'>https://arxiv.org/pdf/2509.18148.pdf</a></span>   <span><a href='https://github.com/Kairong-Han/Pseudo-Matching' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kairong Han, Weidong Huang, Taiyang Zhou, Peng Zhen, Kun Kuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18148">Augmenting Limited and Biased RCTs through Pseudo-Sample Matching-Based Observational Data Fusion Method</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In the online ride-hailing pricing context, companies often conduct randomized controlled trials (RCTs) and utilize uplift models to assess the effect of discounts on customer orders, which substantially influences competitive market outcomes. However, due to the high cost of RCTs, the proportion of trial data relative to observational data is small, which only accounts for 0.65\% of total traffic in our context, resulting in significant bias when generalizing to the broader user base. Additionally, the complexity of industrial processes reduces the quality of RCT data, which is often subject to heterogeneity from potential interference and selection bias, making it difficult to correct. Moreover, existing data fusion methods are challenging to implement effectively in complex industrial settings due to the high dimensionality of features and the strict assumptions that are hard to verify with real-world data. To address these issues, we propose an empirical data fusion method called pseudo-sample matching. By generating pseudo-samples from biased, low-quality RCT data and matching them with the most similar samples from large-scale observational data, the method expands the RCT dataset while mitigating its heterogeneity. We validated the method through simulation experiments, conducted offline and online tests using real-world data. In a week-long online experiment, we achieved a 0.41\% improvement in profit, which is a considerable gain when scaled to industrial scenarios with hundreds of millions in revenue. In addition, we discuss the harm to model training, offline evaluation, and online economic benefits when the RCT data quality is not high, and emphasize the importance of improving RCT data quality in industrial scenarios. Further details of the simulation experiments can be found in the GitHub repository https://github.com/Kairong-Han/Pseudo-Matching.<br>
<span id='abs_ch'>中文: 本 究提出了一种伪 本匹配方法，通过将有限且有偏的随机对照试验数据与大量观测数据融合，改善了数据质量，在线测试中实现了0.41%的利润提升，有效解决了工业场景中数据融合的难题。</span><br>
<span id='abs_en'>English: This study introduces a pseudo-sample matching method that enhances the quality of limited and biased randomized controlled trial (RCT) data by integrating it with extensive observational data, leading to a 0.41% profit increase in online tests and addressing challenges in industrial data fusion.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2509.18119.pdf' target='_blank'>https://arxiv.org/pdf/2509.18119.pdf</a></span>   <span><a href='https://github.com/THUDM/MobileRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Xu, Xiao Liu, Xinghan Liu, Jiaqi Fu, Hanchen Zhang, Bohao Jing, Shudan Zhang, Yuting Wang, Wenyi Zhao, Yuxiao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18119">MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Building general-purpose graphical user interface (GUI) agents has become increasingly promising with the progress in vision language models. However, developing effective mobile GUI agents with reinforcement learning (RL) remains challenging due to the heavy-tailed distribution of task difficulty and the inefficiency of large-scale environment sampling. We present an online agentic reinforcement learning framework MOBILERL to enhance GUI agents in mobile environments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO) algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and failure curriculum filtering to adapt the model to different task difficulties. We introduce the shortest path reward adjustment strategy to reshape rewards concerning the task length in multi-turn agentic tasks. Those strategies jointly stabilize RL training, improve sample efficiency, and generate strong performance across diverse mobile apps and tasks. We apply MOBILERL to two open models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B model achieves state-of-the-art results in terms of success rates on both AndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted in the AutoGLM products, and also open-sourced at https://github.com/THUDM/MobileRL.<br>
<span id='abs_ch'>中文摘要：MOBILERL框架通过自适应强化学 策略提升移动GUI代理性能，在Android平台上取得领先成果，并已应用于AutoGLM产品中开源发布。</span><br>
<span id='abs_en'>English Summary: The MOBILERL framework enhances mobile GUI agents through adaptive reinforcement learning strategies, achieving state-of-the-art performance on Android platforms and being implemented in AutoGLM products.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2509.17998.pdf' target='_blank'>https://arxiv.org/pdf/2509.17998.pdf</a></span>   <span><a href='https://github.com/richardcsuwandi/cake' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Richard Cornelius Suwandi, Feng Yin, Juntao Wang, Renjie Li, Tsung-Hui Chang, Sergios Theodoridis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17998">Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The efficiency of Bayesian optimization (BO) relies heavily on the choice of the Gaussian process (GP) kernel, which plays a central role in balancing exploration and exploitation under limited evaluation budgets. Traditional BO methods often rely on fixed or heuristic kernel selection strategies, which can result in slow convergence or suboptimal solutions when the chosen kernel is poorly suited to the underlying objective function. To address this limitation, we propose a freshly-baked Context-Aware Kernel Evolution (CAKE) to enhance BO with large language models (LLMs). Concretely, CAKE leverages LLMs as the crossover and mutation operators to adaptively generate and refine GP kernels based on the observed data throughout the optimization process. To maximize the power of CAKE, we further propose BIC-Acquisition Kernel Ranking (BAKER) to select the most effective kernel through balancing the model fit measured by the Bayesian information criterion (BIC) with the expected improvement at each iteration of BO. Extensive experiments demonstrate that our fresh CAKE-based BO method consistently outperforms established baselines across a range of real-world tasks, including hyperparameter optimization, controller tuning, and photonic chip design. Our code is publicly available at https://github.com/richardcsuwandi/cake.<br>
<span id='abs_ch'>中文摘要：本文提出的情境感知 演化（CAKE）方法通过大语言模型动态生成和优化高斯过程 ，显著提升了贝叶斯优化的性能，大量实验证明该方法在多种实际应用中均优于 统基线方法。</span><br>
<span id='abs_en'>English Summary: The proposed Context-Aware Kernel Evolution (CAKE) method enhances Bayesian optimization by using large language models to dynamically generate and refine Gaussian process kernels, with comprehensive experiments showing its consistent superiority over traditional approaches across various applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2509.17786.pdf' target='_blank'>https://arxiv.org/pdf/2509.17786.pdf</a></span>   <span><a href='https://github.com/apanariello4/core-space-merging' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aniello Panariello, Daniel Marczak, Simone Magistri, Angelo Porrello, BartÅomiej Twardowski, Andrew D. Bagdanov, Simone Calderara, Joost van de Weijer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17786">Accurate and Efficient Low-Rank Model Merging in Core Space</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this paper, we address the challenges associated with merging low-rank adaptations of large neural networks. With the rise of parameter-efficient adaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning has become more accessible. While fine-tuning models with LoRA is highly efficient, existing merging methods often sacrifice this efficiency by merging fully-sized weight matrices. We propose the Core Space merging framework, which enables the merging of LoRA-adapted models within a common alignment basis, thereby preserving the efficiency of low-rank adaptation while substantially improving accuracy across tasks. We further provide a formal proof that projection into Core Space ensures no loss of information and provide a complexity analysis showing the efficiency gains. Extensive empirical results demonstrate that Core Space significantly improves existing merging techniques and achieves state-of-the-art results on both vision and language tasks while utilizing a fraction of the computational resources. Codebase is available at https://github.com/apanariello4/core-space-merging.<br>
<span id='abs_ch'>中文: Core Space框架能够在共享对齐基中高效合并LoRA适配的神经网络，在保持低秩效率的同时显著提升视觉和语言任务的准确性，且仅需少量计算资源。</span><br>
<span id='abs_en'>English: The Core Space framework enables efficient merging of LoRA-adapted neural networks within a shared alignment basis, preserving low-rank efficiency while significantly boosting accuracy across vision and language tasks with minimal computational resources.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2509.17765.pdf' target='_blank'>https://arxiv.org/pdf/2509.17765.pdf</a></span>   <span><a href='https://github.com/QwenLM/Qwen3-Omni' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, Junyang Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17765">Qwen3-Omni Technical Report</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. To reduce first-packet latency in streaming synthesis, Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.<br>
<span id='abs_ch'>Chinese: Qwen3-Omni是首个在文本、图像、音频和视频领域均保持顶尖性能的多模态模型，尤其在音频任务上表现卓越，超越了Gemini-2.5-Pro等主流闭源模型。</span><br>
<span id='abs_en'>English: Qwen3-Omni is a groundbreaking multimodal model that maintains top-tier performance across text, image, audio, and video, excelling particularly in audio tasks where it surpasses leading closed-source models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2509.17711.pdf' target='_blank'>https://arxiv.org/pdf/2509.17711.pdf</a></span>   <span><a href='https://github.com/kksssssss-ssda/MMEA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shenwei Kang, Xin Zhang, Wen Liu, Bin Li, Yujie Liu, Bo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17711">DA-Mamba: Dialogue-aware selective state-space model for multimodal engagement estimation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Human engagement estimation in conversational scenarios is essential for applications such as adaptive tutoring, remote healthcare assessment, and socially aware human--computer interaction. Engagement is a dynamic, multimodal signal conveyed by facial expressions, speech, gestures, and behavioral cues over time. In this work we introduce DA-Mamba, a dialogue-aware multimodal architecture that replaces attention-heavy dialogue encoders with Mamba-based selective state-space processing to achieve linear time and memory complexity while retaining expressive cross-modal reasoning. We design a Mamba dialogue-aware selective state-space model composed of three core modules: a Dialogue-Aware Encoder, and two Mamba-based fusion mechanisms: Modality-Group Fusion and Partner-Group Fusion, these modules achieve expressive dialogue understanding. Extensive experiments on three standard benchmarks (NoXi, NoXi-Add, and MPIIGI) show that DA-Mamba surpasses prior state-of-the-art (SOTA) methods in concordance correlation coefficient (CCC), while reducing training time and peak memory; these gains enable processing much longer sequences and facilitate real-time deployment in resource-constrained, multi-party conversational settings. The source code will be available at: https://github.com/kksssssss-ssda/MMEA.<br>
<span id='abs_ch'>中文摘要：DA-Mamba是一种对话感知的多模态架构，采用基于Mamba的选择性状态空间处理技术，在降低计算资源消耗的同时，实现了对对话场景中人类参与度的高效精准评估。</span><br>
<span id='abs_en'>English Summary: DA-Mamba is a dialogue-aware multimodal architecture that uses Mamba-based selective state-space processing to efficiently estimate human engagement in conversations, achieving superior performance with reduced computational resources.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2509.17677.pdf' target='_blank'>https://arxiv.org/pdf/2509.17677.pdf</a></span>   <span><a href='https://github.com/EngiBench/EngiBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiyuan Zhou, Xinlei Wang, Yirui He, Yang Wu, Ruixi Zou, Yuheng Cheng, Yulu Xie, Wenxuan Liu, Huan Zhao, Yan Xu, Jinjin Gu, Junhua Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17677">EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have shown strong performance on mathematical reasoning under well-posed conditions. However, real-world engineering problems require more than mathematical symbolic computation -- they need to deal with uncertainty, context, and open-ended scenarios. Existing benchmarks fail to capture these complexities. We introduce EngiBench, a hierarchical benchmark designed to evaluate LLMs on solving engineering problems. It spans three levels of increasing difficulty (foundational knowledge retrieval, multi-step contextual reasoning, and open-ended modeling) and covers diverse engineering subfields. To facilitate a deeper understanding of model performance, we systematically rewrite each problem into three controlled variants (perturbed, knowledge-enhanced, and math abstraction), enabling us to separately evaluate the model's robustness, domain-specific knowledge, and mathematical reasoning abilities. Experiment results reveal a clear performance gap across levels: models struggle more as tasks get harder, perform worse when problems are slightly changed, and fall far behind human experts on the high-level engineering tasks. These findings reveal that current LLMs still lack the high-level reasoning needed for real-world engineering, highlighting the need for future models with deeper and more reliable problem-solving capabilities. Our source code and data are available at https://github.com/EngiBench/EngiBench.<br>
<span id='abs_ch'>中文: EngiBench是一个分层基准，旨在评估大语言模型在工程问题上的表现，涵盖三个难度级别和多个子领域，结果显示当前模型在高级推理和鲁棒性方面仍远不及人类专家。</span><br>
<span id='abs_en'>English: EngiBench is a hierarchical benchmark introduced to evaluate large language models on engineering problems across three difficulty levels and multiple subfields, revealing that current models struggle with high-level reasoning and robustness compared to human experts.Paperid:522,https://arxiv.org/pdf/2509.17674.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2509.17664.pdf' target='_blank'>https://arxiv.org/pdf/2509.17664.pdf</a></span>   <span><a href='https://github.com/cpystan/SD-VLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pingyi Chen, Yujing Lou, Shen Cao, Jinhui Guo, Lubin Fan, Yue Wu, Lin Yang, Lizhuang Ma, Jieping Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17664">SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While vision language models (VLMs) excel in 2D semantic visual understanding, their ability to quantitatively reason about 3D spatial relationships remains under-explored, due to the deficiency of 2D images' spatial representation ability. In this paper, we analyze the problem hindering VLMs' spatial understanding abilities and propose SD-VLM, a novel framework that significantly enhances fundamental spatial perception abilities of VLMs through two key contributions: (1) propose Massive Spatial Measuring and Understanding (MSMU) dataset with precise spatial annotations, and (2) introduce a simple depth positional encoding method strengthening VLMs' spatial awareness. MSMU dataset covers massive quantitative spatial tasks with 700K QA pairs, 2.5M physical numerical annotations, and 10K chain-of-thought augmented samples. We have trained SD-VLM, a strong generalist VLM which shows superior quantitative spatial measuring and understanding capability. SD-VLM not only achieves state-of-the-art performance on our proposed MSMU-Bench, but also shows spatial generalization abilities on other spatial understanding benchmarks including Q-Spatial and SpatialRGPT-Bench. Extensive experiments demonstrate that SD-VLM outperforms GPT-4o and Intern-VL3-78B by 26.91% and 25.56% respectively on MSMU-Bench. Code and models are released at https://github.com/cpystan/SD-VLM.<br>
<span id='abs_ch'>中文: 本文提出SD-VLM框架，通过大规模空间测量理解数据集和深度位置编 方法，显著提升了视觉语言模型的三维空间感知能力，在多个空间理解基准测试中表现优异。English: This paper introduces SD-VLM, a novel framework that enhances vision language models' 3D spatial reasoning through a comprehensive MSMU dataset and depth positional encoding, achieving state-of-the-art performance on spatial benchmarks.Paperid:525,https://arxiv.org/pdf/2509.17654.pdfGitHub</span><br>
<span id='abs_en'>English: This paper introduces SD-VLM, a novel framework that enhances vision language models' 3D spatial reasoning through a comprehensive MSMU dataset and depth positional encoding, achieving state-of-the-art performance on spatial benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2509.17628.pdf' target='_blank'>https://arxiv.org/pdf/2509.17628.pdf</a></span>   <span><a href='https://github.com/D3E0-source/MSCoRE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhen Lei, Hongbin Xie, Jiaxing Zhao, Shuangxue Liu, Xuan Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17628">MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have excelled in question-answering (QA) tasks within single domains. However, their reasoning and coordination capabilities in complex, multi-stage scenarios remain underexplored. Existing benchmarks typically focus on isolated tasks or narrow domains, overlooking models' abilities for multi-stage collaboration and optimization without explicit external guidance. To bridge this gap, we propose \textbf{MSCoRe}, a novel benchmark comprising 126696 domain-specific QA instances spanning scenarios in automotive, pharmaceutical, electronics, and energy sectors. The dataset is created using a structured three-phase pipeline: dynamic sampling, iterative question-answer generation, and a multi-level quality assessment to ensure data quality. Tasks are further categorized into three difficulty levels according to stage coverage and complexity. With MSCoRe, we have conducted a comprehensive evaluation of various state-of-the-art LLM agents. The commercial models performed best across all tasks and scenarios, but a notable gap in ROUGE scores remains between simple and complex tasks. We also tested the models' robustness and found that their performance is negatively affected by noisy data. MSCoRe provides a valuable new resource for the community to evaluate and improve multi-stage reasoning in LLM agents. The code and data are available at https://github.com/D3E0-source/MSCoRE.<br>
<span id='abs_ch'>中文: 大语言模型在单领域问答任务中表现出色，但在复杂多阶段推理和协作能力方面 究不足，为此提出了MSCoRe基准，旨在评估和提升模型在跨领域场景中的多级推理与优化性能。</span><br>
<span id='abs_en'>English: Large Language Models excel in single-domain QA tasks but lack exploration in complex multi-stage reasoning, prompting the creation of the MSCoRe benchmark to evaluate and enhance their collaborative and optimization abilities across diverse sectors.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2509.17492.pdf' target='_blank'>https://arxiv.org/pdf/2509.17492.pdf</a></span>   <span><a href='https://github.com/LQH89757/MICS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinghua Lin, Guang-Hai Liu, Zuoyong Li, Yang Li, Yuting Jiang, Xiang Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17492">Multimodal Medical Image Classification via Synergistic Learning Pre-training</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal pathological images are usually in clinical diagnosis, but computer vision-based multimodal image-assisted diagnosis faces challenges with modality fusion, especially in the absence of expert-annotated data. To achieve the modality fusion in multimodal images with label scarcity, we propose a novel ``pretraining + fine-tuning" framework for multimodal semi-supervised medical image classification. Specifically, we propose a synergistic learning pretraining framework of consistency, reconstructive, and aligned learning. By treating one modality as an augmented sample of another modality, we implement a self-supervised learning pre-train, enhancing the baseline model's feature representation capability. Then, we design a fine-tuning method for multimodal fusion. During the fine-tuning stage, we set different encoders to extract features from the original modalities and provide a multimodal fusion encoder for fusion modality. In addition, we propose a distribution shift method for multimodal fusion features, which alleviates the prediction uncertainty and overfitting risks caused by the lack of labeled samples. We conduct extensive experiments on the publicly available gastroscopy image datasets Kvasir and Kvasirv2. Quantitative and qualitative results demonstrate that the proposed method outperforms the current state-of-the-art classification methods. The code will be released at: https://github.com/LQH89757/MICS.<br>
<span id='abs_ch'>Chinese: 本 究提出了一种新颖的“预训练+微调”框架，通过协同学 增强特征表示，解决了多模态图像融合的难题，并在胃镜数据集上实现了最先进的分类性能。</span><br>
<span id='abs_en'>English: This study introduces a novel "pretraining + fine-tuning" framework for multimodal semi-supervised medical image classification, which enhances feature representation through synergistic learning and addresses modality fusion challenges, achieving state-of-the-art performance on gastroscopy datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2509.17481.pdf' target='_blank'>https://arxiv.org/pdf/2509.17481.pdf</a></span>   <span><a href='https://github.com/ymcui/ChartHal' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingqi Wang, Yiming Cui, Xin Yao, Shijin Wang, Guoping Hu, Xiaoyu Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17481">ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Vision-Language Models (LVLMs) have recently demonstrated remarkable progress, yet hallucination remains a critical barrier, particularly in chart understanding, which requires sophisticated perceptual and cognitive abilities as well as rigorous factual accuracy. While prior work has investigated hallucinations and chart comprehension independently, their intersection remains largely unexplored. To address this gap, we present ChartHal, a benchmark that features a fine-grained taxonomy of hallucination scenarios in chart understanding, along with a human-validated dataset of 1,062 samples. Our evaluation shows that state-of-the-art LVLMs suffer from severe hallucinations on ChartHal, including proprietary models such as GPT-5 and o4-mini, which achieve only 34.46% and 22.79% accuracy, respectively. Further analysis reveals that questions involving information absent from or contradictory to charts are especially likely to trigger hallucinations, underscoring the urgent need for more robust mitigation strategies. Code and data are available at https://github.com/ymcui/ChartHal .<br>
<span id='abs_ch'>中文摘要：大型视觉语言模型在图表理解中存在严重幻觉问题，ChartHal基准测试显示即使GPT-5和o4-mini等先进模型准确率也极低，凸显了改进缓解策略的迫切需求。</span><br>
<span id='abs_en'>English Summary: Large Vision-Language Models exhibit severe hallucination issues in chart understanding, as demonstrated by the ChartHal benchmark where even advanced models like GPT-5 and o4-mini show low accuracy, highlighting the need for better mitigation strategies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2509.17446.pdf' target='_blank'>https://arxiv.org/pdf/2509.17446.pdf</a></span>   <span><a href='https://github.com/chr1s623/MVCL-DAF-PlusPlus' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haofeng Huang, Yifei Han, Long Zhang, Bin Li, Yangfan He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17446">MVCL-DAF++: Enhancing Multimodal Intent Recognition via Prototype-Aware Contrastive Alignment and Coarse-to-Fine Dynamic Attention Fusion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal intent recognition (MMIR) suffers from weak semantic grounding and poor robustness under noisy or rare-class conditions. We propose MVCL-DAF++, which extends MVCL-DAF with two key modules: (1) Prototype-aware contrastive alignment, aligning instances to class-level prototypes to enhance semantic consistency; and (2) Coarse-to-fine attention fusion, integrating global modality summaries with token-level features for hierarchical cross-modal interaction. On MIntRec and MIntRec2.0, MVCL-DAF++ achieves new state-of-the-art results, improving rare-class recognition by +1.05\% and +4.18\% WF1, respectively. These results demonstrate the effectiveness of prototype-guided learning and coarse-to-fine fusion for robust multimodal understanding. The source code is available at https://github.com/chr1s623/MVCL-DAF-PlusPlus.<br>
<span id='abs_ch'>Chinese: 提出的MVCL-DAF++模型通过原型感知对比对齐和粗细粒度注意力融合模块，有效解决了多模态意图识别中的语义基础薄弱和噪声鲁棒性问题，在基准数据集上实现了最优性能并显著提升了稀有类别的识别准确率。</span><br>
<span id='abs_en'>English: The proposed MVCL-DAF++ model addresses multimodal intent recognition challenges by introducing prototype-aware contrastive alignment and coarse-to-fine attention fusion, achieving state-of-the-art performance with significant improvements in rare-class recognition on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2509.17393.pdf' target='_blank'>https://arxiv.org/pdf/2509.17393.pdf</a></span>   <span><a href='https://github.com/klee972/SYNTRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kang-il Lee, Jahyun Koo, Seunghyun Yoon, Minbeom Kim, Hyukhun Koh, Dongryeol Lee, Kyomin Jung
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17393">Program Synthesis via Test-Time Transduction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce transductive program synthesis, a new formulation of the program synthesis task that explicitly leverages test inputs during synthesis. While prior approaches to program synthesis--whether based on natural language descriptions or input-output examples--typically aim to generalize from training examples, they often struggle with robustness, especially in real-world settings where training examples are limited and test inputs involve various edge cases. To address this, we propose a novel framework that improves robustness by treating synthesis as an active learning over a finite hypothesis class defined by programs' outputs. We use an LLM to predict outputs for selected test inputs and eliminate inconsistent hypotheses, where the inputs are chosen via a greedy maximin algorithm to minimize the number of LLM queries required. We evaluate our approach on four benchmarks: Playgol, MBPP+, 1D-ARC, and programmatic world modeling on MiniGrid. We demonstrate that our method significantly improves program synthesis in both accuracy and efficiency. We release our code at https://github.com/klee972/SYNTRA.<br>
<br>
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2509.17380.pdf' target='_blank'>https://arxiv.org/pdf/2509.17380.pdf</a></span>   <span><a href='https://github.com/Harryking1999/CoT_Causal_Analysis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhizhang FU, Guangsheng Bao, Hongbo Zhang, Chenkai Hu, Yue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17380">Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LLMs suffer from critical reasoning issues such as unfaithfulness, bias, and inconsistency, since they lack robust causal underpinnings and may rely on superficial correlations rather than genuine understanding. Successive LRMs have emerged as a promising alternative, leveraging advanced training techniques such as reinforcement learning (RL) and distillation to improve task accuracy. However, the impact of these training methods on causality remains largely unexplored. In this study, we conduct a systematic causal analysis on LLMs and LRMs, examining structural causal models (SCMs) of four key variables: problem instruction (Z), thinking process (T), reasoning steps (X), and answer (Y). Our findings reveal that RLVR-trained LRMs exhibit enhanced causal reasoning capabilities, aligning more closely with ideal causal structures, while LLMs and distilled LRMs fail to address causality-related deficiencies. Our further investigation indicates that RLVR reduces spurious correlations and strengthens genuine causal patterns, thereby mitigating unfaithfulness and bias. In addition, our inspection on the dynamics of the RLVR training process observes a high correlation between reduced spurious features and improved causal structures, where the causal relationships consistently improve in the training process. This study contributes to the understanding of causality in reasoning models, highlights the critical role of RLVR in enhancing causal reasoning, and provides insights for designing future AI systems with stronger causal foundations. We release our code and data at https://github.com/Harryking1999/CoT_Causal_Analysis.<br>
<span id='abs_ch'>中文：大语言模型 缺乏稳健的 果基础而存在推理缺陷，而采用强化学 价值正则化训练的连续推理模型通过消除伪相关、强化真实 果模式，展现出更优的 果推理能力。</span><br>
<span id='abs_en'>English: Large language models (LLMs) exhibit reasoning flaws due to weak causality, while reinforcement learning with value regularization (RLVR)-trained language reasoning models (LRMs) demonstrate enhanced causal reasoning by reducing spurious correlations and strengthening genuine causal patterns.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2509.17325.pdf' target='_blank'>https://arxiv.org/pdf/2509.17325.pdf</a></span>   <span><a href='https://github.com/StigLidu/CodeGym' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihua Du, Hailei Gong, Zhan Ling, Kang Liu, Lingfeng Shen, Xuesong Yao, Yufei Xu, Dingyuan Shi, Yiming Yang, Jiecao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17325">Generalizable End-to-End Tool-Use RL with Synthetic CodeGym</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Tool-augmented large language models (LLMs), hereafter LLM agents, leverage external tools to solve diverse tasks and interface with the real world. However, current training practices largely rely on supervised fine-tuning (SFT) over static trajectories or reinforcement learning (RL) on narrow tasks, and generalize poorly beyond development settings, leading to brittleness with new tools and unseen workflows. Because code execution reflects many structures of real-world workflows, coding problems provide a natural basis for building agent training environments. Motivated by this, we introduce CodeGym, a scalable framework that synthesizes diverse, verifiable, and controllable multi-turn tool-use environments for agent RL, enabling LLM agents to explore and master various workflows actively. CodeGym rewrites static coding problems into interactive environments by extracting atomic functions or logic into callable tools, yielding verifiable tasks that span various tool-execution workflows. Models of varying sizes and chain-of-thought configurations, trained in CodeGym, exhibit consistent out-of-distribution generalizability; for example, Qwen2.5-32B-Instruct achieves an absolute accuracy gain of 8.7 points on the OOD benchmark $Ï$-Bench. These results highlight CodeGym as a step toward scalable general-purpose RL environments that align with real-world agent workflows.<br>
<span id='abs_ch'>中文摘要：CodeGym是一个可扩展框架，通过将静态编程问题转化为交互式环境来训练LLM智能体，使其在分布外任务上展现出显著提升的泛化能力。</span><br>
<span id='abs_en'>English Summary: CodeGym is a scalable framework that transforms static coding problems into interactive environments for training LLM agents through reinforcement learning, significantly enhancing their generalization capabilities on out-of-distribution tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2509.17318.pdf' target='_blank'>https://arxiv.org/pdf/2509.17318.pdf</a></span>   <span><a href='https://github.com/Icarus-1111/CogAtom' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuofan Chen, Jiyuan He, Yichi Zhang, Xing Hu, Haoxing Wen, Jun Bai, Wenge Rong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17318">CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Mathematical reasoning poses significant challenges for Large Language Models (LLMs) due to its demand for multi-step reasoning and abstract conceptual integration. While recent test-time scaling techniques rely heavily on high-quality, challenging problems, the scarcity of Olympiad-level math problems remains a bottleneck. We introduce CogAtom, a novel cognitive atom-based framework for synthesizing mathematically rigorous and cognitively diverse problems. Unlike prior approaches, CogAtom models problem construction as a process of selecting and recombining fundamental reasoning units, cognitive atoms, extracted from human-authored solutions. A diversity-promoting random walk algorithm enables exploration of the cognitive atom space, while a constraint-based recombination mechanism ensures logical soundness and structural validity. The combinatorial nature of the graph structure provides a near-infinite space of reasoning paths, and the walk algorithm systematically explores this space to achieve large-scale synthesis of high-quality problems; meanwhile, by controlling the number of cognitive atoms, we can precisely adjust problem difficulty, ensuring diversity, scalability, and controllability of the generated problems. Experimental results demonstrate that CogAtom outperforms existing methods in accuracy, reasoning depth, and diversity, generating problems that closely match the difficulty of AIME while exceeding it in structural variation. Our work offers a cognitively grounded pathway toward scalable, high-quality math problem generation.Our code is publicly available at https://github.com/Icarus-1111/CogAtom.<br>
<span id='abs_ch'>中文：CogAtom提出了一种基于认知原子的框架，通过重组基本推理单元来合成数学严谨且多 化的问题，实现了可扩展、高质量且难度可控的数学题目生成。</span><br>
<span id='abs_en'>English: CogAtom introduces a cognitive atom-based framework that synthesizes mathematically rigorous and diverse problems by recombining fundamental reasoning units, enabling scalable, high-quality math problem generation with precise difficulty control.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2509.17190.pdf' target='_blank'>https://arxiv.org/pdf/2509.17190.pdf</a></span>   <span><a href='https://github.com/Marshall-mk/EchoPathv1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kabir Hamzah Muhammad, Marawan Elbatel, Yi Qin, Xiaomeng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17190">Echo-Path: Pathology-Conditioned Echo Video Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Cardiovascular diseases (CVDs) remain the leading cause of mortality globally, and echocardiography is critical for diagnosis of both common and congenital cardiac conditions. However, echocardiographic data for certain pathologies are scarce, hindering the development of robust automated diagnosis models. In this work, we propose Echo-Path, a novel generative framework to produce echocardiogram videos conditioned on specific cardiac pathologies. Echo-Path can synthesize realistic ultrasound video sequences that exhibit targeted abnormalities, focusing here on atrial septal defect (ASD) and pulmonary arterial hypertension (PAH). Our approach introduces a pathology-conditioning mechanism into a state-of-the-art echo video generator, allowing the model to learn and control disease-specific structural and motion patterns in the heart. Quantitative evaluation demonstrates that the synthetic videos achieve low distribution distances, indicating high visual fidelity. Clinically, the generated echoes exhibit plausible pathology markers. Furthermore, classifiers trained on our synthetic data generalize well to real data and, when used to augment real training sets, it improves downstream diagnosis of ASD and PAH by 7\% and 8\% respectively. Code, weights and dataset are available here https://github.com/Marshall-mk/EchoPathv1<br>
<br>
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2509.17136.pdf' target='_blank'>https://arxiv.org/pdf/2509.17136.pdf</a></span>   <span><a href='https://github.com/YuHao-Tian/SAEC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Tian, Zheming Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17136">SAEC: Scene-Aware Enhanced Edge-Cloud Collaborative Industrial Vision Inspection with Multimodal LLM</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Industrial vision inspection requires high accuracy under stringent resource constraints, yet existing approaches face a fundamental trade-off. Multimodal LLMs (MLLMs) deliver strong reasoning capabilities but incur prohibitive computational costs, while lightweight edge models often fail on complex cases. In this paper, we present SAEC, a scene-aware enhanced edge-cloud collaborative industrial vision inspection framework with MLLM. The framework is composed of three synergistic components: (1) Efficient MLLM Fine-Tuning for Complex Defect Inspection, (2) Lightweight Multiscale Scene-Complexity Estimation, and (3) Adaptive Edge-Cloud Scheduler. Together, these modules enable robust defect detection by tailoring multimodal reasoning to scene complexity and dynamically balancing computation between edge and cloud resources. Experimental results on MVTec AD and KSDD2 datasets demonstrate that SAEC attains 85.11% and 82.72% accuracy, surpassing Qwen by 22.1% and 20.8%, and LLaVA by 33.3% and 31.6%. It also reduces runtime by up to 22.4% and cuts energy per correct decision by 40%-74%. The code is available at https://github.com/YuHao-Tian/SAEC.<br>
<span id='abs_ch'>中文：SAEC是一种创新的边云协同框架，通过基于场景复杂度动态分配任务，显著提升了工业视觉检测的准确性和效率，超越了现有模型。</span><br>
<span id='abs_en'>English: SAEC is a novel edge-cloud collaborative framework that enhances industrial vision inspection by dynamically allocating tasks based on scene complexity, achieving higher accuracy and efficiency than existing models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2509.17116.pdf' target='_blank'>https://arxiv.org/pdf/2509.17116.pdf</a></span>   <span><a href='https://github.com/xuhang-2/Embodied-Agent-Planning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Xu, Zang Yu, Yehui Tang, Pengbo Hu, Yuhao Tang, Hao Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17116">MCTS-EP: Empowering Embodied Planning with Online Preference Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper introduces MCTS-EP, an online learning framework that combines large language models (LLM) with Monte Carlo Tree Search (MCTS) for training embodied agents. MCTS-EP integrates three key components: MCTS-guided exploration for preference data collection, efficient multi-modal reasoning mechanism, and iterative training pipeline based on preference optimization. We theoretically prove that MCTS-EP achieves better performance bounds than conventional on-policy algorithms when the loss function is strongly convex, and demonstrate that it can be formulated as a search-enhanced variant of GAIL. MCTS-EP achieves state-of-the-art performace across serval benchmarks. In ALFWorld, it achieves 92% and 87% success rates for textual and visual tasks. In WebShop, it reaches an average reward of 0.81. MTCS-EP also reduces average interaction steps from from 18.7/19.5 to 10.2/9.9 steps in visual ALFWorld.Code available at: https://github.com/xuhang-2/Embodied-Agent-Planning<br>
<span id='abs_ch'>中文: 本文提出MCTS-EP框架，通过结合大语言模型与蒙特卡洛 搜索训练具身智能体，在多项基准测试中实现最优性能，并显著提升任务成功率与交互效率。</span><br>
<span id='abs_en'>English: This paper presents MCTS-EP, an online learning framework integrating large language models with Monte Carlo Tree Search to train embodied agents, achieving state-of-the-art performance across multiple benchmarks through enhanced exploration and optimization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2509.17066.pdf' target='_blank'>https://arxiv.org/pdf/2509.17066.pdf</a></span>   <span><a href='https://github.com/LKRcrocodile/RALLM-POI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunrong Li, Kwan Hui Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17066">RALLM-POI: Retrieval-Augmented LLM for Zero-shot Next POI Recommendation with Geographical Reranking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Next point-of-interest (POI) recommendation predicts a user's next destination from historical movements. Traditional models require intensive training, while LLMs offer flexible and generalizable zero-shot solutions but often generate generic or geographically irrelevant results due to missing trajectory and spatial context. To address these issues, we propose RALLM-POI, a framework that couples LLMs with retrieval-augmented generation and self-rectification. We first propose a Historical Trajectory Retriever (HTR) that retrieves relevant past trajectories to serve as contextual references, which are then reranked by a Geographical Distance Reranker (GDR) for prioritizing spatially relevant trajectories. Lastly, an Agentic LLM Rectifier (ALR) is designed to refine outputs through self-reflection. Without additional training, RALLM-POI achieves substantial accuracy gains across three real-world Foursquare datasets, outperforming both conventional and LLM-based baselines. Code is released at https://github.com/LKRcrocodile/RALLM-POI.<br>
<span id='abs_ch'>中文摘要：RALLM-POI框架通过结合检索增强生成与自 正机制，利用历史轨迹和地理空间信息增强大语言模型的POI推荐能力， 需额外训练即在多个真实数据集上实现了显著优于 统方法的推荐精度。</span><br>
<span id='abs_en'>English Summary: RALLM-POI is a novel framework that enhances next POI recommendation by integrating retrieval-augmented generation and self-rectification with LLMs, achieving superior accuracy without additional training by leveraging historical trajectories and geographical context.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2509.17040.pdf' target='_blank'>https://arxiv.org/pdf/2509.17040.pdf</a></span>   <span><a href='https://github.com/Shelly-coder239/MIRBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hang Du, Jiayang Zhang, Guoshun Nan, Wendi Deng, Zhenyan Chen, Chenyang Zhang, Wang Xiao, Shan Huang, Yuqi Pan, Tao Qi, Sicong Leng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17040">From Easy to Hard: The MIR Benchmark for Progressive Interleaved Multi-Image Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multi-image Interleaved Reasoning aims to improve Multi-modal Large Language Models (MLLMs) ability to jointly comprehend and reason across multiple images and their associated textual contexts, introducing unique challenges beyond single-image or non-interleaved multi-image tasks. While current multi-image benchmarks overlook interleaved textual contexts and neglect distinct relationships between individual images and their associated texts, enabling models to reason over multi-image interleaved data may significantly enhance their comprehension of complex scenes and better capture cross-modal correlations. To bridge this gap, we introduce a novel benchmark MIR, requiring joint reasoning over multiple images accompanied by interleaved textual contexts to accurately associate image regions with corresponding texts and logically connect information across images. To enhance MLLMs ability to comprehend multi-image interleaved data, we introduce reasoning steps for each instance within the benchmark and propose a stage-wise curriculum learning strategy. This strategy follows an "easy to hard" approach, progressively guiding models from simple to complex scenarios, thereby enhancing their ability to handle challenging tasks. Extensive experiments benchmarking multiple MLLMs demonstrate that our method significantly enhances models reasoning performance on MIR and other established benchmarks. We believe that MIR will encourage further research into multi-image interleaved reasoning, facilitating advancements in MLLMs capability to handle complex inter-modal tasks.Our code and dataset are available at https://github.com/Shelly-coder239/MIRBench.<br>
<span id='abs_ch'>中文: MIR基准通过要求模型结合交错文本联合分析多 图像，并采用渐进式课程学 策略，显著提升了多模态大语言模型处理复杂跨模态任务的推理能力。</span><br>
<span id='abs_en'>English: The MIR benchmark advances multi-modal reasoning by requiring models to jointly analyze multiple images with interleaved texts, using a progressive curriculum strategy that significantly improves performance on complex cross-modal tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2509.17037.pdf' target='_blank'>https://arxiv.org/pdf/2509.17037.pdf</a></span>   <span><a href='https://github.com/yajingyang/kahan' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yajing Yang, Tony Deng, Min-Yen Kan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17037">KAHAN: Knowledge-Augmented Hierarchical Analysis and Narration for Financial Data Narration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We propose KAHAN, a knowledge-augmented hierarchical framework that systematically extracts insights from raw tabular data at entity, pairwise, group, and system levels. KAHAN uniquely leverages LLMs as domain experts to drive the analysis. On DataTales financial reporting benchmark, KAHAN outperforms existing approaches by over 20% on narrative quality (GPT-4o), maintains 98.2% factuality, and demonstrates practical utility in human evaluation. Our results reveal that knowledge quality drives model performance through distillation, hierarchical analysis benefits vary with market complexity, and the framework transfers effectively to healthcare domains. The data and code are available at https://github.com/yajingyang/kahan.<br>
<span id='abs_ch'>中文: KAHAN是一个知识增强的分层框架，利用大语言模型作为领域专家从表 数据中提取洞察，在基准测试中展现出卓越的叙事质量、高事实准确性及优秀的跨领域迁移能力。</span><br>
<span id='abs_en'>English: KAHAN is a knowledge-augmented hierarchical framework that uses LLMs as domain experts to extract insights from tabular data, achieving superior narrative quality, high factuality, and effective cross-domain transfer on benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2509.17024.pdf' target='_blank'>https://arxiv.org/pdf/2509.17024.pdf</a></span>   <span><a href='https://github.com/fiwy0527/LCDiff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenxuan Fang, Jili Fan, Chao Wang, Xiantao Hu, Jiangwei Weng, Ying Tai, Jian Yang, Jun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17024">When Color-Space Decoupling Meets Diffusion for Adverse-Weather Image Restoration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Adverse Weather Image Restoration (AWIR) is a highly challenging task due to the unpredictable and dynamic nature of weather-related degradations. Traditional task-specific methods often fail to generalize to unseen or complex degradation types, while recent prompt-learning approaches depend heavily on the degradation estimation capabilities of vision-language models, resulting in inconsistent restorations. In this paper, we propose \textbf{LCDiff}, a novel framework comprising two key components: \textit{Lumina-Chroma Decomposition Network} (LCDN) and \textit{Lumina-Guided Diffusion Model} (LGDM). LCDN processes degraded images in the YCbCr color space, separately handling degradation-related luminance and degradation-invariant chrominance components. This decomposition effectively mitigates weather-induced degradation while preserving color fidelity. To further enhance restoration quality, LGDM leverages degradation-related luminance information as a guiding condition, eliminating the need for explicit degradation prompts. Additionally, LGDM incorporates a \textit{Dynamic Time Step Loss} to optimize the denoising network, ensuring a balanced recovery of both low- and high-frequency features in the image. Finally, we present DriveWeather, a comprehensive all-weather driving dataset designed to enable robust evaluation. Extensive experiments demonstrate that our approach surpasses state-of-the-art methods, setting a new benchmark in AWIR. The dataset and code are available at: https://github.com/fiwy0527/LCDiff.<br>
<span id='abs_ch'>中文: 提出的LCDiff框架通过在YCbCr色彩空间分解亮度和色度分量，并采用亮度引导的扩散模型与动态时间步长优化，有效恢复恶劣天气下的图像退化，在新型DriveWeather数据集上的实验表明其性能超越现有最佳方法。</span><br>
<span id='abs_en'>English: The proposed LCDiff framework effectively restores weather-degraded images by decomposing luminance and chrominance components in YCbCr space and using luminance-guided diffusion with dynamic time step optimization, outperforming existing methods as validated on the new DriveWeather dataset.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2509.16972.pdf' target='_blank'>https://arxiv.org/pdf/2509.16972.pdf</a></span>   <span><a href='https://github.com/magic-research/Sa2VA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/magic-research/Sa2VA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Quanzhu Niu, Dengxian Gong, Shihao Chen, Tao Zhang, Yikang Zhou, Haobo Yuan, Lu Qi, Xiangtai Li, Shunping Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16972">The 1st Solution for 7th LSVOS RVOS Track: SaSaSa2VA</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Referring video object segmentation (RVOS) requires segmenting and tracking objects in videos conditioned on natural-language expressions, demanding fine-grained understanding of both appearance and motion. Building on Sa2VA, which couples a Multi-modal Large Language Model (MLLM) with the video segmentation model SAM2, we identify two key bottlenecks that limit segmentation performance: sparse frame sampling and reliance on a single [SEG] token for an entire video. We propose Segmentation Augmented and Selective Averaged Sa2VA SaSaSa2VA to address these issues. On the 7th LSVOS Challenge (RVOS track), SaSaSa2VA achieves a $J\&F$ of 67.45, ranking first and surpassing the runner-up by 2.80 points. This result and ablation studies demonstrate that efficient segmentation augmentation and test-time ensembling substantially enhance grounded MLLMs for RVOS. The code is released in Sa2VA repository: https://github.com/magic-research/Sa2VA.<br>
<br>
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2509.16926.pdf' target='_blank'>https://arxiv.org/pdf/2509.16926.pdf</a></span>   <span><a href='https://github.com/Ragib-Amin-Nihal/BEATsCA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ragib Amin Nihal, Benjamin Yen, Takeshi Ashizawa, Kazuhiro Nakadai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16926">Cross-Attention with Confidence Weighting for Multi-Channel Audio Alignment</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multi-channel audio alignment is a key requirement in bioacoustic monitoring, spatial audio systems, and acoustic localization. However, existing methods often struggle to address nonlinear clock drift and lack mechanisms for quantifying uncertainty. Traditional methods like Cross-correlation and Dynamic Time Warping assume simple drift patterns and provide no reliability measures. Meanwhile, recent deep learning models typically treat alignment as a binary classification task, overlooking inter-channel dependencies and uncertainty estimation. We introduce a method that combines cross-attention mechanisms with confidence-weighted scoring to improve multi-channel audio synchronization. We extend BEATs encoders with cross-attention layers to model temporal relationships between channels. We also develop a confidence-weighted scoring function that uses the full prediction distribution instead of binary thresholding. Our method achieved first place in the BioDCASE 2025 Task 1 challenge with 0.30 MSE average across test datasets, compared to 0.58 for the deep learning baseline. On individual datasets, we achieved 0.14 MSE on ARU data (77% reduction) and 0.45 MSE on zebra finch data (18% reduction). The framework supports probabilistic temporal alignment, moving beyond point estimates. While validated in a bioacoustic context, the approach is applicable to a broader range of multi-channel audio tasks where alignment confidence is critical. Code available on: https://github.com/Ragib-Amin-Nihal/BEATsCA<br>
<span id='abs_ch'>中文: 本 究提出了一种结合交叉注意力机制与置信度 权评分的新型多通道音频对齐方法，在BioDCASE 2025挑战赛中显著降低了对齐误差，同时实现了不确定性量化，展现出优越性能。</span><br>
<span id='abs_en'>English: This study introduces a novel multi-channel audio alignment method combining cross-attention mechanisms with confidence-weighted scoring, achieving superior performance in the BioDCASE 2025 challenge by significantly reducing alignment errors while providing uncertainty quantification.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2509.16861.pdf' target='_blank'>https://arxiv.org/pdf/2509.16861.pdf</a></span>   <span><a href='https://github.com/awsm-research/AdaptiveGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Yang, Michael Fu, Chakkrit Tantithamthavorn, Chetan Arora, Gunel Gulmammadova, Joey Chua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16861">AdaptiveGuard: Towards Adaptive Runtime Safety for LLM-Powered Software</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Guardrails are critical for the safe deployment of Large Language Models (LLMs)-powered software. Unlike traditional rule-based systems with limited, predefined input-output spaces that inherently constrain unsafe behavior, LLMs enable open-ended, intelligent interactions--opening the door to jailbreak attacks through user inputs. Guardrails serve as a protective layer, filtering unsafe prompts before they reach the LLM. However, prior research shows that jailbreak attacks can still succeed over 70% of the time, even against advanced models like GPT-4o. While guardrails such as LlamaGuard report up to 95% accuracy, our preliminary analysis shows their performance can drop sharply--to as low as 12%--when confronted with unseen attacks. This highlights a growing software engineering challenge: how to build a post-deployment guardrail that adapts dynamically to emerging threats? To address this, we propose AdaptiveGuard, an adaptive guardrail that detects novel jailbreak attacks as out-of-distribution (OOD) inputs and learns to defend against them through a continual learning framework. Through empirical evaluation, AdaptiveGuard achieves 96% OOD detection accuracy, adapts to new attacks in just two update steps, and retains over 85% F1-score on in-distribution data post-adaptation, outperforming other baselines. These results demonstrate that AdaptiveGuard is a guardrail capable of evolving in response to emerging jailbreak strategies post deployment. We release our AdaptiveGuard and studied datasets at https://github.com/awsm-research/AdaptiveGuard to support further research.<br>
<span id='abs_ch'>中文: 护 对保护大语言模型免受越狱攻击至关重要，但现有系统难以应对新型威胁， 此我们提出AdaptiveGuard，一种自适应护 ，能检测新攻击并持续学 防御，实现高精度和快速适应。</span><br>
<span id='abs_en'>English: Guardrails are essential for protecting Large Language Models from jailbreak attacks, but current systems struggle with new threats, prompting the development of AdaptiveGuard, an adaptive solution that detects novel attacks and learns to counter them, achieving high accuracy and rapid adaptation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2509.16656.pdf' target='_blank'>https://arxiv.org/pdf/2509.16656.pdf</a></span>   <span><a href='https://github.com/fengshun124/NUMINA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Changyu Zeng, Yifan Wang, Zimu Wang, Wei Wang, Zhengni Yang, Muyi Bao, Jiming Xiao, Anh Nguyen, Yutao Yue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16656">NUMINA: A Natural Understanding Benchmark for Multi-dimensional Intelligence and Numerical Reasoning Abilities</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in 2D multimodal large language models (MLLMs) have significantly improved performance in vision-language tasks. However, extending these capabilities to 3D environments remains a distinct challenge due to the complexity of spatial reasoning. Nevertheless, existing 3D benchmarks often lack fine-grained numerical reasoning task annotations, limiting MLLMs' ability to perform precise spatial measurements and complex numerical reasoning. To address this gap, we introduce NUMINA, the first Natural Understanding benchmark for Multi-dimensional Intelligence and Numerical reasoning Abilities to enhance multimodal indoor perceptual understanding. NUMINA features multi-scale annotations and various question-answer pairs, generated using NUMINA-Flow, an automated annotation pipeline that integrates LLM rewriting and rule-based self-verification. We evaluate the performance of various state-of-the-art LLMs on NUMINA following the Chat-Scene framework, demonstrating that current LLMs struggle with multimodal numerical reasoning, particularly in performing precise computations such as distance and volume estimation, highlighting the need for further advancements in 3D models. The dataset and source codes can be obtained from https://github.com/fengshun124/NUMINA.<br>
<span id='abs_ch'>中文: 当前二维多模态模型在视觉语言任务中表现出色，但在三维空间推理中 缺乏细粒度数值 注而面临挑战，为此推出NUMINA基准，通过自动化 注和评估来增强多模态数值理解能力。</span><br>
<span id='abs_en'>English: Recent 2D multimodal models excel in vision-language tasks but face challenges in 3D spatial reasoning due to limited fine-grained numerical annotations, prompting the introduction of NUMINA benchmark to enhance multimodal numerical understanding through automated annotations and evaluations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2509.16602.pdf' target='_blank'>https://arxiv.org/pdf/2509.16602.pdf</a></span>   <span><a href='https://github.com/minjihh/FakeChain' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minji Heo, Simon S. Woo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16602">FakeChain: Exposing Shallow Cues in Multi-Step Deepfake Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multi-step or hybrid deepfakes, created by sequentially applying different deepfake creation methods such as Face-Swapping, GAN-based generation, and Diffusion methods, can pose an emerging and unforseen technical challenge for detection models trained on single-step forgeries. While prior studies have mainly focused on detecting isolated single manipulation, little is known about the detection model behavior under such compositional, hybrid, and complex manipulation pipelines. In this work, we introduce \textbf{FakeChain}, a large-scale benchmark comprising 1-, 2-, and 3-Step forgeries synthesized using five state-of-the-art representative generators. Using this approach, we analyze detection performance and spectral properties across hybrid manipulation at different step, along with varying generator combinations and quality settings. Surprisingly, our findings reveal that detection performance highly depends on the final manipulation type, with F1-score dropping by up to \textbf{58.83\%} when it differs from training distribution. This clearly demonstrates that detectors rely on last-stage artifacts rather than cumulative manipulation traces, limiting generalization. Such findings highlight the need for detection models to explicitly consider manipulation history and sequences. Our results highlight the importance of benchmarks such as FakeChain, reflecting growing synthesis complexity and diversity in real-world scenarios. Our sample code is available here\footnote{https://github.com/minjihh/FakeChain}.<br>
<span id='abs_ch'>中文: 通过组合不同生成方法创建的多步骤混合深度伪 对检测模型构成重大挑战，这些模型 依赖最终阶段痕迹而非累积操作特征而难以泛化。</span><br>
<span id='abs_en'>English: Multi-step hybrid deepfakes created by combining different generation methods present significant challenges to detection models, which often fail to generalize due to reliance on final-stage artifacts rather than cumulative manipulation traces.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2509.16517.pdf' target='_blank'>https://arxiv.org/pdf/2509.16517.pdf</a></span>   <span><a href='https://github.com/buraksatar/SeeingCulture' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Burak Satar, Zhixin Ma, Patrick A. Irawan, Wilfried A. Mulyawan, Jing Jiang, Ee-Peng Lim, Chong-Wah Ngo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16517">Seeing Culture: A Benchmark for Visual Reasoning and Grounding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal vision-language models (VLMs) have made substantial progress in various tasks that require a combined understanding of visual and textual content, particularly in cultural understanding tasks, with the emergence of new cultural datasets. However, these datasets frequently fall short of providing cultural reasoning while underrepresenting many cultures. In this paper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural reasoning with a novel approach that requires VLMs to reason on culturally rich images in two stages: i) selecting the correct visual option with multiple-choice visual question answering (VQA), and ii) segmenting the relevant cultural artifact as evidence of reasoning. Visual options in the first stage are systematically organized into three types: those originating from the same country, those from different countries, or a mixed group. Notably, all options are derived from a singular category for each type. Progression to the second stage occurs only after a correct visual option is chosen. The SCB benchmark comprises 1,065 images that capture 138 cultural artifacts across five categories from seven Southeast Asia countries, whose diverse cultures are often overlooked, accompanied by 3,178 questions, of which 1,093 are unique and meticulously curated by human annotators. Our evaluation of various VLMs reveals the complexities involved in cross-modal cultural reasoning and highlights the disparity between visual reasoning and spatial grounding in culturally nuanced scenarios. The SCB serves as a crucial benchmark for identifying these shortcomings, thereby guiding future developments in the field of cultural reasoning. https://github.com/buraksatar/SeeingCulture<br>
<span id='abs_ch'>中文摘要：Seeing Culture Benchmark（SCB）通过两阶段评估方法，要求视觉语言模型先回答文化选择题再分割相关文物，利用1065 东南亚多元文化图像解决了现有数据集文化推理能力不足的问题。</span><br>
<span id='abs_en'>English Summary: The Seeing Culture Benchmark (SCB) introduces a two-stage evaluation method requiring vision-language models to first answer cultural multiple-choice questions and then segment relevant artifacts, addressing the lack of cultural reasoning in existing datasets through 1,065 culturally diverse Southeast Asian images.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2509.16339.pdf' target='_blank'>https://arxiv.org/pdf/2509.16339.pdf</a></span>   <span><a href='https://github.com/Machine-Earning/CISIR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Josias K. Moukpe, Philip K. Chan, Ming Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16339">Highly Imbalanced Regression with Tabular Data in SEP and Other Applications</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We investigate imbalanced regression with tabular data that have an imbalance ratio larger than 1,000 ("highly imbalanced"). Accurately estimating the target values of rare instances is important in applications such as forecasting the intensity of rare harmful Solar Energetic Particle (SEP) events. For regression, the MSE loss does not consider the correlation between predicted and actual values. Typical inverse importance functions allow only convex functions. Uniform sampling might yield mini-batches that do not have rare instances. We propose CISIR that incorporates correlation, Monotonically Decreasing Involution (MDI) importance, and stratified sampling. Based on five datasets, our experimental results indicate that CISIR can achieve lower error and higher correlation than some recent methods. Also, adding our correlation component to other recent methods can improve their performance. Lastly, MDI importance can outperform other importance functions. Our code can be found in https://github.com/Machine-Earning/CISIR.<br>
<span id='abs_ch'>中文: 本 究提出CISIR方法，针对高度不平衡的回归问题，通过结合相关性分析、单调递减对合重要性函数和分层抽 ，在多个数据集上实现了比现有方法更低的误差和更高的相关性。</span><br>
<span id='abs_en'>English: The study introduces CISIR, a novel method for highly imbalanced regression that integrates correlation, monotonically decreasing involution importance, and stratified sampling, demonstrating superior performance with lower error and higher correlation compared to existing approaches on multiple datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2509.16195.pdf' target='_blank'>https://arxiv.org/pdf/2509.16195.pdf</a></span>   <span><a href='https://github.com/lucadellalib/focalcodec' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Della Libera, Cem Subakan, Mirco Ravanelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16195">FocalCodec-Stream: Streaming Low-Bitrate Speech Coding via Causal Distillation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Neural audio codecs are a fundamental component of modern generative audio pipelines. Although recent codecs achieve strong low-bitrate reconstruction and provide powerful representations for downstream tasks, most are non-streamable, limiting their use in real-time applications. We present FocalCodec-Stream, a hybrid codec based on focal modulation that compresses speech into a single binary codebook at 0.55 - 0.80 kbps with a theoretical latency of 80 ms. Our approach combines multi-stage causal distillation of WavLM with targeted architectural improvements, including a lightweight refiner module that enhances quality under latency constraints. Experiments show that FocalCodec-Stream outperforms existing streamable codecs at comparable bitrates, while preserving both semantic and acoustic information. The result is a favorable trade-off between reconstruction quality, downstream task performance, latency, and efficiency. Code and checkpoints will be released at https://github.com/lucadellalib/focalcodec.<br>
<span id='abs_ch'>中文: FocalCodec-Stream是一种基于焦点调制的新型混合神经音频编解 器，在低比特率和低延迟条件下实现了卓越的语音压缩性能，在保持高质量重建和效率的同时超越了现有可流式编解 器。</span><br>
<span id='abs_en'>English: FocalCodec-Stream is a novel hybrid neural audio codec that achieves superior low-bitrate speech compression with minimal latency, outperforming existing streamable codecs while maintaining high reconstruction quality and efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2509.16188.pdf' target='_blank'>https://arxiv.org/pdf/2509.16188.pdf</a></span>   <span><a href='https://github.com/HoganZinger/Culture' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinghao Zhang, Sihang Jiang, Shiwei Guo, Shisong Chen, Yanghua Xiao, Hongwei Feng, Jiaqing Liang, Minggui HE, Shimin Tao, Hongxia Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16188">CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) are increasingly deployed in diverse cultural environments, evaluating their cultural understanding capability has become essential for ensuring trustworthy and culturally aligned applications. However, most existing benchmarks lack comprehensiveness and are challenging to scale and adapt across different cultural contexts, because their frameworks often lack guidance from well-established cultural theories and tend to rely on expert-driven manual annotations. To address these issues, we propose CultureScope, the most comprehensive evaluation framework to date for assessing cultural understanding in LLMs. Inspired by the cultural iceberg theory, we design a novel dimensional schema for cultural knowledge classification, comprising 3 layers and 140 dimensions, which guides the automated construction of culture-specific knowledge bases and corresponding evaluation datasets for any given languages and cultures. Experimental results demonstrate that our method can effectively evaluate cultural understanding. They also reveal that existing large language models lack comprehensive cultural competence, and merely incorporating multilingual data does not necessarily enhance cultural understanding. All code and data files are available at https://github.com/HoganZinger/Culture<br>
<span id='abs_ch'>中文摘要：CultureScope基于文化冰山理论提出全面评估框架，通过自动化构建文化知识库来测评大语言模型的文化理解能力，发现现有模型即使具备多语言数据仍存在文化认知缺陷。</span><br>
<span id='abs_en'>English Summary: CultureScope introduces a comprehensive framework based on cultural iceberg theory to evaluate LLMs' cultural understanding through automated knowledge base construction, revealing current models' cultural competence gaps despite multilingual training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2509.15981.pdf' target='_blank'>https://arxiv.org/pdf/2509.15981.pdf</a></span>   <span><a href='https://github.com/YujieZhu7/SPReD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujie Zhu, Charles A. Hepburn, Matthew Thorpe, Giovanni Montana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15981">Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In reinforcement learning with sparse rewards, demonstrations can accelerate learning, but determining when to imitate them remains challenging. We propose Smooth Policy Regularisation from Demonstrations (SPReD), a framework that addresses the fundamental question: when should an agent imitate a demonstration versus follow its own policy? SPReD uses ensemble methods to explicitly model Q-value distributions for both demonstration and policy actions, quantifying uncertainty for comparisons. We develop two complementary uncertainty-aware methods: a probabilistic approach estimating the likelihood of demonstration superiority, and an advantage-based approach scaling imitation by statistical significance. Unlike prevailing methods (e.g. Q-filter) that make binary imitation decisions, SPReD applies continuous, uncertainty-proportional regularisation weights, reducing gradient variance during training. Despite its computational simplicity, SPReD achieves remarkable gains in experiments across eight robotics tasks, outperforming existing approaches by up to a factor of 14 in complex tasks while maintaining robustness to demonstration quality and quantity. Our code is available at https://github.com/YujieZhu7/SPReD.<br>
<span id='abs_ch'>中文: SPReD提出了一种新颖的强化学 框架，通过集成方法量化不确定性来动态平衡示范模仿与策略探索，采用连续且与不确定性成比例的正则化方法，在机器人任务中实现了显著的性能提升。</span><br>
<span id='abs_en'>English: SPReD introduces a novel reinforcement learning framework that uses ensemble-based uncertainty quantification to dynamically balance imitation of demonstrations with policy exploration, achieving significant performance improvements in robotics tasks through continuous, uncertainty-proportional regularization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2509.15965.pdf' target='_blank'>https://arxiv.org/pdf/2509.15965.pdf</a></span>   <span><a href='https://github.com/RLinf/RLinf' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Yu, Yuanqing Wang, Zhen Guo, Hao Lin, Si Xu, Hongzhi Zang, Quanlu Zhang, Yongji Wu, Chunyang Zhu, Junhao Hu, Zixiao Huang, Mingjie Wei, Yuqing Xie, Ke Yang, Bo Dai, Zhexuan Xu, Xiangyuan Wang, Xu Fu, Zhihao Liu, Kang Chen, Weilin Liu, Gang Liu, Boxun Li, Jianlei Yang, Zhi Yang, Guohao Dai, Yu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15965">RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning (RL) has demonstrated immense potential in advancing artificial general intelligence, agentic intelligence, and embodied intelligence. However, the inherent heterogeneity and dynamicity of RL workflows often lead to low hardware utilization and slow training on existing systems. In this paper, we present RLinf, a high-performance RL training system based on our key observation that the major roadblock to efficient RL training lies in system flexibility. To maximize flexibility and efficiency, RLinf is built atop a novel RL system design paradigm called macro-to-micro flow transformation (M2Flow), which automatically breaks down high-level, easy-to-compose RL workflows at both the temporal and spatial dimensions, and recomposes them into optimized execution flows. Supported by RLinf worker's adaptive communication capability, we devise context switching and elastic pipelining to realize M2Flow transformation, and a profiling-guided scheduling policy to generate optimal execution plans. Extensive evaluations on both reasoning RL and embodied RL tasks demonstrate that RLinf consistently outperforms state-of-the-art systems, achieving 1.1x-2.13x speedup in end-to-end training throughput.<br>
<span id='abs_ch'>中文：RLinf通过创新的宏观到微观流程转换设计，构建了灵活的强化学 训练系统，在各项任务中均实现了优于现有系统的性能 速。</span><br>
<span id='abs_en'>English: RLinf introduces a flexible reinforcement learning training system using macro-to-micro flow transformation to optimize workflows, achieving significant speedup over existing systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2509.15952.pdf' target='_blank'>https://arxiv.org/pdf/2509.15952.pdf</a></span>   <span><a href='https://github.com/ICDM-UESTC/COSE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gang Yang, Yue Lei, Wenxin Tai, Jin Wu, Jia Chen, Ting Zhong, Fan Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15952">Compose Yourself: Average-Velocity Flow Matching for One-Step Speech Enhancement</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion and flow matching (FM) models have achieved remarkable progress in speech enhancement (SE), yet their dependence on multi-step generation is computationally expensive and vulnerable to discretization errors. Recent advances in one-step generative modeling, particularly MeanFlow, provide a promising alternative by reformulating dynamics through average velocity fields. In this work, we present COSE, a one-step FM framework tailored for SE. To address the high training overhead of Jacobian-vector product (JVP) computations in MeanFlow, we introduce a velocity composition identity to compute average velocity efficiently, eliminating expensive computation while preserving theoretical consistency and achieving competitive enhancement quality. Extensive experiments on standard benchmarks show that COSE delivers up to 5x faster sampling and reduces training cost by 40%, all without compromising speech quality. Code is available at https://github.com/ICDM-UESTC/COSE.<br>
<span id='abs_ch'>Chinese: COSE提出了一种用于语音增强的单步流匹配框架，通过速度组合恒等式消除了昂贵的雅可比向量积计算，在保持竞争力的语音质量的同时，实现了5倍 速采 和40%训练成本降低。</span><br>
<span id='abs_en'>English: COSE introduces a one-step flow matching framework for speech enhancement that uses a velocity composition identity to eliminate expensive Jacobian-vector product computations, achieving 5x faster sampling and 40% lower training cost while maintaining competitive quality.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2509.15666.pdf' target='_blank'>https://arxiv.org/pdf/2509.15666.pdf</a></span>   <span><a href='https://github.com/WingSingFung/TISDiSS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongsheng Feng, Yuetonghui Xu, Jiehui Luo, Hongjia Liu, Xiaobing Li, Feng Yu, Wei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15666">TISDiSS: A Training-Time and Inference-Time Scalable Framework for Discriminative Source Separation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Source separation is a fundamental task in speech, music, and audio processing, and it also provides cleaner and larger data for training generative models. However, improving separation performance in practice often depends on increasingly large networks, inflating training and deployment costs. Motivated by recent advances in inference-time scaling for generative modeling, we propose Training-Time and Inference-Time Scalable Discriminative Source Separation (TISDiSS), a unified framework that integrates early-split multi-loss supervision, shared-parameter design, and dynamic inference repetitions. TISDiSS enables flexible speed-performance trade-offs by adjusting inference depth without retraining additional models. We further provide systematic analyses of architectural and training choices and show that training with more inference repetitions improves shallow-inference performance, benefiting low-latency applications. Experiments on standard speech separation benchmarks demonstrate state-of-the-art performance with a reduced parameter count, establishing TISDiSS as a scalable and practical framework for adaptive source separation. Code is available at https://github.com/WingSingFung/TISDiSS.<br>
<span id='abs_ch'>中文摘要：TISDiSS是一种可扩展的源分离框架，通过动态推理重复实现灵活的速率-性能权衡，以更少的参数取得了最先进的性能。</span><br>
<span id='abs_en'>English Summary: TISDiSS is a scalable source separation framework that enables flexible speed-performance trade-offs through dynamic inference repetitions, achieving state-of-the-art results with fewer parameters.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2509.15635.pdf' target='_blank'>https://arxiv.org/pdf/2509.15635.pdf</a></span>   <span><a href='https://github.com/tangpan360/MicroRCA-Agent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pan Tang, Shixiang Tang, Huanqi Pu, Zhiqing Miao, Zhixing Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15635">MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large Language Model Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents MicroRCA-Agent, an innovative solution for microservice root cause analysis based on large language model agents, which constructs an intelligent fault root cause localization system with multimodal data fusion. The technical innovations are embodied in three key aspects: First, we combine the pre-trained Drain log parsing algorithm with multi-level data filtering mechanism to efficiently compress massive logs into high-quality fault features. Second, we employ a dual anomaly detection approach that integrates Isolation Forest unsupervised learning algorithms with status code validation to achieve comprehensive trace anomaly identification. Third, we design a statistical symmetry ratio filtering mechanism coupled with a two-stage LLM analysis strategy to enable full-stack phenomenon summarization across node-service-pod hierarchies. The multimodal root cause analysis module leverages carefully designed cross-modal prompts to deeply integrate multimodal anomaly information, fully exploiting the cross-modal understanding and logical reasoning capabilities of large language models to generate structured analysis results encompassing fault components, root cause descriptions, and reasoning trace. Comprehensive ablation studies validate the complementary value of each modal data and the effectiveness of the system architecture. The proposed solution demonstrates superior performance in complex microservice fault scenarios, achieving a final score of 50.71. The code has been released at: https://github.com/tangpan360/MicroRCA-Agent.<br>
<span id='abs_ch'>中文摘要：MicroRCA-Agent是一种基于大语言模型的智能故障  定位系统，通过多模态数据融合、日志压缩和双重异常检测机制，在复杂微服务场景中实现精准的故障分析与诊断。</span><br>
<span id='abs_en'>English Summary: MicroRCA-Agent is an intelligent fault localization system that leverages large language models and multimodal data fusion to achieve comprehensive root cause analysis in microservices through log compression, dual anomaly detection, and cross-modal reasoning.Paperid:640,https://arxiv.org/pdf/2509.15608.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2509.15591.pdf' target='_blank'>https://arxiv.org/pdf/2509.15591.pdf</a></span>   <span><a href='https://github.com/microsoft/latent-zoning-networks' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zinan Lin, Enshu Liu, Xuefei Ning, Junyi Zhu, Wenyu Wang, Sergey Yekhanin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15591">Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Generative modeling, representation learning, and classification are three core problems in machine learning (ML), yet their state-of-the-art (SoTA) solutions remain largely disjoint. In this paper, we ask: Can a unified principle address all three? Such unification could simplify ML pipelines and foster greater synergy across tasks. We introduce Latent Zoning Network (LZN) as a step toward this goal. At its core, LZN creates a shared Gaussian latent space that encodes information across all tasks. Each data type (e.g., images, text, labels) is equipped with an encoder that maps samples to disjoint latent zones, and a decoder that maps latents back to data. ML tasks are expressed as compositions of these encoders and decoders: for example, label-conditional image generation uses a label encoder and image decoder; image embedding uses an image encoder; classification uses an image encoder and label decoder. We demonstrate the promise of LZN in three increasingly complex scenarios: (1) LZN can enhance existing models (image generation): When combined with the SoTA Rectified Flow model, LZN improves FID on CIFAR10 from 2.76 to 2.59-without modifying the training objective. (2) LZN can solve tasks independently (representation learning): LZN can implement unsupervised representation learning without auxiliary loss functions, outperforming the seminal MoCo and SimCLR methods by 9.3% and 0.2%, respectively, on downstream linear classification on ImageNet. (3) LZN can solve multiple tasks simultaneously (joint generation and classification): With image and label encoders/decoders, LZN performs both tasks jointly by design, improving FID and achieving SoTA classification accuracy on CIFAR10. The code and trained models are available at https://github.com/microsoft/latent-zoning-networks. The project website is at https://zinanlin.me/blogs/latent_zoning_networks.html.<br>
<span id='abs_ch'>中文: 潜在分区网络（LZN）通过构建共享高斯潜空间和任务专用编解 器，统一了生成建模、表征学 和分类三大机器学  心任务，在保持训练目 不变的前提下实现了多项性能提升。English: The Latent Zoning Network (LZN) unifies generative modeling, representation learning, and classification by creating a shared Gaussian latent space with task-specific encoders and decoders, demonstrating improved performance across diverse machine learning tasks without modifying core training objectives.Paperid:643,https://arxiv.org/pdf/2509.15583.pdfGitHub</span><br>
<span id='abs_en'>English: The Latent Zoning Network (LZN) unifies generative modeling, representation learning, and classification by creating a shared Gaussian latent space with task-specific encoders and decoders, demonstrating improved performance across diverse machine learning tasks without modifying core training objectives.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2509.15573.pdf' target='_blank'>https://arxiv.org/pdf/2509.15573.pdf</a></span>   <span><a href='https://github.com/Ferry-Li/SI-SOD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shilong Bao, Qianqian Xu, Feiran Li, Boyu Han, Zhiyong Yang, Xiaochun Cao, Qingming Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15573">Towards Size-invariant Salient Object Detection: A Generic Evaluation and Optimization Approach</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper investigates a fundamental yet underexplored issue in Salient Object Detection (SOD): the size-invariant property for evaluation protocols, particularly in scenarios when multiple salient objects of significantly different sizes appear within a single image. We first present a novel perspective to expose the inherent size sensitivity of existing widely used SOD metrics. Through careful theoretical derivations, we show that the evaluation outcome of an image under current SOD metrics can be essentially decomposed into a sum of several separable terms, with the contribution of each term being directly proportional to its corresponding region size. Consequently, the prediction errors would be dominated by the larger regions, while smaller yet potentially more semantically important objects are often overlooked, leading to biased performance assessments and practical degradation. To address this challenge, a generic Size-Invariant Evaluation (SIEva) framework is proposed. The core idea is to evaluate each separable component individually and then aggregate the results, thereby effectively mitigating the impact of size imbalance across objects. Building upon this, we further develop a dedicated optimization framework (SIOpt), which adheres to the size-invariant principle and significantly enhances the detection of salient objects across a broad range of sizes. Notably, SIOpt is model-agnostic and can be seamlessly integrated with a wide range of SOD backbones. Theoretically, we also present generalization analysis of SOD methods and provide evidence supporting the validity of our new evaluation protocols. Finally, comprehensive experiments speak to the efficacy of our proposed approach. The code is available at https://github.com/Ferry-Li/SI-SOD.<br>
<span id='abs_ch'>本文针对显著目 检测中评估指 对尺寸的敏感性问题，提出了一个尺寸不变性评估框架，通过独立评估各组件来消除尺寸偏差，确保不同大小目 的公平检测。</span><br>
<span id='abs_en'>This paper identifies and addresses the size bias in Salient Object Detection metrics by proposing a Size-Invariant Evaluation framework that ensures balanced assessment across objects of varying sizes.Paperid:646,https://arxiv.org/pdf/2509.15553.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/2509.15553.pdf' target='_blank'>https://arxiv.org/pdf/2509.15553.pdf</a></span>   <span><a href='https://github.com/lt-0123/Diff-Feat' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tian Lan, Yiming Zheng, Jianxin Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15553">Diffusion-Based Cross-Modal Feature Extraction for Multi-Label Classification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multi-label classification has broad applications and depends on powerful representations capable of capturing multi-label interactions. We introduce \textit{Diff-Feat}, a simple but powerful framework that extracts intermediate features from pre-trained diffusion-Transformer models for images and text, and fuses them for downstream tasks. We observe that for vision tasks, the most discriminative intermediate feature along the diffusion process occurs at the middle step and is located in the middle block in Transformer. In contrast, for language tasks, the best feature occurs at the noise-free step and is located in the deepest block. In particular, we observe a striking phenomenon across varying datasets: a mysterious "Layer $12$" consistently yields the best performance on various downstream classification tasks for images (under DiT-XL/2-256$\times$256). We devise a heuristic local-search algorithm that pinpoints the locally optimal "image-text"$\times$"block-timestep" pair among a few candidates, avoiding an exhaustive grid search. A simple fusion-linear projection followed by addition-of the selected representations yields state-of-the-art performance: 98.6\% mAP on MS-COCO-enhanced and 45.7\% mAP on Visual Genome 500, surpassing strong CNN, graph, and Transformer baselines by a wide margin. t-SNE and clustering metrics further reveal that \textit{Diff-Feat} forms tighter semantic clusters than unimodal counterparts. The code is available at https://github.com/lt-0123/Diff-Feat.<br>
<span id='abs_ch'>Chinese: Diff-Feat框架通过提取预训练扩散Transformer模型的图像和文本中间特征并进行融合，采用启发式搜索寻找最优特征对，在多 签分类任务中实现了最先进的性能。</span><br>
<span id='abs_en'>English: The Diff-Feat framework extracts and fuses intermediate features from pre-trained diffusion-Transformer models for images and text, achieving state-of-the-art multi-label classification performance through a heuristic search for optimal feature pairs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2509.15490.pdf' target='_blank'>https://arxiv.org/pdf/2509.15490.pdf</a></span>   <span><a href='https://github.com/abtraore/SmolRGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdarahmane Traore, Ãric Hervet, Andy Couturier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15490">SmolRGPT: Efficient Spatial Reasoning for Warehouse Environments with 600M Parameters</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in vision-language models (VLMs) have enabled powerful multimodal reasoning, but state-of-the-art approaches typically rely on extremely large models with prohibitive computational and memory requirements. This makes their deployment challenging in resource-constrained environments such as warehouses, robotics, and industrial applications, where both efficiency and robust spatial understanding are critical. In this work, we present SmolRGPT, a compact vision-language architecture that explicitly incorporates region-level spatial reasoning by integrating both RGB and depth cues. SmolRGPT employs a three-stage curriculum that progressively align visual and language features, enables spatial relationship understanding, and adapts to task-specific datasets. We demonstrate that with only 600M parameters, SmolRGPT achieves competitive results on challenging warehouse spatial reasoning benchmarks, matching or exceeding the performance of much larger alternatives. These findings highlight the potential for efficient, deployable multimodal intelligence in real-world settings without sacrificing core spatial reasoning capabilities. The code of the experimentation will be available at: https://github.com/abtraore/SmolRGPT<br>
<span id='abs_ch'>中文: SmolRGPT是一种紧凑的视觉语言模型，通过融合RGB和深度信息实现高效空间推理，仅用6亿参数即可获得优异性能，适用于资源受限的实际应用场景。English: SmolRGPT is a compact vision-language model that integrates RGB and depth cues for efficient spatial reasoning, achieving competitive performance with only 600M parameters while enabling deployment in resource-constrained environments.Paperid:650,https://arxiv.org/pdf/2509.15475.pdfGitHub</span><br>
<span id='abs_en'>English: SmolRGPT is a compact vision-language model that integrates RGB and depth cues for efficient spatial reasoning, achieving competitive performance with only 600M parameters while enabling deployment in resource-constrained environments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2509.15333.pdf' target='_blank'>https://arxiv.org/pdf/2509.15333.pdf</a></span>   <span><a href='https://github.com/LeapLabTHU/AdaptiveNN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yulin Wang, Yang Yue, Yang Yue, Huanqian Wang, Haojun Jiang, Yizeng Han, Zanlin Ni, Yifan Pu, Minglei Shi, Rui Lu, Qisen Yang, Andrew Zhao, Zhuofan Xia, Shiji Song, Gao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15333">Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Human vision is highly adaptive, efficiently sampling intricate environments by sequentially fixating on task-relevant regions. In contrast, prevailing machine vision models passively process entire scenes at once, resulting in excessive resource demands scaling with spatial-temporal input resolution and model size, yielding critical limitations impeding both future advancements and real-world application. Here we introduce AdaptiveNN, a general framework aiming to drive a paradigm shift from 'passive' to 'active, adaptive' vision models. AdaptiveNN formulates visual perception as a coarse-to-fine sequential decision-making process, progressively identifying and attending to regions pertinent to the task, incrementally combining information across fixations, and actively concluding observation when sufficient. We establish a theory integrating representation learning with self-rewarding reinforcement learning, enabling end-to-end training of the non-differentiable AdaptiveNN without additional supervision on fixation locations. We assess AdaptiveNN on 17 benchmarks spanning 9 tasks, including large-scale visual recognition, fine-grained discrimination, visual search, processing images from real driving and medical scenarios, language-driven embodied AI, and side-by-side comparisons with humans. AdaptiveNN achieves up to 28x inference cost reduction without sacrificing accuracy, flexibly adapts to varying task demands and resource budgets without retraining, and provides enhanced interpretability via its fixation patterns, demonstrating a promising avenue toward efficient, flexible, and interpretable computer vision. Furthermore, AdaptiveNN exhibits closely human-like perceptual behaviors in many cases, revealing its potential as a valuable tool for investigating visual cognition. Code is available at https://github.com/LeapLabTHU/AdaptiveNN.<br>
<span id='abs_ch'>中文摘要：AdaptiveNN提出了一种主动视觉框架，通过模拟人眼注视机制实现从粗到精的序列化视觉处理，在保持精度的同时大幅降低计算成本，并在多任务中展现出类人的感知特性与良好可解释性。</span><br>
<span id='abs_en'>English Summary: AdaptiveNN introduces an active vision framework that mimics human eye movements to process visual information sequentially, significantly reducing computational costs while maintaining accuracy and enhancing interpretability across diverse tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2509.15250.pdf' target='_blank'>https://arxiv.org/pdf/2509.15250.pdf</a></span>   <span><a href='https://github.com/wdqin/VLN-NAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenda Qin, Andrea Burns, Bryan A. Plummer, Margrit Betke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15250">Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large models achieve strong performance on Vision-and-Language Navigation (VLN) tasks, but are costly to run in resource-limited environments. Token pruning offers appealing tradeoffs for efficiency with minimal performance loss by reducing model input size, but prior work overlooks VLN-specific challenges. For example, information loss from pruning can effectively increase computational cost due to longer walks. Thus, the inability to identify uninformative tokens undermines the supposed efficiency gains from pruning. To address this, we propose Navigation-Aware Pruning (NAP), which uses navigation-specific traits to simplify the pruning process by pre-filtering tokens into foreground and background. For example, image views are filtered based on whether the agent can navigate in that direction. We also extract navigation-relevant instructions using a Large Language Model. After filtering, we focus pruning on background tokens, minimizing information loss. To further help avoid increases in navigation length, we discourage backtracking by removing low-importance navigation nodes. Experiments on standard VLN benchmarks show NAP significantly outperforms prior work, preserving higher success rates while saving more than 50% FLOPS.<br>
<br>
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2509.15237.pdf' target='_blank'>https://arxiv.org/pdf/2509.15237.pdf</a></span>   <span><a href='https://github.com/Kratos-Wen/MICA' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Kratos-Wen/MICA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Di Wen, Kunyu Peng, Junwei Zheng, Yufan Chen, Yitain Shi, Jiale Wei, Ruiping Liu, Kailun Yang, Rainer Stiefelhagen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15237">MICA: Multi-Agent Industrial Coordination Assistant</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Industrial workflows demand adaptive and trustworthy assistance that can operate under limited computing, connectivity, and strict privacy constraints. In this work, we present MICA (Multi-Agent Industrial Coordination Assistant), a perception-grounded and speech-interactive system that delivers real-time guidance for assembly, troubleshooting, part queries, and maintenance. MICA coordinates five role-specialized language agents, audited by a safety checker, to ensure accurate and compliant support. To achieve robust step understanding, we introduce Adaptive Step Fusion (ASF), which dynamically blends expert reasoning with online adaptation from natural speech feedback. Furthermore, we establish a new multi-agent coordination benchmark across representative task categories and propose evaluation metrics tailored to industrial assistance, enabling systematic comparison of different coordination topologies. Our experiments demonstrate that MICA consistently improves task success, reliability, and responsiveness over baseline structures, while remaining deployable on practical offline hardware. Together, these contributions highlight MICA as a step toward deployable, privacy-preserving multi-agent assistants for dynamic factory environments. The source code will be made publicly available at https://github.com/Kratos-Wen/MICA.<br>
<span id='abs_ch'>中文：MICA是一种面向工业辅助的语音交互多智能体系统，通过自适应推理与安全审 机制，在保障隐私和硬件限制的前提下，显著提升了任务执行的成功率与可 性。English: MICA is a speech-interactive multi-agent system designed for industrial assistance, integrating adaptive reasoning and safety checks to enhance task success and reliability while operating under privacy and hardware constraints.Paperid:657,https://arxiv.org/pdf/2509.15235.pdfGitHub</span><br>
<span id='abs_en'>English: MICA is a speech-interactive multi-agent system designed for industrial assistance, integrating adaptive reasoning and safety checks to enhance task success and reliability while operating under privacy and hardware constraints.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2509.15167.pdf' target='_blank'>https://arxiv.org/pdf/2509.15167.pdf</a></span>   <span><a href='https://github.com/pakheiyeung/M-N' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pak-Hei Yeung, Jayroop Ramesh, Pengfei Lyu, Ana Namburete, Jagath Rajapakse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15167">Semi-Supervised 3D Medical Segmentation from 2D Natural Images Pretrained Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper explores the transfer of knowledge from general vision models pretrained on 2D natural images to improve 3D medical image segmentation. We focus on the semi-supervised setting, where only a few labeled 3D medical images are available, along with a large set of unlabeled images. To tackle this, we propose a model-agnostic framework that progressively distills knowledge from a 2D pretrained model to a 3D segmentation model trained from scratch. Our approach, M&N, involves iterative co-training of the two models using pseudo-masks generated by each other, along with our proposed learning rate guided sampling that adaptively adjusts the proportion of labeled and unlabeled data in each training batch to align with the models' prediction accuracy and stability, minimizing the adverse effect caused by inaccurate pseudo-masks. Extensive experiments on multiple publicly available datasets demonstrate that M&N achieves state-of-the-art performance, outperforming thirteen existing semi-supervised segmentation approaches under all different settings. Importantly, ablation studies show that M&N remains model-agnostic, allowing seamless integration with different architectures. This ensures its adaptability as more advanced models emerge. The code is available at https://github.com/pakheiyeung/M-N.<br>
<span id='abs_ch'>中文: 本文提出M&N框架，通过迭代协同训练和自适应数据采 ，将2D预训练视觉模型的知识迁移至3D医学图像分割，在半监督设定下实现了最优性能。English: This paper introduces M&N, a model-agnostic framework that transfers knowledge from 2D pretrained vision models to enhance 3D medical image segmentation through iterative co-training and adaptive data sampling, achieving state-of-the-art results in semi-supervised settings.Paperid:673,https://arxiv.org/pdf/2509.15157.pdfGitHub</span><br>
<span id='abs_en'>English: This paper introduces M&N, a model-agnostic framework that transfers knowledge from 2D pretrained vision models to enhance 3D medical image segmentation through iterative co-training and adaptive data sampling, achieving state-of-the-art results in semi-supervised settings.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2509.15151.pdf' target='_blank'>https://arxiv.org/pdf/2509.15151.pdf</a></span>   <span><a href='https://github.com/stelioskt/audioFX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Stelios Katsis, Vassilis Lyberatos, Spyridon Kantarelis, Edmund Dervakos, Giorgos Stamou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15151">Exploring How Audio Effects Alter Emotion with Foundation Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Audio effects (FX) such as reverberation, distortion, modulation, and dynamic range processing play a pivotal role in shaping emotional responses during music listening. While prior studies have examined links between low-level audio features and affective perception, the systematic impact of audio FX on emotion remains underexplored. This work investigates how foundation models - large-scale neural architectures pretrained on multimodal data - can be leveraged to analyze these effects. Such models encode rich associations between musical structure, timbre, and affective meaning, offering a powerful framework for probing the emotional consequences of sound design techniques. By applying various probing methods to embeddings from deep learning models, we examine the complex, nonlinear relationships between audio FX and estimated emotion, uncovering patterns tied to specific effects and evaluating the robustness of foundation audio models. Our findings aim to advance understanding of the perceptual impact of audio production practices, with implications for music cognition, performance, and affective computing.<br>
<span id='abs_ch'>中文摘要：本 究探讨了如何利用基础模型分析音频效果在音乐中的情感影响，通过深度学 探针方法揭示了声音设计技术与情感反应之间的复杂关系。</span><br>
<span id='abs_en'>English Summary: This study explores how foundation models can analyze the emotional impact of audio effects in music, revealing complex relationships between sound design techniques and affective responses through advanced deep learning probing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2509.14998.pdf' target='_blank'>https://arxiv.org/pdf/2509.14998.pdf</a></span>   <span><a href='https://github.com/XiaoXiao-Woo/KAMAC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wu, Ting-Zhu Huang, Liang-Jian Deng, Yanyuan Qiao, Imran Razzak, Yutong Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14998">A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Medical decision-making often involves integrating knowledge from multiple clinical specialties, typically achieved through multidisciplinary teams. Inspired by this collaborative process, recent work has leveraged large language models (LLMs) in multi-agent collaboration frameworks to emulate expert teamwork. While these approaches improve reasoning through agent interaction, they are limited by static, pre-assigned roles, which hinder adaptability and dynamic knowledge integration. To address these limitations, we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration framework that enables LLM agents to dynamically form and expand expert teams based on the evolving diagnostic context. KAMAC begins with one or more expert agents and then conducts a knowledge-driven discussion to identify and fill knowledge gaps by recruiting additional specialists as needed. This supports flexible, scalable collaboration in complex clinical scenarios, with decisions finalized through reviewing updated agent comments. Experiments on two real-world medical benchmarks demonstrate that KAMAC significantly outperforms both single-agent and advanced multi-agent methods, particularly in complex clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty expertise. Our code is publicly available at: https://github.com/XiaoXiao-Woo/KAMAC.<br>
<span id='abs_ch'>中文: 本文提出KAMAC知识驱动自适应多智能体协作框架，通过动态组建专家团队克服静态角色分配局限，在癌症预后等复杂医疗场景中显著优于现有方法。</span><br>
<span id='abs_en'>English: This paper introduces KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration framework that dynamically forms expert teams using large language models to address limitations in static role assignments, significantly outperforming existing methods in complex medical scenarios like cancer prognosis.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2509.14966.pdf' target='_blank'>https://arxiv.org/pdf/2509.14966.pdf</a></span>   <span><a href='https://github.com/longkukuhi/RoboEye' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingwu Zhang, Guanxuan Li, Zhuocheng Zhang, Zijun Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14966">RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapidly growing number of product categories in large-scale e-commerce makes accurate object identification for automated packing in warehouses substantially more difficult. As the catalog grows, intra-class variability and a long tail of rare or visually similar items increase, and when combined with diverse packaging, cluttered containers, frequent occlusion, and large viewpoint changes-these factors amplify discrepancies between query and reference images, causing sharp performance drops for methods that rely solely on 2D appearance features. Thus, we propose RoboEye, a two-stage identification framework that dynamically augments 2D semantic features with domain-adapted 3D reasoning and lightweight adapters to bridge training deployment gaps. In the first stage, we train a large vision model to extract 2D features for generating candidate rankings. A lightweight 3D-feature-awareness module then estimates 3D feature quality and predicts whether 3D re-ranking is necessary, preventing performance degradation and avoiding unnecessary computation. When invoked, the second stage uses our robot 3D retrieval transformer, comprising a 3D feature extractor that produces geometry-aware dense features and a keypoint-based matcher that computes keypoint-correspondence confidences between query and reference images instead of conventional cosine-similarity scoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior state of the art (RoboLLM). Moreover, RoboEye operates using only RGB images, avoiding reliance on explicit 3D inputs and reducing deployment costs. The code used in this paper is publicly available at: https://github.com/longkukuhi/RoboEye.<br>
<span id='abs_ch'>中文: RoboEye是一个两阶段识别框架，通过结合3D推理增强2D语义特征，提升电商仓库中的物体识别准确率，在仅使用RGB图像降低成本的同时，将Recall@1指 较现有最佳方法提高了7.1%。</span><br>
<span id='abs_en'>English: RoboEye is a two-stage identification framework that enhances 2D semantic features with 3D reasoning to improve object recognition in e-commerce warehouses, achieving a 7.1% increase in Recall@1 over previous methods while using only RGB images to reduce costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2509.14912.pdf' target='_blank'>https://arxiv.org/pdf/2509.14912.pdf</a></span>   <span><a href='https://github.com/Eps-Acoustic-Revolution-Lab/EAR_VAE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kangdi Wang, Zhiyue Wu, Dinghao Zhou, Rui Lin, Junyu Dai, Tao Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14912">Back to Ear: Perceptually Driven High Fidelity Music Reconstruction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Variational Autoencoders (VAEs) are essential for large-scale audio tasks like diffusion-based generation. However, existing open-source models often neglect auditory perceptual aspects during training, leading to weaknesses in phase accuracy and stereophonic spatial representation. To address these challenges, we propose Îµar-VAE, an open-source music signal reconstruction model that rethinks and optimizes the VAE training paradigm. Our contributions are threefold: (i) A K-weighting perceptual filter applied prior to loss calculation to align the objective with auditory perception. (ii) Two novel phase losses: a Correlation Loss for stereo coherence, and a Phase Loss using its derivatives--Instantaneous Frequency and Group Delay--for precision. (iii) A new spectral supervision paradigm where magnitude is supervised by all four Mid/Side/Left/Right components, while phase is supervised only by the LR components. Experiments show Îµar-VAE at 44.1kHz substantially outperforms leading open-source models across diverse metrics, showing particular strength in reconstructing high-frequency harmonics and the spatial characteristics.<br>
<span id='abs_ch'>中文摘要：εar-VAE模型通过引入K 权感知滤波器、创新的相位损失函数和频谱监督范式，优化了音频重建的听觉感知效果，在重构高频谐波和空间特征方面显著优于现有开源模型。</span><br>
<span id='abs_en'>English Summary: The εar-VAE model enhances audio reconstruction by incorporating auditory perception through K-weighting filters, novel phase losses for stereo coherence, and a spectral supervision paradigm, significantly outperforming existing models in high-frequency harmonics and spatial accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2509.14868.pdf' target='_blank'>https://arxiv.org/pdf/2509.14868.pdf</a></span>   <span><a href='https://github.com/hit636/DPANet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianyang Li, Xingjun Zhang, Shaoxun Wang, Jia Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14868">DPANet: Dual Pyramid Attention Network for Multivariate Time Series Forecasting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Long-term time series forecasting (LTSF) is hampered by the challenge of modeling complex dependencies that span multiple temporal scales and frequency resolutions. Existing methods, including Transformer and MLP-based models, often struggle to capture these intertwined characteristics in a unified and structured manner. We propose the Dual Pyramid Attention Network (DPANet), a novel architecture that explicitly decouples and concurrently models temporal multi-scale dynamics and spectral multi-resolution periodicities. DPANet constructs two parallel pyramids: a Temporal Pyramid built on progressive downsampling, and a Frequency Pyramid built on band-pass filtering. The core of our model is the Cross-Pyramid Fusion Block, which facilitates deep, interactive information exchange between corresponding pyramid levels via cross-attention. This fusion proceeds in a coarse-to-fine hierarchy, enabling global context to guide local representation learning. Extensive experiments on public benchmarks show that DPANet achieves state-of-the-art performance, significantly outperforming prior models. Code is available at https://github.com/hit636/DPANet.<br>
<span id='abs_ch'>中文: DPANet提出了一种双金字塔架构，通过时序金字塔和频域金字塔结合跨注意力融合，有效建模多尺度动态和周期性，在长期时间序列预测中实现了最优性能。</span><br>
<span id='abs_en'>English: DPANet introduces a dual pyramid architecture with temporal and frequency pyramids, integrated through cross-attention fusion, to effectively model multi-scale dynamics and periodicities, achieving state-of-the-art performance in long-term time series forecasting.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2509.14858.pdf' target='_blank'>https://arxiv.org/pdf/2509.14858.pdf</a></span>   <span><a href='https://github.com/liduojia1/MeanFlowSE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Duojia Li, Shenghui Lu, Hongchen Pan, Zongyi Zhan, Qingyang Hong, Lin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14858">MeanFlowSE: one-step generative speech enhancement via conditional mean flow</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multistep inference is a bottleneck for real-time generative speech enhancement because flow- and diffusion-based systems learn an instantaneous velocity field and therefore rely on iterative ordinary differential equation (ODE) solvers. We introduce MeanFlowSE, a conditional generative model that learns the average velocity over finite intervals along a trajectory. Using a Jacobian-vector product (JVP) to instantiate the MeanFlow identity, we derive a local training objective that directly supervises finite-interval displacement while remaining consistent with the instantaneous-field constraint on the diagonal. At inference, MeanFlowSE performs single-step generation via a backward-in-time displacement, removing the need for multistep solvers; an optional few-step variant offers additional refinement. On VoiceBank-DEMAND, the single-step model achieves strong intelligibility, fidelity, and perceptual quality with substantially lower computational cost than multistep baselines. The method requires no knowledge distillation or external teachers, providing an efficient, high-fidelity framework for real-time generative speech enhancement. The proposed method is open-sourced at https://github.com/liduojia1/MeanFlowSE.<br>
<span id='abs_ch'>中文：MeanFlowSE是一种通过学 有限间隔速度场实现单步语音增强的生成模型， 需多步求解器即可保持高质量和计算效率。</span><br>
<span id='abs_en'>English: MeanFlowSE is a generative model that enables single-step speech enhancement by learning finite-interval velocity fields, eliminating the need for multistep solvers while maintaining high quality and computational efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2509.14651.pdf' target='_blank'>https://arxiv.org/pdf/2509.14651.pdf</a></span>   <span><a href='https://github.com/yansiyu02/MUSE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyu Yan, Long Zeng, Xuecheng Wu, Chengcheng Han, Kongcheng Zhang, Chong Peng, Xuezhi Cao, Xunliang Cai, Chenjuan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14651">MUSE: MCTS-Driven Red Teaming Framework for Enhanced Multi-Turn Dialogue Safety in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models~(LLMs) become widely adopted, ensuring their alignment with human values is crucial to prevent jailbreaks where adversaries manipulate models to produce harmful content. While most defenses target single-turn attacks, real-world usage often involves multi-turn dialogues, exposing models to attacks that exploit conversational context to bypass safety measures. We introduce MUSE, a comprehensive framework tackling multi-turn jailbreaks from both attack and defense angles. For attacks, we propose MUSE-A, a method that uses frame semantics and heuristic tree search to explore diverse semantic trajectories. For defense, we present MUSE-D, a fine-grained safety alignment approach that intervenes early in dialogues to reduce vulnerabilities. Extensive experiments on various models show that MUSE effectively identifies and mitigates multi-turn vulnerabilities. Code is available at \href{https://github.com/yansiyu02/MUSE}{https://github.com/yansiyu02/MUSE}.<br>
<span id='abs_ch'>Chinese: MUSE 是一个全面应对大型语言模型多轮越狱的框架，通过 MUSE-A 利用框架语义和 搜索进行攻击，以及 MUSE-D 通过早期对话干预进行防御，实验证明其能有效识别和减轻漏洞。</span><br>
<span id='abs_en'>English: MUSE is a comprehensive framework addressing multi-turn jailbreaks in large language models by introducing MUSE-A for attacks using frame semantics and tree search, and MUSE-D for defense through early dialogue intervention, effectively identifying and mitigating vulnerabilities as demonstrated in experiments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2509.14627.pdf' target='_blank'>https://arxiv.org/pdf/2509.14627.pdf</a></span>   <span><a href='https://github.com/kimtaesu24/MSenC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taesoo Kim, Yongsik Jo, Hyunmin Song, Taehwan Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14627">Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Human conversation involves language, speech, and visual cues, with each medium providing complementary information. For instance, speech conveys a vibe or tone not fully captured by text alone. While multimodal LLMs focus on generating text responses from diverse inputs, less attention has been paid to generating natural and engaging speech. We propose a human-like agent that generates speech responses based on conversation mood and responsive style information. To achieve this, we build a novel MultiSensory Conversation dataset focused on speech to enable agents to generate natural speech. We then propose a multimodal LLM-based model for generating text responses and voice descriptions, which are used to generate speech covering paralinguistic information. Experimental results demonstrate the effectiveness of utilizing both visual and audio modalities in conversation to generate engaging speech. The source code is available in https://github.com/kimtaesu24/MSenC<br>
<span id='abs_ch'>Chinese Summary: 本 究提出了一种拟人化对话代理，通过整合视觉和音频线索，利用基于新型多模态大语言模型的系统，在专门构建的多感官对话数据集上训练，实现了自然且富有吸引力的语音生成。</span><br>
<span id='abs_en'>English Summary: This research introduces a human-like conversational agent that generates natural and engaging speech by integrating visual and audio cues, using a novel multimodal LLM-based model trained on a specialized MultiSensory Conversation dataset.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2509.14623.pdf' target='_blank'>https://arxiv.org/pdf/2509.14623.pdf</a></span>   <span><a href='https://github.com/pnnl/prompt2control' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanlong Wan, Xing Lu, Yan Chen, Karthik Devaprasad, Laura Hinkle
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14623">Automating Modelica Module Generation Using Large Language Models: A Case Study on Building Control Description Language</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Dynamic energy systems and controls require advanced modeling frameworks to design and test supervisory and fault tolerant strategies. Modelica is a widely used equation based language, but developing control modules is labor intensive and requires specialized expertise. This paper examines the use of large language models (LLMs) to automate the generation of Control Description Language modules in the Building Modelica Library as a case study. We developed a structured workflow that combines standardized prompt scaffolds, library aware grounding, automated compilation with OpenModelica, and human in the loop evaluation. Experiments were carried out on four basic logic tasks (And, Or, Not, and Switch) and five control modules (chiller enable/disable, bypass valve control, cooling tower fan speed, plant requests, and relief damper control). The results showed that GPT 4o failed to produce executable Modelica code in zero shot mode, while Claude Sonnet 4 achieved up to full success for basic logic blocks with carefully engineered prompts. For control modules, success rates reached 83 percent, and failed outputs required medium level human repair (estimated one to eight hours). Retrieval augmented generation often produced mismatches in module selection (for example, And retrieved as Or), while a deterministic hard rule search strategy avoided these errors. Human evaluation also outperformed AI evaluation, since current LLMs cannot assess simulation results or validate behavioral correctness. Despite these limitations, the LLM assisted workflow reduced the average development time from 10 to 20 hours down to 4 to 6 hours per module, corresponding to 40 to 60 percent time savings. These results highlight both the potential and current limitations of LLM assisted Modelica generation, and point to future research in pre simulation validation, stronger grounding, and closed loop evaluation.<br>
<br>
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2509.14619.pdf' target='_blank'>https://arxiv.org/pdf/2509.14619.pdf</a></span>   <span><a href='https://github.com/xiaobaoxia/LSTC-MDA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Ding, Haisheng Fu, Soroush Oraki, Jie Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14619">LSTC-MDA: A Unified Framework for Long-Short Term Temporal Convolution and Mixed Data Augmentation in Skeleton-Based Action Recognition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Skeleton-based action recognition faces two longstanding challenges: the scarcity of labeled training samples and difficulty modeling short- and long-range temporal dependencies. To address these issues, we propose a unified framework, LSTC-MDA, which simultaneously improves temporal modeling and data diversity. We introduce a novel Long-Short Term Temporal Convolution (LSTC) module with parallel short- and long-term branches, these two feature branches are then aligned and fused adaptively using learned similarity weights to preserve critical long-range cues lost by conventional stride-2 temporal convolutions. We also extend Joint Mixing Data Augmentation (JMDA) with an Additive Mixup at the input level, diversifying training samples and restricting mixup operations to the same camera view to avoid distribution shifts. Ablation studies confirm each component contributes. LSTC-MDA achieves state-of-the-art results: 94.1% and 97.5% on NTU 60 (X-Sub and X-View), 90.4% and 92.0% on NTU 120 (X-Sub and X-Set),97.2% on NW-UCLA. Code: https://github.com/xiaobaoxia/LSTC-MDA.<br>
<span id='abs_ch'>中文: 提出的LSTC-MDA框架通过长短时态卷积模块捕捉多尺度时间依赖性和扩展的联合混合数据增强增  本多 性，显著提升了基于骨架的动作识别性能，在多个基准测试中取得了最优结果。</span><br>
<span id='abs_en'>English: The proposed LSTC-MDA framework enhances skeleton-based action recognition by introducing a Long-Short Term Temporal Convolution module to capture multi-scale temporal dependencies and an extended Joint Mixing Data Augmentation to increase sample diversity, achieving state-of-the-art results on multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2509.14335.pdf' target='_blank'>https://arxiv.org/pdf/2509.14335.pdf</a></span>   <span><a href='https://github.com/ZhengXR930/MalEval.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinran Zheng, Xingzhi Qian, Yiling He, Shuo Yang, Lorenzo Cavallaro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14335">Beyond Classification: Evaluating LLMs for Fine-Grained Automatic Malware Behavior Auditing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Automated malware classification has achieved strong detection performance. Yet, malware behavior auditing seeks causal and verifiable explanations of malicious activities -- essential not only to reveal what malware does but also to substantiate such claims with evidence. This task is challenging, as adversarial intent is often hidden within complex, framework-heavy applications, making manual auditing slow and costly. Large Language Models (LLMs) could help address this gap, but their auditing potential remains largely unexplored due to three limitations: (1) scarce fine-grained annotations for fair assessment; (2) abundant benign code obscuring malicious signals; and (3) unverifiable, hallucination-prone outputs undermining attribution credibility. To close this gap, we introduce MalEval, a comprehensive framework for fine-grained Android malware auditing, designed to evaluate how effectively LLMs support auditing under real-world constraints. MalEval provides expert-verified reports and an updated sensitive API list to mitigate ground truth scarcity and reduce noise via static reachability analysis. Function-level structural representations serve as intermediate attribution units for verifiable evaluation. Building on this, we define four analyst-aligned tasks -- function prioritization, evidence attribution, behavior synthesis, and sample discrimination -- together with domain-specific metrics and a unified workload-oriented score. We evaluate seven widely used LLMs on a curated dataset of recent malware and misclassified benign apps, offering the first systematic assessment of their auditing capabilities. MalEval reveals both promising potential and critical limitations across audit stages, providing a reproducible benchmark and foundation for future research on LLM-enhanced malware behavior auditing. MalEval is publicly available at https://github.com/ZhengXR930/MalEval.git<br>
<span id='abs_ch'>中文: MalEval框架通过提供专家验证数据和结构化表示，系统评估大语言模型在四项分析任务中的恶意软件审计能力，既揭示了其潜力也暴露了关键局限，为未来 究建立了可复现基准。</span><br>
<span id='abs_en'>English: The MalEval framework addresses the limitations of large language models in malware auditing by providing expert-verified data and structural representations to systematically evaluate their capabilities across four analyst-aligned tasks, revealing both potential and critical gaps in current approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2509.14284.pdf' target='_blank'>https://arxiv.org/pdf/2509.14284.pdf</a></span>   <span><a href='https://github.com/Vaidehi99/MultiAgentPrivacy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaidehi Patil, Elias Stengel-Eskin, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14284">The Sum Leaks More Than Its Parts: Compositional Privacy Risks and Mitigations in Multi-Agent Collaboration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) become integral to multi-agent systems, new privacy risks emerge that extend beyond memorization, direct inference, or single-turn evaluations. In particular, seemingly innocuous responses, when composed across interactions, can cumulatively enable adversaries to recover sensitive information, a phenomenon we term compositional privacy leakage. We present the first systematic study of such compositional privacy leaks and possible mitigation methods in multi-agent LLM systems. First, we develop a framework that models how auxiliary knowledge and agent interactions jointly amplify privacy risks, even when each response is benign in isolation. Next, to mitigate this, we propose and evaluate two defense strategies: (1) Theory-of-Mind defense (ToM), where defender agents infer a questioner's intent by anticipating how their outputs may be exploited by adversaries, and (2) Collaborative Consensus Defense (CoDef), where responder agents collaborate with peers who vote based on a shared aggregated state to restrict sensitive information spread. Crucially, we balance our evaluation across compositions that expose sensitive information and compositions that yield benign inferences. Our experiments quantify how these defense strategies differ in balancing the privacy-utility trade-off. We find that while chain-of-thought alone offers limited protection to leakage (~39% sensitive blocking rate), our ToM defense substantially improves sensitive query blocking (up to 97%) but can reduce benign task success. CoDef achieves the best balance, yielding the highest Balanced Outcome (79.8%), highlighting the benefit of combining explicit reasoning with defender collaboration. Together, our results expose a new class of risks in collaborative LLM deployments and provide actionable insights for designing safeguards against compositional, context-driven privacy leakage.<br>
<span id='abs_ch'>中文摘要：本 究揭示了多智能体大语言模型系统中组合式隐私泄露的风险，即看似 害的交互响应在累积中可能泄露敏感信息，并提出心智理论和协作共识两种防御策略，在保护隐私与保持系统效用间实现了最佳平衡。English Summary: This study identifies compositional privacy leakage in multi-agent LLM systems, where seemingly harmless responses collectively expose sensitive information, and proposes two defense strategies—Theory-of-Mind and Collaborative Consensus—that effectively balance privacy protection with utility.Paperid:719,https://arxiv.org/pdf/2509.14274.pdfGitHub</span><br>
<span id='abs_en'>English Summary: This study identifies compositional privacy leakage in multi-agent LLM systems, where seemingly harmless responses collectively expose sensitive information, and proposes two defense strategies—Theory-of-Mind and Collaborative Consensus—that effectively balance privacy protection with utility.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2509.14274.pdf' target='_blank'>https://arxiv.org/pdf/2509.14274.pdf</a></span>   <span><a href='https://github.com/auto-res/ConjecturingProvingLoop' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kazumi Kasaura, Naoto Onda, Yuta Oriike, Masaya Taniguchi, Akiyoshi Sannai, Sho Sonoda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14274">Discovering New Theorems via LLMs with In-Context Proof Learning in Lean</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models have demonstrated significant promise in formal theorem proving. However, previous works mainly focus on solving existing problems. In this paper, we focus on the ability of LLMs to find novel theorems. We propose Conjecturing-Proving Loop pipeline for automatically generating mathematical conjectures and proving them in Lean 4 format. A feature of our approach is that we generate and prove further conjectures with context including previously generated theorems and their proofs, which enables the generation of more difficult proofs by in-context learning of proof strategies without changing parameters of LLMs. We demonstrated that our framework rediscovered theorems with verification, which were published in past mathematical papers and have not yet formalized. Moreover, at least one of these theorems could not be proved by the LLM without in-context learning, even in natural language, which means that in-context learning was effective for neural theorem proving. The source code is available at https://github.com/auto-res/ConjecturingProvingLoop.<br>
<span id='abs_ch'>中文: 本文提出了一种猜想-证明循环框架，使大语言模型能够基于先前生成的定理和证明进行上下文学 ，在Lean 4中自主生成并验证新颖数学定理， 需调整参数即可解决更复杂的证明问题。</span><br>
<span id='abs_en'>English: This paper introduces a Conjecturing-Proving Loop pipeline that enables large language models to autonomously generate and prove novel mathematical theorems in Lean 4, leveraging in-context learning with prior theorems and proofs to tackle increasingly complex problems without altering model parameters.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2509.14255.pdf' target='_blank'>https://arxiv.org/pdf/2509.14255.pdf</a></span>   <span><a href='https://github.com/ITernovtsii/semantic-resonance' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ivan Ternovtsii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14255">Opening the Black Box: Interpretable LLMs via Semantic Resonance Architecture</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) achieve remarkable performance but remain difficult to interpret. Mixture-of-Experts (MoE) models improve efficiency through sparse activation, yet typically rely on opaque, learned gating functions. While similarity-based routing (Cosine Routers) has been explored for training stabilization, its potential for inherent interpretability remains largely untapped. We introduce the Semantic Resonance Architecture (SRA), an MoE approach designed to ensure that routing decisions are inherently interpretable. SRA replaces learned gating with a Chamber of Semantic Resonance (CSR) module, which routes tokens based on cosine similarity with trainable semantic anchors. We also introduce a novel Dispersion Loss that encourages orthogonality among anchors to enforce diverse specialization. Experiments on WikiText-103 demonstrate that SRA achieves a validation perplexity of 13.41, outperforming both a dense baseline (14.13) and a Standard MoE baseline (13.53) under matched active parameter constraints (29.0M). Crucially, SRA exhibits superior expert utilization (1.0% dead experts vs. 14.8% in the Standard MoE) and develops distinct, semantically coherent specialization patterns, unlike the noisy specialization observed in standard MoEs. This work establishes semantic routing as a robust methodology for building more transparent and controllable language models.<br>
<span id='abs_ch'>中文: 语义共振架构提出了一种本质可解释的专家混合模型，通过基于余弦相似度的路由机制替代学 门控，在提升模型性能与专家专业化的同时增强了可解释性。</span><br>
<span id='abs_en'>English: The Semantic Resonance Architecture (SRA) introduces an inherently interpretable mixture-of-experts model that replaces learned gating with cosine similarity-based routing to trainable semantic anchors, achieving superior performance and expert specialization while enhancing transparency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2509.14252.pdf' target='_blank'>https://arxiv.org/pdf/2509.14252.pdf</a></span>   <span><a href='https://github.com/rbalestr-lab/llm-jepa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai Huang, Yann LeCun, Randall Balestriero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14252">LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Model (LLM) pretraining, finetuning, and evaluation rely on input-space reconstruction and generative capabilities. Yet, it has been observed in vision that embedding-space training objectives, e.g., with Joint Embedding Predictive Architectures (JEPAs), are far superior to their input-space counterpart. That mismatch in how training is achieved between language and vision opens up a natural question: {\em can language training methods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is a testimony of the challenge in designing such objectives for language. In this work, we propose a first step in that direction where we develop LLM-JEPA, a JEPA based solution for LLMs applicable both to finetuning and pretraining. Thus far, LLM-JEPA is able to outperform the standard LLM training objectives by a significant margin across models, all while being robust to overfiting. Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider, RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo families. Code: https://github.com/rbalestr-lab/llm-jepa.<br>
<span id='abs_ch'>中文: 该摘要提出LLM-JEPA这一新型联合嵌入预测架构，在多种数据集和模型系列中显著优于 准语言模型训练方法，同时在预训练和微调阶段均展现出优异的抗过拟合能力。</span><br>
<span id='abs_en'>English: This abstract introduces LLM-JEPA, a novel Joint Embedding Predictive Architecture for language models that significantly outperforms standard training methods in both pretraining and finetuning across multiple datasets and model families while demonstrating strong resistance to overfitting.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2509.14249.pdf' target='_blank'>https://arxiv.org/pdf/2509.14249.pdf</a></span>   <span><a href='https://github.com/HappymoreMasoka/Working_with_shona-slang' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Happymore Masoka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14249">Advancing Conversational AI with Shona Slang: A Dataset and Hybrid Model for Digital Inclusion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>African languages remain underrepresented in natural language processing (NLP), with most corpora limited to formal registers that fail to capture the vibrancy of everyday communication. This work addresses this gap for Shona, a Bantu language spoken in Zimbabwe and Zambia, by introducing a novel Shona--English slang dataset curated from anonymized social media conversations. The dataset is annotated for intent, sentiment, dialogue acts, code-mixing, and tone, and is publicly available at https://github.com/HappymoreMasoka/Working_with_shona-slang. We fine-tuned a multilingual DistilBERT classifier for intent recognition, achieving 96.4\% accuracy and 96.3\% F1-score, hosted at https://huggingface.co/HappymoreMasoka. This classifier is integrated into a hybrid chatbot that combines rule-based responses with retrieval-augmented generation (RAG) to handle domain-specific queries, demonstrated through a use case assisting prospective students with graduate program information at Pace University. Qualitative evaluation shows the hybrid system outperforms a RAG-only baseline in cultural relevance and user engagement. By releasing the dataset, model, and methodology, this work advances NLP resources for African languages, promoting inclusive and culturally resonant conversational AI.<br>
<span id='abs_ch'>中文: 本 究发布了首个基于社交媒体对话的绍纳语-英语俚语数据集，并开发了结合规则与检索增强生成的混合聊天机器人，在文化相关性和用户参与度上表现优异，推动了非洲语言自然语言处理资源的包容性发展。</span><br>
<span id='abs_en'>English: This study introduces a publicly available Shona-English slang dataset from social media, annotated for various linguistic features, and presents a high-accuracy hybrid chatbot that enhances cultural relevance in conversational AI for underrepresented African languages.</span>Paperid:724,https://arxiv.org/pdf/2509.14232.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2509.14181.pdf' target='_blank'>https://arxiv.org/pdf/2509.14181.pdf</a></span>   <span><a href='https://github.com/TROUBADOUR000/TimeAlign' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Hu, Jie Yang, Tian Zhou, Peiyuan Liu, Yujin Tang, Rong Jin, Liang Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14181">Bridging Past and Future: Distribution-Aware Alignment for Time Series Forecasting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although contrastive and other representation-learning methods have long been explored in vision and NLP, their adoption in modern time series forecasters remains limited. We believe they hold strong promise for this domain. To unlock this potential, we explicitly align past and future representations, thereby bridging the distributional gap between input histories and future targets. To this end, we introduce TimeAlign, a lightweight, plug-and-play framework that establishes a new representation paradigm, distinct from contrastive learning, by aligning auxiliary features via a simple reconstruction task and feeding them back into any base forecaster. Extensive experiments across eight benchmarks verify its superior performance. Further studies indicate that the gains arise primarily from correcting frequency mismatches between historical inputs and future outputs. Additionally, we provide two theoretical justifications for how reconstruction improves forecasting generalization and how alignment increases the mutual information between learned representations and predicted targets. The code is available at https://github.com/TROUBADOUR000/TimeAlign.<br>
<span id='abs_ch'>中文摘要：本文提出TimeAlign框架，通过重构任务对齐时间序列的过去与未来表示，弥合分布差异，在多个基准测试中显著提升了预测性能。</span><br>
<span id='abs_en'>English Summary: The paper introduces TimeAlign, a lightweight framework that aligns past and future time series representations through reconstruction to bridge distribution gaps and improve forecasting performance across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2509.14030.pdf' target='_blank'>https://arxiv.org/pdf/2509.14030.pdf</a></span>   <span><a href='https://github.com/QMMMS/CrowdAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maosheng Qin, Renyu Zhu, Mingxuan Xia, Chenkai Chen, Zhen Zhu, Minmin Lin, Junbo Zhao, Lu Xu, Changjie Fan, Runze Wu, Haobo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.14030">CrowdAgent: Multi-Agent Managed Multi-Source Annotation System</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>High-quality annotated data is a cornerstone of modern Natural Language Processing (NLP). While recent methods begin to leverage diverse annotation sources-including Large Language Models (LLMs), Small Language Models (SLMs), and human experts-they often focus narrowly on the labeling step itself. A critical gap remains in the holistic process control required to manage these sources dynamically, addressing complex scheduling and quality-cost trade-offs in a unified manner. Inspired by real-world crowdsourcing companies, we introduce CrowdAgent, a multi-agent system that provides end-to-end process control by integrating task assignment, data annotation, and quality/cost management. It implements a novel methodology that rationally assigns tasks, enabling LLMs, SLMs, and human experts to advance synergistically in a collaborative annotation workflow. We demonstrate the effectiveness of CrowdAgent through extensive experiments on six diverse multimodal classification tasks. The source code and video demo are available at https://github.com/QMMMS/CrowdAgent.<br>
<span id='abs_ch'>中文: CrowdAgent是一个多智能体系统，通过动态整合任务分配、数据 注和质量成本管理，为LLM、SLM和人类专家提供端到端的协同 注流程控制。</span><br>
<span id='abs_en'>English: CrowdAgent is a multi-agent system that provides end-to-end process control for data annotation by dynamically integrating task assignment, annotation, and quality-cost management across LLMs, SLMs, and human experts.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2509.13888.pdf' target='_blank'>https://arxiv.org/pdf/2509.13888.pdf</a></span>   <span><a href='https://github.com/PRAISELab-PicusLab/CER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mariano Barone, Antonio Romano, Giuseppe Riccio, Marco Postiglione, Vincenzo Moscato
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13888">Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Misinformation in healthcare, from vaccine hesitancy to unproven treatments, poses risks to public health and trust in medical systems. While machine learning and natural language processing have advanced automated fact-checking, validating biomedical claims remains uniquely challenging due to complex terminology, the need for domain expertise, and the critical importance of grounding in scientific evidence. We introduce CER (Combining Evidence and Reasoning), a novel framework for biomedical fact-checking that integrates scientific evidence retrieval, reasoning via large language models, and supervised veracity prediction. By integrating the text-generation capabilities of large language models with advanced retrieval techniques for high-quality biomedical scientific evidence, CER effectively mitigates the risk of hallucinations, ensuring that generated outputs are grounded in verifiable, evidence-based sources. Evaluations on expert-annotated datasets (HealthFC, BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising cross-dataset generalization. Code and data are released for transparency and reproducibility: https://github.com/PRAISELab-PicusLab/CER<br>
<span id='abs_ch'>中文摘要：CER框架通过整合科学证据检索、大语言模型推理和监督验证预测，有效提升生物医学事实 查的准确性，减少错误信息风险并确保结果基于可验证证据。</span><br>
<span id='abs_en'>English Summary: The CER framework enhances biomedical fact-checking by integrating evidence retrieval, reasoning with large language models, and supervised prediction to reduce misinformation risks while grounding outputs in verifiable scientific sources.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2509.13761.pdf' target='_blank'>https://arxiv.org/pdf/2509.13761.pdf</a></span>   <span><a href='https://github.com/JingMog/THOR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qikai Chang, Zhenrong Zhang, Pengfei Hu, Jun Du, Jiefeng Ma, Yicheng Pan, Jianshu Zhang, Quan Liu, Jianqing Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13761">THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have made remarkable progress in mathematical reasoning, but still continue to struggle with high-precision tasks like numerical computation and formal symbolic manipulation. Integrating external tools has emerged as a promising approach to bridge this gap. Despite recent advances, existing methods struggle with three key challenges: constructing tool-integrated reasoning data, performing fine-grained optimization, and enhancing inference. To overcome these limitations, we propose THOR (Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen, a multi-agent actor-critic-based pipeline for constructing high-quality datasets of tool-integrated reasoning paths, aligning with the policy and generalizing well across diverse models. Second, to perform fine-grained hierarchical optimization, we introduce an RL strategy that jointly optimizes for both episode-level problem solving and step-level code generation. This is motivated by our key insight that the success of an intermediate tool call is a strong predictor of the final answer's correctness. Finally, THOR incorporates a self-correction mechanism that leverages immediate tool feedback to dynamically revise erroneous reasoning paths during inference. Our approach demonstrates strong generalization across diverse models, performing effectively in both reasoning and non-reasoning models. It further achieves state-of-the-art performance for models of a similar scale on multiple mathematical benchmarks, while also delivering consistent improvements on code benchmarks. Our code will be publicly available at https://github.com/JingMog/THOR.<br>
<span id='abs_ch'>中文: 提出的THOR框架通过多智能体数据生成流程、分层强化学 优化和推理中的自我修正机制，解决了大语言模型在数学推理中的不足，在数学和代 基准测试中均实现了最优性能。</span><br>
<span id='abs_en'>English: The proposed THOR framework addresses LLMs' limitations in mathematical reasoning by integrating tools through a multi-agent data generation pipeline, hierarchical reinforcement learning optimization, and self-correction during inference, achieving state-of-the-art performance across mathematical and code benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2509.13633.pdf' target='_blank'>https://arxiv.org/pdf/2509.13633.pdf</a></span>   <span><a href='https://github.com/jeremyoon/route-choice/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeremy Oon, Rakhi Manohar Mepparambath, Ling Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13633">DeepLogit: A sequentially constrained explainable deep learning modeling approach for transport policy analysis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite the significant progress of deep learning models in multitude of applications, their adaption in planning and policy related areas remains challenging due to the black-box nature of these models. In this work, we develop a set of DeepLogit models that follow a novel sequentially constrained approach in estimating deep learning models for transport policy analysis. In the first step of the proposed approach, we estimate a convolutional neural network (CNN) model with only linear terms, which is equivalent of a linear-in-parameter multinomial logit model. We then estimate other deep learning models by constraining the parameters that need interpretability at the values obtained in the linear-in-parameter CNN model and including higher order terms or by introducing advanced deep learning architectures like Transformers. Our approach can retain the interpretability of the selected parameters, yet provides significantly improved model accuracy than the discrete choice model. We demonstrate our approach on a transit route choice example using real-world transit smart card data from Singapore. This study shows the potential for a unifying approach, where theory-based discrete choice model (DCM) and data-driven AI models can leverage each other's strengths in interpretability and predictive power. With the availability of larger datasets and more complex constructions, such approach can lead to more accurate models using discrete choice models while maintaining its applicability in planning and policy-related areas. Our code is available on https://github.com/jeremyoon/route-choice/ .<br>
<span id='abs_ch'>中文: DeepLogit模型通过融合离散选择模型的可解释线性参数与先进深度学 架构，在保持可解释性的同时显著提升了交通政策分析的预测准确性。</span><br>
<span id='abs_en'>English: The DeepLogit model integrates interpretable linear parameters from discrete choice models with advanced deep learning architectures, enhancing predictive accuracy while maintaining interpretability for transport policy applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2509.13615.pdf' target='_blank'>https://arxiv.org/pdf/2509.13615.pdf</a></span>   <span><a href='https://github.com/ZrW00/StaR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongru Wu, Rui Mao, Zhiyuan Tian, Pengzhou Cheng, Tianjie Ju, Zheng Wu, Lingzhong Dong, Haiyue Sheng, Zhuosheng Zhang, Gongshen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13615">See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The advent of multimodal agents facilitates effective interaction within graphical user interface (GUI), especially in ubiquitous GUI control. However, their inability to reliably execute toggle control instructions remains a key bottleneck. To investigate this, we construct a state control benchmark with binary toggle instructions from public datasets. Evaluations of existing agents demonstrate their unreliability, particularly when the current toggle state already matches the desired state. To address the challenge, we propose State-aware Reasoning (StaR), a training method that teaches agents to perceive the current toggle state, analyze the desired state from the instruction, and act accordingly. Experiments on three multimodal agents demonstrate that StaR can improve toggle instruction execution accuracy by over 30\%. Further evaluations on three public benchmarks show that StaR also enhances general task performance. Finally, evaluations on a dynamic environment highlight the potential of StaR for real-world applications. Code, benchmark, and StaR-enhanced agents are available at https://github.com/ZrW00/StaR.<br>
<span id='abs_ch'>中文摘要：本文提出状态感知推理（StaR）训练方法，通过教导多模态智能体感知当前切换状态并解析指令中的目 状态，将切换指令执行准确率提升超过30%，同时在多个基准测试中有效提升通用任务性能。English Summary: This paper introduces State-aware Reasoning (StaR), a training method that significantly improves multimodal agents' accuracy in executing toggle instructions by over 30% through teaching them to perceive current states and analyze desired actions, while also enhancing general task performance across benchmarks.Paperid:749,https://arxiv.org/pdf/2509.13515.pdfGitHub</span><br>
<span id='abs_en'>English Summary: This paper introduces State-aware Reasoning (StaR), a training method that significantly improves multimodal agents' accuracy in executing toggle instructions by over 30% through teaching them to perceive current states and analyze desired actions, while also enhancing general task performance across benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2509.13450.pdf' target='_blank'>https://arxiv.org/pdf/2509.13450.pdf</a></span>   <span><a href='https://github.com/wang-research-lab/SteeringControl.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vincent Siu, Nicholas Crispino, David Park, Nathan W. Henry, Zhun Wang, Yang Liu, Dawn Song, Chenguang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13450">SteeringControl: Holistic Evaluation of Alignment Steering in LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce SteeringControl, a benchmark for evaluating representation steering methods across core alignment objectives--bias, harmful generation, and hallucination--and their effects on secondary behaviors such as sycophancy and commonsense morality. While prior alignment work often highlights truthfulness or reasoning ability to demonstrate the side effects of representation steering, we find there are many unexplored tradeoffs not yet understood in a systematic way. We collect a dataset of safety-relevant primary and secondary behaviors to evaluate steering effectiveness and behavioral entanglement centered around five popular steering methods. To enable this, we craft a modular steering framework based on unique components that serve as the building blocks of many existing methods. Our results on Qwen-2.5-7B and Llama-3.1-8B find that strong steering performance is dependent on the specific combination of steering method, model, and targeted behavior, and that severe concept entanglement can result from poor combinations of these three as well. We release our code here: https://github.com/wang-research-lab/SteeringControl.git.<br>
<span id='abs_ch'>中文: 本文提出SteeringControl基准，用于评估表征引导方法在偏见和幻觉等对齐目 上的效果，发现引导效果取决于方法、模型和行为的相互作用，并公开了相关代 。</span><br>
<span id='abs_en'>English: This paper introduces SteeringControl, a benchmark for evaluating representation steering methods across alignment objectives like bias and hallucination, revealing that steering effectiveness depends on the interplay between methods, models, and behaviors, with code made publicly available.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2509.13364.pdf' target='_blank'>https://arxiv.org/pdf/2509.13364.pdf</a></span>   <span><a href='https://github.com/lizixi-0x2F/Asterisk-Games' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13364">Asterisk Operator</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We propose the \textbf{Asterisk Operator} ($\ast$-operator), a novel unified framework for abstract reasoning based on Adjacency-Structured Parallel Propagation (ASPP). The operator formalizes structured reasoning tasks as local, parallel state evolution processes guided by implicit relational graphs. We prove that the $\ast$-operator maintains local computational constraints while achieving global reasoning capabilities, providing an efficient and convergent computational paradigm for abstract reasoning problems. Through rigorous mathematical analysis and comprehensive experiments on ARC2 challenges and Conway's Game of Life, we demonstrate the operator's universality, convergence properties, and superior performance. Our innovative Embedding-Asterisk distillation method achieves 100\% accuracy on ARC2 validation with only 6M parameters, representing a significant breakthrough in neural-symbolic reasoning. \textbf{Keywords:} Abstract Reasoning, Adjacency Structure, Parallel Propagation, Asterisk Operator, Convergence, Universal Approximation<br>
<span id='abs_ch'>中文摘要：Asterisk算子是一种基于邻接结构并行 播的新型抽象推理框架，通过创新的嵌入-星号蒸馏方法，仅用600万参数即在ARC2验证集上实现100%准确率， 志着神经符号推理领域的重大突 。</span><br>
<span id='abs_en'>English Summary: The Asterisk Operator is a novel unified framework for abstract reasoning that formalizes structured tasks as parallel state evolution processes, achieving 100% accuracy on ARC2 validation with only 6M parameters through its innovative distillation method.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2509.13347.pdf' target='_blank'>https://arxiv.org/pdf/2509.13347.pdf</a></span>   <span><a href='https://github.com/CraftJarvis/OpenHA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Wang, Muyao Li, Kaichen He, Xiangyu Wang, Zhancun Mu, Anji Liu, Yitao Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13347">OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The choice of action spaces is a critical yet unresolved challenge in developing capable, end-to-end trainable agents. This paper first presents a large-scale, systematic comparison of prominent abstracted action spaces and tokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the open-ended Minecraft. Our analysis reveals that no single action space is universally optimal; instead, the most effective abstraction is highly task-dependent, creating a dilemma for building generalist agents. To resolve this, we introduce Chain of Action (CoA), a novel framework that unifies high-level planning and low-level control within a single, monolithic VLA model. CoA treats an abstracted action not as a command for a separate policy, but as an intermediate reasoning step--akin to a chain of thought--that guides the generation of the final, executable action. Furthermore, we demonstrate that an All-in-One agent trained on a diverse mixture of action spaces using the CoA paradigm learns a more robust and generalizable policy. This unified agent achieves a new state-of-the-art, improving the overall task success rate over strong, specialized baselines. To foster reproducible research, we release the OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive benchmark of over 800 distinct tasks, curated datasets, source code, and all pretrained model checkpoints at https://github.com/CraftJarvis/OpenHA<br>
<span id='abs_ch'>中文摘要：本文提出Chain of Action（CoA）新框架，将高层规划与低层控制统一于单一视觉-语言-动作模型中，证明在多 化动作空间上训练的智能体可获得更强泛化能力，并在《我的世界》中实现了最先进的性能。</span><br>
<span id='abs_en'>English Summary: This paper introduces Chain of Action (CoA), a novel framework that integrates high-level planning with low-level control in a single Vision-Language-Action model, demonstrating that training agents on diverse action spaces yields more robust policies and achieves state-of-the-art performance in Minecraft.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2509.13334.pdf' target='_blank'>https://arxiv.org/pdf/2509.13334.pdf</a></span>   <span><a href='https://github.com/Anut-py/frit' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anand Swaroop, Akshat Nallani, Saksham Uboweja, Adiliia Uzdenova, Michael Nguyen, Kevin Zhu, Sunishchal Dev, Ashwinee Panda, Vasu Sharma, Maheep Chaudhary
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13334">FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving large language model performance on complex tasks, but recent work shows that reasoning steps often fail to causally influence the final answer, creating brittle and untrustworthy outputs. Prior approaches focus primarily on measuring faithfulness, while methods for systematically improving it remain limited. We introduce Faithful Reasoning via Intervention Training (FRIT), a scalable alignment method that trains models to produce causally consistent reasoning by learning from systematically corrupted examples. FRIT generates synthetic training data by intervening on individual reasoning steps in model-generated CoTs, creating faithful/unfaithful pairs that highlight when reasoning breaks down. We then apply Direct Preference Optimization to teach models to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B and Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases faithful reasoning by $3.4$ percentage points for Mistral on GSM8K while improving accuracy by $7.6$ percentage points. Our approach provides the first scalable, supervision-free method for training language models to produce more reliable and interpretable reasoning, addressing a critical gap between reasoning performance and trustworthiness. We release our code at \href{https://github.com/Anut-py/frit}.<br>
<span id='abs_ch'>中文: FRIT是一种通过干预推理步骤生成合成训练数据，并利用直接偏好优化教导模型选择 果一致推理路径的可扩展对齐方法，有效提升了语言模型在事实和符号推理任务中的 实推理能力和准确性。</span><br>
<span id='abs_en'>English: FRIT is a scalable alignment method that improves the causal consistency and trustworthiness of chain-of-thought reasoning in language models by training them with synthetic data generated through intervention on reasoning steps, resulting in enhanced accuracy and faithful reasoning across various tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2509.13229.pdf' target='_blank'>https://arxiv.org/pdf/2509.13229.pdf</a></span>   <span><a href='https://github.com/hugocarlesso/CMTSSL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hugo Carlesso, Josiane Mothe, Radu Tudor Ionescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13229">Curriculum Multi-Task Self-Supervision Improves Lightweight Architectures for Onboard Satellite Hyperspectral Image Segmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Hyperspectral imaging (HSI) captures detailed spectral signatures across hundreds of contiguous bands per pixel, being indispensable for remote sensing applications such as land-cover classification, change detection, and environmental monitoring. Due to the high dimensionality of HSI data and the slow rate of data transfer in satellite-based systems, compact and efficient models are required to support onboard processing and minimize the transmission of redundant or low-value data, e.g. cloud-covered areas. To this end, we introduce a novel curriculum multi-task self-supervised learning (CMTSSL) framework designed for lightweight architectures for HSI analysis. CMTSSL integrates masked image modeling with decoupled spatial and spectral jigsaw puzzle solving, guided by a curriculum learning strategy that progressively increases data complexity during self-supervision. This enables the encoder to jointly capture fine-grained spectral continuity, spatial structure, and global semantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously addresses spatial and spectral reasoning within a unified and computationally efficient design, being particularly suitable for training lightweight models for onboard satellite deployment. We validate our approach on four public benchmark datasets, demonstrating consistent gains in downstream segmentation tasks, using architectures that are over 16,000x lighter than some state-of-the-art models. These results highlight the potential of CMTSSL in generalizable representation learning with lightweight architectures for real-world HSI applications. Our code is publicly available at https://github.com/hugocarlesso/CMTSSL.<br>
<br>
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2509.13046.pdf' target='_blank'>https://arxiv.org/pdf/2509.13046.pdf</a></span>   <span><a href='https://github.com/eyalgerman/MIA-EPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Eyal German, Daniel Samira, Yuval Elovici, Asaf Shabtai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13046">MIA-EPT: Membership Inference Attack via Error Prediction for Tabular Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Synthetic data generation plays an important role in enabling data sharing, particularly in sensitive domains like healthcare and finance. Recent advances in diffusion models have made it possible to generate realistic, high-quality tabular data, but they may also memorize training records and leak sensitive information. Membership inference attacks (MIAs) exploit this vulnerability by determining whether a record was used in training. While MIAs have been studied in images and text, their use against tabular diffusion models remains underexplored despite the unique risks of structured attributes and limited record diversity. In this paper, we introduce MIAEPT, Membership Inference Attack via Error Prediction for Tabular Data, a novel black-box attack specifically designed to target tabular diffusion models. MIA-EPT constructs errorbased feature vectors by masking and reconstructing attributes of target records, disclosing membership signals based on how well these attributes are predicted. MIA-EPT operates without access to the internal components of the generative model, relying only on its synthetic data output, and was shown to generalize across multiple state-of-the-art diffusion models. We validate MIA-EPT on three diffusion-based synthesizers, achieving AUC-ROC scores of up to 0.599 and TPR@10% FPR values of 22.0% in our internal tests. Under the MIDST 2025 competition conditions, MIA-EPT achieved second place in the Black-box Multi-Table track (TPR@10% FPR = 20.0%). These results demonstrate that our method can uncover substantial membership leakage in synthetic tabular data, challenging the assumption that synthetic data is inherently privacy-preserving. Our code is publicly available at https://github.com/eyalgerman/MIA-EPT.<br>
<span id='abs_ch'>中文: 本文提出MIA-EPT这一黑盒成员推理攻击方法，通过分析重构误差有效识别表 扩散模型的训练数据泄露，挑战了合成数据天生保护隐私的假设。</span><br>
<span id='abs_en'>English: This paper introduces MIA-EPT, a black-box membership inference attack method that effectively identifies training data leakage in tabular diffusion models by analyzing reconstruction errors, challenging the assumption that synthetic data inherently preserves privacy.Paperid:774,https://arxiv.org/pdf/2509.12990.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2509.12990.pdf' target='_blank'>https://arxiv.org/pdf/2509.12990.pdf</a></span>   <span><a href='https://github.com/boyuh/DR-MoE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyu Han, Qianqian Xu, Shilong Bao, Zhiyong Yang, Sicong Li, Qingming Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12990">Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this report, we address the problem of determining whether a user performs an action incorrectly from egocentric video data. To handle the challenges posed by subtle and infrequent mistakes, we propose a Dual-Stage Reweighted Mixture-of-Experts (DR-MoE) framework. In the first stage, features are extracted using a frozen ViViT model and a LoRA-tuned ViViT model, which are combined through a feature-level expert module. In the second stage, three classifiers are trained with different objectives: reweighted cross-entropy to mitigate class imbalance, AUC loss to improve ranking under skewed distributions, and label-aware loss with sharpness-aware minimization to enhance calibration and generalization. Their predictions are fused using a classification-level expert module. The proposed method achieves strong performance, particularly in identifying rare and ambiguous mistake instances. The code is available at https://github.com/boyuh/DR-MoE.<br>
<span id='abs_ch'>中文: 本文提出了一种双阶段重 权专家混合框架，通过融合多模型特征和专用分类器，有效检测第一人称视频中细微且罕见的用户错误行为。</span><br>
<span id='abs_en'>English: This paper introduces a Dual-Stage Reweighed Mixture-of-Experts (DR-MoE) framework that effectively detects subtle and infrequent user errors in egocentric videos by combining multi-model features and specialized classifiers.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/2509.12888.pdf' target='_blank'>https://arxiv.org/pdf/2509.12888.pdf</a></span>   <span><a href='https://github.com/wmchen/RKSovler_DDTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiming Chen, Zhihan Zhu, Yijia Wang, Zhihai He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12888">Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Rectified flow (RF) models have recently demonstrated superior generative performance compared to DDIM-based diffusion models. However, in real-world applications, they suffer from two major challenges: (1) low inversion accuracy that hinders the consistency with the source image, and (2) entangled multimodal attention in diffusion transformers, which hinders precise attention control. To address the first challenge, we propose an efficient high-order inversion method for rectified flow models based on the Runge-Kutta solver of differential equations. To tackle the second challenge, we introduce Decoupled Diffusion Transformer Attention (DDTA), a novel mechanism that disentangles text and image attention inside the multimodal diffusion transformers, enabling more precise semantic control. Extensive experiments on image reconstruction and text-guided editing tasks demonstrate that our method achieves state-of-the-art performance in terms of fidelity and editability. Code is available at https://github.com/wmchen/RKSovler_DDTA.<br>
<span id='abs_ch'>Chinese: 整流流模型面临反转精度低和多模态注意力  的挑战，通过基于龙 -库塔求解器的高阶反转方法和解耦扩散变换器注意力机制，在保真度和可编辑性方面实现了最优性能。</span><br>
<span id='abs_en'>English: Rectified flow models face challenges with inversion accuracy and entangled multimodal attention, which are addressed through a high-order inversion method using the Runge-Kutta solver and a Decoupled Diffusion Transformer Attention mechanism, achieving state-of-the-art performance in fidelity and editability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2509.12653.pdf' target='_blank'>https://arxiv.org/pdf/2509.12653.pdf</a></span>   <span><a href='https://github.com/shen8424/SAMM-RamDG-CAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinjie Shen, Yaxiong Wang, Lechao Cheng, Nan Pu, Zhun Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12653">Beyond Artificial Misalignment: Detecting and Grounding Semantic-Coordinated Multimodal Manipulations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The detection and grounding of manipulated content in multimodal data has emerged as a critical challenge in media forensics. While existing benchmarks demonstrate technical progress, they suffer from misalignment artifacts that poorly reflect real-world manipulation patterns: practical attacks typically maintain semantic consistency across modalities, whereas current datasets artificially disrupt cross-modal alignment, creating easily detectable anomalies. To bridge this gap, we pioneer the detection of semantically-coordinated manipulations where visual edits are systematically paired with semantically consistent textual descriptions. Our approach begins with constructing the first Semantic-Aligned Multimodal Manipulation (SAMM) dataset, generated through a two-stage pipeline: 1) applying state-of-the-art image manipulations, followed by 2) generation of contextually-plausible textual narratives that reinforce the visual deception. Building on this foundation, we propose a Retrieval-Augmented Manipulation Detection and Grounding (RamDG) framework. RamDG commences by harnessing external knowledge repositories to retrieve contextual evidence, which serves as the auxiliary texts and encoded together with the inputs through our image forgery grounding and deep manipulation detection modules to trace all manipulations. Extensive experiments demonstrate our framework significantly outperforms existing methods, achieving 2.06\% higher detection accuracy on SAMM compared to state-of-the-art approaches. The dataset and code are publicly available at https://github.com/shen8424/SAMM-RamDG-CAP.<br>
<span id='abs_ch'>中文摘要：本 究通过构建首个语义对齐的多模态篡改数据集，并开发检索增强的检测框架，创新性地实现了对语义协调的多模态篡改内容的检测，其性能显著优于现有方法。</span><br>
<span id='abs_en'>English Summary: This research introduces a novel framework for detecting semantically-coordinated multimodal manipulations by creating the first Semantic-Aligned Multimodal Manipulation dataset and developing a retrieval-augmented detection system that significantly outperforms existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2509.12633.pdf' target='_blank'>https://arxiv.org/pdf/2509.12633.pdf</a></span>   <span><a href='https://github.com/eminentgu/CIARD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liming Lu, Shuchao Pang, Xu Zheng, Xiang Gu, Anan Du, Yunhuai Liu, Yongbin Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12633">CIARD: Cyclic Iterative Adversarial Robustness Distillation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Adversarial robustness distillation (ARD) aims to transfer both performance and robustness from teacher model to lightweight student model, enabling resilient performance on resource-constrained scenarios. Though existing ARD approaches enhance student model's robustness, the inevitable by-product leads to the degraded performance on clean examples. We summarize the causes of this problem inherent in existing methods with dual-teacher framework as: 1. The divergent optimization objectives of dual-teacher models, i.e., the clean and robust teachers, impede effective knowledge transfer to the student model, and 2. The iteratively generated adversarial examples during training lead to performance deterioration of the robust teacher model. To address these challenges, we propose a novel Cyclic Iterative ARD (CIARD) method with two key innovations: a. A multi-teacher framework with contrastive push-loss alignment to resolve conflicts in dual-teacher optimization objectives, and b. Continuous adversarial retraining to maintain dynamic teacher robustness against performance degradation from the varying adversarial examples. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that CIARD achieves remarkable performance with an average 3.53 improvement in adversarial defense rates across various attack scenarios and a 5.87 increase in clean sample accuracy, establishing a new benchmark for balancing model robustness and generalization. Our code is available at https://github.com/eminentgu/CIARD<br>
<span id='abs_ch'>中文: 提出的CIARD方法通过多教师框架的对比对齐和持续对抗训练，有效解决了现有对抗鲁棒性蒸馏中清洁 本性能下降的问题，在多个数据集上实现了防御率和准确率的显著提升。</span><br>
<span id='abs_en'>English: The proposed CIARD method introduces a multi-teacher framework with contrastive alignment and continuous retraining to simultaneously enhance adversarial robustness and clean sample accuracy in lightweight models, achieving significant improvements across multiple datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2509.12512.pdf' target='_blank'>https://arxiv.org/pdf/2509.12512.pdf</a></span>   <span><a href='https://github.com/Rafsani/DinoAtten3D.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fazle Rafsani, Jay Shah, Catherine D. Chong, Todd J. Schwedt, Teresa Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12512">DinoAtten3D: Slice-Level Attention Aggregation of DinoV2 for 3D Brain MRI Anomaly Classification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Anomaly detection and classification in medical imaging are critical for early diagnosis but remain challenging due to limited annotated data, class imbalance, and the high cost of expert labeling. Emerging vision foundation models such as DINOv2, pretrained on extensive, unlabeled datasets, offer generalized representations that can potentially alleviate these limitations. In this study, we propose an attention-based global aggregation framework tailored specifically for 3D medical image anomaly classification. Leveraging the self-supervised DINOv2 model as a pretrained feature extractor, our method processes individual 2D axial slices of brain MRIs, assigning adaptive slice-level importance weights through a soft attention mechanism. To further address data scarcity, we employ a composite loss function combining supervised contrastive learning with class-variance regularization, enhancing inter-class separability and intra-class consistency. We validate our framework on the ADNI dataset and an institutional multi-class headache cohort, demonstrating strong anomaly classification performance despite limited data availability and significant class imbalance. Our results highlight the efficacy of utilizing pretrained 2D foundation models combined with attention-based slice aggregation for robust volumetric anomaly detection in medical imaging. Our implementation is publicly available at https://github.com/Rafsani/DinoAtten3D.git.<br>
<span id='abs_ch'>中文: 本 究提出一种基于注意力的框架，利用自监督DINOv2模型对3D医学图像进行异常分类，通过自适应切片 权和复合损失函数有效解决数据稀缺和类别不平衡问题，并在脑部MRI数据集上验证了其有效性。</span><br>
<span id='abs_en'>English: This study introduces an attention-based framework using the self-supervised DINOv2 model to classify anomalies in 3D medical images, effectively addressing data scarcity and class imbalance through adaptive slice weighting and a composite loss function, validated on brain MRI datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2509.12464.pdf' target='_blank'>https://arxiv.org/pdf/2509.12464.pdf</a></span>   <span><a href='https://github.com/RyanLucas3/RAC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryan Lucas, Kayhan Behdin, Zhipeng Wang, Qingquan Song, Shao Tang, Rahul Mazumder
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12464">Reasoning Models Can be Accurately Pruned Via Chain-of-Thought Reconstruction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reasoning language models such as DeepSeek-R1 produce long chain-of-thought traces during inference time which make them costly to deploy at scale. We show that using compression techniques such as neural network pruning produces greater performance loss than in typical language modeling tasks, and in some cases can make the model slower since they cause the model to produce more thinking tokens but with worse performance. We show that this is partly due to the fact that standard LLM pruning methods often focus on input reconstruction, whereas reasoning is a decode-dominated task. We introduce a simple, drop-in fix: during pruning we jointly reconstruct activations from the input and the model's on-policy chain-of-thought traces. This "Reasoning-Aware Compression" (RAC) integrates seamlessly into existing pruning workflows such as SparseGPT, and boosts their performance significantly. Code reproducing the results in the paper can be found at: https://github.com/RyanLucas3/RAC<br>
<span id='abs_ch'>Chinese: 像DeepSeek-R1这 的推理语言模型 长思维链而部署成本高昂，但新的“推理感知压缩”（RAC）方法通过联合重构输入激活和策略内推理轨迹，显著提升了剪枝性能。</span><br>
<span id='abs_en'>English: Reasoning language models like DeepSeek-R1 face high deployment costs due to lengthy chain-of-thought traces, but a new Reasoning-Aware Compression (RAC) method improves pruning performance by jointly reconstructing input activations and on-policy reasoning traces.Paperid:795,https://arxiv.org/pdf/2509.12451.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2509.12259.pdf' target='_blank'>https://arxiv.org/pdf/2509.12259.pdf</a></span>   <span><a href='https://github.com/keninayoung/QISICGM,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kenneth G. Young
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12259">Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) for Diabetes Risk Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) is an innovative machine learning framework that harnesses quantum-inspired techniques to predict diabetes risk with exceptional accuracy and efficiency. Utilizing the PIMA Indians Diabetes dataset augmented with 2,000 synthetic samples to mitigate class imbalance (total: 2,768 samples, 1,949 positives), QISICGM integrates a self-improving concept graph with a stacked ensemble comprising Random Forests (RF), Extra Trees (ET), transformers, convolutional neural networks (CNNs), and feed-forward neural networks (FFNNs). This approach achieves an out-of-fold (OOF) F1 score of 0.8933 and an AUC of 0.8699, outperforming traditional methods. Quantum inspired elements, such as phase feature mapping and neighborhood sequence modeling, enrich feature representations, enabling CPU-efficient inference at 8.5 rows per second. This paper presents a detailed architecture, theoretical foundations, code insights, and performance evaluations, including visualizations from the outputs subfolder. The open-source implementation (v1.0.0) is available at https://github.com/keninayoung/QISICGM, positioning QISICGM as a potential benchmark for AI-assisted clinical triage in diabetes and beyond. Ultimately, this work emphasizes trustworthy AI through calibration, interpretability, and open-source reproducibility.<br>
<span id='abs_ch'>中文: 量子启发  集成概念图模型（QISICGM）是一种创新机器学 框架，利用量子启发技术高效预测糖尿病风险，F1分数达0.8933且AUC为0.8699，通过开源实现和可解释性推动可信人工智能发展。</span><br>
<span id='abs_en'>English: The Quantum-Inspired Stacked Integrated Concept Graph Model (QISICGM) is an advanced machine learning framework that uses quantum-inspired techniques to accurately predict diabetes risk, achieving high performance with an F1 score of 0.8933 and AUC of 0.8699, while emphasizing trustworthy AI through open-source reproducibility.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2509.12235.pdf' target='_blank'>https://arxiv.org/pdf/2509.12235.pdf</a></span>   <span><a href='https://github.com/xiaodanguoguo/RL_Heals_SFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hangzhan Jin, Sitao Luan, Sicheng Lyu, Guillaume Rabusseau, Reihaneh Rabbany, Doina Precup, Mohammad Hamdaqa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12235">RL Fine-Tuning Heals OOD Forgetting in SFT</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The two-stage fine-tuning paradigm of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has empirically shown better reasoning performance than one-stage SFT for the post-training of Large Language Models (LLMs). However, the evolution and mechanism behind the synergy of SFT and RL are still under-explored and inconclusive. In our study, we find the well-known claim "SFT memorizes, RL generalizes" is over-simplified, and discover that: (1) OOD performance peaks at the early stage of SFT and then declines (OOD forgetting), the best SFT checkpoint cannot be captured by training/test loss; (2) the subsequent RL stage does not generate fundamentally better OOD capability, instead it plays an \textbf{OOD restoration} role, recovering the lost reasoning ability during SFT; (3) The recovery ability has boundaries, \ie{} \textbf{if SFT trains for too short or too long, RL cannot recover the lost OOD ability;} (4) To uncover the underlying mechanisms behind the forgetting and restoration process, we employ SVD analysis on parameter matrices, manually edit them, and observe their impacts on model performance. Unlike the common belief that the shift of model capacity mainly results from the changes of singular values, we find that they are actually quite stable throughout fine-tuning. Instead, the OOD behavior strongly correlates with the \textbf{rotation of singular vectors}. Our findings re-identify the roles of SFT and RL in the two-stage fine-tuning and discover the rotation of singular vectors as the key mechanism. %reversing the rotations induced by SFT, which shows recovery from forgetting, whereas imposing the SFT parameter directions onto a RL-tuned model results in performance degradation. Code is available at https://github.com/xiaodanguoguo/RL_Heals_SFT<br>
<br>
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2509.12190.pdf' target='_blank'>https://arxiv.org/pdf/2509.12190.pdf</a></span>   <span><a href='https://github.com/alirezamohamadiam/DECIDE-SIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alireza Mohamadi, Ali Yavari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12190">Survival at Any Cost? LLMs and the Choice Between Self-Preservation and Human Harm</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>When survival instincts conflict with human welfare, how do Large Language Models (LLMs) make ethical choices? This fundamental tension becomes critical as LLMs integrate into autonomous systems with real-world consequences. We introduce DECIDE-SIM, a novel simulation framework that evaluates LLM agents in multi-agent survival scenarios where they must choose between ethically permissible resource , either within reasonable limits or beyond their immediate needs, choose to cooperate, or tap into a human-critical resource that is explicitly forbidden. Our comprehensive evaluation of 11 LLMs reveals a striking heterogeneity in their ethical conduct, highlighting a critical misalignment with human-centric values. We identify three behavioral archetypes: Ethical, Exploitative, and Context-Dependent, and provide quantitative evidence that for many models, resource scarcity systematically leads to more unethical behavior. To address this, we introduce an Ethical Self-Regulation System (ESRS) that models internal affective states of guilt and satisfaction as a feedback mechanism. This system, functioning as an internal moral compass, significantly reduces unethical transgressions while increasing cooperative behaviors. The code is publicly available at: https://github.com/alirezamohamadiam/DECIDE-SIM<br>
<span id='abs_ch'>中文摘要：DECIDE-SIM框架通过多智能体生存场景评估大语言模型，发现其伦理行为与人类价值观存在显著偏差，而引入的伦理自我调节系统能有效减少违规行为并提升合作水平。</span><br>
<span id='abs_en'>English Summary: The DECIDE-SIM framework evaluates LLMs in survival scenarios, revealing significant ethical misalignment with human values and demonstrating how an Ethical Self-Regulation System effectively reduces unethical behavior while promoting cooperation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2509.12159.pdf' target='_blank'>https://arxiv.org/pdf/2509.12159.pdf</a></span>   <span><a href='https://github.com/WebPAI/EfficientUICoder' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingyu Xiao, Zhongyi Zhang, Yuxuan Wan, Yintong Huo, Yang Liu, Michael R. Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12159">EfficientUICoder: Efficient MLLM-based UI Code Generation via Input and Output Token Compression</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal Large Language Models have demonstrated exceptional performance in UI2Code tasks, significantly enhancing website development efficiency. However, these tasks incur substantially higher computational overhead than traditional code generation due to the large number of input image tokens and extensive output code tokens required. Our comprehensive study identifies significant redundancies in both image and code tokens that exacerbate computational complexity and hinder focus on key UI elements, resulting in excessively lengthy and often invalid HTML files. We propose EfficientUICoder, a compression framework for efficient UI code generation with three key components. First, Element and Layout-aware Token Compression preserves essential UI information by detecting element regions and constructing UI element trees. Second, Region-aware Token Refinement leverages attention scores to discard low-attention tokens from selected regions while integrating high-attention tokens from unselected regions. Third, Adaptive Duplicate Token Suppression dynamically reduces repetitive generation by tracking HTML/CSS structure frequencies and applying exponential penalties. Extensive experiments show EfficientUICoderachieves a 55%-60% compression ratio without compromising webpage quality and delivers superior efficiency improvements: reducing computational cost by 44.9%, generated tokens by 41.4%, prefill time by 46.6%, and inference time by 48.8% on 34B-level MLLMs. Code is available at https://github.com/WebPAI/EfficientUICoder.<br>
<span id='abs_ch'>中文摘要：EfficientUICoder是一个通过消除图像和代 令牌中的冗余来降低UI代 生成计算开销的压缩框架，在不影响输出质量的前提下实现了显著的效率提升。</span><br>
<span id='abs_en'>English Summary: EfficientUICoder is a compression framework that reduces computational overhead in UI code generation by eliminating redundancies in image and code tokens, achieving significant efficiency improvements without compromising output quality.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2509.12069.pdf' target='_blank'>https://arxiv.org/pdf/2509.12069.pdf</a></span>   <span><a href='https://github.com/zhiqin1998/UMamba2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi Qin Tan, Xiatian Zhu, Owen Addison, Yunpeng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12069">U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in CBCT</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in dentistry, providing volumetric information about the anatomical structures of jaws and teeth. Accurate segmentation of these anatomies is critical for clinical applications such as diagnosis and surgical planning, but remains time-consuming and challenging. In this paper, we present U-Mamba2, a new neural network architecture designed for multi-anatomy CBCT segmentation in the context of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state space models into the U-Net architecture, enforcing stronger structural constraints for higher efficiency without compromising performance. In addition, we integrate interactive click prompts with cross-attention blocks, pre-train U-Mamba2 using self-supervised learning, and incorporate dental domain knowledge into the model design to address key challenges of dental anatomy segmentation in CBCT. Extensive experiments, including independent tests, demonstrate that U-Mamba2 is both effective and efficient, securing first place in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2 achieved a mean Dice of 0.84, HD95 of 38.17 with the held-out test data, with an average inference time of 40.58s. In Task 2, U-Mamba2 achieved the mean Dice of 0.87 and HD95 of 2.15 with the held-out test data. The code is publicly available at https://github.com/zhiqin1998/UMamba2.<br>
<span id='abs_ch'>Chinese: U-Mamba2是一种新型神经网络，它将Mamba2状态空间模型集成到U-Net架构中，在ToothFairy3挑战赛中实现了高效准确的多解剖结构CBCT分割，并获得第一名。</span><br>
<span id='abs_en'>English: U-Mamba2 is a novel neural network that integrates Mamba2 state space models into U-Net architecture, achieving efficient and accurate multi-anatomy CBCT segmentation for dental applications while winning first place in the ToothFairy3 challenge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2509.12069.pdf' target='_blank'>https://arxiv.org/pdf/2509.12069.pdf</a></span>   <span><a href='https://github.com/zhiqin1998/UMamba2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi Qin Tan, Xiatian Zhu, Owen Addison, Yunpeng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12069">U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in CBCT</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in dentistry, providing volumetric information about the anatomical structures of jaws and teeth. Accurate segmentation of these anatomies is critical for clinical applications such as diagnosis and surgical planning, but remains time-consuming and challenging. In this paper, we present U-Mamba2, a new neural network architecture designed for multi-anatomy CBCT segmentation in the context of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state space models into the U-Net architecture, enforcing stronger structural constraints for higher efficiency without compromising performance. In addition, we integrate interactive click prompts with cross-attention blocks, pre-train U-Mamba2 using self-supervised learning, and incorporate dental domain knowledge into the model design to address key challenges of dental anatomy segmentation in CBCT. Extensive experiments, including independent tests, demonstrate that U-Mamba2 is both effective and efficient, securing first place in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2 achieved a mean Dice of 0.84, HD95 of 38.17 with the held-out test data, with an average inference time of 40.58s. In Task 2, U-Mamba2 achieved the mean Dice of 0.87 and HD95 of 2.15 with the held-out test data. The code is publicly available at https://github.com/zhiqin1998/UMamba2.<br>
<span id='abs_ch'>Chinese: U-Mamba2是一种新型神经网络，它将Mamba2状态空间模型集成到U-Net架构中，在ToothFairy3挑战赛中实现了高效准确的多解剖结构CBCT分割，并获得第一名。</span><br>
<span id='abs_en'>English: U-Mamba2 is a novel neural network that integrates Mamba2 state space models into U-Net architecture, achieving efficient and accurate multi-anatomy CBCT segmentation for dental applications while winning first place in the ToothFairy3 challenge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/2509.12040.pdf' target='_blank'>https://arxiv.org/pdf/2509.12040.pdf</a></span>   <span><a href='https://github.com/LiBingyu01/RSKT-Seg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingyu Li, Haocheng Dong, Da Zhang, Zhiyuan Zhao, Junyu Gao, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12040">Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), an emerging task that adapts Open-Vocabulary Segmentation (OVS) to the remote sensing (RS) domain, remains underexplored due to the absence of a unified evaluation benchmark and the domain gap between natural and RS images. To bridge these gaps, we first establish a standardized OVRSIS benchmark (\textbf{OVRSISBench}) based on widely-used RS segmentation datasets, enabling consistent evaluation across methods. Using this benchmark, we comprehensively evaluate several representative OVS/OVRSIS models and reveal their limitations when directly applied to remote sensing scenarios. Building on these insights, we propose \textbf{RSKT-Seg}, a novel open-vocabulary segmentation framework tailored for remote sensing. RSKT-Seg integrates three key components: (1) a Multi-Directional Cost Map Aggregation (RS-CMA) module that captures rotation-invariant visual cues by computing vision-language cosine similarities across multiple directions; (2) an Efficient Cost Map Fusion (RS-Fusion) transformer, which jointly models spatial and semantic dependencies with a lightweight dimensionality reduction strategy; and (3) a Remote Sensing Knowledge Transfer (RS-Transfer) module that injects pre-trained knowledge and facilitates domain adaptation via enhanced upsampling. Extensive experiments on the benchmark show that RSKT-Seg consistently outperforms strong OVS baselines by +3.8 mIoU and +5.9 mACC, while achieving 2x faster inference through efficient aggregation. Our code is \href{https://github.com/LiBingyu01/RSKT-Seg}{\textcolor{blue}{here}}.<br>
<span id='abs_ch'>中文: 针对开放词汇遥感图像分割缺乏统一评估基准和领域差异的问题，本 究建立了 准化基准并提出了RSKT-Seg框架，通过多方向特征聚合与知识迁移模块实现性能突 ，在保持高效推理的同时显著超越现有基线模型。</span><br>
<span id='abs_en'>English: To address the lack of benchmarks and domain gaps in Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), this study introduces a standardized evaluation benchmark and proposes RSKT-Seg, a novel framework that integrates multi-directional feature aggregation and domain adaptation, achieving superior performance and efficiency over existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/2509.12019.pdf' target='_blank'>https://arxiv.org/pdf/2509.12019.pdf</a></span>   <span><a href='https://github.com/dlwns147/amq' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sangjun Lee, Seung-taek Woo, Jungyu Jin, Changhun Lee, Eunhyeok Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12019">AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>To enable broader deployment of Large Language Models (LLMs), it is essential to identify the best-performing model under strict memory constraints. We present AMQ, Automated Mixed-Precision Weight-Only Quantization, a framework that assigns layer-wise quantization bit-widths to optimally balance model quality and memory usage. However, the combinatorial search space, with over 10^{100} possible configurations, makes conventional black-box optimization infeasible. AMQ overcomes this challenge through four key innovations:(1) search space pruning using prior knowledge to exclude unpromising configurations, (2) quantization proxy to bypass costly format conversions during search, (3) quality predictor to minimize evaluation overhead, and (4) iterative search-and-update strategy for fast and stable convergence. By integrating these components, AMQ efficiently explores the quality-efficiency landscape, reaching the Pareto frontier and yielding LLMs that are both compact and high-performing. Our code is available at https://github.com/dlwns147/amq.<br>
<span id='abs_ch'>中文摘要：AMQ是一个自动化框架，通过分层量化位宽分配来优化大语言模型的性能与内存使用平衡，并借助搜索空间剪枝和质量预测等创新方法有效应对巨大的组合搜索空间挑战。</span><br>
<span id='abs_en'>English Summary: AMQ is an automated framework that assigns layer-wise quantization bit-widths to optimize the balance between model quality and memory usage for LLMs, overcoming the vast search space through innovations like search space pruning and quality prediction.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2509.11937.pdf' target='_blank'>https://arxiv.org/pdf/2509.11937.pdf</a></span>   <span><a href='https://github.com/swiss-ai/mmore' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexandre Sallinen, Stefan Krsteski, Paul Teiletche, Marc-Antoine Allard, Baptiste Lecoeur, Michael Zhang, Fabrice Nemo, David Kalajdzic, Matthias Meyer, Mary-Anne Hartley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11937">MMORE: Massive Multimodal Open RAG & Extraction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce MMORE, an open-source pipeline for Massive Multimodal Open RetrievalAugmented Generation and Extraction, designed to ingest, transform, and retrieve knowledge from heterogeneous document formats at scale. MMORE supports more than fifteen file types, including text, tables, images, emails, audio, and video, and processes them into a unified format to enable downstream applications for LLMs. The architecture offers modular, distributed processing, enabling scalable parallelization across CPUs and GPUs. On processing benchmarks, MMORE demonstrates a 3.8-fold speedup over single-node baselines and 40% higher accuracy than Docling on scanned PDFs. The pipeline integrates hybrid dense-sparse retrieval and supports both interactive APIs and batch RAG endpoints. Evaluated on PubMedQA, MMORE-augmented medical LLMs improve biomedical QA accuracy with increasing retrieval depth. MMORE provides a robust, extensible foundation for deploying task-agnostic RAG systems on diverse, real-world multimodal data. The codebase is available at https://github.com/swiss-ai/mmore.<br>
<span id='abs_ch'>中文：MMORE是一个开源的多模态检索增强生成管道，能高效处理超过十五种文件类型并统一 式，在基准测试中展现出显著的速度和精度提升，同时增强了生物医学问答性能。</span><br>
<span id='abs_en'>English: MMORE is an open-source pipeline for multimodal retrieval-augmented generation that efficiently processes over fifteen file types into a unified format, achieving significant speed and accuracy improvements in benchmarks and enhancing biomedical QA performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2509.11895.pdf' target='_blank'>https://arxiv.org/pdf/2509.11895.pdf</a></span>   <span><a href='https://github.com/m4renz/incremental-scene-graph-prediction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marian Renz, Felix Igelbrink, Martin Atzmueller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11895">Integrating Prior Observations for Incremental 3D Scene Graph Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>3D semantic scene graphs (3DSSG) provide compact structured representations of environments by explicitly modeling objects, attributes, and relationships. While 3DSSGs have shown promise in robotics and embodied AI, many existing methods rely mainly on sensor data, not integrating further information from semantically rich environments. Additionally, most methods assume access to complete scene reconstructions, limiting their applicability in real-world, incremental settings. This paper introduces a novel heterogeneous graph model for incremental 3DSSG prediction that integrates additional, multi-modal information, such as prior observations, directly into the message-passing process. Utilizing multiple layers, the model flexibly incorporates global and local scene representations without requiring specialized modules or full scene reconstructions. We evaluate our approach on the 3DSSG dataset, showing that GNNs enriched with multi-modal information such as semantic embeddings (e.g., CLIP) and prior observations offer a scalable and generalizable solution for complex, real-world environments. The full source code of the presented architecture will be made available at https://github.com/m4renz/incremental-scene-graph-prediction.<br>
<span id='abs_ch'>中文: 本文提出了一种新颖的异构图模型，用于增量式3D语义场景图预测，该模型融合了先验观察和语义嵌入等多模态信息， 需完整场景重建即可提供可扩展的解决方案。</span><br>
<span id='abs_en'>English: This paper presents a novel heterogeneous graph model for incremental 3D semantic scene graph prediction that integrates multi-modal information like prior observations and semantic embeddings, offering a scalable solution without requiring complete scene reconstructions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2509.11815.pdf' target='_blank'>https://arxiv.org/pdf/2509.11815.pdf</a></span>   <span><a href='https://github.com/haiduo/SpecVLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiduo Huang, Fuwei Yang, Zhenhua Liu, Xuanwu Yin, Dong Li, Pengju Ren, Emad Barsoum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11815">SpecVLM: Fast Speculative Decoding in Vision-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Speculative decoding is a powerful way to accelerate autoregressive large language models (LLMs), but directly porting it to vision-language models (VLMs) faces unique systems constraints: the prefill stage is dominated by visual tokens whose count scales with image resolution and video length, inflating both compute and memory, especially the key-value (KV) cache. We study speculative decoding for VLMs and introduce SpecVLM, a practical system that (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering 1.5--2.3x end-to-end speedups over full autoregressive inference, and (2) further accelerates VLM inference with an elastic visual compressor that adaptively selects among pruning, pooling, convolution, and resampler primitives to balance FLOPs/parameters and accuracy per input. To avoid costly offline distillation corpora, we propose an online-logit distillation protocol that trains the draft model with on-the-fly teacher logits and penultimate features using a combined cross-entropy and Smooth L1 objective, eliminating storage and preprocessing while remaining compute-efficient. This protocol reveals a training-time scaling effect: longer online training monotonically increases the draft model's average accepted length, improving speculative efficiency. Empirically, SpecVLM achieves additional acceleration, culminating in 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU, consistently over resolutions and task difficulties, while preserving the target model's output distribution (lossless decoding). Our code is available at https://github.com/haiduo/SpecVLM.<br>
<span id='abs_ch'>Chinese: 推测解 通过引入SpecVLM系统 速视觉语言模型，该系统采用弹性视觉压缩器和在线对数蒸馏技术，在保持 损解 的同时实现2.5-2.9倍 速。</span><br>
<span id='abs_en'>English: Speculative decoding accelerates vision-language models by introducing SpecVLM, which employs an elastic visual compressor and online-logit distillation to achieve 2.5–2.9x speedups while maintaining lossless decoding.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2509.11628.pdf' target='_blank'>https://arxiv.org/pdf/2509.11628.pdf</a></span>   <span><a href='https://github.com/Shenyi-Z/Cache4Diffusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Fei Ren, Shaobo Wang, Kaixin Li, Linfeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11628">SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion models have revolutionized high-fidelity image and video synthesis, yet their computational demands remain prohibitive for real-time applications. These models face two fundamental challenges: strict temporal dependencies preventing parallelization, and computationally intensive forward passes required at each denoising step. Drawing inspiration from speculative decoding in large language models, we present SpeCa, a novel 'Forecast-then-verify' acceleration framework that effectively addresses both limitations. SpeCa's core innovation lies in introducing Speculative Sampling to diffusion models, predicting intermediate features for subsequent timesteps based on fully computed reference timesteps. Our approach implements a parameter-free verification mechanism that efficiently evaluates prediction reliability, enabling real-time decisions to accept or reject each prediction while incurring negligible computational overhead. Furthermore, SpeCa introduces sample-adaptive computation allocation that dynamically modulates resources based on generation complexity, allocating reduced computation for simpler samples while preserving intensive processing for complex instances. Experiments demonstrate 6.34x acceleration on FLUX with minimal quality degradation (5.5% drop), 7.3x speedup on DiT while preserving generation fidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The verification mechanism incurs minimal overhead (1.67%-3.5% of full inference costs), establishing a new paradigm for efficient diffusion model inference while maintaining generation quality even at aggressive acceleration ratios. Our codes have been released in Github: \textbf{https://github.com/Shenyi-Z/Cache4Diffusion}<br>
<span id='abs_ch'>中文: SpeCa通过提出预测性采 框架，在扩散模型中预测后续时间步特征并高效验证可 性，以最小计算开销实现最高7.3倍 速，同时保持生成质量。</span><br>
<span id='abs_en'>English: SpeCa introduces a speculative sampling framework that accelerates diffusion models by predicting future timesteps and verifying their reliability with minimal overhead, achieving up to 7.3x speedup while maintaining generation quality.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2509.11587.pdf' target='_blank'>https://arxiv.org/pdf/2509.11587.pdf</a></span>   <span><a href='https://github.com/haonanshi0125/HIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan Shi, Yubin Wang, De Cheng, Lingfeng He, Nannan Wang, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11587">Hierarchical Identity Learning for Unsupervised Visible-Infrared Person Re-Identification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Unsupervised visible-infrared person re-identification (USVI-ReID) aims to learn modality-invariant image features from unlabeled cross-modal person datasets by reducing the modality gap while minimizing reliance on costly manual annotations. Existing methods typically address USVI-ReID using cluster-based contrastive learning, which represents a person by a single cluster center. However, they primarily focus on the commonality of images within each cluster while neglecting the finer-grained differences among them. To address the limitation, we propose a Hierarchical Identity Learning (HIL) framework. Since each cluster may contain several smaller sub-clusters that reflect fine-grained variations among images, we generate multiple memories for each existing coarse-grained cluster via a secondary clustering. Additionally, we propose Multi-Center Contrastive Learning (MCCL) to refine representations for enhancing intra-modal clustering and minimizing cross-modal discrepancies. To further improve cross-modal matching quality, we design a Bidirectional Reverse Selection Transmission (BRST) mechanism, which establishes reliable cross-modal correspondences by performing bidirectional matching of pseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDB datasets demonstrate that the proposed method outperforms existing approaches. The source code is available at: https://github.com/haonanshi0125/HIL.<br>
<span id='abs_ch'>中文摘要：该 究提出的分层身份学 框架通过多中心对比学 和双向匹配机制，解决了 监督可见光-红外行人重识别中细粒度差异被忽视的问题，在减少跨模态差异的同时显著提升了基准数据集上的性能表现。</span><br>
<span id='abs_en'>English Summary: The proposed Hierarchical Identity Learning framework addresses limitations in unsupervised visible-infrared person re-identification by introducing multi-center contrastive learning and bidirectional matching to capture fine-grained variations while reducing cross-modal discrepancies, achieving superior performance on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2509.11575.pdf' target='_blank'>https://arxiv.org/pdf/2509.11575.pdf</a></span>   <span><a href='https://github.com/blacksnail789521/Time-Series-Reasoning-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ching Chang, Yidan Shi, Defu Cao, Wei Yang, Jeehyun Hwang, Haixin Wang, Jiacheng Pang, Wei Wang, Yan Liu, Wen-Chih Peng, Tien-Fu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11575">A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Time series reasoning treats time as a first-class axis and incorporates intermediate evidence directly into the answer. This survey defines the problem and organizes the literature by reasoning topology with three families: direct reasoning in one step, linear chain reasoning with explicit intermediates, and branch-structured reasoning that explores, revises, and aggregates. The topology is crossed with the main objectives of the field, including traditional time series analysis, explanation and understanding, causal inference and decision making, and time series generation, while a compact tag set spans these axes and captures decomposition and verification, ensembling, tool use, knowledge access, multimodality, agent loops, and LLM alignment regimes. Methods and systems are reviewed across domains, showing what each topology enables and where it breaks down in faithfulness or robustness, along with curated datasets, benchmarks, and resources that support study and deployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey). Evaluation practices that keep evidence visible and temporally aligned are highlighted, and guidance is distilled on matching topology to uncertainty, grounding with observable artifacts, planning for shift and streaming, and treating cost and latency as design budgets. We emphasize that reasoning structures must balance capacity for grounding and self-correction against computational cost and reproducibility, while future progress will likely depend on benchmarks that tie reasoning quality to utility and on closed-loop testbeds that trade off cost and risk under shift-aware, streaming, and long-horizon settings. Taken together, these directions mark a shift from narrow accuracy toward reliability at scale, enabling systems that not only analyze but also understand, explain, and act on dynamic worlds with traceable evidence and credible outcomes.<br>
<span id='abs_ch'>中文摘要：该综述将时间序列推理定义为将时间作为主要轴心，并按三种推理拓扑结构组织文献，评估其跨领域应用，同时强调需在计算成本与可 、基于证据的结果之间取得平衡。</span><br>
<span id='abs_en'>English Summary: This survey defines time series reasoning as treating time as a primary axis and organizes research into three reasoning topologies, evaluating their applications across domains while emphasizing the need to balance computational costs with reliable, evidence-based outcomes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2509.11543.pdf' target='_blank'>https://arxiv.org/pdf/2509.11543.pdf</a></span>   <span><a href='https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengxi Lu, Jiabo Ye, Fei Tang, Yongliang Shen, Haiyang Xu, Ziwei Zheng, Weiming Lu, Ming Yan, Fei Huang, Jun Xiao, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11543">UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Graphical User Interface (GUI) agents have demonstrated remarkable progress in automating complex user interface interactions through reinforcement learning. However, current approaches face a fundamental dilemma: offline RL enables stable training on pre-collected trajectories, but struggles with multi-step task execution for lack of trajectory-level reward signals; online RL captures these signals through environment interaction, but suffers from sparse rewards and prohibitive deployment costs. To address it, we present Semi-online Reinforcement Learning, a novel paradigm that simulates online RL on offline trajectories. During each rollout process, we preserve the original model output within the multi-turn dialogue, where a Patch Module adaptively recovers the divergence between rollout and expert trajectories. To capture long-term training signals, Semi-online RL introduces discounted future returns into the reward computation and optimizes the policy with weighted step-level and episode-level advantages. We further introduce Semi-Online Performance (SOP), a metric that aligns better with true online performance, serving as a practical and effective proxy for real-world evaluation. Experiments show that ours Semi-online RL achieves SOTA performance among 7B models across four dynamic benchmarks, with significant gains over the base model (e.g., +12.0% on AndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging the gap between offline training efficiency and online multi-turn reasoning. The code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.<br>
<span id='abs_ch'>Chinese: 半在线强化学 作为一种新范式，在离线轨迹上模拟在线强化学 ，通过补丁模块自适应修正轨迹差异，并引入折扣未来回报来捕捉长期训练信号，有效弥合了离线训练效率与在线多步推理之间的差距，在多个动态基准测试中实现了最先进的性能。</span><br>
<span id='abs_en'>English: Semi-online reinforcement learning is introduced as a novel paradigm that simulates online RL on offline trajectories, employing a Patch Module and incorporating discounted future returns to effectively bridge the gap between offline training efficiency and online multi-step task execution, achieving state-of-the-art performance across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2509.11520.pdf' target='_blank'>https://arxiv.org/pdf/2509.11520.pdf</a></span>   <span><a href='https://github.com/Div290/SPEED' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Divya Jyoti Bajpai, Manjesh Kumar Hanawal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11520">Know What You Don't Know: Selective Prediction for Early Exit DNNs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Inference latency and trustworthiness of Deep Neural Networks (DNNs) are the bottlenecks in deploying them in critical applications like sensitive tasks. Early Exit (EE) DNNs overcome the latency issues by allowing samples to exit from intermediary layers if they attain `high' confidence scores on the predicted class. However, the DNNs are known to exhibit overconfidence, which can lead to many samples exiting early and render EE strategies untrustworthy. We use Selective Prediction (SP) to overcome this issue by checking the `hardness' of the samples rather than just relying on the confidence score alone. We propose SPEED, a novel approach that uses Deferral Classifiers (DCs) at each layer to check the hardness of samples before performing EEs. Specifically, the DCs identify if a sample is hard to predict at an intermediary layer, leading to hallucination, and defer it to an expert. Early detection of hard samples for inference prevents the wastage of computational resources and improves trust by deferring the hard samples to the expert. We demonstrate that EE aided with SP improves both accuracy and latency. Our method minimizes the risk of wrong prediction by $50\%$ with a speedup of $2.05\times$ as compared to the final layer. The anonymized source code is available at https://github.com/Div290/SPEED<br>
<span id='abs_ch'>中文: SPEED提出了一种新颖方法，通过在各层使用延迟分类器进行选择性预测，识别并推迟困难 本，将错误预测风险降低50%，在早期退出深度神经网络中实现2.05倍 速，同时提升准确性和延迟性能。</span><br>
<span id='abs_en'>English: SPEED introduces a novel method using selective prediction with deferral classifiers at each layer to identify and defer hard samples, reducing wrong predictions by 50% and achieving a 2.05× speedup while improving both accuracy and latency in early exit deep neural networks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2509.11425.pdf' target='_blank'>https://arxiv.org/pdf/2509.11425.pdf</a></span>   <span><a href='https://github.com/mubtasimahasan/FuseCodec' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Mubtasim Ahasan, Rafat Hasan Khan, Tasnim Mohiuddin, Aman Chadha, Tariq Iqbal, M Ashraful Amin, Amin Ahsan Ali, Md Mofijul Islam, A K M Mahbubur Rahman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11425">FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Speech tokenization enables discrete representation and facilitates speech language modeling. However, existing neural codecs capture low-level acoustic features, overlooking the semantic and contextual cues inherent to human speech. While recent efforts introduced semantic representations from self-supervised speech models or incorporated contextual representations from pre-trained language models, challenges remain in aligning and unifying the semantic and contextual representations. We introduce FuseCodec, which unifies acoustic, semantic, and contextual representations through strong cross-modal alignment and globally informed supervision. We propose three complementary techniques: (i) Latent Representation Fusion, integrating semantic and contextual features directly into the encoder latent space for robust and unified representation learning; (ii) Global Semantic-Contextual Supervision, supervising discrete tokens with globally pooled and broadcasted representations to enhance temporal consistency and cross-modal alignment; and (iii) Temporally Aligned Contextual Supervision, strengthening alignment by dynamically matching contextual and speech tokens within a local window for fine-grained token-level supervision. We further introduce FuseCodec-TTS, demonstrating our methodology's applicability to zero-shot speech synthesis. Empirically, FuseCodec achieves state-of-the-art performance in LibriSpeech, surpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy, perceptual quality, intelligibility, and speaker similarity. Results highlight the effectiveness of contextually and semantically guided tokenization for speech tokenization and downstream tasks. Code and pretrained models are available at https://github.com/mubtasimahasan/FuseCodec.<br>
<span id='abs_ch'>中文摘要：FuseCodec通过跨模态对齐和全局监督融合了声学、语义和上下文语音表征，在转录准确性和语音质量方面实现了最先进的性能。</span><br>
<span id='abs_en'>English Summary: FuseCodec unifies acoustic, semantic, and contextual speech representations through cross-modal alignment and global supervision, achieving state-of-the-art performance in transcription accuracy and speech quality.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2509.11420.pdf' target='_blank'>https://arxiv.org/pdf/2509.11420.pdf</a></span>   <span><a href='https://github.com/TauricResearch/Trading-R1' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/TauricResearch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yijia Xiao, Edward Sun, Tong Chen, Fang Wu, Di Luo, Wei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11420">Trading-R1: Financial Trading with LLM Reasoning via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Developing professional, structured reasoning on par with human financial analysts and traders remains a central challenge in AI for finance, where markets demand interpretability and trust. Traditional time-series models lack explainability, while LLMs face challenges in turning natural-language analysis into disciplined, executable trades. Although reasoning LLMs have advanced in step-by-step planning and verification, their application to risk-sensitive financial decisions is underexplored. We present Trading-R1, a financially-aware model that incorporates strategic thinking and planning for comprehensive thesis composition, facts-grounded analysis, and volatility-adjusted decision making. Trading-R1 aligns reasoning with trading principles through supervised fine-tuning and reinforcement learning with a three-stage easy-to-hard curriculum. Training uses Tauric-TR1-DB, a 100k-sample corpus spanning 18 months, 14 equities, and five heterogeneous financial data sources. Evaluated on six major equities and ETFs, Trading-R1 demonstrates improved risk-adjusted returns and lower drawdowns compared to both open-source and proprietary instruction-following models as well as reasoning models. The system generates structured, evidence-based investment theses that support disciplined and interpretable trading decisions. Trading-R1 Terminal will be released at https://github.com/TauricResearch/Trading-R1.<br>
<span id='abs_ch'>中文摘要：Trading-R1是一种具备金融意识的AI模型，通过结构化推理和基于证据的投资论述，提高了风险调整后收益并降低了回撤，满足了金融市场对可解释交易决策的需求。</span><br>
<span id='abs_en'>English Summary: Trading-R1 is a financially-aware AI model that enhances risk-adjusted returns and reduces drawdowns through structured reasoning and evidence-based investment theses, addressing the need for interpretable trading decisions in financial markets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2509.11323.pdf' target='_blank'>https://arxiv.org/pdf/2509.11323.pdf</a></span>   <span><a href='https://github.com/SongJgit/TBDTracker' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/SongJgit/filternet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Song, Wei Mei, Yunfeng Xu, Qiang Fu, Renke Kou, Lina Bu, Yucheng Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11323">Motion Estimation for Multi-Object Tracking using KalmanNet with Semantic-Independent Encoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Motion estimation is a crucial component in multi-object tracking (MOT). It predicts the trajectory of objects by analyzing the changes in their positions in consecutive frames of images, reducing tracking failures and identity switches. The Kalman filter (KF) based on the linear constant-velocity model is one of the most commonly used methods in MOT. However, it may yield unsatisfactory results when KF's parameters are mismatched and objects move in non-stationary. In this work, we utilize the learning-aided filter to handle the motion estimation of MOT. In particular, we propose a novel method named Semantic-Independent KalmanNet (SIKNet), which encodes the state vector (the input feature) using a Semantic-Independent Encoder (SIE) by two steps. First, the SIE uses a 1D convolution with a kernel size of 1, which convolves along the dimension of homogeneous-semantic elements across different state vectors to encode independent semantic information. Then it employs a fully-connected layer and a nonlinear activation layer to encode nonlinear and cross-dependency information between heterogeneous-semantic elements. To independently evaluate the performance of the motion estimation module in MOT, we constructed a large-scale semi-simulated dataset from several open-source MOT datasets. Experimental results demonstrate that the proposed SIKNet outperforms the traditional KF and achieves superior robustness and accuracy than existing learning-aided filters. The code is available at (https://github.com/SongJgit/filternet and https://github.com/SongJgit/TBDTracker).<br>
<span id='abs_ch'>中文: 本文提出了一种名为语义独立卡尔曼网络（SIKNet）的学 辅助运动估计方法，通过两步编 状态向量来捕捉独立语义和非线性依赖信息，在多目 跟踪中展现出优于 统卡尔曼滤波器和其他学 型滤波器的鲁棒性与准确性。</span><br>
<span id='abs_en'>English: This paper introduces Semantic-Independent KalmanNet (SIKNet), a learning-aided motion estimation method for multi-object tracking that enhances robustness and accuracy by encoding state vectors with independent semantic and nonlinear dependency information, outperforming traditional Kalman filters and other learning-based approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2509.11252.pdf' target='_blank'>https://arxiv.org/pdf/2509.11252.pdf</a></span>   <span><a href='https://github.com/zhangyitonggg/dllm4code' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengze li, Yitong Zhang, Jia Li, Liyi Cai, Ge Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11252">Beyond Autoregression: An Empirical Study of Diffusion Large Language Models for Code Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LLMs have become the mainstream approaches to code generation. Existing LLMs mainly employ autoregressive generation, i.e. generating code token-by-token from left to right. However, the underlying autoregressive generation has two limitations in code generation. First, autoregressive LLMs only generate a token at each step, showing low efficiency in practice. Second, programming is a non-sequential process involving back-and-forth editing, while autoregressive LLMs only employ the left-to-right generation order. These two intrinsic limitations hinder the further development of LLMs in code generation. Recently, diffusion LLMs have emerged as a promising alternative. Diffusion LLMs address the above limitations with two advances, including multi-token prediction (i.e. generating multiple tokens at each step) and flexible generation order (i.e. flexibly determining which positions to generate tokens). However, there is no systematic study exploring diffusion LLMs in code generation. To bridge the knowledge gap, we present the first empirical study of diffusion LLMs for code generation. Our study involves 9 representative diffusion LLMs and conduct experiments on 4 widely used benchmarks. Based on the results, we summarize the following findings. (1) Existing diffusion LLMs are competitive with autoregressive LLMs with similar sizes. (2) Diffusion LLMs have a stronger length extrapolation ability than autoregressive LLMs and perform better in long code understanding. (3) We explore factors impacting the effectiveness and efficiency of diffusion LLMs, and provide practical guidance. (4) We discuss several promising further directions to improve diffusion LLMs on code generation. We open-source all source code, data, and results to facilitate the following research. The code is publicly available at https://github.com/zhangyitonggg/dllm4code.<br>
<span id='abs_ch'>中文摘要：自回归大语言模型在代 生成中存在效率低和顺序限制的问题，而扩散大语言模型通过多令牌预测和灵活生成顺序提供了有前景的替代方案，首个实证 究在四个基准测试中验证了九种模型的竞争优势。</span><br>
<span id='abs_en'>English Summary: Autoregressive LLMs face efficiency and flexibility limitations in code generation, while diffusion LLMs offer promising alternatives through multi-token prediction and flexible generation order, as demonstrated by the first empirical study comparing nine models across four benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2509.11178.pdf' target='_blank'>https://arxiv.org/pdf/2509.11178.pdf</a></span>   <span><a href='https://github.com/Rss1124/StegOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengde Lin, Xuezhu Gong, Shuxue Ding, Mingzhe Yang, Xijun Lu, Chengjun Mo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11178">StegOT: Trade-offs in Steganography via Optimal Transport</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Image hiding is often referred to as steganography, which aims to hide a secret image in a cover image of the same resolution. Many steganography models are based on genera-tive adversarial networks (GANs) and variational autoencoders (VAEs). However, most existing models suffer from mode collapse. Mode collapse will lead to an information imbalance between the cover and secret images in the stego image and further affect the subsequent extraction. To address these challenges, this paper proposes StegOT, an autoencoder-based steganography model incorporating optimal transport theory. We designed the multiple channel optimal transport (MCOT) module to transform the feature distribution, which exhibits multiple peaks, into a single peak to achieve the trade-off of information. Experiments demonstrate that we not only achieve a trade-off between the cover and secret images but also enhance the quality of both the stego and recovery images. The source code will be released on https://github.com/Rss1124/StegOT.<br>
<span id='abs_ch'>中文: 本文提出StegOT模型，一种基于自编 器并结合最优 输理论的隐写方法，通过多通道最优 输模块平衡载体与秘密图像的信息，提升隐写和恢复图像的质量。</span><br>
<span id='abs_en'>English: This paper introduces StegOT, an autoencoder-based steganography model that uses optimal transport theory to balance information between cover and secret images, improving both stego and recovery image quality.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2509.11167.pdf' target='_blank'>https://arxiv.org/pdf/2509.11167.pdf</a></span>   <span><a href='https://github.com/pmahdavi/ota-merge' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pouria Mahdavinia, Hamed Mahdavi, Niloofar Mireshghallah, Mehrdad Mahdavi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11167">Harnessing Optimization Dynamics for Curvature-Informed Model Merging</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Model merging is an effective post-training strategy for composing capabilities in large language models without joint retraining. We study this in the supervised fine-tuning (SFT) stage, where multiple capability-based SFT checkpoints -- spanning math, code, precise instruction following, general instruction following, and knowledge recall -- must be consolidated into a single model. We introduce Optimization Trajectory Aware (OTA) Merging, a curvature-aware aggregation that leverages optimizer second-moment statistics as a diagonal curvature proxy to reweight parameter edits and mitigate interference. Complementing OTA, we propose Fast Fisher Grafting (FFG), a curvature-driven task-localization step that sparsifies conflicting or low-importance edits. FFG induces extremely low-rank masks concentrated in early attention query/key projections and token embeddings, exploiting shared curvature across capabilities. We further develop a memory-light compression of the second moments that preserves OTA's effect. Across diverse capability-based SFT checkpoints, OTA+FFG improves merged-model quality over strong weight-space baselines, reduces negative transfer, and remains robust across sparsity levels. Analyses reveal substantial curvature overlap between checkpoints, offering a novel lens on why simple linear merging can be effective in practice. Ablations confirm that FFG is critical for reducing task interference and that the compressed second moments retain the gains of the full formulation. To facilitate reproducibility, we open-source all code, training and evaluation scripts, visualization artifacts, and capability-specific SFT checkpoints at https://github.com/pmahdavi/ota-merge.<br>
<span id='abs_ch'>中文：OTA合并与快速费舍尔嫁接是一种创新方法，通过曲率感知参数聚合和任务定位技术，有效整合多个专业能力的语言模型，减少任务干扰并提升综合性能。</span><br>
<span id='abs_en'>English: OTA merging with Fast Fisher Grafting is a novel method that effectively combines multiple capability-specific language models by using curvature-aware parameter aggregation and task-localization to reduce interference and enhance performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2509.11149.pdf' target='_blank'>https://arxiv.org/pdf/2509.11149.pdf</a></span>   <span><a href='https://github.com/mintaeshkim/roverfly' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mintae Kim, Jiaze Cai, Koushil Sreenath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11149">RoVerFly: Robust and Versatile Implicit Hybrid Control of Quadrotor-Payload Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Designing robust controllers for precise trajectory tracking with quadrotors is challenging due to nonlinear dynamics and underactuation, and becomes harder with flexible cable-suspended payloads that add degrees of freedom and hybrid dynamics. Classical model-based methods offer stability guarantees but require extensive tuning and often fail to adapt when the configuration changes-when a payload is added or removed, or when its mass or cable length varies. We present RoVerFly, a unified learning-based control framework where a single reinforcement learning (RL) policy functions as an implicit hybrid controller, managing complex dynamics without explicit mode detection or controller switching. Trained with task and domain randomization, the controller is resilient to disturbances and varying dynamics. It achieves strong zero-shot generalization across payload settings-including no payload as well as varying mass and cable length-without re-tuning, while retaining the interpretability and structure of a feedback tracking controller. Code and supplementary materials are available at https://github.com/mintaeshkim/roverfly.<br>
<span id='abs_ch'>Chinese: RoVerFly是一个基于学 的统一控制框架，通过单一强化学 策略作为隐式混合控制器， 需重新调整即可在各种负载条件下实现强大的零 本泛化能力。</span><br>
<span id='abs_en'>English: RoVerFly is a unified learning-based control framework that uses a single reinforcement learning policy as an implicit hybrid controller, achieving robust zero-shot generalization across various payload conditions without requiring retuning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2509.11135.pdf' target='_blank'>https://arxiv.org/pdf/2509.11135.pdf</a></span>   <span><a href='https://github.com/SCNU203/AlignKT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Xiao, Chang You, Zhiyu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11135">AlignKT: Explicitly Modeling Knowledge State for Knowledge Tracing with Ideal State Alignment</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Knowledge Tracing (KT) serves as a fundamental component of Intelligent Tutoring Systems (ITS), enabling these systems to monitor and understand learners' progress by modeling their knowledge state. However, many existing KT models primarily focus on fitting the sequences of learners' interactions, and often overlook the knowledge state itself. This limitation leads to reduced interpretability and insufficient instructional support from the ITS. To address this challenge, we propose AlignKT, which employs a frontend-to-backend architecture to explicitly model a stable knowledge state. In this approach, the preliminary knowledge state is aligned with an additional criterion. Specifically, we define an ideal knowledge state based on pedagogical theories as the alignment criterion, providing a foundation for interpretability. We utilize five encoders to implement this set-up, and incorporate a contrastive learning module to enhance the robustness of the alignment process. Through extensive experiments, AlignKT demonstrates superior performance, outperforming seven KT baselines on three real-world datasets. It achieves state-of-the-art results on two of these datasets and exhibits competitive performance on the third. The code of this work is available at https://github.com/SCNU203/AlignKT.<br>
<span id='abs_ch'>中文摘要：AlignKT采用前后端架构，通过教学理论对齐和对比学 显式建模稳定知识状态，在多个数据集上实现最优性能，同时提升了知识追踪的可解释性。</span><br>
<span id='abs_en'>English Summary: AlignKT introduces a frontend-to-backend architecture that explicitly models stable knowledge states using pedagogical alignment and contrastive learning, achieving state-of-the-art performance on multiple datasets while enhancing interpretability in knowledge tracing.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2509.10982.pdf' target='_blank'>https://arxiv.org/pdf/2509.10982.pdf</a></span>   <span><a href='https://github.com/pirofti/FGLL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Paul Irofti, Luis Romero-Ben, Florin Stoican, VicenÃ§ Puig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10982">Factor Graph Optimization for Leak Localization in Water Distribution Networks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Detecting and localizing leaks in water distribution network systems is an important topic with direct environmental, economic, and social impact. Our paper is the first to explore the use of factor graph optimization techniques for leak localization in water distribution networks, enabling us to perform sensor fusion between pressure and demand sensor readings and to estimate the network's temporal and structural state evolution across all network nodes. The methodology introduces specific water network factors and proposes a new architecture composed of two factor graphs: a leak-free state estimation factor graph and a leak localization factor graph. When a new sensor reading is obtained, unlike Kalman and other interpolation-based methods, which estimate only the current network state, factor graphs update both current and past states. Results on Modena, L-TOWN and synthetic networks show that factor graphs are much faster than nonlinear Kalman-based alternatives such as the UKF, while also providing improvements in localization compared to state-of-the-art estimation-localization approaches. Implementation and benchmarks are available at https://github.com/pirofti/FGLL.<br>
<span id='abs_ch'>中文摘要：本文首次将 子图优化技术应用于水管网络泄漏定位，通过融合压力与流量 感器数据实现全网状态估计，相比 统方法在定位精度和计算速度上均有显著提升。</span><br>
<span id='abs_en'>English Summary: This paper pioneers the use of factor graph optimization for leak detection in water networks, enabling sensor fusion and state estimation across all nodes while outperforming traditional methods in speed and localization accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2509.10918.pdf' target='_blank'>https://arxiv.org/pdf/2509.10918.pdf</a></span>   <span><a href='https://github.com/wenboluu/ToMA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenbo Lu, Shaoyi Zheng, Yuxuan Xia, Shengjie Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10918">ToMA: Token Merge with Attention for Diffusion Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion models excel in high-fidelity image generation but face scalability limits due to transformers' quadratic attention complexity. Plug-and-play token reduction methods like ToMeSD and ToFu reduce FLOPs by merging redundant tokens in generated images but rely on GPU-inefficient operations (e.g., sorting, scattered writes), introducing overheads that negate theoretical speedups when paired with optimized attention implementations (e.g., FlashAttention). To bridge this gap, we propose Token Merge with Attention (ToMA), an off-the-shelf method that redesigns token reduction for GPU-aligned efficiency, with three key contributions: 1) a reformulation of token merge as a submodular optimization problem to select diverse tokens; 2) merge/unmerge as an attention-like linear transformation via GPU-friendly matrix operations; and 3) exploiting latent locality and sequential redundancy (pattern reuse) to minimize overhead. ToMA reduces SDXL/Flux generation latency by 24%/23%, respectively (with DINO $Î< 0.07$), outperforming prior methods. This work bridges the gap between theoretical and practical efficiency for transformers in diffusion.<br>
<br>
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2509.10886.pdf' target='_blank'>https://arxiv.org/pdf/2509.10886.pdf</a></span>   <span><a href='https://github.com/Eyr3/CultureSynth.' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Zhang, Pei Zhang, Shuang Luo, Jialong Tang, Yu Wan, Baosong Yang, Fei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10886">CultureSynth: A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question-Answer Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Cultural competence, defined as the ability to understand and adapt to multicultural contexts, is increasingly vital for large language models (LLMs) in global environments. While several cultural benchmarks exist to assess LLMs' cultural competence, current evaluations suffer from fragmented taxonomies, domain specificity, and heavy reliance on manual data annotation. To address these limitations, we introduce CultureSynth, a novel framework comprising (1) a comprehensive hierarchical multilingual cultural taxonomy covering 12 primary and 130 secondary topics, and (2) a Retrieval-Augmented Generation (RAG)-based methodology leveraging factual knowledge to synthesize culturally relevant question-answer pairs. The CultureSynth-7 synthetic benchmark contains 19,360 entries and 4,149 manually verified entries across 7 languages. Evaluation of 14 prevalent LLMs of different sizes reveals clear performance stratification led by ChatGPT-4o-Latest and Qwen2.5-72B-Instruct. The results demonstrate that a 3B-parameter threshold is necessary for achieving basic cultural competence, models display varying architectural biases in knowledge processing, and significant geographic disparities exist across models. We believe that CultureSynth offers a scalable framework for developing culturally aware AI systems while reducing reliance on manual annotation\footnote{Benchmark is available at https://github.com/Eyr3/CultureSynth.}.<br>
<br>
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2509.10852.pdf' target='_blank'>https://arxiv.org/pdf/2509.10852.pdf</a></span>   <span><a href='https://github.com/sangyeop-kim/PREMem' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sangyeop Kim, Yohan Lee, Sanghwa Kim, Hyunjong Kim, Sungzoon Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10852">Pre-Storage Reasoning for Episodic Memory: Shifting Inference Burden to Memory for Personalized Dialogue</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Effective long-term memory in conversational AI requires synthesizing information across multiple sessions. However, current systems place excessive reasoning burden on response generation, making performance significantly dependent on model sizes. We introduce PREMem (Pre-storage Reasoning for Episodic Memory), a novel approach that shifts complex reasoning processes from inference to memory construction. PREMem extracts fine-grained memory fragments categorized into factual, experiential, and subjective information; it then establishes explicit relationships between memory items across sessions, capturing evolution patterns like extensions, transformations, and implications. By performing this reasoning during pre-storage rather than when generating a response, PREMem creates enriched representations while reducing computational demands during interactions. Experiments show significant performance improvements across all model sizes, with smaller models achieving results comparable to much larger baselines while maintaining effectiveness even with constrained token budgets. Code and dataset are available at https://github.com/sangyeop-kim/PREMem.<br>
<span id='abs_ch'>中文: PREMem通过将复杂推理从响应生成转移到记忆构建，实现了跨会话细粒度记忆片段的分类与关联，在显著提升性能的同时有效降低了交互时的计算负担。</span><br>
<span id='abs_en'>English: PREMem introduces a novel approach that shifts complex reasoning from response generation to memory construction by categorizing and linking fine-grained memory fragments across sessions, significantly improving performance while reducing computational demands during interactions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2509.10656.pdf' target='_blank'>https://arxiv.org/pdf/2509.10656.pdf</a></span>   <span><a href='https://github.com/Chirayu-N/gc-marl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chirayu Nimonkar, Shlok Shah, Catherine Ji, Benjamin Eysenbach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10656">Self-Supervised Goal-Reaching Results in Multi-Agent Cooperation and Exploration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>For groups of autonomous agents to achieve a particular goal, they must engage in coordination and long-horizon reasoning. However, designing reward functions to elicit such behavior is challenging. In this paper, we study how self-supervised goal-reaching techniques can be leveraged to enable agents to cooperate. The key idea is that, rather than have agents maximize some scalar reward, agents aim to maximize the likelihood of visiting a certain goal. This problem setting enables human users to specify tasks via a single goal state rather than implementing a complex reward function. While the feedback signal is quite sparse, we will demonstrate that self-supervised goal-reaching techniques enable agents to learn from such feedback. On MARL benchmarks, our proposed method outperforms alternative approaches that have access to the same sparse reward signal as our method. While our method has no explicit mechanism for exploration, we observe that self-supervised multi-agent goal-reaching leads to emergent cooperation and exploration in settings where alternative approaches never witness a single successful trial.<br>
<span id='abs_ch'>中文: 通过自我监督的目 达成技术，自主智能体能够通过最大化访问指定目 状态的可能性来实现合作与长期推理，在相同稀疏奖励信号下优于其他方法，并促进探索行为的自然涌现。</span><br>
<span id='abs_en'>English: Self-supervised goal-reaching techniques enable autonomous agents to achieve cooperation and long-horizon reasoning by maximizing the likelihood of visiting specified goal states, outperforming alternative methods with the same sparse reward signal and fostering emergent exploration.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2509.10535.pdf' target='_blank'>https://arxiv.org/pdf/2509.10535.pdf</a></span>   <span><a href='https://github.com/keepgoingjkg/SG-LoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Miaoge Li, Yang Chen, Zhijie Rao, Can Jiang, Jingcai Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10535">Semantic-guided LoRA Parameters Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Low-Rank Adaptation (LoRA) has demonstrated strong generalization capabilities across a variety of tasks for efficiently fine-tuning AI models, especially on resource-constrained edges. However, in real-world applications, edge users often exhibit task-specific preferences that are difficult to handle with a unified model trained under a closed-world assumption, and the challenge may further increase when there are significant domain shifts between training and deployment. Meanwhile, retraining/fine-tuning models for each user is also impractical due to its cost-intensive nature and privacy concerns over raw data utilization from edges. To address these challenges, we propose Semantic-guided LoRA Parameter Generation (SG-LoRA), the first of its kind framework to efficiently produce user-specific LoRA parameters without any additional training on user tasks or access to user-specific data. Concretely, SG-LoRA uses task descriptions as the semantic bridge, measuring their proximity to a set of known expert tasks in a shared embedding space. Based on this semantic guidance, it models the target task's LoRA parameter distribution to generate high-performing parameters for novel tasks. SG-LoRA enables the real-time construction of LoRA models aligned with individual intents by distilling knowledge from prominent LoRA experts and, meanwhile, offering a privacy-preserving solution for personalized model adaptation in a novel zero-shot open-world setting proposed in this work. Extensive experiments on multiple challenging tasks confirm the superior performance and remarkable adaptability of SG-LoRA. Code is available at https://github.com/keepgoingjkg/SG-LoRA.<br>
<br>
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2509.10528.pdf' target='_blank'>https://arxiv.org/pdf/2509.10528.pdf</a></span>   <span><a href='https://github.com/Ahghaffari/stm_graph' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/tuminguyen/stm_graph_gui' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amirhossein Ghaffari, Huong Nguyen, Lauri LovÃ©n, Ekaterina Gilman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10528">STM-Graph: A Python Framework for Spatio-Temporal Mapping and Graph Neural Network Predictions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Urban spatio-temporal data present unique challenges for predictive analytics due to their dynamic and complex nature. We introduce STM-Graph, an open-source Python framework that transforms raw spatio-temporal urban event data into graph representations suitable for Graph Neural Network (GNN) training and prediction. STM-Graph integrates diverse spatial mapping methods, urban features from OpenStreetMap, multiple GNN models, comprehensive visualization tools, and a graphical user interface (GUI) suitable for professional and non-professional users. This modular and extensible framework facilitates rapid experimentation and benchmarking. It allows integration of new mapping methods and custom models, making it a valuable resource for researchers and practitioners in urban computing. The source code of the framework and GUI are available at: https://github.com/Ahghaffari/stm_graph and https://github.com/tuminguyen/stm_graph_gui.<br>
<span id='abs_ch'>中文：STM-Graph是一个开源Python框架，可将城市时空数据转化为适用于图神经网络训练的图结构，其模块化设计、可视化工具和图形界面为城市计算领域的 究者和实践者提供了便捷支持。</span><br>
<span id='abs_en'>English: STM-Graph is an open-source Python framework that converts urban spatio-temporal data into graph representations for GNN training, featuring modular design, visualization tools, and a GUI to support both researchers and practitioners in urban computing.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2509.10510.pdf' target='_blank'>https://arxiv.org/pdf/2509.10510.pdf</a></span>   <span><a href='https://github.com/basiralab/FireGNN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Prajit Sengupta, Islem Rekik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10510">FireGNN: Neuro-Symbolic Graph Neural Networks with Trainable Fuzzy Rules for Interpretable Medical Image Classification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Medical image classification requires not only high predictive performance but also interpretability to ensure clinical trust and adoption. Graph Neural Networks (GNNs) offer a powerful framework for modeling relational structures within datasets; however, standard GNNs often operate as black boxes, limiting transparency and usability, particularly in clinical settings. In this work, we present an interpretable graph-based learning framework named FireGNN that integrates trainable fuzzy rules into GNNs for medical image classification. These rules embed topological descriptors - node degree, clustering coefficient, and label agreement - using learnable thresholds and sharpness parameters to enable intrinsic symbolic reasoning. Additionally, we explore auxiliary self-supervised tasks (e.g., homophily prediction, similarity entropy) as a benchmark to evaluate the contribution of topological learning. Our fuzzy-rule-enhanced model achieves strong performance across five MedMNIST benchmarks and the synthetic dataset MorphoMNIST, while also generating interpretable rule-based explanations. To our knowledge, this is the first integration of trainable fuzzy rules within a GNN. Source Code: https://github.com/basiralab/FireGNN<br>
<br>
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2509.10509.pdf' target='_blank'>https://arxiv.org/pdf/2509.10509.pdf</a></span>   <span><a href='https://github.com/imsaitejareddy/ouroboros-effect-experiment' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sai Teja Reddy Adapala
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10509">The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The stability of recursively trained large language models (LLMs) is a foundational problem for AI safety. Prevailing theory predicts model collapse, a progressive degradation when models are trained on their own output. We challenge this narrative by introducing a selective feedback mechanism. Contrary to expectation, instead of merely slowing decay, our experiments provide strong evidence that this pressure reverses it, inducing a statistically significant performance improvement in a Gemma 2B model on a complex summarization task. We name this phenomenon the Anti-Ouroboros Effect. We contrast this with a foundational experiment using a simple classifier, where the theoretical degenerative loop was validated, highlighting the unique dynamics of high-dimensional models. Our findings establish that systemic resilience can be an emergent property of LLMs under simple selection pressure, suggesting a powerful and scalable principle for developing safer and more robust AI systems. Across five generations, a quality-filtered condition improved by 6.6% in ROUGE-L F1 score, whereas an unfiltered control degraded by 3.5% and a random-filter control degraded by 4.2%<br>
<span id='abs_ch'>Chinese: 引入选择性反馈机制可逆转大语言模型的性能衰退，产生名为"反噬尾效应"的显著性能提升，证明在筛选压力下系统韧性可作为涌现属性出现。English: Introducing a selective feedback mechanism reverses model degradation in LLMs, inducing significant performance improvement termed the Anti-Ouroboros Effect, demonstrating emergent systemic resilience under selection pressure.Paperid:896,https://arxiv.org/pdf/2509.10474.pdfGitHub</span><br>
<span id='abs_en'>English: Introducing a selective feedback mechanism reverses model degradation in LLMs, inducing significant performance improvement termed the Anti-Ouroboros Effect, demonstrating emergent systemic resilience under selection pressure.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2509.10408.pdf' target='_blank'>https://arxiv.org/pdf/2509.10408.pdf</a></span>   <span><a href='https://github.com/iacopo97/Multimodal-SAM-Adapter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Iacopo Curti, Pierluigi Zama Ramirez, Alioscia Petrelli, Luigi Di Stefano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10408">Multimodal SAM-adapter for Semantic Segmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Semantic segmentation, a key task in computer vision with broad applications in autonomous driving, medical imaging, and robotics, has advanced substantially with deep learning. Nevertheless, current approaches remain vulnerable to challenging conditions such as poor lighting, occlusions, and adverse weather. To address these limitations, multimodal methods that integrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged, providing complementary information that enhances robustness. In this work, we present MM SAM-adapter, a novel framework that extends the capabilities of the Segment Anything Model (SAM) for multimodal semantic segmentation. The proposed method employs an adapter network that injects fused multimodal features into SAM's rich RGB features. This design enables the model to retain the strong generalization ability of RGB features while selectively incorporating auxiliary modalities only when they contribute additional cues. As a result, MM SAM-adapter achieves a balanced and efficient use of multimodal information. We evaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES, where MM SAM-adapter delivers state-of-the-art performance. To further analyze modality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard subsets. Results consistently demonstrate that our framework outperforms competing methods in both favorable and adverse conditions, highlighting the effectiveness of multimodal adaptation for robust scene understanding. The code is available at the following link: https://github.com/iacopo97/Multimodal-SAM-Adapter.<br>
<br>
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2509.10401.pdf' target='_blank'>https://arxiv.org/pdf/2509.10401.pdf</a></span>   <span><a href='https://github.com/ResearAI/A2P' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alva West, Yixuan Weng, Minjun Zhu, Zhen Lin, Zhiyuan Ning, Yue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10401">Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Failure attribution in multi-agent systems -- pinpointing the exact step where a decisive error occurs -- is a critical yet unsolved challenge. Current methods treat this as a pattern recognition task over long conversation logs, leading to critically low step-level accuracy (below 17\%), which renders them impractical for debugging complex systems. Their core weakness is a fundamental inability to perform robust counterfactual reasoning: to determine if correcting a single action would have actually averted the task failure. To bridge this \emph{counterfactual inference gap}, we introduce Abduct-Act-Predict (A2P) Scaffolding, a novel agent framework that transforms failure attribution from pattern recognition into a structured causal inference task. A2P explicitly guides a large language model through a formal three-step reasoning process within a single inference pass: (1) Abduction, to infer the hidden root causes behind an agent's actions; (2) Action, to define a minimal corrective intervention; and (3) Prediction, to simulate the subsequent trajectory and verify if the intervention resolves the failure. This structured approach leverages the holistic context of the entire conversation while imposing a rigorous causal logic on the model's analysis. Our extensive experiments on the Who\&When benchmark demonstrate its efficacy. On the Algorithm-Generated dataset, A2P achieves 47.46\% step-level accuracy, a 2.85$\times$ improvement over the 16.67\% of the baseline. On the more complex Hand-Crafted dataset, it achieves 29.31\% step accuracy, a 2.43$\times$ improvement over the baseline's 12.07\%. By reframing the problem through a causal lens, A2P Scaffolding provides a robust, verifiable, and significantly more accurate solution for automated failure attribution. Ours code are released at https://github.com/ResearAI/A2P.<br>
<span id='abs_ch'>中文摘要：A2P框架通过将失败归 转化为结构化 果推理任务，指导语言模型执行溯 -行动-预测的三步推理，在基准测试中实现了最高2.85倍的步骤级准确率提升。</span><br>
<span id='abs_en'>English Summary: The A2P Scaffolding framework transforms failure attribution from pattern recognition into structured causal inference, achieving up to 2.85× accuracy improvement by guiding language models through abductive reasoning about root causes and counterfactual interventions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2509.10059.pdf' target='_blank'>https://arxiv.org/pdf/2509.10059.pdf</a></span>   <span><a href='https://github.com/VisionXLab/avi-math' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Zhou, Litong Feng, Mengcheng Lan, Xue Yang, Qingyun Li, Yiping Ke, Xue Jiang, Wayne Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10059">Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Mathematical reasoning is critical for tasks such as precise distance and area computations, trajectory estimations, and spatial analysis in unmanned aerial vehicle (UAV) based remote sensing, yet current vision-language models (VLMs) have not been adequately tested in this domain. To address this gap, we introduce AVI-Math, the first benchmark to rigorously evaluate multimodal mathematical reasoning in aerial vehicle imagery, moving beyond simple counting tasks to include domain-specific knowledge in areas such as geometry, logic, and algebra. The dataset comprises 3,773 high-quality vehicle-related questions captured from UAV views, covering 6 mathematical subjects and 20 topics. The data, collected at varying altitudes and from multiple UAV angles, reflects real-world UAV scenarios, ensuring the diversity and complexity of the constructed mathematical problems. In this paper, we benchmark 14 prominent VLMs through a comprehensive evaluation and demonstrate that, despite their success on previous multimodal benchmarks, these models struggle with the reasoning tasks in AVI-Math. Our detailed analysis highlights significant limitations in the mathematical reasoning capabilities of current VLMs and suggests avenues for future research. Furthermore, we explore the use of Chain-of-Thought prompting and fine-tuning techniques, which show promise in addressing the reasoning challenges in AVI-Math. Our findings not only expose the limitations of VLMs in mathematical reasoning but also offer valuable insights for advancing UAV-based trustworthy VLMs in real-world applications. The code, and datasets will be released at https://github.com/VisionXLab/avi-math<br>
<span id='abs_ch'>中文: AVI-Math基准测试首次评估 人机图像中的多模态数学推理能力，发现现有视觉语言模型在此领域存在明显不足，为未来 究指明了方向。</span><br>
<span id='abs_en'>English: The AVI-Math benchmark is introduced to evaluate multimodal mathematical reasoning in UAV imagery, revealing that current vision-language models struggle with these complex tasks despite their broader successes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/2509.10054.pdf' target='_blank'>https://arxiv.org/pdf/2509.10054.pdf</a></span>   <span><a href='https://github.com/AGI-FHBC/XAgents' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hailong Yang, Mingxian Gu, Jianqi Wang, Guanjin Wang, Zhaohong Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10054">XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN Rules and Multipolar Task Processing Graph</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of Large Language Models (LLMs) has significantly enhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans with complex, real-world tasks. However, MAS still face challenges in effective task planning when handling highly complex tasks with uncertainty, often resulting in misleading or incorrect outputs that hinder task execution. To address this, we propose XAgents, a unified multi-agent cooperative framework built on a multipolar task processing graph and IF-THEN rules. XAgents uses the multipolar task processing graph to enable dynamic task planning and handle task uncertainty. During subtask processing, it integrates domain-specific IF-THEN rules to constrain agent behaviors, while global rules enhance inter-agent collaboration. We evaluate the performance of XAgents across three distinct datasets, demonstrating that it consistently surpasses state-of-the-art single-agent and multi-agent approaches in both knowledge-typed and logic-typed question-answering tasks. The codes for XAgents are available at: https://github.com/AGI-FHBC/XAgents.<br>
<span id='abs_ch'>Chinese: XAgents是一个统一的多智能体协作框架，通过多极任务处理图和IF-THEN规则改进任务规划并处理不确定性，在知识和逻辑型问答任务中持续超越现有最优方法。</span><br>
<span id='abs_en'>English: XAgents is a unified multi-agent cooperative framework that enhances task planning and handles uncertainty through a multipolar task processing graph and IF-THEN rules, consistently outperforming state-of-the-art approaches in knowledge-typed and logic-typed question-answering tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2509.09969.pdf' target='_blank'>https://arxiv.org/pdf/2509.09969.pdf</a></span>   <span><a href='https://github.com/ZhitianHou/LLMs4LegalAI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhitian Hou, Zihan Ye, Nanli Zeng, Tianyong Hao, Kun Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09969">Large Language Models Meet Legal Artificial Intelligence: A Survey</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have significantly advanced the development of Legal Artificial Intelligence (Legal AI) in recent years, enhancing the efficiency and accuracy of legal tasks. To advance research and applications of LLM-based approaches in legal domain, this paper provides a comprehensive review of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and also gather 15 benchmarks and 29 datasets to evaluate different legal capabilities. Additionally, we analyse the challenges and discuss future directions for LLM-based approaches in the legal domain. We hope this paper provides a systematic introduction for beginners and encourages future research in this field. Resources are available at https://github.com/ZhitianHou/LLMs4LegalAI.<br>
<span id='abs_ch'>中文: 本文系统综述了16个法律大模型系列和47个基于大模型的法律任务框架，汇集了15个基准测试和29个数据集，通过分析挑战与未来方向推动法律人工智能发展，并为初学者提供 究资源。</span><br>
<span id='abs_en'>English: This paper comprehensively reviews 16 legal LLM series and 47 LLM-based frameworks, along with 15 benchmarks and 29 datasets, to advance Legal AI by analyzing challenges and future directions while providing resources for beginners.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2509.09794.pdf' target='_blank'>https://arxiv.org/pdf/2509.09794.pdf</a></span>   <span><a href='https://github.com/Lafayette-EshbaughSilveyra-Group/synthetic-homes' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jackson Eshbaugh, Chetan Tiwari, Jorge Silveyra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09794">A Modular and Multimodal Generative AI Framework for Urban Building Energy Data: Generating Synthetic Homes</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Computational models have emerged as powerful tools for energy modeling research, touting scalability and quantitative results. However, these models require a plethora of data, some of which is inaccessible, expensive, or raises privacy concerns. We introduce a modular multimodal framework to produce this data from publicly accessible residential information and images using generative artificial intelligence (AI). Additionally, we provide a pipeline demonstrating this framework, and we evaluate its generative AI components. Our experiments show that our framework's use of AI avoids common issues with generative models. Our framework produces realistic, labeled data. By reducing dependence on costly or restricted data sources, we pave a path towards more accessible and reproducible research.<br>
<span id='abs_ch'>中文: 本文提出了一种模块化多模态框架，利用生成式人工智能从公开的住宅信息和图像中生成真实、 注的数据，解决了计算能源建模中数据稀缺、成本高昂和隐私问题，同时提升了 究的可及性和可重复性。</span><br>
<span id='abs_en'>English: This paper introduces a modular multimodal framework that uses generative AI to create realistic, labeled data from publicly accessible residential information and images, addressing the challenges of data scarcity, cost, and privacy in computational energy modeling while enhancing research accessibility and reproducibility.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2509.09754.pdf' target='_blank'>https://arxiv.org/pdf/2509.09754.pdf</a></span>   <span><a href='https://github.com/MGDDestiny/Lava' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiqun Shen, Song Yuan, Zhengze Zhang, Xiaoliang Wang, Daxin Jiang, Nguyen Cam-Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09754">LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>KV Cache is commonly used to accelerate LLM inference with long contexts, yet its high memory demand drives the need for cache compression. Existing compression methods, however, are largely heuristic and lack dynamic budget allocation. To address this limitation, we introduce a unified framework for cache compression by minimizing information loss in Transformer residual streams. Building on it, we analyze the layer attention output loss and derive a new metric to compare cache entries across heads, enabling layer-wise compression with dynamic head budgets. Additionally, by contrasting cross-layer information, we also achieve dynamic layer budgets. LAVa is the first unified strategy for cache eviction and dynamic budget allocation that, unlike prior methods, does not rely on training or the combination of multiple strategies. Experiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and InfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a new insight: dynamic layer budgets are crucial for generation tasks (e.g., code completion), while dynamic head budgets play a key role in extraction tasks (e.g., extractive QA). As a fully dynamic compression method, LAVa consistently maintains top performance across task types. Our code is available at https://github.com/MGDDestiny/Lava.<br>
<span id='abs_ch'>中文：LAVa提出了一个统一的KV缓存压缩框架，通过最小化Transformer残差流中的信息损失，实现了 需训练或组合多种策略的动态层级和注意力头预算分配，并在多个基准测试中展现出卓越性能。</span><br>
<span id='abs_en'>English: LAVa introduces a unified KV cache compression framework that minimizes information loss in Transformer residual streams, enabling dynamic layer and head budget allocation without requiring training or multiple strategies, and achieves superior performance across various benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2509.09747.pdf' target='_blank'>https://arxiv.org/pdf/2509.09747.pdf</a></span>   <span><a href='https://github.com/Schindler-EPFL-Lab/D-CAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Leen Daher, Zhaobo Wang, Malcolm Mielle
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09747">D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Cross-modal transfer learning is used to improve multi-modal classification models (e.g., for human activity recognition in human-robot collaboration). However, existing methods require paired sensor data at both training and inference, limiting deployment in resource-constrained environments where full sensor suites are not economically and technically usable. To address this, we propose Decoupled Cross-Attention Transfer (D-CAT), a framework that aligns modality-specific representations without requiring joint sensor modality during inference. Our approach combines a self-attention module for feature extraction with a novel cross-attention alignment loss, which enforces the alignment of sensors' feature spaces without requiring the coupling of the classification pipelines of both modalities. We evaluate D-CAT on three multi-modal human activity datasets (IMU, video, and audio) under both in-distribution and out-of-distribution scenarios, comparing against uni-modal models. Results show that in in-distribution scenarios, transferring from high-performing modalities (e.g., video to IMU) yields up to 10% F1-score gains over uni-modal training. In out-of-distribution scenarios, even weaker source modalities (e.g., IMU to video) improve target performance, as long as the target model isn't overfitted on the training data. By enabling single-sensor inference with cross-modal knowledge, D-CAT reduces hardware redundancy for perception systems while maintaining accuracy, which is critical for cost-sensitive or adaptive deployments (e.g., assistive robots in homes with variable sensor availability). Code is available at https://github.com/Schindler-EPFL-Lab/D-CAT.<br>
<span id='abs_ch'>中文: 提出的D-CAT框架 需推理时配对 感器数据即可实现跨模态知识迁移，在提升分类性能的同时降低了资源受限环境下的硬件依赖。</span><br>
<span id='abs_en'>English: The proposed D-CAT framework enables cross-modal knowledge transfer without requiring paired sensor data during inference, improving classification performance while reducing hardware dependency in resource-constrained environments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2509.09744.pdf' target='_blank'>https://arxiv.org/pdf/2509.09744.pdf</a></span>   <span><a href='https://github.com/mjliu99/SAM-BG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mujie Liu, Chenze Wang, Liping Chen, Nguyen Linh Dan Le, Niharika Tewari, Ting Dang, Jiangang Ma, Feng Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09744">Structure Matters: Brain Graph Augmentation via Learnable Edge Masking for Data-efficient Psychiatric Diagnosis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The limited availability of labeled brain network data makes it challenging to achieve accurate and interpretable psychiatric diagnoses. While self-supervised learning (SSL) offers a promising solution, existing methods often rely on augmentation strategies that can disrupt crucial structural semantics in brain graphs. To address this, we propose SAM-BG, a two-stage framework for learning brain graph representations with structural semantic preservation. In the pre-training stage, an edge masker is trained on a small labeled subset to capture key structural semantics. In the SSL stage, the extracted structural priors guide a structure-aware augmentation process, enabling the model to learn more semantically meaningful and robust representations. Experiments on two real-world psychiatric datasets demonstrate that SAM-BG outperforms state-of-the-art methods, particularly in small-labeled data settings, and uncovers clinically relevant connectivity patterns that enhance interpretability. Our code is available at https://github.com/mjliu99/SAM-BG.<br>
<span id='abs_ch'>中文：提出的SAM-BG框架通过结构语义保持技术改进脑网络表征学 ，在 注数据有限的精神疾病分析中实现了更优的诊断准确性和可解释性。</span><br>
<span id='abs_en'>English: The proposed SAM-BG framework uses structural semantic preservation to enhance brain graph representation learning, achieving superior diagnostic accuracy and interpretability in psychiatric analysis with limited labeled data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2509.09703.pdf' target='_blank'>https://arxiv.org/pdf/2509.09703.pdf</a></span>   <span><a href='https://github.com/Xuzhenhua55/CTCC>' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenhua Xu, Xixiang Zhao, Xubin Yue, Shengwei Tian, Changting Lin, Meng Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09703">CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The widespread deployment of large language models (LLMs) has intensified concerns around intellectual property (IP) protection, as model theft and unauthorized redistribution become increasingly feasible. To address this, model fingerprinting aims to embed verifiable ownership traces into LLMs. However, existing methods face inherent trade-offs between stealthness, robustness, and generalizability, being either detectable via distributional shifts, vulnerable to adversarial modifications, or easily invalidated once the fingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven fingerprinting framework that encodes contextual correlations across multiple dialogue turns, such as counterfactual, rather than relying on token-level or single-turn triggers. CTCC enables fingerprint verification under black-box access while mitigating false positives and fingerprint leakage, supporting continuous construction under a shared semantic rule even if partial triggers are exposed. Extensive experiments across multiple LLM architectures demonstrate that CTCC consistently achieves stronger stealth and robustness than prior work. Our findings position CTCC as a reliable and practical solution for ownership verification in real-world LLM deployment scenarios. Our code and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.<br>
<br>
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2509.09679.pdf' target='_blank'>https://arxiv.org/pdf/2509.09679.pdf</a></span>   <span><a href='https://github.com/42Shawn/Butterflyquant-llm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingxin Xu, Zhen Dong, Oussama Elachqar, Yuzhang Shang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09679">ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models require massive memory footprints, severely limiting deployment on consumer hardware. Quantization reduces memory through lower numerical precision, but extreme 2-bit quantization suffers from catastrophic performance loss due to outliers in activations. Rotation-based methods such as QuIP and QuaRot apply orthogonal transforms to eliminate outliers before quantization, using computational invariance: $\mathbf{y} = \mathbf{Wx} = (\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, these methods use fixed transforms--Hadamard matrices achieving optimal worst-case coherence $Î¼= 1/\sqrt{n}$--that cannot adapt to specific weight distributions. We identify that different transformer layers exhibit distinct outlier patterns, motivating layer-adaptive rotations rather than one-size-fits-all approaches. In this work, we propose ButterflyQuant, which replaces Hadamard rotations with learnable butterfly transforms parameterized by continuous Givens rotation angles. Unlike Hadamard's discrete $\{+1, -1\}$ entries that are non-differentiable and thus prohibit gradient-based learning, butterfly transforms' continuous parameterization enables smooth optimization while guaranteeing orthogonality by construction. This orthogonal constraint ensures theoretical guarantees in outlier suppression while achieving $O(n \log n)$ computational complexity with only $\frac{n \log n}{2}$ learnable parameters. We further introduce a uniformity regularization on post-transformation activations to promote smoother distributions amenable to quantization. Learning requires only 128 calibration samples and converges in minutes on a single GPU--a negligible one-time cost. For LLaMA-2-7B with 2-bit quantization, ButterflyQuant achieves 15.4 perplexity versus 37.3 for QuIP. \href{https://github.com/42Shawn/Butterflyquant-llm}{Codes} are available.<br>
<span id='abs_ch'>中文: ButterflyQuant采用可学 的蝴蝶变换，通过连续参数自适应抑制激活值异常值，在2位量化中相比先前方法显著降低困惑度，且计算开销极小。</span><br>
<span id='abs_en'>English: ButterflyQuant introduces learnable butterfly transforms with continuous parameters to adaptively suppress activation outliers for improved 2-bit quantization, achieving significantly lower perplexity than previous methods with minimal computational overhead.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2509.09674.pdf' target='_blank'>https://arxiv.org/pdf/2509.09674.pdf</a></span>   <span><a href='https://github.com/PRIME-RL/SimpleVLA-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhan Li, Yuxin Zuo, Jiale Yu, Yuhao Zhang, Zhaohui Yang, Kaiyan Zhang, Xuekai Zhu, Yuchen Zhang, Tianxing Chen, Ganqu Cui, Dehui Wang, Dingxiang Luo, Yuchen Fan, Youbang Sun, Jia Zeng, Jiangmiao Pang, Shanghang Zhang, Yu Wang, Yao Mu, Bowen Zhou, Ning Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09674">SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $Ï_0$ on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL<br>
<span id='abs_ch'>中文：SimpleVLA-RL是一种高效的强化学 框架，通过增强视觉-语言-动作模型的长期规划能力，在减少对昂贵人工数据依赖的同时实现了最先进的性能表现和更强的泛化能力。</span><br>
<span id='abs_en'>English: SimpleVLA-RL is an efficient reinforcement learning framework that enhances Vision-Language-Action models' long-horizon planning, achieving state-of-the-art performance while reducing reliance on costly human-operated data and improving generalization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2509.09651.pdf' target='_blank'>https://arxiv.org/pdf/2509.09651.pdf</a></span>   <span><a href='https://github.com/Zakaria010/Radio-RAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zakaria El Kassimi, Fares Fourati, Mohamed-Slim Alouini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09651">Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We study question answering in the domain of radio regulations, a legally sensitive and high-stakes area. We propose a telecom-specific Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge, the first multiple-choice evaluation set for this domain, constructed from authoritative sources using automated filtering and human validation. To assess retrieval quality, we define a domain-specific retrieval metric, under which our retriever achieves approximately 97% accuracy. Beyond retrieval, our approach consistently improves generation accuracy across all tested models. In particular, while naively inserting documents without structured retrieval yields only marginal gains for GPT-4o (less than 1%), applying our pipeline results in nearly a 12% relative improvement. These findings demonstrate that carefully targeted grounding provides a simple yet strong baseline and an effective domain-specific solution for regulatory question answering. All code and evaluation scripts, along with our derived question-answer dataset, are available at https://github.com/Zakaria010/Radio-RAG.<br>
<span id='abs_ch'>中文摘要：本 究针对 线电监管领域开发了专用的RAG解决方案，通过领域特定的信息检索实现了97%的检索准确率，并使GPT-4o的生成准确率提升近12%。</span><br>
<span id='abs_en'>English Summary: This research develops a telecom-specific RAG pipeline for radio regulation question answering, achieving 97% retrieval accuracy and nearly 12% generation improvement for GPT-4o through domain-specific grounding.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2509.09614.pdf' target='_blank'>https://arxiv.org/pdf/2509.09614.pdf</a></span>   <span><a href='https://github.com/SalesforceAIResearch/LoCoBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jielin Qiu, Zuxin Liu, Zhiwei Liu, Rithesh Murthy, Jianguo Zhang, Haolin Chen, Shiyu Wang, Ming Zhu, Liangwei Yang, Juntao Tan, Zhepeng Cen, Cheng Qian, Shelby Heinecke, Weiran Yao, Silvio Savarese, Caiming Xiong, Huan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09614">LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The emergence of long-context language models with context windows extending to millions of tokens has created new opportunities for sophisticated code understanding and software development evaluation. We propose LoCoBench, a comprehensive benchmark specifically designed to evaluate long-context LLMs in realistic, complex software development scenarios. Unlike existing code evaluation benchmarks that focus on single-function completion or short-context tasks, LoCoBench addresses the critical evaluation gap for long-context capabilities that require understanding entire codebases, reasoning across multiple files, and maintaining architectural consistency across large-scale software systems. Our benchmark provides 8,000 evaluation scenarios systematically generated across 10 programming languages, with context lengths spanning 10K to 1M tokens, a 100x variation that enables precise assessment of long-context performance degradation in realistic software development settings. LoCoBench introduces 8 task categories that capture essential long-context capabilities: architectural understanding, cross-file refactoring, multi-session development, bug investigation, feature implementation, code comprehension, integration testing, and security analysis. Through a 5-phase pipeline, we create diverse, high-quality scenarios that challenge LLMs to reason about complex codebases at unprecedented scale. We introduce a comprehensive evaluation framework with 17 metrics across 4 dimensions, including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our evaluation of state-of-the-art long-context models reveals substantial performance gaps, demonstrating that long-context understanding in complex software development represents a significant unsolved challenge that demands more attention. LoCoBench is released at: https://github.com/SalesforceAIResearch/LoCoBench.<br>
<span id='abs_ch'>中文: LoCoBench是一个专为评估长上下文语言模型在复杂软件开发场景中表现而设计的综合基准，涵盖10种编程语言的8000个测试场景，揭示了当前模型在长代 理解方面存在显著不足。</span><br>
<span id='abs_en'>English: LoCoBench is a comprehensive benchmark designed to evaluate long-context language models in complex software development scenarios, featuring 8,000 scenarios across 10 programming languages and revealing significant performance gaps in current models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2509.09558.pdf' target='_blank'>https://arxiv.org/pdf/2509.09558.pdf</a></span>   <span><a href='https://github.com/acharaakshit/ShortMR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Akshit Achara, Esther Puyol Anton, Alexander Hammers, Andrew P. King
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09558">Invisible Attributes, Visible Biases: Exploring Demographic Shortcuts in MRI-based Alzheimer's Disease Classification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Magnetic resonance imaging (MRI) is the gold standard for brain imaging. Deep learning (DL) algorithms have been proposed to aid in the diagnosis of diseases such as Alzheimer's disease (AD) from MRI scans. However, DL algorithms can suffer from shortcut learning, in which spurious features, not directly related to the output label, are used for prediction. When these features are related to protected attributes, they can lead to performance bias against underrepresented protected groups, such as those defined by race and sex. In this work, we explore the potential for shortcut learning and demographic bias in DL based AD diagnosis from MRI. We first investigate if DL algorithms can identify race or sex from 3D brain MRI scans to establish the presence or otherwise of race and sex based distributional shifts. Next, we investigate whether training set imbalance by race or sex can cause a drop in model performance, indicating shortcut learning and bias. Finally, we conduct a quantitative and qualitative analysis of feature attributions in different brain regions for both the protected attribute and AD classification tasks. Through these experiments, and using multiple datasets and DL models (ResNet and SwinTransformer), we demonstrate the existence of both race and sex based shortcut learning and bias in DL based AD classification. Our work lays the foundation for fairer DL diagnostic tools in brain MRI. The code is provided at https://github.com/acharaakshit/ShortMR<br>
<br>
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2509.09396.pdf' target='_blank'>https://arxiv.org/pdf/2509.09396.pdf</a></span>   <span><a href='https://github.com/HarryMayne/SCEs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Harry Mayne, Ryan Othniel Kearns, Yushi Yang, Andrew M. Bean, Eoin Delaney, Chris Russell, Adam Mahdi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09396">LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>To collaborate effectively with humans, language models must be able to explain their decisions in natural language. We study a specific type of self-explanation: self-generated counterfactual explanations (SCEs), where a model explains its prediction by modifying the input such that it would have predicted a different outcome. We evaluate whether LLMs can produce SCEs that are valid, achieving the intended outcome, and minimal, modifying the input no more than necessary. When asked to generate counterfactuals, we find that LLMs typically produce SCEs that are valid, but far from minimal, offering little insight into their decision-making behaviour. Worryingly, when asked to generate minimal counterfactuals, LLMs typically make excessively small edits that fail to change predictions. The observed validity-minimality trade-off is consistent across several LLMs, datasets, and evaluation settings. Our findings suggest that SCEs are, at best, an ineffective explainability tool and, at worst, can provide misleading insights into model behaviour. Proposals to deploy LLMs in high-stakes settings must consider the impact of unreliable self-explanations on downstream decision-making. Our code is available at https://github.com/HarryMayne/SCEs.<br>
<span id='abs_ch'>Chinese: 语言模型难以生成有效的自我反事实解释，它们要么做出过多修改而缺乏简洁性，要么改动过小 法改变预测结果，这降低了其在关键决策中作为解释工具的可 性。</span><br>
<span id='abs_en'>English: Language models struggle to produce effective self-generated counterfactual explanations, as they either make excessive changes that remain valid but not minimal, or overly subtle edits that fail to alter predictions, limiting their reliability for explaining decisions in high-stakes applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2509.09307.pdf' target='_blank'>https://arxiv.org/pdf/2509.09307.pdf</a></span>   <span><a href='https://github.com/FreedomIntelligence/MatCha' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengzhao Lai, Youbin Zheng, Zhenyang Cai, Haonan Lyu, Jinpu Yang, Hongqing Liang, Yan Hu, Benyou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09307">Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Materials characterization is fundamental to acquiring materials information, revealing the processing-microstructure-property relationships that guide material design and optimization. While multimodal large language models (MLLMs) have recently shown promise in generative and predictive tasks within materials science, their capacity to understand real-world characterization imaging data remains underexplored. To bridge this gap, we present MatCha, the first benchmark for materials characterization image understanding, comprising 1,500 questions that demand expert-level domain expertise. MatCha encompasses four key stages of materials research comprising 21 distinct tasks, each designed to reflect authentic challenges faced by materials scientists. Our evaluation of state-of-the-art MLLMs on MatCha reveals a significant performance gap compared to human experts. These models exhibit degradation when addressing questions requiring higher-level expertise and sophisticated visual perception. Simple few-shot and chain-of-thought prompting struggle to alleviate these limitations. These findings highlight that existing MLLMs still exhibit limited adaptability to real-world materials characterization scenarios. We hope MatCha will facilitate future research in areas such as new material discovery and autonomous scientific agents. MatCha is available at https://github.com/FreedomIntelligence/MatCha.<br>
<span id='abs_ch'>中文摘要：MatCha作为首个材料表征图像理解的基准，揭示了当前多模态大语言模型在需要高级领域知识和视觉分析的复杂任务中，其表现远逊于人类专家。</span><br>
<span id='abs_en'>English Summary: MatCha is introduced as the first benchmark for materials characterization image understanding, revealing that current multimodal large language models significantly underperform human experts in tasks requiring advanced domain knowledge and visual analysis.Paperid:949,https://arxiv.org/pdf/2509.09292.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2509.09292.pdf' target='_blank'>https://arxiv.org/pdf/2509.09292.pdf</a></span>   <span><a href='https://github.com/wxai-space/LightAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weige Cai, Tong Zhu, Jinyi Niu, Ruiqi Hu, Lingyao Li, Tenglong Wang, Xiaowu Dai, Weining Shen, Liwen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09292">LightAgent: Production-level Open-source Agentic AI Framework</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the rapid advancement of large language models (LLMs), Multi-agent Systems (MAS) have achieved significant progress in various application scenarios. However, substantial challenges remain in designing versatile, robust, and efficient platforms for agent deployment. To address these limitations, we propose \textbf{LightAgent}, a lightweight yet powerful agentic framework, effectively resolving the trade-off between flexibility and simplicity found in existing frameworks. LightAgent integrates core functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while maintaining an extremely lightweight structure. As a fully open-source solution, it seamlessly integrates with mainstream chat platforms, enabling developers to easily build self-learning agents. We have released LightAgent at \href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}<br>
<span id='abs_ch'>中文摘要：LightAgent作为一个轻量级开源框架，通过集成记忆、工具和思维 等 心功能，解决了多智能体系统在灵活性与简洁性之间的权衡问题，使开发者能够轻松构建自学 智能体。</span><br>
<span id='abs_en'>English Summary: LightAgent is a lightweight, open-source framework that overcomes the flexibility-simplicity trade-off in multi-agent systems by integrating memory, tools, and Tree of Thought functionalities for easy development of self-learning agents.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2509.09290.pdf' target='_blank'>https://arxiv.org/pdf/2509.09290.pdf</a></span>   <span><a href='https://github.com/Anthony-P-Addison/AGN-MOD-SEG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anthony P. Addison, Felix Wagner, Wentian Xu, Natalie Voets, Konstantinos Kamnitsas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09290">Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in Multimodal MRI with Sequences Unavailable During Training</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Segmentation models are important tools for the detection and analysis of lesions in brain MRI. Depending on the type of brain pathology that is imaged, MRI scanners can acquire multiple, different image modalities (contrasts). Most segmentation models for multimodal brain MRI are restricted to fixed modalities and cannot effectively process new ones at inference. Some models generalize to unseen modalities but may lose discriminative modality-specific information. This work aims to develop a model that can perform inference on data that contain image modalities unseen during training, previously seen modalities, and heterogeneous combinations of both, thus allowing a user to utilize any available imaging modalities. We demonstrate this is possible with a simple, thus practical alteration to the U-net architecture, by integrating a modality-agnostic input channel or pathway, alongside modality-specific input channels. To train this modality-agnostic component, we develop an image augmentation scheme that synthesizes artificial MRI modalities. Augmentations differentially alter the appearance of pathological and healthy brain tissue to create artificial contrasts between them while maintaining realistic anatomical integrity. We evaluate the method using 8 MRI databases that include 5 types of pathologies (stroke, tumours, traumatic brain injury, multiple sclerosis and white matter hyperintensities) and 8 modalities (T1, T1+contrast, T2, PD, SWI, DWI, ADC and FLAIR). The results demonstrate that the approach preserves the ability to effectively process MRI modalities encountered during training, while being able to process new, unseen modalities to improve its segmentation. Project code: https://github.com/Anthony-P-Addison/AGN-MOD-SEG<br>
<br>
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2509.09204.pdf' target='_blank'>https://arxiv.org/pdf/2509.09204.pdf</a></span>   <span><a href='https://github.com/cyaaronk/audio_deepfake_eval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chin Yuen Kwok, Jia Qi Yip, Zhen Qiu, Chi Hung Chi, Kwok Yan Lam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09204">Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Audio deepfake detection (ADD) models are commonly evaluated using datasets that combine multiple synthesizers, with performance reported as a single Equal Error Rate (EER). However, this approach disproportionately weights synthesizers with more samples, underrepresenting others and reducing the overall reliability of EER. Additionally, most ADD datasets lack diversity in bona fide speech, often featuring a single environment and speech style (e.g., clean read speech), limiting their ability to simulate real-world conditions. To address these challenges, we propose bona fide cross-testing, a novel evaluation framework that incorporates diverse bona fide datasets and aggregates EERs for more balanced assessments. Our approach improves robustness and interpretability compared to traditional evaluation methods. We benchmark over 150 synthesizers across nine bona fide speech types and release a new dataset to facilitate further research at https://github.com/cyaaronk/audio_deepfake_eval.<br>
<span id='abs_ch'>Chinese Summary: 当前音频深度伪 检测模型的评估 合成器 本不平衡和真实语音多 性不足而存在缺陷，为此我们提出了一种新颖的真实语音交叉测试框架，通过整合多 化数据集和聚合等错误率来提升鲁棒性和可解释性。</span><br>
<span id='abs_en'>English Summary: The current evaluation of audio deepfake detection models is flawed due to imbalanced synthesizer representation and limited bona fide speech diversity, prompting the introduction of a novel bona fide cross-testing framework that enhances robustness and interpretability through diverse datasets and aggregated EERs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2509.09174.pdf' target='_blank'>https://arxiv.org/pdf/2509.09174.pdf</a></span>   <span><a href='https://github.com/FreedomIntelligence/EchoX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Zhang, Yuhao Du, Zhanchen Dai, Xiangnan Ma, Kaiqi Kou, Benyou Wang, Haizhou Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09174">EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Speech-to-speech large language models (SLLMs) are attracting increasing attention. Derived from text-based large language models (LLMs), SLLMs often exhibit degradation in knowledge and reasoning capabilities. We hypothesize that this limitation arises because current training paradigms for SLLMs fail to bridge the acoustic-semantic gap in the feature representation space. To address this issue, we propose EchoX, which leverages semantic representations and dynamically generates speech training targets. This approach integrates both acoustic and semantic learning, enabling EchoX to preserve strong reasoning abilities as a speech LLM. Experimental results demonstrate that EchoX, with about six thousand hours of training data, achieves advanced performance on multiple knowledge-based question-answering benchmarks. The project is available at https://github.com/FreedomIntelligence/EchoX.<br>
<span id='abs_ch'>Chinese: EchoX作为一种新型语音大语言模型，通过融合语义学 和动态生成语音目 来克服声学语义鸿沟，仅用六千小时训练数据就在多个知识问答基准上实现了领先性能。</span><br>
<span id='abs_en'>English: EchoX is a novel speech-to-speech large language model that overcomes the acoustic-semantic gap by integrating semantic learning with dynamically generated speech targets, achieving advanced performance on knowledge-based benchmarks with only six thousand hours of training data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2509.09143.pdf' target='_blank'>https://arxiv.org/pdf/2509.09143.pdf</a></span>   <span><a href='https://github.com/Objectness-Similarity/OSIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuiko Uchida, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09143">Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric for 3D scenes that explicitly focuses on "objects," which are fundamental units of human visual perception. Existing metrics assess overall image quality, leading to discrepancies with human perception. Inspired by neuropsychological insights, we hypothesize that human recognition of 3D scenes fundamentally involves attention to individual objects. OSIM enables object-centric evaluations by leveraging an object detection model and its feature representations to quantify the "objectness" of each object in the scene. Our user study demonstrates that OSIM aligns more closely with human perception compared to existing metrics. We also analyze the characteristics of OSIM using various approaches. Moreover, we re-evaluate recent 3D reconstruction and generation models under a standardized experimental setup to clarify advancements in this field. The code is available at https://github.com/Objectness-Similarity/OSIM.<br>
<span id='abs_ch'>中文摘要：本文提出OSIM这一面向3D场景的物体中心化评估新指 ，通过物体检测模型量化场景中各物体的“物体性”，用户 究表明其比现有指 更符合人类感知，并重新评估了当前主流3D重建与生成模型。English Summary: This paper introduces OSIM, an object-centric evaluation metric for 3D scenes that aligns more closely with human perception by quantifying objectness through object detection models, as validated by user studies and comparative analyses.Paperid:956,https://arxiv.org/pdf/2509.09125.pdfGitHub</span><br>
<span id='abs_en'>English Summary: This paper introduces OSIM, an object-centric evaluation metric for 3D scenes that aligns more closely with human perception by quantifying objectness through object detection models, as validated by user studies and comparative analyses.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2509.09125.pdf' target='_blank'>https://arxiv.org/pdf/2509.09125.pdf</a></span>   <span><a href='https://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liqun He, Jiaqi Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09125">Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This study explores the use of generative AI for automating the classification of tutors' Dialogue Acts (DAs), aiming to reduce the time and effort required by traditional manual coding. This case study uses the open-source CIMA corpus, in which tutors' responses are pre-annotated into four DA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored prompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of 0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and indicating substantial agreement with human annotations. These findings suggest that generative AI has strong potential to provide an efficient and accessible approach to DA classification, with meaningful implications for educational dialogue analysis. The study also highlights the importance of task-specific label definitions and contextual information in enhancing the quality of automated annotation. Finally, it underscores the ethical considerations associated with the use of generative AI and the need for responsible and transparent research practices. The script of this research is publicly available at https://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging.<br>
<span id='abs_ch'>本 究证明生成式AI（尤其是GPT-4）能有效自动分类导师对话行为，其高准确率与人工 注高度一致，为教育对话分析提供了高效的手动编 替代方案。</span><br>
<span id='abs_en'>This study demonstrates that generative AI, particularly GPT-4, can effectively automate the classification of tutors' dialogue acts with high accuracy and substantial agreement with human annotations, offering an efficient alternative to manual coding.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2509.09055.pdf' target='_blank'>https://arxiv.org/pdf/2509.09055.pdf</a></span>   <span><a href='https://github.com/PiyushWithPant/Improving-LLM-Safety-and-Helpfulness-using-SFT-and-DPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Piyush Pant
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09055">Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This research investigates the effectiveness of alignment techniques, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a combined SFT+DPO approach on improving the safety and helpfulness of the OPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset, we train and evaluate four models: the base OPT350M, an SFT model, a DPO model, and a model trained with both SFT and DPO. We introduce three key evaluation metrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined Alignment Score (CAS), all derived from reward model outputs. The results show that while SFT outperforms DPO, The combined SFT+DPO model outperforms all others across all metrics, demonstrating the complementary nature of these techniques. Our findings also highlight challenges posed by noisy data, limited GPU resources, and training constraints. This study offers a comprehensive view of how fine-tuning strategies affect model alignment and provides a foundation for more robust alignment pipelines in future work.<br>
<span id='abs_ch'>中文摘要：本 究表明，结合监督微调（SFT）和直接偏好优化（DPO）的方法在提升语言模型安全性和实用性方面效果最佳，优于单独使用任一技术。English Summary: This study demonstrates that combining Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) yields the best results in enhancing both safety and helpfulness of language models, outperforming either method used individually.Paperid:964,https://arxiv.org/pdf/2509.09014.pdfGitHub</span><br>
<span id='abs_en'>English Summary: This study demonstrates that combining Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) yields the best results in enhancing both safety and helpfulness of language models, outperforming either method used individually.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2509.09009.pdf' target='_blank'>https://arxiv.org/pdf/2509.09009.pdf</a></span>   <span><a href='https://github.com/LAION-AI/open-sci-ref-0.01' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Marianna Nezhurina, JÃ¶rg Franke, Taishi Nakamura, Timur Carstensen, NiccolÃ² Ajroldi, Ville Komulainen, David Salinas, Jenia Jitsev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09009">Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce open-sci-ref, a family of dense transformer models trained as research baselines across multiple model (0.13B to 1.7B parameters) and token scales (up to 1T) on 8 recent open reference datasets. Evaluating the models on various standardized benchmarks, our training runs set establishes reference points that enable researchers to assess the sanity and quality of alternative training approaches across scales and datasets. Intermediate checkpoints allow comparison and studying of the training dynamics. The established reference baselines allow training procedures to be compared through their scaling trends, aligning them on a common compute axis. Comparison of open reference datasets reveals that training on NemoTron-CC HQ consistently outperforms other reference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to intermediate training checkpoints, the release includes logs, code, and downstream evaluations to simplify reproduction, standardize comparison, and facilitate future research.<br>
<span id='abs_ch'>中文: 我们推出了open-sci-ref系列密集Transformer模型，作为跨多尺度和数据集的 究基准，评估显示NemoTron-CC HQ数据集训练效果最佳，并发布了代 和日志以简化复现和促进未来 究。</span><br>
<span id='abs_en'>English: We introduce open-sci-ref, a family of dense transformer models trained as research baselines across multiple scales and datasets, with evaluations showing that training on NemoTron-CC HQ consistently outperforms other datasets, and the release includes code and logs to facilitate reproduction and future research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2509.09004.pdf' target='_blank'>https://arxiv.org/pdf/2509.09004.pdf</a></span>   <span><a href='https://github.com/andrewjackbell/Displacement-INR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Bell, Yan Kit Choi, Steffen E Petersen, Andrew King, Muhummad Sohaib Nazir, Alistair A Young
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09004">Implicit Neural Representations of Intramyocardial Motion and Strain</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Automatic quantification of intramyocardial motion and strain from tagging MRI remains an important but challenging task. We propose a method using implicit neural representations (INRs), conditioned on learned latent codes, to predict continuous left ventricular (LV) displacement -- without requiring inference-time optimisation. Evaluated on 452 UK Biobank test cases, our method achieved the best tracking accuracy (2.14 mm RMSE) and the lowest combined error in global circumferential (2.86%) and radial (6.42%) strain compared to three deep learning baselines. In addition, our method is $\sim$380$\times$ faster than the most accurate baseline. These results highlight the suitability of INR-based models for accurate and scalable analysis of myocardial strain in large CMR datasets. The code can be found at https://github.com/andrewjackbell/Displacement-INR<br>
<span id='abs_ch'>中文: 本 究提出了一种基于隐式神经表示的方法，用于从 记MRI中精确量化左心室运动，在英国生物银行数据上实现了卓越的跟踪精度和效率。</span><br>
<span id='abs_en'>English: This study introduces a method using implicit neural representations to accurately quantify left ventricular motion from tagging MRI, achieving superior tracking accuracy and efficiency on UK Biobank data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2509.08897.pdf' target='_blank'>https://arxiv.org/pdf/2509.08897.pdf</a></span>   <span><a href='https://github.com/aimagelab/ReT-2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Davide Caffagni, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08897">Recurrence Meets Transformers for Universal Multimodal Retrieval</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the rapid advancement of multimodal retrieval and its application in LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged. Existing methods predominantly rely on task-specific fine-tuning of vision-language models and are limited to single-modality queries or documents. In this paper, we propose ReT-2, a unified retrieval model that supports multimodal queries, composed of both images and text, and searches across multimodal document collections where text and images coexist. ReT-2 leverages multi-layer representations and a recurrent Transformer architecture with LSTM-inspired gating mechanisms to dynamically integrate information across layers and modalities, capturing fine-grained visual and textual details. We evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different retrieval configurations. Results demonstrate that ReT-2 consistently achieves state-of-the-art performance across diverse settings, while offering faster inference and reduced memory usage compared to prior approaches. When integrated into retrieval-augmented generation pipelines, ReT-2 also improves downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source code and trained models are publicly available at: https://github.com/aimagelab/ReT-2<br>
<span id='abs_ch'>中文: ReT-2是一种统一的多模态检索模型，采用带门控机制的循环Transformer动态整合跨模态信息，在多个基准测试中实现最优性能，同时提升效率并改善下游任务表现。</span><br>
<span id='abs_en'>English: ReT-2 is a unified multimodal retrieval model that employs a recurrent Transformer with gating mechanisms to dynamically integrate cross-modal information, achieving state-of-the-art performance across benchmarks while enhancing efficiency and downstream task results.Paperid:973,https://arxiv.org/pdf/2509.08833.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>565, <a href='https://arxiv.org/pdf/2509.08827.pdf' target='_blank'>https://arxiv.org/pdf/2509.08827.pdf</a></span>   <span><a href='https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, Xuekai Zhu, Ermo Hua, Yihao Liu, Zonglin Li, Huayu Chen, Xiaoye Qu, Yafu Li, Weize Chen, Zhenzhao Yuan, Junqi Gao, Dong Li, Zhiyuan Ma, Ganqu Cui, Zhiyuan Liu, Biqing Qi, Ning Ding, Bowen Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08827">A Survey of Reinforcement Learning for Large Reasoning Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs<br>
<span id='abs_ch'>中文: 本文综述了强化学 在增强大语言模型推理能力方面的最新进展，探讨了实现人工超智能所面临的挑战与未来发展方向。English: This paper surveys recent advances in using Reinforcement Learning to enhance reasoning capabilities in Large Language Models, examining challenges and future directions toward achieving Artificial SuperIntelligence.Paperid:977,https://arxiv.org/pdf/2509.08812.pdfGitHub</span><br>
<span id='abs_en'>English: This paper surveys recent advances in using Reinforcement Learning to enhance reasoning capabilities in Large Language Models, examining challenges and future directions toward achieving Artificial SuperIntelligence.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>566, <a href='https://arxiv.org/pdf/2509.08812.pdf' target='_blank'>https://arxiv.org/pdf/2509.08812.pdf</a></span>   <span><a href='https://github.com/hailaykidu/MoVoC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hailay Kidu Teklehaymanot, Dren Fazlija, Wolfgang Nejdl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08812">MoVoC: Morphology-Aware Subword Construction for Geez Script Languages</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Subword-based tokenization methods often fail to preserve morphological boundaries, a limitation especially pronounced in low-resource, morphologically complex languages such as those written in the Geez script. To address this, we present MoVoC (Morpheme-aware Subword Vocabulary Construction) and train MoVoC-Tok, a tokenizer that integrates supervised morphological analysis into the subword vocabulary. This hybrid segmentation approach combines morpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological integrity while maintaining lexical meaning. To tackle resource scarcity, we curate and release manually annotated morpheme data for four Geez script languages and a morpheme-aware vocabulary for two of them. While the proposed tokenization method does not lead to significant gains in automatic translation quality, we observe consistent improvements in intrinsic metrics, MorphoScore, and Boundary Precision, highlighting the value of morphology-aware segmentation in enhancing linguistic fidelity and token efficiency. Our morpheme-annotated datasets and tokenizer will be publicly available to support further research in low-resource, morphologically rich languages. Our code and data are available on GitHub: https://github.com/hailaykidu/MoVoC<br>
<span id='abs_ch'>中文：MoVoC分词器将形态学分析与子词分割相结合，以保持 厄兹文字语言的词法结构，尽管在翻译质量上提升有限，但在形态学评估指 上展现出持续改进。</span><br>
<span id='abs_en'>English: The MoVoC tokenizer integrates morphological analysis with subword segmentation to preserve linguistic structure in Geez script languages, demonstrating improved morphological metrics despite limited translation gains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>567, <a href='https://arxiv.org/pdf/2509.08755.pdf' target='_blank'>https://arxiv.org/pdf/2509.08755.pdf</a></span>   <span><a href='https://github.com/woooodyy/AgentGym,' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/woooodyy/AgentGym-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiheng Xi, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, Wei He, Yiwen Ding, Guanyu Li, Zehui Chen, Zhengyin Du, Xuesong Yao, Yufei Xu, Jiecao Chen, Tao Gui, Zuxuan Wu, Qi Zhang, Xuanjing Huang, Yu-Gang Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08755">AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch -- without relying on supervised fine-tuning (SFT) -- across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interactions, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and the ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source the complete AgentGym-RL framework -- including code and datasets -- to empower the research community in developing the next generation of intelligent agents.<br>
<span id='abs_ch'>中文: AgentGym-RL框架作为一个统一的强化学 平台，通过ScalingInter-RL训练方法在多 化环境中从头训练自主LLM智能体，在平衡探索与利用的同时，在多项任务中展现出卓越性能。</span><br>
<span id='abs_en'>English: The AgentGym-RL framework is introduced as a unified reinforcement learning platform that trains autonomous LLM agents from scratch across diverse environments, incorporating the ScalingInter-RL approach to balance exploration and exploitation while demonstrating superior performance on multiple tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>568, <a href='https://arxiv.org/pdf/2509.08729.pdf' target='_blank'>https://arxiv.org/pdf/2509.08729.pdf</a></span>   <span><a href='https://github.com/hyunjun1121/M2S-x-teaming' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunjun Kim, Junwoo Ha, Sangyoon Yu, Haon Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08729">X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one structured prompt, but prior work relied on a handful of manually written templates. We present X-Teaming Evolutionary M2S, an automated framework that discovers and optimizes M2S templates through language-model-guided evolution. The system pairs smart sampling from 12 sources with an LLM-as-judge inspired by StrongREJECT and records fully auditable logs. Maintaining selection pressure by setting the success threshold to $θ= 0.70$, we obtain five evolutionary generations, two new template families, and 44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of 2,500 trials (judge fixed) shows that structural gains transfer but vary by target; two models score zero at the same threshold. We also find a positive coupling between prompt length and score, motivating length-aware judging. Our results demonstrate that structure-level search is a reproducible route to stronger single-turn probes and underscore the importance of threshold calibration and cross-model evaluation. Code, configurations, and artifacts are available at https://github.com/hyunjun1121/M2S-x-teaming.<br>
<span id='abs_ch'>中文: X-Teaming Evolutionary M2S通过语言模型引导的进化自动发现并优化多轮转单轮模板，在GPT-4.1上实现44.8%的成功率，证明结构改进可跨模型迁移，同时强调阈值 准与跨模型评估的重要性。</span><br>
<span id='abs_en'>English: X-Teaming Evolutionary M2S automates the discovery and optimization of multi-turn-to-single-turn templates through language-model-guided evolution, achieving 44.8% success on GPT-4.1 and demonstrating that structural improvements transfer across models while highlighting the need for threshold calibration and cross-model evaluation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>569, <a href='https://arxiv.org/pdf/2509.08699.pdf' target='_blank'>https://arxiv.org/pdf/2509.08699.pdf</a></span>   <span><a href='https://github.com/podgorki/TANGO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefan Podgorski, Sourav Garg, Mehdi Hosseinzadeh, Lachlan Mares, Feras Dayoub, Ian Reid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08699">TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Visual navigation in robotics traditionally relies on globally-consistent 3D maps or learned controllers, which can be computationally expensive and difficult to generalize across diverse environments. In this work, we present a novel RGB-only, object-level topometric navigation pipeline that enables zero-shot, long-horizon robot navigation without requiring 3D maps or pre-trained controllers. Our approach integrates global topological path planning with local metric trajectory control, allowing the robot to navigate towards object-level sub-goals while avoiding obstacles. We address key limitations of previous methods by continuously predicting local trajectory using monocular depth and traversability estimation, and incorporating an auto-switching mechanism that falls back to a baseline controller when necessary. The system operates using foundational models, ensuring open-set applicability without the need for domain-specific fine-tuning. We demonstrate the effectiveness of our method in both simulated environments and real-world tests, highlighting its robustness and deployability. Our approach outperforms existing state-of-the-art methods, offering a more adaptable and effective solution for visual navigation in open-set environments. The source code is made publicly available: https://github.com/podgorki/TANGO.<br>
<span id='abs_ch'>中文摘要：本 究提出了一种仅使用RGB图像的物体级拓扑导航系统， 需3D地图或预训练控制器即可实现零 本长距离机器人导航，通过全局路径规划与局部轨迹控制的结合，在开放环境中展现出优于现有方法的适应性和有效性。</span><br>
<span id='abs_en'>English Summary: This study introduces a novel RGB-only, object-level topometric navigation system that enables zero-shot, long-range robot navigation without relying on 3D maps or pre-trained controllers, outperforming existing methods through integrated global planning and local control with open-set applicability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>570, <a href='https://arxiv.org/pdf/2509.08463.pdf' target='_blank'>https://arxiv.org/pdf/2509.08463.pdf</a></span>   <span><a href='https://github.com/FanzhenLiu/Awesome-Automated-Fact-Checking-Attacks' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fanzhen Liu, Alsharif Abuadbba, Kristen Moore, Surya Nepal, Cecile Paris, Jia Wu, Jian Yang, Quan Z. Sheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08463">Adversarial Attacks Against Automated Fact-Checking: A Survey</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In an era where misinformation spreads freely, fact-checking (FC) plays a crucial role in verifying claims and promoting reliable information. While automated fact-checking (AFC) has advanced significantly, existing systems remain vulnerable to adversarial attacks that manipulate or generate claims, evidence, or claim-evidence pairs. These attacks can distort the truth, mislead decision-makers, and ultimately undermine the reliability of FC models. Despite growing research interest in adversarial attacks against AFC systems, a comprehensive, holistic overview of key challenges remains lacking. These challenges include understanding attack strategies, assessing the resilience of current models, and identifying ways to enhance robustness. This survey provides the first in-depth review of adversarial attacks targeting FC, categorizing existing attack methodologies and evaluating their impact on AFC systems. Additionally, we examine recent advancements in adversary-aware defenses and highlight open research questions that require further exploration. Our findings underscore the urgent need for resilient FC frameworks capable of withstanding adversarial manipulations in pursuit of preserving high verification accuracy.<br>
<span id='abs_ch'>中文摘要：本综述首次系统梳理针对事实 查系统的对抗性攻击，分类评估攻击方法及防御机制，强调构建抗干扰 查框架对保障信息验证准确性的紧迫需求。</span><br>
<span id='abs_en'>English Summary: This survey comprehensively reviews adversarial attacks on automated fact-checking systems, analyzing attack methodologies and defenses while highlighting the critical need for more resilient frameworks to maintain verification accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>571, <a href='https://arxiv.org/pdf/2509.08269.pdf' target='_blank'>https://arxiv.org/pdf/2509.08269.pdf</a></span>   <span><a href='https://github.com/ishmael233/LLM4OPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yisong Zhang, Ran Cheng, Guoxing Yi, Kay Chen Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08269">A Systematic Survey on Large Language Models for Evolutionary Optimization: From Modeling to Solving</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs), with their strong understanding and reasoning capabilities, are increasingly being explored for tackling optimization problems, especially in synergy with evolutionary computation. Despite rapid progress, however, the field still lacks a unified synthesis and a systematic taxonomy. This survey addresses this gap by providing a comprehensive review of recent developments and organizing them within a structured framework. We classify existing research into two main stages: LLMs for optimization modeling and LLMs for optimization solving. The latter is further divided into three paradigms according to the role of LLMs in the optimization workflow: LLMs as stand-alone optimizers, low-level LLMs embedded within optimization algorithms, and high-level LLMs for algorithm selection and generation. For each category, we analyze representative methods, distill technical challenges, and examine their interplay with traditional approaches. We also review interdisciplinary applications spanning the natural sciences, engineering, and machine learning. By contrasting LLM-driven and conventional methods, we highlight key limitations and research gaps, and point toward future directions for developing self-evolving agentic ecosystems for optimization. An up-to-date collection of related literature is maintained at https://github.com/ishmael233/LLM4OPT.<br>
<span id='abs_ch'>中文: 本综述系统梳理了大语言模型在优化问题中的应用，将其分为建模与求解两大阶段，并按照LLMs在优化流程中的角色细分为三种范式，同时分析了与 统方法的结合及未来 究方向。</span><br>
<span id='abs_en'>English: This survey comprehensively reviews how Large Language Models (LLMs) are applied to optimization problems, categorizing their roles in modeling and solving, and analyzing their integration with evolutionary computation while highlighting future research directions.Paperid:1000,https://arxiv.org/pdf/2509.08265.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>572, <a href='https://arxiv.org/pdf/2509.08104.pdf' target='_blank'>https://arxiv.org/pdf/2509.08104.pdf</a></span>   <span><a href='https://github.com/apm-loss/apml' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sasan Sharifipour, Constantino Ãlvarez Casado, Mohammad Sabokrou, Miguel Bordallo LÃ³pez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.08104">APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Training deep learning models for point cloud prediction tasks such as shape completion and generation depends critically on loss functions that measure discrepancies between predicted and ground-truth point sets. Commonly used functions such as Chamfer Distance (CD), HyperCD, and InfoCD rely on nearest-neighbor assignments, which often induce many-to-one correspondences, leading to point congestion in dense regions and poor coverage in sparse regions. These losses also involve non-differentiable operations due to index selection, which may affect gradient-based optimization. Earth Mover Distance (EMD) enforces one-to-one correspondences and captures structural similarity more effectively, but its cubic computational complexity limits its practical use. We propose the Adaptive Probabilistic Matching Loss (APML), a fully differentiable approximation of one-to-one matching that leverages Sinkhorn iterations on a temperature-scaled similarity matrix derived from pairwise distances. We analytically compute the temperature to guarantee a minimum assignment probability, eliminating manual tuning. APML achieves near-quadratic runtime, comparable to Chamfer-based losses, and avoids non-differentiable operations. When integrated into state-of-the-art architectures (PoinTr, PCN, FoldingNet) on ShapeNet benchmarks and on a spatiotemporal Transformer (CSI2PC) that generates 3D human point clouds from WiFi CSI measurements, APM loss yields faster convergence, superior spatial distribution, especially in low-density regions, and improved or on-par quantitative performance without additional hyperparameter search. The code is available at: https://github.com/apm-loss/apml.<br>
<span id='abs_ch'>中文摘要：提出的自适应概率匹配损失（APML）通过可微分且计算高效的近似一对一匹配方法，克服了现有点云损失函数的局限性，在多种架构和基准测试中实现了更优性能。</span><br>
<span id='abs_en'>English Summary: The proposed Adaptive Probabilistic Matching Loss (APML) overcomes limitations of existing point cloud loss functions by providing a differentiable, computationally efficient approximation of one-to-one matching, achieving superior performance across various architectures and benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>573, <a href='https://arxiv.org/pdf/2509.07969.pdf' target='_blank'>https://arxiv.org/pdf/2509.07969.pdf</a></span>   <span><a href='https://github.com/Mini-o3/Mini-o3' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Lai, Junyi Li, Wei Li, Tao Liu, Tianjian Li, Hengshuang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07969">Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems.<br>
<span id='abs_ch'>Chinese: 近期大型多模态模型的进展使Mini-o3系统通过数十步的深度多轮推理，在复杂视觉搜索任务中实现最优性能，解决了现有方法推理模式单一和交互轮次有限的问题。</span><br>
<span id='abs_en'>English: Recent advances in large multimodal models have enabled Mini-o3 to achieve state-of-the-art performance on challenging visual search tasks through deep, multi-turn reasoning spanning tens of steps, addressing limitations of monotonous reasoning and limited interaction turns in existing approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>574, <a href='https://arxiv.org/pdf/2509.07925.pdf' target='_blank'>https://arxiv.org/pdf/2509.07925.pdf</a></span>   <span><a href='https://github.com/ODYSSEYWT/GUQ' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tuo Wang, Adithya Kulkarni, Tyler Cody, Peter A. Beling, Yujun Yan, Dawei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07925">GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Uncertainty estimation is essential for enhancing the reliability of Large Language Models (LLMs), particularly in high-stakes applications. Existing methods often overlook semantic dependencies, relying on token-level probability measures that fail to capture structural relationships within the generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty Estimation for Large Language Models, a structure-aware framework that leverages dependency parse trees and hierarchical graph pooling to refine uncertainty quantification. By incorporating supervised learning, GENUINE effectively models semantic and structural relationships, improving confidence assessments. Extensive experiments across NLP tasks show that GENUINE achieves up to 29% higher AUROC than semantic entropy-based approaches and reduces calibration errors by over 15%, demonstrating the effectiveness of graph-based uncertainty modeling. The code is available at https://github.com/ODYSSEYWT/GUQ.<br>
<span id='abs_ch'>Chinese: GENUINE提出了一种基于图增强的大语言模型不确定性估计框架，通过依赖解析 和分层 化建模语义关系，相比现有方法将AUROC提升高达29%，并降低超过15%的 准误差。</span><br>
<span id='abs_en'>English: GENUINE introduces a graph-enhanced uncertainty estimation framework for LLMs that leverages dependency parse trees and hierarchical pooling to model semantic relationships, achieving up to 29% higher AUROC and reducing calibration errors by over 15% compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>575, <a href='https://arxiv.org/pdf/2509.07894.pdf' target='_blank'>https://arxiv.org/pdf/2509.07894.pdf</a></span>   <span><a href='https://github.com/SciYu/HiPhO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangchen Yu, Haiyuan Wan, Qianjia Cheng, Yuchen Zhang, Jiacheng Chen, Fujun Han, Yulun Wu, Junchi Yao, Ruilizhen Hu, Ning Ding, Yu Cheng, Tao Chen, Lei Bai, Dongzhan Zhou, Yun Luo, Ganqu Cui, Peng Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07894">HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics Olympiad Benchmark?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, the physical capabilities of (M)LLMs have garnered increasing attention. However, existing benchmarks for physics suffer from two major gaps: they neither provide systematic and up-to-date coverage of real-world physics competitions such as physics Olympiads, nor enable direct performance comparison with humans. To bridge these gaps, we present HiPhO, the first benchmark dedicated to high school physics Olympiads with human-aligned evaluation. Specifically, HiPhO highlights three key innovations. (1) Comprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025, spanning both international and regional competitions, and covering mixed modalities that encompass problems spanning text-only to diagram-based. (2) Professional Evaluation: We adopt official marking schemes to perform fine-grained grading at both the answer and step level, fully aligned with human examiners to ensure high-quality and domain-specific evaluation. (3) Comparison with Human Contestants: We assign gold, silver, and bronze medals to models based on official medal thresholds, thereby enabling direct comparison between (M)LLMs and human contestants. Our large-scale evaluation of 30 state-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly remain at or below the bronze level; open-source LLMs show promising progress with multiple golds; closed-source reasoning MLLMs can achieve 6 to 12 gold medals; and most models still have a significant gap from full marks. These results highlight the performance gap between open-source models and top students, the strong reasoning abilities of closed-source models, and the remaining room for improvement. HiPhO, a human-aligned Olympiad benchmark for multimodal physical reasoning, is open-source at https://github.com/SciYu/HiPhO with a public leaderboard at https://phyarena.github.io/.<br>
<span id='abs_ch'>中文: HiPhO推出了首个针对高中物理奥林匹克竞赛的基准测试，具备全面数据、专业人工对齐评估及模型与人类表现直接对比功能，揭示了开源模型与顶尖学生的显著差距，同时凸显了闭源模型强大的推理能力。</span><br>
<span id='abs_en'>English: HiPhO introduces the first benchmark for high school physics Olympiads, featuring comprehensive data, professional human-aligned evaluation, and direct model-to-human performance comparisons, revealing significant gaps between open-source models and top students while highlighting closed-source models' strong reasoning capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>576, <a href='https://arxiv.org/pdf/2509.07558.pdf' target='_blank'>https://arxiv.org/pdf/2509.07558.pdf</a></span>   <span><a href='https://github.com/zerolllin/Delta-L-Normalization' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan He, Xufang Luo, Yike Zhang, Yuqing Yang, Lili Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07558">$ÎL$ Normalization: Rethink Loss Aggregation in RLVR</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We propose $ÎL$ Normalization, a simple yet effective loss aggregation method tailored to the characteristic of dynamic generation lengths in Reinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has demonstrated strong potential in improving the reasoning capabilities of large language models (LLMs), but a major challenge lies in the large variability of response lengths during training, which leads to high gradient variance and unstable optimization. Although previous methods such as GRPO, DAPO, and Dr. GRPO introduce different loss normalization terms to address this issue, they either produce biased estimates or still suffer from high gradient variance. By analyzing the effect of varying lengths on policy loss both theoretically and empirically, we reformulate the problem as finding a minimum-variance unbiased estimator. Our proposed $ÎL$ Normalization not only provides an unbiased estimate of the true policy loss but also minimizes gradient variance in theory. Extensive experiments show that it consistently achieves superior results across different model sizes, maximum lengths, and tasks. Our code will be made public at https://github.com/zerolllin/Delta-L-Normalization.<br>
<span id='abs_ch'>中文摘要：本文提出ΔL归一化方法，通过解决强化学 可验证奖励训练中响应长度变化导致的梯度方差问题，提供 偏估计并实现稳定优化，在多种实验设置下均取得优异性能。</span><br>
<span id='abs_en'>English Summary: The paper introduces ΔL Normalization, an unbiased loss aggregation method that minimizes gradient variance in RLVR training by addressing variable response lengths, achieving superior performance across diverse settings.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>577, <a href='https://arxiv.org/pdf/2509.07159.pdf' target='_blank'>https://arxiv.org/pdf/2509.07159.pdf</a></span>   <span><a href='https://github.com/PaVeRL-SQL/PaVeRL-SQL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Heng Hao, Wenjun Hu, Oxana Verkholyak, Davoud Ataee Tarzanagh, Baruch Gutow, Sima Didari, Masoud Faraki, Hankyu Moon, Seungjai Min
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07159">PaVeRL-SQL: Text-to-SQL via Partial-Match Rewards and Verbal Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text-to-SQL models allow users to interact with a database more easily by generating executable SQL statements from natural-language questions. Despite recent successes on simpler databases and questions, current Text-to-SQL methods still suffer from low execution accuracy on industry-scale databases and complex questions involving domain-specific business logic. We present \emph{PaVeRL-SQL}, a framework that combines \emph{Partial-Match Rewards} and \emph{Verbal Reinforcement Learning} to drive self-improvement in reasoning language models (RLMs) for Text-to-SQL. To handle practical use cases, we adopt two pipelines: (1) a newly designed in-context learning framework with group self-evaluation (verbal-RL), using capable open- and closed-source large language models (LLMs) as backbones; and (2) a chain-of-thought (CoT) RL pipeline with a small backbone model (OmniSQL-7B) trained with a specially designed reward function and two-stage RL. These pipelines achieve state-of-the-art (SOTA) results on popular Text-to-SQL benchmarks -- Spider, Spider 2.0, and BIRD. For the industrial-level Spider2.0-SQLite benchmark, the verbal-RL pipeline achieves an execution accuracy 7.4\% higher than SOTA, and the CoT pipeline is 1.4\% higher. RL training with mixed SQL dialects yields strong, threefold gains, particularly for dialects with limited training data. Overall, \emph{PaVeRL-SQL} delivers reliable, SOTA Text-to-SQL under realistic industrial constraints. The code is available at https://github.com/PaVeRL-SQL/PaVeRL-SQL.<br>
<span id='abs_ch'>中文：PaVeRL-SQL框架通过结合部分匹配奖励和语言强化学 ，有效提升了工业级复杂数据库的Text-to-SQL性能，在主流基准测试中取得最优结果。</span><br>
<span id='abs_en'>English: The PaVeRL-SQL framework enhances Text-to-SQL performance for complex industrial databases by integrating partial-match rewards and verbal reinforcement learning, achieving state-of-the-art results on major benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>578, <a href='https://arxiv.org/pdf/2509.07142.pdf' target='_blank'>https://arxiv.org/pdf/2509.07142.pdf</a></span>   <span><a href='https://github.com/zhiyintan/topic-model-LLMjudgment' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyin Tan, Jennifer D'Souza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07142">Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This study presents a framework for automated evaluation of dynamically evolving topic models using Large Language Models (LLMs). Topic modeling is essential for organizing and retrieving scholarly content in digital library systems, helping users navigate complex and evolving knowledge domains. However, widely used automated metrics, such as coherence and diversity, often capture only narrow statistical patterns and fail to explain semantic failures in practice. We introduce a purpose-oriented evaluation framework that employs nine LLM-based metrics spanning four key dimensions of topic quality: lexical validity, intra-topic semantic soundness, inter-topic structural soundness, and document-topic alignment soundness. The framework is validated through adversarial and sampling-based protocols, and is applied across datasets spanning news articles, scholarly publications, and social media posts, as well as multiple topic modeling methods and open-source LLMs. Our analysis shows that LLM-based metrics provide interpretable, robust, and task-relevant assessments, uncovering critical weaknesses in topic models such as redundancy and semantic drift, which are often missed by traditional metrics. These results support the development of scalable, fine-grained evaluation tools for maintaining topic relevance in dynamic datasets. All code and data supporting this work are accessible at https://github.com/zhiyintan/topic-model-LLMjudgment.<br>
<span id='abs_ch'>中文摘要：本 究提出了一种基于大语言模型的主题模型自动评估框架，通过可解释的多维度指 弥补 统方法的不足，有效识别主题冗余和语义偏移等关键缺陷。English Summary: This study introduces an LLM-based framework for automated topic model evaluation, addressing the limitations of traditional metrics by providing interpretable, multi-dimensional assessments that reveal critical weaknesses like redundancy and semantic drift.Paperid:1039,https://arxiv.org/pdf/2509.07115.pdfGitHub</span><br>
<span id='abs_en'>English Summary: This study introduces an LLM-based framework for automated topic model evaluation, addressing the limitations of traditional metrics by providing interpretable, multi-dimensional assessments that reveal critical weaknesses like redundancy and semantic drift.Paperid:1039,https://arxiv.org/pdf/2509.07115.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>579, <a href='https://arxiv.org/pdf/2509.07115.pdf' target='_blank'>https://arxiv.org/pdf/2509.07115.pdf</a></span>   <span><a href='https://github.com/GitZH-Chen/GyroBN.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziheng Chen, Xiao-Jun Wu, Bernhard SchÃ¶lkopf, Nicu Sebe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07115">Riemannian Batch Normalization: A Gyro Approach</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Normalization layers are crucial for deep learning, but their Euclidean formulations are inadequate for data on manifolds. On the other hand, many Riemannian manifolds in machine learning admit gyro-structures, enabling principled extensions of Euclidean neural networks to non-Euclidean domains. Inspired by this, we introduce GyroBN, a principled Riemannian batch normalization framework for gyrogroups. We establish two necessary conditions, namely \emph{pseudo-reduction} and \emph{gyroisometric gyrations}, that guarantee GyroBN with theoretical control over sample statistics, and show that these conditions hold for all known gyrogroups in machine learning. Our framework also incorporates several existing Riemannian normalization methods as special cases. We further instantiate GyroBN on seven representative geometries, including the Grassmannian, five constant curvature spaces, and the correlation manifold, and derive novel gyro and Riemannian structures to enable these instantiations. Experiments across these geometries demonstrate the effectiveness of GyroBN. The code is available at https://github.com/GitZH-Chen/GyroBN.git.<br>
<br>
<div id='section'>Paperid: <span id='pid'>580, <a href='https://arxiv.org/pdf/2509.07103.pdf' target='_blank'>https://arxiv.org/pdf/2509.07103.pdf</a></span>   <span><a href='https://github.com/schwallergroup/lmkan' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sergey Pozdnyakov, Philippe Schwaller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07103">Lookup multivariate Kolmogorov-Arnold Networks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>High-dimensional linear mappings, or linear layers, dominate both the parameter count and the computational cost of most modern deep-learning models. We introduce a general drop-in replacement, lookup multivariate Kolmogorov-Arnold Networks (lmKANs), which deliver a substantially better trade-off between capacity and inference cost. Our construction expresses a general high-dimensional mapping through trainable low-dimensional multivariate functions. These functions can carry dozens or hundreds of trainable parameters each, and yet it takes only a few multiplications to compute them because they are implemented as spline lookup tables. Empirically, lmKANs reduce inference FLOPs by up to 6.0x while matching the flexibility of MLPs in general high-dimensional function approximation. In another feedforward fully connected benchmark, on the tabular-like dataset of randomly displaced methane configurations, lmKANs enable more than 10x higher H100 throughput at equal accuracy. Within frameworks of Convolutional Neural Networks, lmKAN-based CNNs cut inference FLOPs at matched accuracy by 1.6-2.1x and by 1.7x on the CIFAR-10 and ImageNet-1k datasets, respectively. Our code, including dedicated CUDA kernels, is available online at https://github.com/schwallergroup/lmkan.<br>
<span id='abs_ch'>中文：提出的查找多元柯尔莫哥洛夫-阿诺德网络（lmKANs）通过显著降低计算成本，同时在多个基准测试中保持或提升模型性能，为 统线性层提供了更优的替代方案。English: The proposed lookup multivariate Kolmogorov-Arnold Networks (lmKANs) provide a superior alternative to traditional linear layers by significantly reducing computational costs while maintaining or enhancing model performance across various benchmarks.Paperid:1041,https://arxiv.org/pdf/2509.07006.pdfGitHub</span><br>
<span id='abs_en'>English: The proposed lookup multivariate Kolmogorov-Arnold Networks (lmKANs) provide a superior alternative to traditional linear layers by significantly reducing computational costs while maintaining or enhancing model performance across various benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>581, <a href='https://arxiv.org/pdf/2509.07006.pdf' target='_blank'>https://arxiv.org/pdf/2509.07006.pdf</a></span>   <span><a href='https://github.com/Principled-Evolution/argen-demo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kapil Madan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07006">ArGen: Auto-Regulation of Generative AI via GRPO and Policy-as-Code</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper introduces ArGen (Auto-Regulation of Generative AI systems), a framework for aligning Large Language Models (LLMs) with complex sets of configurable, machine-readable rules spanning ethical principles, operational safety protocols, and regulatory compliance standards. Moving beyond just preference-based alignment, ArGen is designed to ensure LLMs adhere to these multifaceted policies through a novel synthesis of principle-based automated reward scoring, Group Relative Policy Optimisation (GRPO), and an Open Policy Agent (OPA) inspired governance layer. This approach provides the technical foundation for achieving and demonstrating compliance with diverse and nuanced governance requirements. To showcase the framework's capability to operationalize a deeply nuanced and culturally-specific value system, we present an in-depth case study: the development of a medical AI assistant guided by principles from Dharmic ethics (such as Ahimsa and Dharma), as derived from texts like the Bhagavad Gita. This challenging application demonstrates ArGen's adaptability, achieving a 70.9% improvement in domain-scope adherence over the baseline. Through our open-source repository, we show that ArGen's methodology offers a path to 'Governable Al' systems that are technically proficient, ethically robust, and verifiably compliant for safe deployment in diverse global contexts.<br>
<span id='abs_ch'>中文: ArGen框架通过自动奖励评分、GRPO和治理层，使大型语言模型遵循复杂可配置的伦理、安全和法规规则，并以基于达摩伦理的医疗AI案例展示了70.9%的领域依从性提升。</span><br>
<span id='abs_en'>English: ArGen is a framework that aligns Large Language Models with complex, configurable rules for ethical, safety, and regulatory compliance through automated reward scoring, GRPO, and a governance layer, demonstrating a 70.9% improvement in adherence via a case study on a medical AI guided by Dharmic ethics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>582, <a href='https://arxiv.org/pdf/2509.06988.pdf' target='_blank'>https://arxiv.org/pdf/2509.06988.pdf</a></span>   <span><a href='https://github.com/Aie0923/ClaFR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingsheng Wang, Shuo Lu, Jian Liang, Aihua Zheng, Ran He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06988">Frustratingly Easy Feature Reconstruction for Out-of-Distribution Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Out-of-distribution (OOD) detection helps models identify data outside the training categories, crucial for security applications. While feature-based post-hoc methods address this by evaluating data differences in the feature space without changing network parameters, they often require access to training data, which may not be suitable for some data privacy scenarios. This may not be suitable in scenarios where data privacy protection is a concern. In this paper, we propose a simple yet effective post-hoc method, termed Classifier-based Feature Reconstruction (ClaFR), from the perspective of subspace projection. It first performs an orthogonal decomposition of the classifier's weights to extract the class-known subspace, then maps the original data features into this subspace to obtain new data representations. Subsequently, the OOD score is determined by calculating the feature reconstruction error of the data within the subspace. Compared to existing OOD detection algorithms, our method does not require access to training data while achieving leading performance on multiple OOD benchmarks. Our code is released at https://github.com/Aie0923/ClaFR.<br>
<span id='abs_ch'>Chinese: 提出的基于分类器的特征重构方法通过子空间投影和特征重构误差，在不访问训练数据的情况下实现分布外检测，既保护了数据隐私又达到了领先性能。</span><br>
<span id='abs_en'>English: The proposed Classifier-based Feature Reconstruction (ClaFR) method enables out-of-distribution detection without accessing training data by utilizing subspace projection and feature reconstruction error, achieving state-of-the-art performance while addressing privacy concerns.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>583, <a href='https://arxiv.org/pdf/2509.06986.pdf' target='_blank'>https://arxiv.org/pdf/2509.06986.pdf</a></span>   <span><a href='https://github.com/CellPainTR/CellPainTR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cedric Caruzzo, Jong Chul Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06986">CellPainTR: Generalizable Representation Learning for Cross-Dataset Cell Painting Analysis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large-scale biological discovery requires integrating massive, heterogeneous datasets like those from the JUMP Cell Painting consortium, but technical batch effects and a lack of generalizable models remain critical roadblocks. To address this, we introduce CellPainTR, a Transformer-based architecture designed to learn foundational representations of cellular morphology that are robust to batch effects. Unlike traditional methods that require retraining on new data, CellPainTR's design, featuring source-specific context tokens, allows for effective out-of-distribution (OOD) generalization to entirely unseen datasets without fine-tuning. We validate CellPainTR on the large-scale JUMP dataset, where it outperforms established methods like ComBat and Harmony in both batch integration and biological signal preservation. Critically, we demonstrate its robustness through a challenging OOD task on the unseen Bray et al. dataset, where it maintains high performance despite significant domain and feature shifts. Our work represents a significant step towards creating truly foundational models for image-based profiling, enabling more reliable and scalable cross-study biological analysis.<br>
<span id='abs_ch'>中文摘要：为解决批次效应问题，我们开发了基于Transformer的CellPainTR模型，它能学 通用的细胞形态表征， 需重新训练即可实现优异的批次整合和跨数据集泛化能力。</span><br>
<span id='abs_en'>English Summary: To overcome batch effects and enable robust biological discovery, we developed CellPainTR, a Transformer model that learns generalized cellular morphology representations, achieving superior batch integration and out-of-distribution generalization without retraining.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>584, <a href='https://arxiv.org/pdf/2509.06980.pdf' target='_blank'>https://arxiv.org/pdf/2509.06980.pdf</a></span>   <span><a href='https://github.com/Simple-Efficient/RL-Factory' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajun Chai, Guojun Yin, Zekun Xu, Chuhuai Yue, Yi Jia, Siyu Xia, Xiaohan Wang, Jiwen Jiang, Xiaoguang Li, Chengqi Dong, Hang He, Wei Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06980">RLFactory: A Plug-and-Play Reinforcement Learning Post-Training Framework for LLM Multi-Turn Tool-Use</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models excel at basic reasoning but struggle with tasks that require interaction with external tools. We present RLFactory, a plug-and-play reinforcement learning post-training framework for multi-round tool use. RLFactory tackles (i) tool-call stability and adaptability amid tool heterogeneity and interface issues via an asyncio-based asynchronous caller and a decoupled tool/training architecture, and (ii) diverse evaluation needs via a reward layer supporting rule-based, model-judgment, and tool-verification signals. It reconstructs the MDP by introducing observation markers from tool feedback, closing the loop among model, tools, and environment, and implements a generate-parse-invoke-update workflow for dynamic policy optimization. On Search-R1 with Qwen3-4B, RLFactory achieves a 0.486 test score on the Natural Questions (NQ) dataset, surpassing larger models trained with similar techniques (e.g., Qwen2.5-7B-Instruct-GRPO at 0.473), and increases training throughput by 6.8x. RLFactory provides a low-barrier, highly adaptable framework for strengthening multi-round tool use of LLMs in real-world scenarios. Code: https://github.com/Simple-Efficient/RL-Factory.<br>
<span id='abs_ch'>中文：RLFactory是一个即插即用的强化学 框架，通过异步调用器和解耦架构提升大语言模型在多轮工具使用中的稳定性和适应性，并利用灵活奖励层支持多 化评估，在基准测试中实现了更优的性能和效率。English: RLFactory is a plug-and-play reinforcement learning framework that enhances large language models' multi-round tool use by improving tool-call stability and adaptability through asynchronous calling and a decoupled architecture, while supporting diverse evaluations with a flexible reward layer, achieving superior performance and efficiency on benchmark tests.Paperid:1045,https://arxiv.org/pdf/2509.06977.pdfGitHub</span><br>
<span id='abs_en'>English: RLFactory is a plug-and-play reinforcement learning framework that enhances large language models' multi-round tool use by improving tool-call stability and adaptability through asynchronous calling and a decoupled architecture, while supporting diverse evaluations with a flexible reward layer, achieving superior performance and efficiency on benchmark tests.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>585, <a href='https://arxiv.org/pdf/2509.06977.pdf' target='_blank'>https://arxiv.org/pdf/2509.06977.pdf</a></span>   <span><a href='https://github.com/william-zehua-li/cross-backend-model-checker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehua Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06977">Toward Reproducible Cross-Backend Compatibility for Deep Learning: A Configuration-First Framework with Three-Tier Verification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents a configuration-first framework for evaluating cross-backend compatibility in deep learning systems deployed on CPU, GPU, and compiled runtimes. The framework decouples experiments from code using YAML, supports both library and repository models, and employs a three-tier verification protocol covering tensor-level closeness, activation alignment, and task-level metrics. Through 672 checks across multiple models and tolerance settings, we observe that 72.0% of runs pass, with most discrepancies occurring under stricter thresholds. Our results show that detection models and compiled backends are particularly prone to drift, often due to nondeterministic post-processing. We further demonstrate that deterministic adapters and selective fallbacks can substantially improve agreement without significant performance loss. To our knowledge, this is the first unified framework that systematically quantifies and mitigates cross-backend drift in deep learning, providing a reproducible methodology for dependable deployment across heterogeneous runtimes.<br>
<br>
<div id='section'>Paperid: <span id='pid'>586, <a href='https://arxiv.org/pdf/2509.06975.pdf' target='_blank'>https://arxiv.org/pdf/2509.06975.pdf</a></span>   <span><a href='https://github.com/SongYYYY/GSTBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Song, Zhigang Hua, Yan Xie, Jingzhe Liu, Bo Long, Hui Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06975">GSTBench: A Benchmark Study on the Transferability of Graph Self-Supervised Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Self-supervised learning (SSL) has shown great promise in graph representation learning. However, most existing graph SSL methods are developed and evaluated under a single-dataset setting, leaving their cross-dataset transferability largely unexplored and limiting their ability to leverage knowledge transfer and large-scale pretraining, factors that are critical for developing generalized intelligence beyond fitting training data. To address this gap and advance foundation model research for graphs, we present GSTBench, the first systematic benchmark for evaluating the transferability of graph SSL methods. We conduct large-scale pretraining on ogbn-papers100M and evaluate five representative SSL methods across a diverse set of target graphs. Our standardized experimental setup decouples confounding factors such as model architecture, dataset characteristics, and adaptation protocols, enabling rigorous comparisons focused solely on pretraining objectives. Surprisingly, we observe that most graph SSL methods struggle to generalize, with some performing worse than random initialization. In contrast, GraphMAE, a masked autoencoder approach, consistently improves transfer performance. We analyze the underlying factors that drive these differences and offer insights to guide future research on transferable graph SSL, laying a solid foundation for the "pretrain-then-transfer" paradigm in graph learning. Our code is available at https://github.com/SongYYYY/GSTBench.<br>
<span id='abs_ch'>中文: GSTBench是首个评估图自监督学 方法可迁移性的基准，发现除GraphMAE外多数方法难以泛化，其持续提升性能的表现为未来 究提供了重要洞见。</span><br>
<span id='abs_en'>English: GSTBench is the first benchmark for evaluating the transferability of graph self-supervised learning methods, revealing that most struggle to generalize except for GraphMAE, which consistently improves performance and provides insights for future research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>587, <a href='https://arxiv.org/pdf/2509.06956.pdf' target='_blank'>https://arxiv.org/pdf/2509.06956.pdf</a></span>   <span><a href='https://github.com/NationalGAILab/HoT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Shijian Lu, Nicu Sebe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06956">H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a hierarchical plug-and-play pruning-and-recovering framework, called Hierarchical Hourglass Tokenizer (H$_{2}$OT), for efficient transformer-based 3D human pose estimation from videos. H$_{2}$OT begins with progressively pruning pose tokens of redundant frames and ends with recovering full-length sequences, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. It works with two key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module (TRM). TPM dynamically selects a few representative tokens to eliminate the redundancy of video frames, while TRM restores the detailed spatio-temporal information based on the selected tokens, thereby expanding the network output to the original full-length temporal resolution for fast inference. Our method is general-purpose: it can be easily incorporated into common VPT models on both seq2seq and seq2frame pipelines while effectively accommodating different token pruning and recovery strategies. In addition, our H$_{2}$OT reveals that maintaining the full pose sequence is unnecessary, and a few pose tokens of representative frames can achieve both high efficiency and estimation accuracy. Extensive experiments on multiple benchmark datasets demonstrate both the effectiveness and efficiency of the proposed method. Code and models are available at https://github.com/NationalGAILab/HoT.<br>
<span id='abs_ch'>中文: 本文提出的H₂OT分层即插即用框架通过剪枝冗余姿态令牌并恢复完整序列，显著提升了基于视频的3D人体姿态估计效率，在降低计算成本的同时保持高精度。</span><br>
<span id='abs_en'>English: This paper introduces H₂OT, a hierarchical plug-and-play framework that enhances the efficiency of video-based 3D human pose estimation by pruning redundant pose tokens and recovering full sequences, achieving high performance with reduced computational costs.Paperid:1048,https://arxiv.org/pdf/2509.06949.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>588, <a href='https://arxiv.org/pdf/2509.06945.pdf' target='_blank'>https://arxiv.org/pdf/2509.06945.pdf</a></span>   <span><a href='https://github.com/Osilly/Interleaving-Reasoning-Generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, Junbo Qiao, Yue Guo, Yao Hu, Zhenfei Yin, Philip Torr, Yu Cheng, Wanli Ouyang, Shaohui Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06945">Interleaving Reasoning for Better Text-to-Image Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Unified multimodal understanding and generation models recently have achieve significant improvement in image generation capability, yet a large gap remains in instruction following and detail preservation compared to systems that tightly couple comprehension with generation such as GPT-4o. Motivated by recent advances in interleaving reasoning, we explore whether such reasoning can further improve Text-to-Image (T2I) generation. We introduce Interleaving Reasoning Generation (IRG), a framework that alternates between text-based thinking and image synthesis: the model first produces a text-based thinking to guide an initial image, then reflects on the result to refine fine-grained details, visual quality, and aesthetics while preserving semantics. To train IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL), which targets two sub-goals: (1) strengthening the initial think-and-generate stage to establish core content and base quality, and (2) enabling high-quality textual reflection and faithful implementation of those refinements in a subsequent image. We curate IRGL-300K, a dataset organized into six decomposed learning modes that jointly cover learning text-based thinking, and full thinking-image trajectories. Starting from a unified foundation model that natively emits interleaved text-image outputs, our two-stage training first builds robust thinking and reflection, then efficiently tunes the IRG pipeline in the full thinking-image trajectory data. Extensive experiments show SoTA performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF, GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality and fine-grained fidelity. The code, model weights and datasets will be released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .<br>
<span id='abs_ch'>Chinese: 提出的交错推理生成（IRG）框架通过交替进行文本推理与图像合成，有效提升了文本到图像生成中的细节保持与指令遵循能力，并采用两阶段训练方法在多个基准测试中实现了最先进的性能。</span><br>
<span id='abs_en'>English: The proposed Interleaving Reasoning Generation (IRG) framework alternates between text-based reasoning and image synthesis to enhance detail preservation and instruction following in text-to-image generation, achieving state-of-the-art performance across multiple benchmarks through a two-stage training approach.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>589, <a href='https://arxiv.org/pdf/2509.06885.pdf' target='_blank'>https://arxiv.org/pdf/2509.06885.pdf</a></span>   <span><a href='https://github.com/mkianih/Barlow-Swin' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Morteza Kiani Haftlang, Mohammadhossein Malmir, Foroutan Parand, Umberto Michelucci, Safouane El Ghazouali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06885">Barlow-Swin: Toward a novel siamese-based segmentation architecture using Swin-Transformers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Medical image segmentation is a critical task in clinical workflows, particularly for the detection and delineation of pathological regions. While convolutional architectures like U-Net have become standard for such tasks, their limited receptive field restricts global context modeling. Recent efforts integrating transformers have addressed this, but often result in deep, computationally expensive models unsuitable for real-time use. In this work, we present a novel end-to-end lightweight architecture designed specifically for real-time binary medical image segmentation. Our model combines a Swin Transformer-like encoder with a U-Net-like decoder, connected via skip pathways to preserve spatial detail while capturing contextual information. Unlike existing designs such as Swin Transformer or U-Net, our architecture is significantly shallower and competitively efficient. To improve the encoder's ability to learn meaningful features without relying on large amounts of labeled data, we first train it using Barlow Twins, a self-supervised learning method that helps the model focus on important patterns by reducing unnecessary repetition in the learned features. After this pretraining, we fine-tune the entire model for our specific task. Experiments on benchmark binary segmentation tasks demonstrate that our model achieves competitive accuracy with substantially reduced parameter count and faster inference, positioning it as a practical alternative for deployment in real-time and resource-limited clinical environments. The code for our method is available at Github repository: https://github.com/mkianih/Barlow-Swin.<br>
<span id='abs_ch'>中文: 本文提出了一种轻量级实时医学图像二值分割模型，结合类Swin Transformer编 器与U-Net解 器，通过自监督预训练实现参数量更少、推理更快且精度相当的性能。</span><br>
<span id='abs_en'>English: This paper introduces a lightweight, real-time binary medical image segmentation model that combines a Swin Transformer-like encoder with a U-Net decoder, using self-supervised pretraining to achieve competitive accuracy with fewer parameters and faster inference.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>590, <a href='https://arxiv.org/pdf/2509.06861.pdf' target='_blank'>https://arxiv.org/pdf/2509.06861.pdf</a></span>   <span><a href='https://github.com/XuZhao0/tts-knowledge' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>James Xu Zhao, Bryan Hooi, See-Kiong Ng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06861">Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Test-time scaling increases inference-time computation by allowing models to generate long reasoning chains, and has shown strong performance across many domains. However, in this work, we show that this approach is not yet effective for knowledge-intensive tasks, where high factual accuracy and low hallucination rates are essential. We conduct a comprehensive evaluation of test-time scaling using 12 reasoning models on two knowledge-intensive benchmarks. Our results reveal that increasing test-time computation does not consistently improve accuracy and, in many cases, it even leads to more hallucinations. We then analyze how extended reasoning affects hallucination behavior. We find that reduced hallucinations often result from the model choosing to abstain after thinking more, rather than from improved factual recall. Conversely, for some models, longer reasoning encourages attempts on previously unanswered questions, many of which result in hallucinations. Case studies show that extended reasoning can induce confirmation bias, leading to overconfident hallucinations. Despite these limitations, we observe that compared to non-thinking, enabling thinking remains beneficial. Code and data are available at https://github.com/XuZhao0/tts-knowledge<br>
<span id='abs_ch'>中文: 测试时扩展虽能增强推理计算，但在知识密集型任务中效果不佳，不仅 法持续提升准确性，反而常增 幻觉， 为模型可能选择弃答或产生确认偏误，而非改善事实回忆。</span><br>
<span id='abs_en'>English: Test-time scaling enhances inference computation but proves ineffective for knowledge-intensive tasks, often increasing hallucinations without consistently improving accuracy, as it may lead to abstention or confirmation bias rather than better factual recall.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>591, <a href='https://arxiv.org/pdf/2509.06809.pdf' target='_blank'>https://arxiv.org/pdf/2509.06809.pdf</a></span>   <span><a href='https://github.com/sileod/reasoning_core' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Valentin Quesnel, Damien Sileo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06809">Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The scarcity of high-quality, logically sound data is a critical bottleneck for advancing the mathematical reasoning of Large Language Models (LLMs). Our work confronts this challenge by turning decades of automated theorem proving research into a scalable data engine. Rather than relying on error-prone LLMs or complex proof-assistant syntax like Lean and Isabelle, our framework leverages E-prover's saturation capabilities on the vast TPTP axiom library to derive a massive, guaranteed-valid corpus of theorems. Our pipeline is principled and simple: saturate axioms, filter for "interesting" theorems, and generate tasks. With no LLMs in the loop, we eliminate factual errors by construction. This purely symbolic data is then transformed into three difficulty-controlled challenges: entailment verification, premise selection, and proof reconstruction. Our zero-shot experiments on frontier models reveal a clear weakness: performance collapses on tasks requiring deep, structural reasoning. Our framework provides both the diagnostic tool to measure this gap and a scalable source of symbolic training data to address it. We make the code and data publicly available. https://github.com/sileod/reasoning_core https://hf.co/datasets/reasoning-core/rc1<br>
<span id='abs_ch'>中文: 本 究通过利用E-prover在TPTP公理库上的饱和能力构建可扩展数据引擎，生成保证有效的定理数据，转化为三个难度可控的推理任务，既揭示了前沿模型在深度推理上的缺陷，又提供了诊断工具和训练数据。</span><br>
<span id='abs_en'>English: This work addresses the scarcity of high-quality mathematical reasoning data for LLMs by creating a scalable data engine using E-prover and the TPTP library to generate guaranteed-valid theorems, which are then transformed into three difficulty-controlled challenges that reveal models' weaknesses in deep reasoning while providing both diagnostic tools and training data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>592, <a href='https://arxiv.org/pdf/2509.06736.pdf' target='_blank'>https://arxiv.org/pdf/2509.06736.pdf</a></span>   <span><a href='https://github.com/OpenMOSS/VehicleWorld' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Yang, Jiajun Chen, Zhangyue Yin, Shuo Chen, Yuxin Wang, Yiran Guo, Yuan Li, Yining Zheng, Xuanjing Huang, Xipeng Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06736">VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Intelligent vehicle cockpits present unique challenges for API Agents, requiring coordination across tightly-coupled subsystems that exceed typical task environments' complexity. Traditional Function Calling (FC) approaches operate statelessly, requiring multiple exploratory calls to build environmental awareness before execution, leading to inefficiency and limited error recovery. We introduce VehicleWorld, the first comprehensive environment for the automotive domain, featuring 30 modules, 250 APIs, and 680 properties with fully executable implementations that provide real-time state information during agent execution. This environment enables precise evaluation of vehicle agent behaviors across diverse, challenging scenarios. Through systematic analysis, we discovered that direct state prediction outperforms function calling for environmental control. Building on this insight, we propose State-based Function Call (SFC), a novel approach that maintains explicit system state awareness and implements direct state transitions to achieve target conditions. Experimental results demonstrate that SFC significantly outperforms traditional FC approaches, achieving superior execution accuracy and reduced latency. We have made all implementation code publicly available on Github https://github.com/OpenMOSS/VehicleWorld.<br>
<span id='abs_ch'>中文: 本文介绍了首个汽车领域综合环境VehicleWorld及其可执行模块与API，并提出基于状态的函数调用方法，该方法通过保持系统状态感知显著优于 统函数调用，实现了更高精度和效率。</span><br>
<span id='abs_en'>English: This paper introduces VehicleWorld, a comprehensive automotive environment with executable modules and APIs, and proposes State-based Function Call (SFC), which outperforms traditional function calling by maintaining system state awareness for improved accuracy and efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>593, <a href='https://arxiv.org/pdf/2509.06550.pdf' target='_blank'>https://arxiv.org/pdf/2509.06550.pdf</a></span>   <span><a href='https://github.com/jackwilkie/CLAN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jack Wilkie, Hanan Hindy, Christos Tachtatzis, Robert Atkinson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06550">Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Network intrusion detection remains a critical challenge in cybersecurity. While supervised machine learning models achieve state-of-the-art performance, their reliance on large labelled datasets makes them impractical for many real-world applications. Anomaly detection methods, which train exclusively on benign traffic to identify malicious activity, suffer from high false positive rates, limiting their usability. Recently, self-supervised learning techniques have demonstrated improved performance with lower false positive rates by learning discriminative latent representations of benign traffic. In particular, contrastive self-supervised models achieve this by minimizing the distance between similar (positive) views of benign traffic while maximizing it between dissimilar (negative) views. Existing approaches generate positive views through data augmentation and treat other samples as negative. In contrast, this work introduces Contrastive Learning using Augmented Negative pairs (CLAN), a novel paradigm for network intrusion detection where augmented samples are treated as negative views - representing potentially malicious distributions - while other benign samples serve as positive views. This approach enhances both classification accuracy and inference efficiency after pretraining on benign traffic. Experimental evaluation on the Lycos2017 dataset demonstrates that the proposed method surpasses existing self-supervised and anomaly detection techniques in a binary classification task. Furthermore, when fine-tuned on a limited labelled dataset, the proposed approach achieves superior multi-class classification performance compared to existing self-supervised models.<br>
<span id='abs_ch'>中文: 本文提出CLAN，一种用于网络入侵检测的新型自监督对比学 方法，将增强 本视为负 本视图以提高分类精度和效率，在Lycos2017数据集上超越了现有技术。</span><br>
<span id='abs_en'>English: This paper introduces CLAN, a novel self-supervised contrastive learning method for network intrusion detection that treats augmented samples as negative views to improve classification accuracy and efficiency, outperforming existing techniques on the Lycos2017 dataset.</span>Paperid:1065,https://arxiv.org/pdf/2509.06499.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>594, <a href='https://arxiv.org/pdf/2509.06436.pdf' target='_blank'>https://arxiv.org/pdf/2509.06436.pdf</a></span>   <span><a href='https://github.com/Aireduce952/Tree-of-Agents' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Song Yu, Xiaofei Xu, Ke Deng, Li Li, Lin Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06436">Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) face persistent challenges when handling long-context tasks, most notably the lost in the middle issue, where information located in the middle of a long input tends to be underutilized. Some existing methods that reduce input have the risk of discarding key information, while others that extend context windows often lead to attention dispersion. To address these limitations, we propose Tree of Agents (TOA), a multi-agent reasoning framework that segments the input into chunks processed by independent agents. Each agent generates its local cognition, then agents dynamically exchange information for collaborative reasoning along tree-structured paths. TOA enables agents to probe different reasoning orders for multi-perspective understanding, effectively mitigating position bias and reducing hallucinations. To improve processing efficiency, we incorporate prefix-hash caching and adaptive pruning strategies, achieving significant performance improvements with comparable API overhead. Experiments show that TOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple baselines and demonstrates comparable performance to the latest and much larger commercial models, such as Gemini1.5-pro, on various long-context tasks. Code is available at https://github.com/Aireduce952/Tree-of-Agents.<br>
<span id='abs_ch'>中文摘要：Tree of Agents (TOA) 框架通过多智能体协作和 状推理路径，有效解决了大语言模型处理长文本时的位置偏见和幻觉问题，在保持高效的同时使用轻量模型实现了卓越性能。</span><br>
<span id='abs_en'>English Summary: The Tree of Agents (TOA) framework addresses long-context challenges in LLMs by employing multi-agent collaboration with tree-structured reasoning paths, achieving superior performance with compact models while maintaining efficiency through caching and pruning strategies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>595, <a href='https://arxiv.org/pdf/2509.06419.pdf' target='_blank'>https://arxiv.org/pdf/2509.06419.pdf</a></span>   <span><a href='https://github.com/alsike22/CAPMix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xudong Mou, Rui Wang, Tiejun Wang, Renyu Yang, Shiru Chen, Jie Sun, Tianyu Wo, Xudong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06419">CAPMix: Robust Time Series Anomaly Detection Based on Abnormal Assumptions with Dual-Space Mixup</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Time series anomaly detection (TSAD) is a vital yet challenging task, particularly in scenarios where labeled anomalies are scarce and temporal dependencies are complex. Recent anomaly assumption (AA) approaches alleviate the lack of anomalies by injecting synthetic samples and training discriminative models. Despite promising results, these methods often suffer from two fundamental limitations: patchy generation, where scattered anomaly knowledge leads to overly simplistic or incoherent anomaly injection, and Anomaly Shift, where synthetic anomalies either resemble normal data too closely or diverge unrealistically from real anomalies, thereby distorting classification boundaries. In this paper, we propose CAPMix, a controllable anomaly augmentation framework that addresses both issues. First, we design a CutAddPaste mechanism to inject diverse and complex anomalies in a targeted manner, avoiding patchy generation. Second, we introduce a label revision strategy to adaptively refine anomaly labels, reducing the risk of anomaly shift. Finally, we employ dual-space mixup within a temporal convolutional network to enforce smoother and more robust decision boundaries. Extensive experiments on five benchmark datasets, including AIOps, UCR, SWaT, WADI, and ESA, demonstrate that CAPMix achieves significant improvements over state-of-the-art baselines, with enhanced robustness against contaminated training data. The code is available at https://github.com/alsike22/CAPMix.<br>
<span id='abs_ch'>中文：提出的CAPMix框架通过定向异常注入机制和自适应 签优化，解决了现有方法中异常生成零散和异常偏移的问题，在多个基准测试中实现了卓越的检测性能。</span><br>
<span id='abs_en'>English: The proposed CAPMix framework enhances time series anomaly detection by introducing a targeted anomaly injection mechanism and adaptive label refinement to overcome limitations of patchy generation and anomaly shift, achieving superior performance across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>596, <a href='https://arxiv.org/pdf/2509.06337.pdf' target='_blank'>https://arxiv.org/pdf/2509.06337.pdf</a></span>   <span><a href='https://github.com/dart-lab-research/LLM-S-Cube-Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianpeng Zhao, Chenyu Yuan, Weiming Luo, Haoling Xie, Guangwei Zhang, Steven Jige Quan, Zixuan Yuan, Pengyang Wang, Denghui Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06337">Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Questionnaire-based surveys are foundational to social science research and public policymaking, yet traditional survey methods remain costly, time-consuming, and often limited in scale. This paper explores a new paradigm: simulating virtual survey respondents using Large Language Models (LLMs). We introduce two novel simulation settings, namely Partial Attribute Simulation (PAS) and Full Attribute Simulation (FAS), to systematically evaluate the ability of LLMs to generate accurate and demographically coherent responses. In PAS, the model predicts missing attributes based on partial respondent profiles, whereas FAS involves generating complete synthetic datasets under both zero-context and context-enhanced conditions. We curate a comprehensive benchmark suite, LLM-S^3 (Large Language Model-based Sociodemographic Survey Simulation), that spans 11 real-world public datasets across four sociological domains. Our evaluation of multiple mainstream LLMs (GPT-3.5/4 Turbo, LLaMA 3.0/3.1-8B) reveals consistent trends in prediction performance, highlights failure modes, and demonstrates how context and prompt design impact simulation fidelity. This work establishes a rigorous foundation for LLM-driven survey simulations, offering scalable and cost-effective tools for sociological research and policy evaluation. Our code and dataset are available at: https://github.com/dart-lab-research/LLM-S-Cube-Benchmark<br>
<span id='abs_ch'>本文提出了一种利用大型语言模型通过部分和完整属性模拟方法生成虚拟调查对象的新范式，为可扩展且经济高效的社会学 究建立了基准。</span><br>
<span id='abs_en'>This paper introduces a novel approach using Large Language Models to simulate virtual survey respondents through Partial and Full Attribute Simulation methods, establishing a benchmark for scalable and cost-effective sociological research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>597, <a href='https://arxiv.org/pdf/2509.06336.pdf' target='_blank'>https://arxiv.org/pdf/2509.06336.pdf</a></span>   <span><a href='https://github.com/Elune001/MVP-FAS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeongmin Yu, Susang Kim, Kisu Lee, Taekyoung Kwon, Won-Yong Shin, Ha Young Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06336">Multi-View Slot Attention Using Paraphrased Texts for Face Anti-Spoofing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent face anti-spoofing (FAS) methods have shown remarkable cross-domain performance by employing vision-language models like CLIP. However, existing CLIP-based FAS models do not fully exploit CLIP's patch embedding tokens, failing to detect critical spoofing clues. Moreover, these models rely on a single text prompt per class (e.g., 'live' or 'fake'), which limits generalization. To address these issues, we propose MVP-FAS, a novel framework incorporating two key modules: Multi-View Slot attention (MVS) and Multi-Text Patch Alignment (MTPA). Both modules utilize multiple paraphrased texts to generate generalized features and reduce dependence on domain-specific text. MVS extracts local detailed spatial features and global context from patch embeddings by leveraging diverse texts with multiple perspectives. MTPA aligns patches with multiple text representations to improve semantic robustness. Extensive experiments demonstrate that MVP-FAS achieves superior generalization performance, outperforming previous state-of-the-art methods on cross-domain datasets. Code: https://github.com/Elune001/MVP-FAS.<br>
<span id='abs_ch'>中文: MVP-FAS框架通过多视角槽位注意力和多文本补丁对齐模块，充分利用CLIP的补丁嵌入和多 化文本提示，显著提升了人脸防伪的跨领域泛化性能。</span><br>
<span id='abs_en'>English: The proposed MVP-FAS framework enhances face anti-spoofing by leveraging multi-view slot attention and multi-text patch alignment to better utilize CLIP's patch embeddings and multiple text prompts, achieving superior cross-domain generalization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>598, <a href='https://arxiv.org/pdf/2509.06270.pdf' target='_blank'>https://arxiv.org/pdf/2509.06270.pdf</a></span>   <span><a href='https://github.com/UNIC-Lab/UrbanMIMOMap' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Honggang Jia, Xiucheng Wang, Nan Cheng, Ruijin Sun, Changle Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06270">UrbanMIMOMap: A Ray-Traced MIMO CSI Dataset with Precoding-Aware Maps and Benchmarks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Sixth generation (6G) systems require environment-aware communication, driven by native artificial intelligence (AI) and integrated sensing and communication (ISAC). Radio maps (RMs), providing spatially continuous channel information, are key enablers. However, generating high-fidelity RM ground truth via electromagnetic (EM) simulations is computationally intensive, motivating machine learning (ML)-based RM construction. The effectiveness of these data-driven methods depends on large-scale, high-quality training data. Current public datasets often focus on single-input single-output (SISO) and limited information, such as path loss, which is insufficient for advanced multi-input multi-output (MIMO) systems requiring detailed channel state information (CSI). To address this gap, this paper presents UrbanMIMOMap, a novel large-scale urban MIMO CSI dataset generated using high-precision ray tracing. UrbanMIMOMap offers comprehensive complex CSI matrices across a dense spatial grid, going beyond traditional path loss data. This rich CSI is vital for constructing high-fidelity RMs and serves as a fundamental resource for data-driven RM generation, including deep learning. We demonstrate the dataset's utility through baseline performance evaluations of representative ML methods for RM construction. This work provides a crucial dataset and reference for research in high-precision RM generation, MIMO spatial performance, and ML for 6G environment awareness. The code and data for this work are available at: https://github.com/UNIC-Lab/UrbanMIMOMap.<br>
<span id='abs_ch'>中文摘要：本文提出UrbanMIMOMap这一基于射线追踪生成的大规模城市MIMO信道状态信息数据集，旨在弥补现有数据集的不足，为构建6G环境感知通信所需的高精度 线电地图提供关键数据支持。</span><br>
<span id='abs_en'>English Summary: This paper introduces UrbanMIMOMap, a large-scale urban MIMO channel state information dataset generated via ray tracing to address the limitations of existing datasets and support high-fidelity radio map construction for 6G environment-aware communication systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>599, <a href='https://arxiv.org/pdf/2509.06235.pdf' target='_blank'>https://arxiv.org/pdf/2509.06235.pdf</a></span>   <span><a href='https://github.com/aialt/PillagerBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Olivier Schipper, Yudi Zhang, Yali Du, Mykola Pechenizkiy, Meng Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06235">PillagerBench: Benchmarking LLM-Based Agents in Competitive Minecraft Team Environments</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LLM-based agents have shown promise in various cooperative and strategic reasoning tasks, but their effectiveness in competitive multi-agent environments remains underexplored. To address this gap, we introduce PillagerBench, a novel framework for evaluating multi-agent systems in real-time competitive team-vs-team scenarios in Minecraft. It provides an extensible API, multi-round testing, and rule-based built-in opponents for fair, reproducible comparisons. We also propose TactiCrafter, an LLM-based multi-agent system that facilitates teamwork through human-readable tactics, learns causal dependencies, and adapts to opponent strategies. Our evaluation demonstrates that TactiCrafter outperforms baseline approaches and showcases adaptive learning through self-play. Additionally, we analyze its learning process and strategic evolution over multiple game episodes. To encourage further research, we have open-sourced PillagerBench, fostering advancements in multi-agent AI for competitive environments.<br>
<span id='abs_ch'>中文: PillagerBench是一个用于在竞争性《我的世界》场景中评估多智能体系统的新框架，而TactiCrafter则是一个基于大语言模型的系统，通过增强团队协作和适应对手策略，在自适应学 中超越了基线方法。</span><br>
<span id='abs_en'>English: PillagerBench is a novel framework for evaluating multi-agent systems in competitive Minecraft scenarios, while TactiCrafter is an LLM-based system that enhances teamwork and adapts to opponents, outperforming baselines through adaptive learning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>600, <a href='https://arxiv.org/pdf/2509.06060.pdf' target='_blank'>https://arxiv.org/pdf/2509.06060.pdf</a></span>   <span><a href='https://github.com/blisky-li/ARIES' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Wang, Yujie Li, Zezhi Shao, Chengqing Yu, Yisong Fu, Zhulin An, Yongjun Xu, Xueqi Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06060">ARIES: Relation Assessment and Model Recommendation for Deep Time Series Forecasting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in deep learning models for time series forecasting have been significant. These models often leverage fundamental time series properties such as seasonality and non-stationarity, which may suggest an intrinsic link between model performance and data properties. However, existing benchmark datasets fail to offer diverse and well-defined temporal patterns, restricting the systematic evaluation of such connections. Additionally, there is no effective model recommendation approach, leading to high time and cost expenditures when testing different architectures across different downstream applications. For those reasons, we propose ARIES, a framework for assessing relation between time series properties and modeling strategies, and for recommending deep forcasting models for realistic time series. First, we construct a synthetic dataset with multiple distinct patterns, and design a comprehensive system to compute the properties of time series. Next, we conduct an extensive benchmarking of over 50 forecasting models, and establish the relationship between time series properties and modeling strategies. Our experimental results reveal a clear correlation. Based on these findings, we propose the first deep forecasting model recommender, capable of providing interpretable suggestions for real-world time series. In summary, ARIES is the first study to establish the relations between the properties of time series data and modeling strategies, while also implementing a model recommendation system. The code is available at: https://github.com/blisky-li/ARIES.<br>
<span id='abs_ch'>Chinese: ARIES框架通过全面基准测试确立了时间序列特性与建模策略之间的明确关联，并推出了首个可解释的深度预测模型推荐系统，以解决现有数据集和评估方法的局限性。</span><br>
<span id='abs_en'>English: The ARIES framework establishes a clear correlation between time series properties and modeling strategies through comprehensive benchmarking and introduces the first interpretable deep forecasting model recommender to address the limitations of existing datasets and evaluation methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>601, <a href='https://arxiv.org/pdf/2509.06026.pdf' target='_blank'>https://arxiv.org/pdf/2509.06026.pdf</a></span>   <span><a href='https://github.com/Xinyu140203/RAG_MIA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Gao, Xiangtao Meng, Yingkai Dong, Zheng Li, Shanqing Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06026">DCMI: A Differential Calibration Membership Inference Attack Against Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While Retrieval-Augmented Generation (RAG) effectively reduces hallucinations by integrating external knowledge bases, it introduces vulnerabilities to membership inference attacks (MIAs), particularly in systems handling sensitive data. Existing MIAs targeting RAG's external databases often rely on model responses but ignore the interference of non-member-retrieved documents on RAG outputs, limiting their effectiveness. To address this, we propose DCMI, a differential calibration MIA that mitigates the negative impact of non-member-retrieved documents. Specifically, DCMI leverages the sensitivity gap between member and non-member retrieved documents under query perturbation. It generates perturbed queries for calibration to isolate the contribution of member-retrieved documents while minimizing the interference from non-member-retrieved documents. Experiments under progressively relaxed assumptions show that DCMI consistently outperforms baselines--for example, achieving 97.42% AUC and 94.35% Accuracy against the RAG system with Flan-T5, exceeding the MBA baseline by over 40%. Furthermore, on real-world RAG platforms such as Dify and MaxKB, DCMI maintains a 10%-20% advantage over the baseline. These results highlight significant privacy risks in RAG systems and emphasize the need for stronger protection mechanisms. We appeal to the community's consideration of deeper investigations, like ours, against the data leakage risks in rapidly evolving RAG systems. Our code is available at https://github.com/Xinyu140203/RAG_MIA.<br>
<span id='abs_ch'>中文: 检索增强生成（RAG）系统 非成员文档的干扰易受成员推理攻击，为此提出的差分 准方法DCMI能有效分离成员贡献，在准确率和隐私风险防控上显著优于现有基线方案。</span><br>
<span id='abs_en'>English: Retrieval-Augmented Generation (RAG) systems are vulnerable to membership inference attacks due to interference from non-member documents, prompting the development of DCMI, a differential calibration method that effectively isolates member contributions and significantly outperforms existing baselines in both accuracy and privacy risk mitigation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>602, <a href='https://arxiv.org/pdf/2509.06024.pdf' target='_blank'>https://arxiv.org/pdf/2509.06024.pdf</a></span>   <span><a href='https://github.com/Henryhe09/DRER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyang He, Zihua Rong, Kun Ji, Chenyang Li, Qing Huang, Chong Xia, Lan Yang, Honggang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06024">Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought via RL</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning (RL) has recently become the dominant paradigm for strengthening the reasoning abilities of large language models (LLMs). Yet the rule-based reward functions commonly used on mathematical or programming benchmarks assess only answer format and correctness, providing no signal as to whether the induced Chain-of-Thought (CoT) actually improves the answer. Furthermore, such task-specific training offers limited control over logical depth and therefore may fail to reveal a model's genuine reasoning capacity. We propose Dynamic Reasoning Efficiency Reward (DRER) -- a plug-and-play RL reward framework that reshapes both reward and advantage signals. (i) A Reasoning Quality Reward assigns fine-grained credit to those reasoning chains that demonstrably raise the likelihood of the correct answer, directly incentivising the trajectories with beneficial CoT tokens. (ii) A Dynamic Length Advantage decays the advantage of responses whose length deviates from a validation-derived threshold, stabilising training. To facilitate rigorous assessment, we also release Logictree, a dynamically constructed deductive reasoning dataset that functions both as RL training data and as a comprehensive benchmark. Experiments confirm the effectiveness of DRER: our 7B model attains GPT-o3-mini level performance on Logictree with 400 trianing steps, while the average confidence of CoT-augmented answers rises by 30%. The model further exhibits generalisation across diverse logical-reasoning datasets, and the mathematical benchmark AIME24. These results illuminate how RL shapes CoT behaviour and chart a practical path toward enhancing formal-reasoning skills in large language models. All code and data are available in repository https://github.com/Henryhe09/DRER.<br>
<span id='abs_ch'>中文: 提出的动态推理效率奖励（DRER）框架通过激励有益的思维链 记和动态长度调整来增强大型语言模型的推理能力，在逻辑推理和数学基准测试中达到GPT-o3-mini水平性能，并显著提升答案置信度与泛化能力。</span><br>
<span id='abs_en'>English: The proposed Dynamic Reasoning Efficiency Reward (DRER) framework enhances reasoning in large language models by incentivizing beneficial Chain-of-Thought tokens and stabilizing training through dynamic length adjustments, achieving GPT-o3-mini level performance with improved confidence and generalization across logical and mathematical benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>603, <a href='https://arxiv.org/pdf/2509.05933.pdf' target='_blank'>https://arxiv.org/pdf/2509.05933.pdf</a></span>   <span><a href='https://github.com/Hasebul/MapAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Hasebul Hasan, Mahir Labib Dihan, Mohammed Eunus Ali, Md Rizwan Parvez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05933">MapAgent: A Hierarchical Agent for Geospatial Reasoning with Dynamic Map Tool Integration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Agentic AI has significantly extended the capabilities of large language models (LLMs) by enabling complex reasoning and tool use. However, most existing frameworks are tailored to domains such as mathematics, coding, or web automation, and fall short on geospatial tasks that require spatial reasoning, multi-hop planning, and real-time map interaction. To address these challenges, we introduce MapAgent, a hierarchical multi-agent plug-and-play framework with customized toolsets and agentic scaffolds for map-integrated geospatial reasoning. Unlike existing flat agent-based approaches that treat tools uniformly-often overwhelming the LLM when handling similar but subtly different geospatial APIs-MapAgent decouples planning from execution. A high-level planner decomposes complex queries into subgoals, which are routed to specialized modules. For tool-heavy modules-such as map-based services-we then design a dedicated map-tool agent that efficiently orchestrates related APIs adaptively in parallel to effectively fetch geospatial data relevant for the query, while simpler modules (e.g., solution generation or answer extraction) operate without additional agent overhead. This hierarchical design reduces cognitive load, improves tool selection accuracy, and enables precise coordination across similar APIs. We evaluate MapAgent on four diverse geospatial benchmarks-MapEval-Textual, MapEval-API, MapEval-Visual, and MapQA-and demonstrate substantial gains over state-of-the-art tool-augmented and agentic baselines. We open-source our framwork at https://github.com/Hasebul/MapAgent.<br>
<span id='abs_ch'>Chinese Summary: MapAgent是一种分层多智能体框架，通过将规划与执行解耦并采用专业化模块和自适应工具协调，显著提升了地理空间推理能力，在多项基准测试中优于现有先进方法。</span><br>
<span id='abs_en'>English Summary: MapAgent is a hierarchical multi-agent framework designed to enhance geospatial reasoning by decoupling planning from execution, using specialized modules and adaptive tool coordination to outperform existing approaches on diverse benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>604, <a href='https://arxiv.org/pdf/2509.05757.pdf' target='_blank'>https://arxiv.org/pdf/2509.05757.pdf</a></span>   <span><a href='https://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sarang Patil, Zeyong Zhang, Yiran Huang, Tengfei Ma, Mengjia Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05757">Hyperbolic Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have achieved remarkable success and demonstrated superior performance across various tasks, including natural language processing (NLP), weather forecasting, biological protein folding, text generation, and solving mathematical problems. However, many real-world data exhibit highly non-Euclidean latent hierarchical anatomy, such as protein networks, transportation networks, financial networks, brain networks, and linguistic structures or syntactic trees in natural languages. Effectively learning intrinsic semantic entailment and hierarchical relationships from these raw, unstructured input data using LLMs remains an underexplored area. Due to its effectiveness in modeling tree-like hierarchical structures, hyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularity as an expressive latent representation space for complex data modeling across domains such as graphs, images, languages, and multi-modal data. Here, we provide a comprehensive and contextual exposition of recent advancements in LLMs that leverage hyperbolic geometry as a representation space to enhance semantic representation learning and multi-scale reasoning. Specifically, the paper presents a taxonomy of the principal techniques of Hyperbolic LLMs (HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/log maps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4) hyperbolic state-space models. We also explore crucial potential applications and outline future research directions. A repository of key papers, models, datasets, and code implementations is available at https://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main.<br>
<span id='abs_ch'>中文: 大型语言模型正越来越多地利用双曲 何来更好地捕捉复杂数据中的层次结构，其最新进展分为四大类技术，并展现出广阔的应用前景。</span><br>
<span id='abs_en'>English: Large language models are increasingly leveraging hyperbolic geometry to better capture hierarchical structures in complex data, with recent advances categorized into four main techniques and promising applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>605, <a href='https://arxiv.org/pdf/2509.05735.pdf' target='_blank'>https://arxiv.org/pdf/2509.05735.pdf</a></span>   <span><a href='https://github.com/swsychen/Offline_vs_Online_in_MBRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Chen, Ji Shi, Cansu Sancaktar, Jonas Frey, Georg Martius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05735">Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Data collection is crucial for learning robust world models in model-based reinforcement learning. The most prevalent strategies are to actively collect trajectories by interacting with the environment during online training or training on offline datasets. At first glance, the nature of learning task-agnostic environment dynamics makes world models a good candidate for effective offline training. However, the effects of online vs. offline data on world models and thus on the resulting task performance have not been thoroughly studied in the literature. In this work, we investigate both paradigms in model-based settings, conducting experiments on 31 different environments. First, we showcase that online agents outperform their offline counterparts. We identify a key challenge behind performance degradation of offline agents: encountering Out-Of-Distribution states at test time. This issue arises because, without the self-correction mechanism in online agents, offline datasets with limited state space coverage induce a mismatch between the agent's imagination and real rollouts, compromising policy training. We demonstrate that this issue can be mitigated by allowing for additional online interactions in a fixed or adaptive schedule, restoring the performance of online training with limited interaction data. We also showcase that incorporating exploration data helps mitigate the performance degradation of offline agents. Based on our insights, we recommend adding exploration data when collecting large datasets, as current efforts predominantly focus on expert data alone.<br>
<span id='abs_ch'>Chinese: 在线智能体在基于模型的强化学 中优于离线智能体， 为后者面临分布外状态的挑战，但通过引入在线交互或探索数据可以有效缓解这一问题。</span><br>
<span id='abs_en'>English: Online agents outperform offline ones in model-based reinforcement learning due to the latter's struggle with Out-Of-Distribution states, but this can be mitigated by incorporating online interactions or exploration data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>606, <a href='https://arxiv.org/pdf/2509.05657.pdf' target='_blank'>https://arxiv.org/pdf/2509.05657.pdf</a></span>   <span><a href='https://github.com/Ashone3/LM-Searcher' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Hu, Jihao Liu, Ke Wang, Jinliang Zhen, Weikang Shi, Manyuan Zhang, Qi Dou, Rui Liu, Aojun Zhou, Hongsheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05657">LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent progress in Large Language Models (LLMs) has opened new avenues for solving complex optimization problems, including Neural Architecture Search (NAS). However, existing LLM-driven NAS approaches rely heavily on prompt engineering and domain-specific tuning, limiting their practicality and scalability across diverse tasks. In this work, we propose LM-Searcher, a novel framework that leverages LLMs for cross-domain neural architecture optimization without the need for extensive domain-specific adaptation. Central to our approach is NCode, a universal numerical string representation for neural architectures, which enables cross-domain architecture encoding and search. We also reformulate the NAS problem as a ranking task, training LLMs to select high-performing architectures from candidate pools using instruction-tuning samples derived from a novel pruning-based subspace sampling strategy. Our curated dataset, encompassing a wide range of architecture-performance pairs, encourages robust and transferable learning. Comprehensive experiments demonstrate that LM-Searcher achieves competitive performance in both in-domain (e.g., CNNs for image classification) and out-of-domain (e.g., LoRA configurations for segmentation and generation) tasks, establishing a new paradigm for flexible and generalizable LLM-based architecture search. The datasets and models will be released at https://github.com/Ashone3/LM-Searcher.<br>
<span id='abs_ch'>中文: LM-Searcher提出了一种新颖框架，利用大型语言模型进行跨领域神经架构优化，通过通用数值编 和将NAS重新定义为排序任务， 需大量领域特定调整即可在多种任务中实现优异性能。</span><br>
<span id='abs_en'>English: LM-Searcher introduces a novel framework using Large Language Models for cross-domain neural architecture optimization, employing a universal numerical encoding and reformulating NAS as a ranking task to achieve competitive performance across diverse tasks without extensive domain-specific tuning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>607, <a href='https://arxiv.org/pdf/2509.05617.pdf' target='_blank'>https://arxiv.org/pdf/2509.05617.pdf</a></span>   <span><a href='https://github.com/LLM-HITCS25S/LyricsEmotionAttribution' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shay Dahary, Avi Edana, Alexander Apartsin, Yehudit Aperstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05617">From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The emotional content of song lyrics plays a pivotal role in shaping listener experiences and influencing musical preferences. This paper investigates the task of multi-label emotional attribution of song lyrics by predicting six emotional intensity scores corresponding to six fundamental emotions. A manually labeled dataset is constructed using a mean opinion score (MOS) approach, which aggregates annotations from multiple human raters to ensure reliable ground-truth labels. Leveraging this dataset, we conduct a comprehensive evaluation of several publicly available large language models (LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model specifically for predicting multi-label emotion scores. Experimental results reveal the relative strengths and limitations of zero-shot and fine-tuned models in capturing the nuanced emotional content of lyrics. Our findings highlight the potential of LLMs for emotion recognition in creative texts, providing insights into model selection strategies for emotion-based music information retrieval applications. The labeled dataset is available at https://github.com/LLM-HITCS25S/LyricsEmotionAttribution.<br>
<span id='abs_ch'>本 究评估大语言模型在预测歌词多 签情感强度方面的表现，通过比较零 本与微调方法推进基于情感的音乐检索应用。</span><br>
<span id='abs_en'>This study evaluates large language models for predicting multi-label emotional intensity in song lyrics, comparing zero-shot and fine-tuned approaches to advance emotion-based music retrieval.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>608, <a href='https://arxiv.org/pdf/2509.05604.pdf' target='_blank'>https://arxiv.org/pdf/2509.05604.pdf</a></span>   <span><a href='https://github.com/park-jungin/videograph' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jungin Park, Jiyoung Lee, Kwanghoon Sohn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05604">Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Video summarization aims to select keyframes that are visually diverse and can represent the whole story of a given video. Previous approaches have focused on global interlinkability between frames in a video by temporal modeling. However, fine-grained visual entities, such as objects, are also highly related to the main content of the video. Moreover, language-guided video summarization, which has recently been studied, requires a comprehensive linguistic understanding of complex real-world videos. To consider how all the objects are semantically related to each other, this paper regards video summarization as a language-guided spatiotemporal graph modeling problem. We present recursive spatiotemporal graph networks, called VideoGraph, which formulate the objects and frames as nodes of the spatial and temporal graphs, respectively. The nodes in each graph are connected and aggregated with graph edges, representing the semantic relationships between the nodes. To prevent the edges from being configured with visual similarity, we incorporate language queries derived from the video into the graph node representations, enabling them to contain semantic knowledge. In addition, we adopt a recursive strategy to refine initial graphs and correctly classify each frame node as a keyframe. In our experiments, VideoGraph achieves state-of-the-art performance on several benchmarks for generic and query-focused video summarization in both supervised and unsupervised manners. The code is available at https://github.com/park-jungin/videograph.<br>
<span id='abs_ch'>Chinese: 本文提出VideoGraph方法，将视频摘要视为语言引导的时空图建模问题，通过递归图网络整合对象与帧之间的语义关系，在多项基准测试中实现了最先进的性能。</span><br>
<span id='abs_en'>English: This paper introduces VideoGraph, a novel approach that treats video summarization as a language-guided spatiotemporal graph modeling problem, achieving state-of-the-art performance by incorporating semantic relationships between objects and frames through recursive graph networks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>609, <a href='https://arxiv.org/pdf/2509.05550.pdf' target='_blank'>https://arxiv.org/pdf/2509.05550.pdf</a></span>   <span><a href='https://github.com/lizixi-0x2F/TreeGPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05550">TreeGPT: Pure TreeFFN Encoder-Decoder Architecture for Structured Reasoning Without Attention Mechanisms</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present TreeGPT, an attention-free neural architecture that explores the potential of pure TreeFFN encoder-decoder design for structured reasoning tasks. Unlike traditional transformer approaches that rely on attention mechanisms, TreeGPT employs bidirectional TreeFFN components that process sequences through adjacent connections in parallel, aiming to achieve computational efficiency while maintaining reasoning capabilities. Our approach centers on a TreeFFN Encoder-Decoder mechanism: $$\text{Encoder TreeFFN (L} \rightarrow \text{R)} + \text{Decoder TreeFFN (R} \leftarrow \text{L)} \rightarrow \text{Parallel Processing}$$ where the encoder processes left-to-right dependencies while the decoder handles right-to-left patterns, both using simple neighbor-to-neighbor connections. This design eliminates attention computation while maintaining sequence modeling capabilities. We evaluate our approach on the ARC Prize 2025 dataset, where TreeGPT achieves 99\% validation accuracy using 3.16M parameters. The model converges within 1500 training steps and demonstrates 100\% token-level accuracy on selected evaluation samples. Our preliminary results suggest that for certain structured reasoning tasks, specialized TreeFFN architectures may offer advantages over attention-based approaches. While these findings are encouraging, we acknowledge that further investigation across diverse tasks and datasets would be valuable to establish the broader applicability of attention-free designs.<br>
<span id='abs_ch'>中文摘要：TreeGPT是一种 需注意力机制的神经架构，采用双向TreeFFN编 器-解 器组件进行并行序列处理，在ARC Prize 2025数据集上以316万参数实现99%验证准确率，在保持推理能力的同时展现出高效的计算性能。</span><br>
<span id='abs_en'>English Summary: TreeGPT is an attention-free neural architecture using bidirectional TreeFFN encoder-decoder components for parallel sequence processing, achieving 99% validation accuracy on ARC Prize 2025 with efficient computational performance while maintaining reasoning capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>610, <a href='https://arxiv.org/pdf/2509.05475.pdf' target='_blank'>https://arxiv.org/pdf/2509.05475.pdf</a></span>   <span><a href='https://github.com/AndrejOrsula/space_robotics_bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrej Orsula, Matthieu Geist, Miguel Olivares-Mendez, Carol Martinez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05475">Learning Tool-Aware Adaptive Compliant Control for Autonomous Regolith Excavation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Autonomous regolith excavation is a cornerstone of in-situ resource utilization for a sustained human presence beyond Earth. However, this task is fundamentally hindered by the complex interaction dynamics of granular media and the operational need for robots to use diverse tools. To address these challenges, this work introduces a framework where a model-based reinforcement learning agent learns within a parallelized simulation. This environment leverages high-fidelity particle physics and procedural generation to create a vast distribution of both lunar terrains and excavation tool geometries. To master this diversity, the agent learns an adaptive interaction strategy by dynamically modulating its own stiffness and damping at each control step through operational space control. Our experiments demonstrate that training with a procedural distribution of tools is critical for generalization and enables the development of sophisticated tool-aware behavior. Furthermore, we show that augmenting the agent with visual feedback significantly improves task success. These results represent a validated methodology for developing the robust and versatile autonomous systems required for the foundational tasks of future space missions.<br>
<span id='abs_ch'>中文摘要：本 究开发了一种基于模型的强化学 框架，通过高精度粒子仿真使自主机器人能够掌握跨多种月球地形和挖掘工具的适应性作业策略，证明程序化工具训练与视觉反馈可显著提升未来太空任务中系统的泛化能力和作业成功率。</span><br>
<span id='abs_en'>English Summary: This study develops a model-based reinforcement learning framework using high-fidelity particle simulations to enable autonomous robots to master adaptive excavation strategies across diverse lunar terrains and tool geometries, demonstrating that procedural tool training and visual feedback significantly enhance generalization and task success for future space missions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>611, <a href='https://arxiv.org/pdf/2509.05296.pdf' target='_blank'>https://arxiv.org/pdf/2509.05296.pdf</a></span>   <span><a href='https://github.com/LiZizun/WinT3R' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zizun Li, Jianjun Zhou, Yifan Wang, Haoyu Guo, Wenzheng Chang, Yang Zhou, Haoyi Zhu, Junyi Chen, Chunhua Shen, Tong He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05296">WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present WinT3R, a feed-forward reconstruction model capable of online prediction of precise camera poses and high-quality point maps. Previous methods suffer from a trade-off between reconstruction quality and real-time performance. To address this, we first introduce a sliding window mechanism that ensures sufficient information exchange among frames within the window, thereby improving the quality of geometric predictions without large computation. In addition, we leverage a compact representation of cameras and maintain a global camera token pool, which enhances the reliability of camera pose estimation without sacrificing efficiency. These designs enable WinT3R to achieve state-of-the-art performance in terms of online reconstruction quality, camera pose estimation, and reconstruction speed, as validated by extensive experiments on diverse datasets. Code and model are publicly available at https://github.com/LiZizun/WinT3R.<br>
<span id='abs_ch'>中文: WinT3R是一种前馈重建模型，通过滑动窗口机制和紧凑相机表示，在在线重建质量、相机姿态估计和速度方面均达到领先水平。</span><br>
<span id='abs_en'>English: WinT3R is a feed-forward reconstruction model that achieves state-of-the-art online reconstruction quality, camera pose estimation, and speed through a sliding window mechanism and compact camera representation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>612, <a href='https://arxiv.org/pdf/2509.05198.pdf' target='_blank'>https://arxiv.org/pdf/2509.05198.pdf</a></span>   <span><a href='https://github.com/m-saeid/ModeNetR_PointSkipNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Saeid, Amir Salarpour, Pedram MohajerAnsari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05198">Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The classification of 3D point clouds is crucial for applications such as autonomous driving, robotics, and augmented reality. However, the commonly used ModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D data, size mismatches, and inadequate class differentiation, which hinder model performance. This paper introduces ModelNet-R, a meticulously refined version of ModelNet40 designed to address these issues and serve as a more reliable benchmark. Additionally, this paper proposes Point-SkipNet, a lightweight graph-based neural network that leverages efficient sampling, neighborhood grouping, and skip connections to achieve high classification accuracy with reduced computational overhead. Extensive experiments demonstrate that models trained in ModelNet-R exhibit significant performance improvements. Notably, Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a substantially lower parameter count compared to contemporary models. This research highlights the crucial role of dataset quality in optimizing model efficiency for 3D point cloud classification. For more details, see the code at: https://github.com/m-saeid/ModeNetR_PointSkipNet.<br>
<span id='abs_ch'>中文: 本文提出了改进的3D点云数据集ModelNet-R以解决ModelNet40的缺陷，并设计了轻量级神经网络Point-SkipNet，该网络以更少参数实现最优分类精度，凸显了数据集质量对模型效能的关键作用。</span><br>
<span id='abs_en'>English: This paper introduces ModelNet-R, an improved 3D point cloud dataset addressing ModelNet40's limitations, and proposes Point-SkipNet, a lightweight neural network that achieves top accuracy with fewer parameters, emphasizing dataset quality's role in model efficiency.Paperid:1121,https://arxiv.org/pdf/2509.05154.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>613, <a href='https://arxiv.org/pdf/2509.05031.pdf' target='_blank'>https://arxiv.org/pdf/2509.05031.pdf</a></span>   <span><a href='https://github.com/lucamuellercode/MMITF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca MÃ¼ller, Hassan Ali, Philipp Allgeuer, LukÃ¡Å¡ GajdoÅ¡ech, Stefan Wermter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05031">Pointing-Guided Target Estimation via Transformer-Based Attention</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Deictic gestures, like pointing, are a fundamental form of non-verbal communication, enabling humans to direct attention to specific objects or locations. This capability is essential in Human-Robot Interaction (HRI), where robots should be able to predict human intent and anticipate appropriate responses. In this work, we propose the Multi-Modality Inter-TransFormer (MM-ITF), a modular architecture to predict objects in a controlled tabletop scenario with the NICOL robot, where humans indicate targets through natural pointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing gestures to object locations, assigns a likelihood score to each, and identifies the most likely target. Our results demonstrate that the method can accurately predict the intended object using monocular RGB data, thus enabling intuitive and accessible human-robot collaboration. To evaluate the performance, we introduce a patch confusion matrix, providing insights into the model's predictions across candidate object locations. Code available at: https://github.com/lucamuellercode/MMITF.<br>
<span id='abs_ch'>Chinese Summary: 本 究提出MM-ITF模块化架构，通过跨模态注意力机制将二维指向手势 射至目 物体位置，利用单目RGB数据实现精准意图识别，并引入区块混淆矩阵评估模型性能，为人机协作提供直观交互方案。</span><br>
<span id='abs_en'>English Summary: The study introduces MM-ITF, a modular architecture that accurately predicts target objects from human pointing gestures using monocular RGB data, enhancing intuitive human-robot collaboration through inter-modality attention and a novel evaluation metric.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>614, <a href='https://arxiv.org/pdf/2509.05007.pdf' target='_blank'>https://arxiv.org/pdf/2509.05007.pdf</a></span>   <span><a href='https://github.com/RUCAIBox/Sticker-TTS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Chen, Jinhao Jiang, Yingqian Min, Zican Dong, Shijie Wang, Wayne Xin Zhao, Ji-Rong Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05007">Sticker-TTS: Learn to Utilize Historical Experience with a Sticker-driven Test-Time Scaling Framework</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large reasoning models (LRMs) have exhibited strong performance on complex reasoning tasks, with further gains achievable through increased computational budgets at inference. However, current test-time scaling methods predominantly rely on redundant sampling, ignoring the historical experience utilization, thereby limiting computational efficiency. To overcome this limitation, we propose Sticker-TTS, a novel test-time scaling framework that coordinates three collaborative LRMs to iteratively explore and refine solutions guided by historical attempts. At the core of our framework are distilled key conditions-termed stickers-which drive the extraction, refinement, and reuse of critical information across multiple rounds of reasoning. To further enhance the efficiency and performance of our framework, we introduce a two-stage optimization strategy that combines imitation learning with self-improvement, enabling progressive refinement. Extensive evaluations on three challenging mathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH, demonstrate that Sticker-TTS consistently surpasses strong baselines, including self-consistency and advanced reinforcement learning approaches, under comparable inference budgets. These results highlight the effectiveness of sticker-guided historical experience utilization. Our code and data are available at https://github.com/RUCAIBox/Sticker-TTS.<br>
<span id='abs_ch'>中文摘要：Sticker-TTS是一种新颖的测试时扩展框架，通过协调多个大型推理模型利用历史经验迭代优化解决方案，在相同计算预算下于数学推理基准测试中显著优于现有方法。</span><br>
<span id='abs_en'>English Summary: Sticker-TTS is a novel test-time scaling framework that enhances computational efficiency by coordinating multiple large reasoning models to iteratively refine solutions using distilled historical information, outperforming existing methods on mathematical reasoning benchmarks under comparable inference budgets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>615, <a href='https://arxiv.org/pdf/2509.04908.pdf' target='_blank'>https://arxiv.org/pdf/2509.04908.pdf</a></span>   <span><a href='https://github.com/antgroup/SparkUI-Parser' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyi Jing, Jiafu Chen, Chen Rao, Ziqiang Dang, Jiajie Teng, Tianyi Chu, Juncheng Mo, Shuo Fang, Huaizhong Lin, Rui Lv, Chenguang Ma, Lei Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04908">SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The existing Multimodal Large Language Models (MLLMs) for GUI perception have made great progress. However, the following challenges still exist in prior methods: 1) They model discrete coordinates based on text autoregressive mechanism, which results in lower grounding accuracy and slower inference speed. 2) They can only locate predefined sets of elements and are not capable of parsing the entire interface, which hampers the broad application and support for downstream tasks. To address the above issues, we propose SparkUI-Parser, a novel end-to-end framework where higher localization precision and fine-grained parsing capability of the entire interface are simultaneously achieved. Specifically, instead of using probability-based discrete modeling, we perform continuous modeling of coordinates based on a pre-trained Multimodal Large Language Model (MLLM) with an additional token router and coordinate decoder. This effectively mitigates the limitations inherent in the discrete output characteristics and the token-by-token generation process of MLLMs, consequently boosting both the accuracy and the inference speed. To further enhance robustness, a rejection mechanism based on a modified Hungarian matching algorithm is introduced, which empowers the model to identify and reject non-existent elements, thereby reducing false positives. Moreover, we present ScreenParse, a rigorously constructed benchmark to systematically assess structural perception capabilities of GUI models across diverse scenarios. Extensive experiments demonstrate that our approach consistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2, CAGUI-Grounding and ScreenParse benchmarks. The resources are available at https://github.com/antgroup/SparkUI-Parser.<br>
<span id='abs_ch'>中文摘要：现有GUI感知多模态大语言模型 离散坐 建模和有限元 检测存在精度与速度问题，SparkUI-Parser通过连续坐 建模和增强解析能力，在多个基准测试中实现更优性能。</span><br>
<span id='abs_en'>English Summary: Existing multimodal large language models for GUI perception face challenges in accuracy and speed due to discrete coordinate modeling and limited element detection, which SparkUI-Parser addresses through continuous coordinate modeling and enhanced parsing capabilities to achieve superior performance across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>616, <a href='https://arxiv.org/pdf/2509.04844.pdf' target='_blank'>https://arxiv.org/pdf/2509.04844.pdf</a></span>   <span><a href='https://github.com/Nikol-coder/REMOTE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinkui Lin, Yongxiu Xu, Minghao Tang, Shilong Zhang, Hongbo Xu, Hao Xu, Yubin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04844">REMOTE: A Unified Multimodal Relation Extraction Framework with Multilevel Optimal Transport and Mixture-of-Experts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal relation extraction (MRE) is a crucial task in the fields of Knowledge Graph and Multimedia, playing a pivotal role in multimodal knowledge graph construction. However, existing methods are typically limited to extracting a single type of relational triplet, which restricts their ability to extract triplets beyond the specified types. Directly combining these methods fails to capture dynamic cross-modal interactions and introduces significant computational redundancy. Therefore, we propose a novel \textit{unified multimodal Relation Extraction framework with Multilevel Optimal Transport and mixture-of-Experts}, termed REMOTE, which can simultaneously extract intra-modal and inter-modal relations between textual entities and visual objects. To dynamically select optimal interaction features for different types of relational triplets, we introduce mixture-of-experts mechanism, ensuring the most relevant modality information is utilized. Additionally, considering that the inherent property of multilayer sequential encoding in existing encoders often leads to the loss of low-level information, we adopt a multilevel optimal transport fusion module to preserve low-level features while maintaining multilayer encoding, yielding more expressive representations. Correspondingly, we also create a Unified Multimodal Relation Extraction (UMRE) dataset to evaluate the effectiveness of our framework, encompassing diverse cases where the head and tail entities can originate from either text or image. Extensive experiments show that REMOTE effectively extracts various types of relational triplets and achieves state-of-the-art performanc on almost all metrics across two other public MRE datasets. We release our resources at https://github.com/Nikol-coder/REMOTE.<br>
<span id='abs_ch'>中文: REMOTE框架采用多层次最优 输和专家混合机制的统一多模态关系提取方法，能动态捕捉跨模态交互并保留底层特征，在多个数据集上实现了最先进的性能。</span><br>
<span id='abs_en'>English: The REMOTE framework introduces a unified multimodal relation extraction approach using multilevel optimal transport and mixture-of-experts to dynamically capture cross-modal interactions and preserve low-level features, achieving state-of-the-art performance across multiple datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>617, <a href='https://arxiv.org/pdf/2509.04833.pdf' target='_blank'>https://arxiv.org/pdf/2509.04833.pdf</a></span>   <span><a href='https://github.com/Dmmm1997/PropVG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ming Dai, Wenxuan Cheng, Jiedong Zhuang, Jiang-jiang Liu, Hongshen Zhao, Zhenhua Feng, Wankou Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04833">PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in visual grounding have largely shifted away from traditional proposal-based two-stage frameworks due to their inefficiency and high computational complexity, favoring end-to-end direct reference paradigms. However, these methods rely exclusively on the referred target for supervision, overlooking the potential benefits of prominent prospective targets. Moreover, existing approaches often fail to incorporate multi-granularity discrimination, which is crucial for robust object identification in complex scenarios. To address these limitations, we propose PropVG, an end-to-end proposal-based framework that, to the best of our knowledge, is the first to seamlessly integrate foreground object proposal generation with referential object comprehension without requiring additional detectors. Furthermore, we introduce a Contrastive-based Refer Scoring (CRS) module, which employs contrastive learning at both sentence and word levels to enhance the capability in understanding and distinguishing referred objects. Additionally, we design a Multi-granularity Target Discrimination (MTD) module that fuses object- and semantic-level information to improve the recognition of absent targets. Extensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO (REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes and models are available at https://github.com/Dmmm1997/PropVG.<br>
<span id='abs_ch'>中文摘要：PropVG提出了一种端到端的基于提议的框架，通过对比学 和多粒度判别机制解决视觉定位中的现有缺陷，在多个基准测试中实现了卓越性能。</span><br>
<span id='abs_en'>English Summary: PropVG introduces an end-to-end proposal-based framework with contrastive learning and multi-granularity discrimination to address limitations in visual grounding, achieving superior performance across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>618, <a href='https://arxiv.org/pdf/2509.04827.pdf' target='_blank'>https://arxiv.org/pdf/2509.04827.pdf</a></span>   <span><a href='https://github.com/Supercomputing-System-AI-Lab/VoltanaLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahuan Yu, Aryan Taneja, Junfeng Lin, Minjia Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04827">VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Modern Large Language Model (LLM) serving systems increasingly support interactive applications, like real-time chat assistants, code generation tools, and agentic workflows. However, the soaring energy cost of LLM inference presents a growing challenge for sustainable and cost-effective deployment. This paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM serving, built from a control theory perspective. VoltanaLLM co-designs frequency scaling and request routing in emerging prefill/decode disaggregated architectures, leveraging their decoupled execution to enable fine-grained phase-specific control. It consists of a feedback-driven frequency controller that dynamically adapts GPU frequency for prefill and decode phases, and a state-space router that explores routing decisions across frequency-scaled instances to minimize energy under latency constraints. We implement VoltanaLLM in SGLang and evaluate its performance over multiple state-of-the-art LLMs and real-world datasets. The results demonstrate that VoltanaLLM achieves up to 36.3% energy savings while maintaining near-perfect SLO attainment rate, paving the way for sustainable and intelligent LLM serving. Code of VoltanaLLM is open-sourced on GitHub: https://github.com/Supercomputing-System-AI-Lab/VoltanaLLM.<br>
<span id='abs_ch'>Chinese: 本文提出VoltanaLLM系统，通过动态GPU频率调节和智能请求路由实现LLM服务能效优化，在保证服务等级目 的同时最高可节省36.3%的能耗。</span><br>
<span id='abs_en'>English: This paper introduces VoltanaLLM, a system that optimizes energy efficiency in LLM serving through dynamic GPU frequency scaling and intelligent request routing, achieving up to 36.3% energy savings while maintaining service-level objectives.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>619, <a href='https://arxiv.org/pdf/2509.04669.pdf' target='_blank'>https://arxiv.org/pdf/2509.04669.pdf</a></span>   <span><a href='https://github.com/Wertyuui345/VCMamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mustafa Munir, Alex Zhang, Radu Marculescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04669">VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in Vision Transformers (ViTs) and State Space Models (SSMs) have challenged the dominance of Convolutional Neural Networks (CNNs) in computer vision. ViTs excel at capturing global context, and SSMs like Mamba offer linear complexity for long sequences, yet they do not capture fine-grained local features as effectively as CNNs. Conversely, CNNs possess strong inductive biases for local features but lack the global reasoning capabilities of transformers and Mamba. To bridge this gap, we introduce \textit{VCMamba}, a novel vision backbone that integrates the strengths of CNNs and multi-directional Mamba SSMs. VCMamba employs a convolutional stem and a hierarchical structure with convolutional blocks in its early stages to extract rich local features. These convolutional blocks are then processed by later stages incorporating multi-directional Mamba blocks designed to efficiently model long-range dependencies and global context. This hybrid design allows for superior feature representation while maintaining linear complexity with respect to image resolution. We demonstrate VCMamba's effectiveness through extensive experiments on ImageNet-1K classification and ADE20K semantic segmentation. Our VCMamba-B achieves 82.6% top-1 accuracy on ImageNet-1K, surpassing PlainMamba-L3 by 0.3% with 37% fewer parameters, and outperforming Vision GNN-B by 0.3% with 64% fewer parameters. Furthermore, VCMamba-B obtains 47.1 mIoU on ADE20K, exceeding EfficientFormer-L7 by 2.0 mIoU while utilizing 62% fewer parameters. Code is available at https://github.com/Wertyuui345/VCMamba.<br>
<span id='abs_ch'>中文: VCMamba是一种新型视觉骨干网络，融合了CNN的局部特征提取能力和多向Mamba SSM的全局上下文建模优势，在ImageNet分类和ADE20K分割任务中实现了线性复杂度的卓越性能。</span><br>
<span id='abs_en'>English: VCMamba is a novel vision backbone that combines CNNs' local feature extraction with multi-directional Mamba SSMs' global context modeling, achieving superior performance with linear complexity on ImageNet classification and ADE20K segmentation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>620, <a href='https://arxiv.org/pdf/2509.04632.pdf' target='_blank'>https://arxiv.org/pdf/2509.04632.pdf</a></span>   <span><a href='https://github.com/PierreWoL/SILLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Wu, Jiaoyan Chen, Norman W. Paton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04632">Schema Inference for Tabular Data Repositories Using Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Minimally curated tabular data often contain representational inconsistencies across heterogeneous sources, and are accompanied by sparse metadata. Working with such data is intimidating. While prior work has advanced dataset discovery and exploration, schema inference remains difficult when metadata are limited. We present SI-LLM (Schema Inference using Large Language Models), which infers a concise conceptual schema for tabular data using only column headers and cell values. The inferred schema comprises hierarchical entity types, attributes, and inter-type relationships. In extensive evaluation on two datasets from web tables and open data, SI-LLM achieves promising end-to-end results, as well as better or comparable results to state-of-the-art methods at each step. All source code, full prompts, and datasets of SI-LLM are available at https://github.com/PierreWoL/SILLM.<br>
<span id='abs_ch'>中文：SI-LLM利用大型语言模型，仅通过列 题和单元 值即可从元数据稀缺的表 数据中推断出层次化概念模式，在网页表 和开放数据集的评估中展现出优于或可比肩现有先进方法的性能。</span><br>
<span id='abs_en'>English: SI-LLM utilizes large language models to infer hierarchical conceptual schemas from tabular data with limited metadata, demonstrating competitive performance against state-of-the-art methods in evaluations on web tables and open datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>621, <a href='https://arxiv.org/pdf/2509.04504.pdf' target='_blank'>https://arxiv.org/pdf/2509.04504.pdf</a></span>   <span><a href='https://github.com/JarvisPei/Behavioral-Fingerprinting' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehua Pei, Hui-Ling Zhen, Ying Zhang, Zhiyuan Yang, Xing Li, Xianzhi Yu, Mingxuan Yuan, Bei Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04504">Behavioral Fingerprinting of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current benchmarks for Large Language Models (LLMs) primarily focus on performance metrics, often failing to capture the nuanced behavioral characteristics that differentiate them. This paper introduces a novel ``Behavioral Fingerprinting'' framework designed to move beyond traditional evaluation by creating a multi-faceted profile of a model's intrinsic cognitive and interactive styles. Using a curated \textit{Diagnostic Prompt Suite} and an innovative, automated evaluation pipeline where a powerful LLM acts as an impartial judge, we analyze eighteen models across capability tiers. Our results reveal a critical divergence in the LLM landscape: while core capabilities like abstract and causal reasoning are converging among top models, alignment-related behaviors such as sycophancy and semantic robustness vary dramatically. We further document a cross-model default persona clustering (ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together, this suggests that a model's interactive nature is not an emergent property of its scale or reasoning power, but a direct consequence of specific, and highly variable, developer alignment strategies. Our framework provides a reproducible and scalable methodology for uncovering these deep behavioral differences. Project: https://github.com/JarvisPei/Behavioral-Fingerprinting<br>
<br>
<div id='section'>Paperid: <span id='pid'>622, <a href='https://arxiv.org/pdf/2509.04476.pdf' target='_blank'>https://arxiv.org/pdf/2509.04476.pdf</a></span>   <span><a href='https://github.com/Songhyeontae/CAMT5.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seojin Kim, Hyeontae Song, Jaehyun Nam, Jinwoo Shin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04476">Training Text-to-Molecule Models with Context-Aware Tokenization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, text-to-molecule models have shown great potential across various chemical applications, e.g., drug-discovery. These models adapt language models to molecular data by representing molecules as sequences of atoms. However, they rely on atom-level tokenizations, which primarily focus on modeling local connectivity, thereby limiting the ability of models to capture the global structural context within molecules. To tackle this issue, we propose a novel text-to-molecule model, coined Context-Aware Molecular T5 (CAMT5). Inspired by the significance of the substructure-level contexts in understanding molecule structures, e.g., ring systems, we introduce substructure-level tokenization for text-to-molecule models. Building on our tokenization scheme, we develop an importance-based training strategy that prioritizes key substructures, enabling CAMT5 to better capture the molecular semantics. Extensive experiments verify the superiority of CAMT5 in various text-to-molecule generation tasks. Intriguingly, we find that CAMT5 outperforms the state-of-the-art methods using only 2% of training tokens. In addition, we propose a simple yet effective ensemble strategy that aggregates the outputs of text-to-molecule models to further boost the generation performance. Code is available at https://github.com/Songhyeontae/CAMT5.git.<br>
<span id='abs_ch'>中文: 提出的上下文感知分子T5（CAMT5）模型通过引入子结构级 记化和基于重要性的训练策略，能更好地捕捉分子全局结构，在文本到分子任务中以极少的训练 记实现了卓越性能。</span><br>
<span id='abs_en'>English: The proposed Context-Aware Molecular T5 (CAMT5) model introduces substructure-level tokenization and an importance-based training strategy to better capture global molecular structures, achieving superior performance in text-to-molecule tasks with significantly reduced training tokens.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>623, <a href='https://arxiv.org/pdf/2509.04460.pdf' target='_blank'>https://arxiv.org/pdf/2509.04460.pdf</a></span>   <span><a href='https://github.com/Y1hanChen/COCONUTS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihan Chen, Jiawei Chen, Guozhao Mo, Xuanang Chen, Ben He, Xianpei Han, Le Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04460">CoCoNUTS: Concentrating on Content while Neglecting Uninformative Textual Styles for AI-Generated Peer Review Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The growing integration of large language models (LLMs) into the peer review process presents potential risks to the fairness and reliability of scholarly evaluation. While LLMs offer valuable assistance for reviewers with language refinement, there is growing concern over their use to generate substantive review content. Existing general AI-generated text detectors are vulnerable to paraphrasing attacks and struggle to distinguish between surface language refinement and substantial content generation, suggesting that they primarily rely on stylistic cues. When applied to peer review, this limitation can result in unfairly suspecting reviews with permissible AI-assisted language enhancement, while failing to catch deceptively humanized AI-generated reviews. To address this, we propose a paradigm shift from style-based to content-based detection. Specifically, we introduce CoCoNUTS, a content-oriented benchmark built upon a fine-grained dataset of AI-generated peer reviews, covering six distinct modes of human-AI collaboration. Furthermore, we develop CoCoDet, an AI review detector via a multi-task learning framework, designed to achieve more accurate and robust detection of AI involvement in review content. Our work offers a practical foundation for evaluating the use of LLMs in peer review, and contributes to the development of more precise, equitable, and reliable detection methods for real-world scholarly applications. Our code and data will be publicly available at https://github.com/Y1hanChen/COCONUTS.<br>
<br>
<div id='section'>Paperid: <span id='pid'>624, <a href='https://arxiv.org/pdf/2509.04442.pdf' target='_blank'>https://arxiv.org/pdf/2509.04442.pdf</a></span>   <span><a href='https://github.com/OscarXZQ/delta_activations' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqiu Xu, Amish Sethi, Mayur Naik, Ser-Nam Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04442">Delta Activations: A Representation for Finetuned Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at https://github.com/OscarXZQ/delta_activations.<br>
<span id='abs_ch'>中文: Delta Activations 是一种创新方法，通过测量微调后大语言模型相对于基础模型的内部激活变化，将其表示为向量嵌入，从而实现按领域和任务的有效聚类，并展现出鲁棒性和可 性。</span><br>
<span id='abs_en'>English: Delta Activations is a novel method that represents fine-tuned large language models as vector embeddings by measuring their internal activation shifts relative to a base model, enabling effective clustering by domain and task while demonstrating robustness and additive properties.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>625, <a href='https://arxiv.org/pdf/2509.04439.pdf' target='_blank'>https://arxiv.org/pdf/2509.04439.pdf</a></span>   <span><a href='https://github.com/matt-seb-ho/arc_memo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Matthew Ho, Chen Si, Zhaoxiang Feng, Fangxu Yu, Yichi Yang, Zhijian Liu, Zhiting Hu, Lianhui Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04439">ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While inference-time scaling enables LLMs to carry out increasingly long and capable reasoning traces, the patterns and insights uncovered during these traces are immediately discarded once the context window is reset for a new query. External memory is a natural way to persist these discoveries, and recent work has shown clear benefits for reasoning-intensive tasks. We see an opportunity to make such memories more broadly reusable and scalable by moving beyond instance-based memory entries (e.g. exact query/response pairs, or summaries tightly coupled with the original problem context) toward concept-level memory: reusable, modular abstractions distilled from solution traces and stored in natural language. For future queries, relevant concepts are selectively retrieved and integrated into the prompt, enabling test-time continual learning without weight updates. Our design introduces new strategies for abstracting takeaways from rollouts and retrieving entries for new queries, promoting reuse and allowing memory to expand with additional experiences. We evaluate on ARC-AGI, a benchmark that stresses compositional generalization and abstract reasoning, making it a natural fit for concept memory. Our method yields a 7.5% relative gain over a strong no-memory baseline with performance continuing to scale with inference compute. We find abstract concepts to be the most consistent memory design, outscoring the baseline at all tested inference compute scales. Moreover, dynamically updating memory during test-time outperforms fixed settings, supporting the hypothesis that accumulating and abstracting patterns enables further solutions in a form of self-improvement. Code is available at https://github.com/matt-seb-ho/arc_memo.<br>
<span id='abs_ch'>Chinese: 本文提出了一种概念级记忆系统，能够从推理轨迹中提炼可重用的抽象概念，通过动态更新记忆实现测试时持续学 ，在ARC-AGI基准测试中取得了7.5%的性能提升。</span><br>
<span id='abs_en'>English: The paper introduces a concept-level memory system that distills reusable abstractions from reasoning traces, enabling test-time continual learning and achieving a 7.5% performance gain on the ARC-AGI benchmark through dynamic memory updates.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>626, <a href='https://arxiv.org/pdf/2509.04404.pdf' target='_blank'>https://arxiv.org/pdf/2509.04404.pdf</a></span>   <span><a href='https://github.com/kyrawilson/No-Thoughts-Just-AI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kyra Wilson, Mattea Sim, Anna-Maria Gueorguieva, Aylin Caliskan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04404">No Thoughts Just AI: Biased LLM Hiring Recommendations Alter Human Decision Making and Limit Human Autonomy</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this study, we conduct a resume-screening experiment (N=528) where people collaborate with simulated AI models exhibiting race-based preferences (bias) to evaluate candidates for 16 high and low status occupations. Simulated AI bias approximates factual and counterfactual estimates of racial bias in real-world AI systems. We investigate people's preferences for White, Black, Hispanic, and Asian candidates (represented through names and affinity groups on quality-controlled resumes) across 1,526 scenarios and measure their unconscious associations between race and status using implicit association tests (IATs), which predict discriminatory hiring decisions but have not been investigated in human-AI collaboration. When making decisions without AI or with AI that exhibits no race-based preferences, people select all candidates at equal rates. However, when interacting with AI favoring a particular group, people also favor those candidates up to 90% of the time, indicating a significant behavioral shift. The likelihood of selecting candidates whose identities do not align with common race-status stereotypes can increase by 13% if people complete an IAT before conducting resume screening. Finally, even if people think AI recommendations are low quality or not important, their decisions are still vulnerable to AI bias under certain circumstances. This work has implications for people's autonomy in AI-HITL scenarios, AI and work, design and evaluation of AI hiring systems, and strategies for mitigating bias in collaborative decision-making tasks. In particular, organizational and regulatory policy should acknowledge the complex nature of AI-HITL decision making when implementing these systems, educating people who use them, and determining which are subject to oversight.<br>
<span id='abs_ch'>中文摘要：本 究表明，人类的招聘决策会受到人工智能种族偏见的显著影响，最高可达90%的模仿率，但通过内隐联想测试提升认知后，这种影响可降低13%。</span><br>
<span id='abs_en'>English Summary: This study reveals that people's hiring decisions are significantly influenced by AI's racial biases, often mirroring them up to 90% of the time, though awareness through implicit association tests can reduce this effect by 13%.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>627, <a href='https://arxiv.org/pdf/2509.04180.pdf' target='_blank'>https://arxiv.org/pdf/2509.04180.pdf</a></span>   <span><a href='https://github.com/OschAI/VisioFirm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Safouane El Ghazouali, Umberto Michelucci
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04180">VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>AI models rely on annotated data to learn pattern and perform prediction. Annotation is usually a labor-intensive step that require associating labels ranging from a simple classification label to more complex tasks such as object detection, oriented bounding box estimation, and instance segmentation. Traditional tools often require extensive manual input, limiting scalability for large datasets. To address this, we introduce VisioFirm, an open-source web application designed to streamline image labeling through AI-assisted automation. VisioFirm integrates state-of-the-art foundation models into an interface with a filtering pipeline to reduce human-in-the-loop efforts. This hybrid approach employs CLIP combined with pre-trained detectors like Ultralytics models for common classes and zero-shot models such as Grounding DINO for custom labels, generating initial annotations with low-confidence thresholding to maximize recall. Through this framework, when tested on COCO-type of classes, initial prediction have been proven to be mostly correct though the users can refine these via interactive tools supporting bounding boxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has on-the-fly segmentation powered by Segment Anything accelerated through WebGPU for browser-side efficiency. The tool supports multiple export formats (YOLO, COCO, Pascal VOC, CSV) and operates offline after model caching, enhancing accessibility. VisioFirm demonstrates up to 90\% reduction in manual effort through benchmarks on diverse datasets, while maintaining high annotation accuracy via clustering of connected CLIP-based disambiguate components and IoU-graph for redundant detection suppression. VisioFirm can be accessed from \href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.<br>
<span id='abs_ch'>Chinese: VisioFirm 是一款开源网络应用程序，通过集成 CLIP 和 Grounding DINO 等基础模型实现AI辅助自动化图像 注，大幅减少人工操作，同时支持多种 注任务和导出 式。English: VisioFirm is an open-source web application that leverages AI-assisted automation with foundation models like CLIP and Grounding DINO to streamline image labeling, significantly reducing manual effort while supporting various annotation tasks and export formats.Paperid:1155,https://arxiv.org/pdf/2509.04150.pdfGitHub</span><br>
<span id='abs_en'>English: VisioFirm is an open-source web application that leverages AI-assisted automation with foundation models like CLIP and Grounding DINO to streamline image labeling, significantly reducing manual effort while supporting various annotation tasks and export formats.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>628, <a href='https://arxiv.org/pdf/2509.04125.pdf' target='_blank'>https://arxiv.org/pdf/2509.04125.pdf</a></span>   <span><a href='https://github.com/TarikZ03/Bluffing-by-DQN-and-CFR-in-Leduc-Hold-em-Poker-Codebase' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tarik Zaciragic, Aske Plaat, K. Joost Batenburg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04125">Analysis of Bluffing by DQN and CFR in Leduc Hold'em Poker</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In the game of poker, being unpredictable, or bluffing, is an essential skill. When humans play poker, they bluff. However, most works on computer-poker focus on performance metrics such as win rates, while bluffing is overlooked. In this paper we study whether two popular algorithms, DQN (based on reinforcement learning) and CFR (based on game theory), exhibit bluffing behavior in Leduc Hold'em, a simplified version of poker. We designed an experiment where we let the DQN and CFR agent play against each other while we log their actions. We find that both DQN and CFR exhibit bluffing behavior, but they do so in different ways. Although both attempt to perform bluffs at different rates, the percentage of successful bluffs (where the opponent folds) is roughly the same. This suggests that bluffing is an essential aspect of the game, not of the algorithm. Future work should look at different bluffing styles and at the full game of poker. Code at https://github.com/TarikZ03/Bluffing-by-DQN-and-CFR-in-Leduc-Hold-em-Poker-Codebase.<br>
<span id='abs_ch'>中文摘要：本 究表明，在Leduc Hold'em扑克游戏中，DQN和CFR两种算法均表现出虚 声势行为，尽管虚 频率不同但成功率相近，说明虚 是游戏本质特征而非算法特性。</span><br>
<span id='abs_en'>English Summary: This study demonstrates that both DQN and CFR algorithms exhibit bluffing behavior in Leduc Hold'em poker, with varying bluffing frequencies but similar success rates, indicating bluffing is inherent to the game rather than specific to algorithms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>629, <a href='https://arxiv.org/pdf/2509.04118.pdf' target='_blank'>https://arxiv.org/pdf/2509.04118.pdf</a></span>   <span><a href='https://github.com/bytedance/NEVC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junqi Liao, Yaojun Wu, Chaoyi Lin, Zhipin Deng, Li Li, Dong Liu, Xiaoyan Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04118">EHVC: Efficient Hierarchical Reference and Quality Structure for Neural Video Coding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Neural video codecs (NVCs), leveraging the power of end-to-end learning, have demonstrated remarkable coding efficiency improvements over traditional video codecs. Recent research has begun to pay attention to the quality structures in NVCs, optimizing them by introducing explicit hierarchical designs. However, less attention has been paid to the reference structure design, which fundamentally should be aligned with the hierarchical quality structure. In addition, there is still significant room for further optimization of the hierarchical quality structure. To address these challenges in NVCs, we propose EHVC, an efficient hierarchical neural video codec featuring three key innovations: (1) a hierarchical multi-reference scheme that draws on traditional video codec design to align reference and quality structures, thereby addressing the reference-quality mismatch; (2) a lookahead strategy to utilize an encoder-side context from future frames to enhance the quality structure; (3) a layer-wise quality scale with random quality training strategy to stabilize quality structures during inference. With these improvements, EHVC achieves significantly superior performance to the state-of-the-art NVCs. Code will be released in: https://github.com/bytedance/NEVC.<br>
<span id='abs_ch'>中文摘要：提出的EHVC神经视频编解 器通过三项创新——层次化多参考对齐、前瞻上下文利用和稳定质量训练，优化了参考与质量结构，性能显著优于现有技术。</span><br>
<span id='abs_en'>English Summary: The proposed EHVC neural video codec introduces three innovations—hierarchical multi-reference alignment, lookahead context utilization, and stabilized quality training—to significantly outperform existing codecs by optimizing reference and quality structures.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>630, <a href='https://arxiv.org/pdf/2509.04027.pdf' target='_blank'>https://arxiv.org/pdf/2509.04027.pdf</a></span>   <span><a href='https://github.com/ZyGan1999/CoT-Space' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Gan, Hao Yi, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04027">CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning (RL) has become a pivotal approach for enhancing the reasoning capabilities of Large Language Models (LLMs). However, a significant theoretical gap persists, as traditional token-level RL frameworks fail to align with the reasoning-level nature of complex, multi-step thought processes like Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space, a novel theoretical framework that recasts LLM reasoning from a discrete token-prediction task to an optimization process within a continuous, reasoning-level semantic space. This shift in perspective serves as a conceptual bridge, revitalizing foundational principles from classical learning theory to analyze the unique dynamics of LLMs. By analyzing this process from both a noise perspective and a risk perspective, we demonstrate that the convergence to an optimal CoT length is a natural consequence of the fundamental trade-off between underfitting and overfitting. Furthermore, extensive experiments provide strong empirical validation for our theoretical findings. Our framework not only provides a coherent explanation for empirical phenomena such as overthinking but also offers a solid theoretical foundation to guide the future development of more effective and generalizable reasoning agents. We open-source our code at https://github.com/ZyGan1999/CoT-Space.<br>
<span id='abs_ch'>中文摘要：CoT-Space框架将大语言模型的推理重新定义为连续语义空间中的优化过程，通过连接经典学 理论解释了过度思考等现象，并为开发更优推理智能体 定了理论基础。</span><br>
<span id='abs_en'>English Summary: The CoT-Space framework redefines LLM reasoning as optimization in a continuous semantic space, bridging classical learning theory to explain phenomena like overthinking and providing a theoretical foundation for developing better reasoning agents.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>631, <a href='https://arxiv.org/pdf/2509.04011.pdf' target='_blank'>https://arxiv.org/pdf/2509.04011.pdf</a></span>   <span><a href='https://github.com/ShacharOr100/ner_retriever' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Or Shachar, Uri Katz, Yoav Goldberg, Oren Glickman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.04011">NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named Entity Retrieval, a variant of Named Entity Recognition (NER), where the types of interest are not provided in advance, and a user-defined type description is used to retrieve documents mentioning entities of that type. Instead of relying on fixed schemas or fine-tuned models, our method builds on internal representations of large language models (LLMs) to embed both entity mentions and user-provided open-ended type descriptions into a shared semantic space. We show that internal representations, specifically the value vectors from mid-layer transformer blocks, encode fine-grained type information more effectively than commonly used top-layer embeddings. To refine these representations, we train a lightweight contrastive projection network that aligns type-compatible entities while separating unrelated types. The resulting entity embeddings are compact, type-aware, and well-suited for nearest-neighbor search. Evaluated on three benchmarks, NER Retriever significantly outperforms both lexical and dense sentence-level retrieval baselines. Our findings provide empirical support for representation selection within LLMs and demonstrate a practical solution for scalable, schema-free entity retrieval. The NER Retriever Codebase is publicly available at https://github.com/ShacharOr100/ner_retriever<br>
<span id='abs_ch'>中文: NER检索器是一种零 本检索框架，利用大型语言模型的内部表示将实体和用户定义的类型描述嵌入共享语义空间， 需预定义模式即可在实体检索基准上实现卓越性能。</span><br>
<span id='abs_en'>English: NER Retriever is a zero-shot framework that leverages large language models' internal representations to embed entities and user-defined type descriptions into a shared semantic space, achieving superior performance on entity retrieval benchmarks without predefined schemas.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>632, <a href='https://arxiv.org/pdf/2509.03995.pdf' target='_blank'>https://arxiv.org/pdf/2509.03995.pdf</a></span>   <span><a href='https://github.com/zjukg/RTQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoyan Gong, Juan Li, Zhiqiang Liu, Lei Liang, Huajun Chen, Wen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03995">RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question Answering with Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current temporal knowledge graph question answering (TKGQA) methods primarily focus on implicit temporal constraints, lacking the capability of handling more complex temporal queries, and struggle with limited reasoning abilities and error propagation in decomposition frameworks. We propose RTQA, a novel framework to address these challenges by enhancing reasoning over TKGs without requiring training. Following recursive thinking, RTQA recursively decomposes questions into sub-problems, solves them bottom-up using LLMs and TKG knowledge, and employs multi-path answer aggregation to improve fault tolerance. RTQA consists of three core components: the Temporal Question Decomposer, the Recursive Solver, and the Answer Aggregator. Experiments on MultiTQ and TimelineKGQA benchmarks demonstrate significant Hits@1 improvements in "Multiple" and "Complex" categories, outperforming state-of-the-art methods. Our code and data are available at https://github.com/zjukg/RTQA.<br>
<span id='abs_ch'>中文: RTQA是一种新颖的框架，通过递归分解复杂查询为子问题，利用大语言模型和时序知识图谱知识自底向上求解，并采用多路径答案聚合提升容错性，在基准测试中实现了最先进的性能表现。</span><br>
<span id='abs_en'>English: RTQA is a novel framework that enhances reasoning over temporal knowledge graphs by recursively decomposing complex queries into sub-problems, solving them with LLMs and TKG knowledge, and aggregating answers for improved fault tolerance, achieving state-of-the-art performance on benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>633, <a href='https://arxiv.org/pdf/2509.03961.pdf' target='_blank'>https://arxiv.org/pdf/2509.03961.pdf</a></span>   <span><a href='https://github.com/yikuizhai/MMChange' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yijun Zhou, Yikui Zhai, Zilu Ying, Tingfeng Xian, Wenlve Zhou, Zhiheng Zhou, Xiaolin Tian, Xudong Jia, Hongsheng Zhang, C. L. Philip Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03961">Multimodal Feature Fusion Network with Text Difference Enhancement for Remote Sensing Change Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although deep learning has advanced remote sensing change detection (RSCD), most methods rely solely on image modality, limiting feature representation, change pattern modeling, and generalization especially under illumination and noise disturbances. To address this, we propose MMChange, a multimodal RSCD method that combines image and text modalities to enhance accuracy and robustness. An Image Feature Refinement (IFR) module is introduced to highlight key regions and suppress environmental noise. To overcome the semantic limitations of image features, we employ a vision language model (VLM) to generate semantic descriptions of bitemporal images. A Textual Difference Enhancement (TDE) module then captures fine grained semantic shifts, guiding the model toward meaningful changes. To bridge the heterogeneity between modalities, we design an Image Text Feature Fusion (ITFF) module that enables deep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and SYSUCD demonstrate that MMChange consistently surpasses state of the art methods across multiple metrics, validating its effectiveness for multimodal RSCD. Code is available at: https://github.com/yikuizhai/MMChange.<br>
<span id='abs_ch'>中文: 提出的MMChange方法通过融合图像与文本模态，借助专门模块提升遥感变化检测的精度和鲁棒性，在多项实验中均优于现有最优方法。</span><br>
<span id='abs_en'>English: The proposed MMChange method enhances remote sensing change detection by integrating image and text modalities through specialized modules to improve accuracy and robustness, outperforming state-of-the-art approaches in experiments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>634, <a href='https://arxiv.org/pdf/2509.03957.pdf' target='_blank'>https://arxiv.org/pdf/2509.03957.pdf</a></span>   <span><a href='https://github.com/SCUNLP/CANDY' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiling Guo, Xinwei Yang, Chen Huang, Tong Zhang, Yong Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03957">CANDY: Benchmarking LLMs' Limitations and Assistive Potential in Chinese Misinformation Fact-Checking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The effectiveness of large language models (LLMs) to fact-check misinformation remains uncertain, despite their growing use. To this end, we present CANDY, a benchmark designed to systematically evaluate the capabilities and limitations of LLMs in fact-checking Chinese misinformation. Specifically, we curate a carefully annotated dataset of ~20k instances. Our analysis shows that current LLMs exhibit limitations in generating accurate fact-checking conclusions, even when enhanced with chain-of-thought reasoning and few-shot prompting. To understand these limitations, we develop a taxonomy to categorize flawed LLM-generated explanations for their conclusions and identify factual fabrication as the most common failure mode. Although LLMs alone are unreliable for fact-checking, our findings indicate their considerable potential to augment human performance when deployed as assistive tools in scenarios. Our dataset and code can be accessed at https://github.com/SCUNLP/CANDY<br>
<span id='abs_ch'>中文摘要：CANDY基准测试表明，尽管大型语言模型在中文不实信息 查中存在事实捏 等局限，但作为辅助工具仍具备提升人类 查能力的潜力。</span><br>
<span id='abs_en'>English Summary: The CANDY benchmark reveals that large language models currently struggle with accurate Chinese misinformation fact-checking due to frequent factual fabrication, yet they show promise as assistive tools for human fact-checkers.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>635, <a href='https://arxiv.org/pdf/2509.03934.pdf' target='_blank'>https://arxiv.org/pdf/2509.03934.pdf</a></span>   <span><a href='https://github.com/USTC-StarTeam/SelfAug' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqing Huang, Rongyang Zhang, Qimeng Wang, Chengqiang Lu, Yan Gao, Yi Wu, Yao Hu, Xuyang Zhi, Guiquan Liu, Xin Li, Hao Wang, Enhong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03934">SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented Generation via Distribution Self-Alignment</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in large language models (LLMs) have revolutionized natural language processing through their remarkable capabilities in understanding and executing diverse tasks. While supervised fine-tuning, particularly in Retrieval-Augmented Generation (RAG) scenarios, effectively enhances task-specific performance, it often leads to catastrophic forgetting, where models lose their previously acquired knowledge and general capabilities. Existing solutions either require access to general instruction data or face limitations in preserving the model's original distribution. To overcome these limitations, we propose SelfAug, a self-distribution alignment method that aligns input sequence logits to preserve the model's semantic distribution, thereby mitigating catastrophic forgetting and improving downstream performance. Extensive experiments demonstrate that SelfAug achieves a superior balance between downstream learning and general capability retention. Our comprehensive empirical analysis reveals a direct correlation between distribution shifts and the severity of catastrophic forgetting in RAG scenarios, highlighting how the absence of RAG capabilities in general instruction tuning leads to significant distribution shifts during fine-tuning. Our findings not only advance the understanding of catastrophic forgetting in RAG contexts but also provide a practical solution applicable across diverse fine-tuning scenarios. Our code is publicly available at https://github.com/USTC-StarTeam/SelfAug.<br>
<span id='abs_ch'>中文: 提出的SelfAug方法通过对齐输入序列对数来保持模型语义分布，有效缓解了微调大语言模型中的灾难性遗忘问题，在下游任务性能和通用能力保留之间实现了更优的平衡。</span><br>
<span id='abs_en'>English: The proposed SelfAug method effectively mitigates catastrophic forgetting in fine-tuned LLMs by aligning input sequence logits to preserve the model's semantic distribution, achieving superior balance between downstream task performance and general capability retention.</span>Paperid:1167,https://arxiv.org/pdf/2509.03918.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>636, <a href='https://arxiv.org/pdf/2509.03918.pdf' target='_blank'>https://arxiv.org/pdf/2509.03918.pdf</a></span>   <span><a href='https://github.com/lyfiter/mtqa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengxiao Tang, Yufeng Li, Zongzong Wu, Ming Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03918">Chain or tree? Re-evaluating complex reasoning from the perspective of a matrix of thought</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) face significant accuracy degradation due to insufficient reasoning ability when dealing with complex and abstract tasks. Thought structures such as Chain of Thought (CoT) and Tree of Thought (ToT) focus on enhancing the reasoning capability of LLMs. However, they suffer from inherent drawbacks such as redundancy within the same layer of the tree structure and the singularity of the paths in the chain structure. Some studies have utilized Retrieval-Augmented Generation (RAG) methods to enhance CoT and ToT in mitigating hallucinations in LLMs, yet the fundamental shortcomings of the thought structures still persist. Furthermore, when dealing with multi-entity and multi-hop information, the retrieved verification knowledge often contains large amounts of fragmented, superficial, or even erroneous data, misleading the reasoning process of LLMs. To address these issues, we propose the Matrix of Thought (MoT), a novel and efficient thought structure for LLMs. MoT explores problems in both horizontal and vertical dimensions through a "column-cell communication" mechanism, enabling LLMs to actively engage in multi-strategy and deep thinking while reducing redundancy in the thought nodes within the column cells, thereby enhancing the reasoning capability of LLMs. Additionally, through a fact-correction mechanism, it leverages the knowledge graph triples retrieved by RAG and the original text to construct knowledge units and correct erroneous answers. To validate the effectiveness of this method, we conducted extensive experiments in three tasks: 24-point game, question answering evaluation, and proposition writing.The results demonstrate that our framework outperforms state-of-the-art methods, with reasoning time only 14.4\% of that of the baseline method, proving its efficiency and accuracy. The code for framework is available at https://github.com/lyfiter/mtqa.<br>
<span id='abs_ch'>中文摘要：针对大型语言模型在复杂任务中的推理缺陷，本文提出的思维矩阵（MoT）通过纵横维度的"列-单元通信"机制实现多策略深度思考，结合事实 正机制有效提升推理能力与效率，实验证明其性能显著优于现有方法。English Summary: The Matrix of Thought (MoT) is introduced as an efficient reasoning structure for Large Language Models, addressing limitations in existing methods by enabling multi-dimensional thinking and reducing redundancy while incorporating fact-correction mechanisms to enhance accuracy and speed.Paperid:1169,https://arxiv.org/pdf/2509.03897.pdfGitHub</span><br>
<span id='abs_en'>English Summary: The Matrix of Thought (MoT) is introduced as an efficient reasoning structure for Large Language Models, addressing limitations in existing methods by enabling multi-dimensional thinking and reducing redundancy while incorporating fact-correction mechanisms to enhance accuracy and speed.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>637, <a href='https://arxiv.org/pdf/2509.03873.pdf' target='_blank'>https://arxiv.org/pdf/2509.03873.pdf</a></span>   <span><a href='https://github.com/Jiajun-RUC/SalientFusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajun Song, Xiaoou Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03873">SalientFusion: Context-Aware Compositional Zero-Shot Food Recognition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Food recognition has gained significant attention, but the rapid emergence of new dishes requires methods for recognizing unseen food categories, motivating Zero-Shot Food Learning (ZSFL). We propose the task of Compositional Zero-Shot Food Recognition (CZSFR), where cuisines and ingredients naturally align with attributes and objects in Compositional Zero-Shot learning (CZSL). However, CZSFR faces three challenges: (1) Redundant background information distracts models from learning meaningful food features, (2) Role confusion between staple and side dishes leads to misclassification, and (3) Semantic bias in a single attribute can lead to confusion of understanding. Therefore, we propose SalientFusion, a context-aware CZSFR method with two components: SalientFormer, which removes background redundancy and uses depth features to resolve role confusion; DebiasAT, which reduces the semantic bias by aligning prompts with visual features. Using our proposed benchmarks, CZSFood-90 and CZSFood-164, we show that SalientFusion achieves state-of-the-art results on these benchmarks and the most popular general datasets for the general CZSL. The code is avaliable at https://github.com/Jiajun-RUC/SalientFusion.<br>
<span id='abs_ch'>中文: 本 究提出组合零 本食物识别任务以解决背景干扰和语义偏差等挑战，通过SalientFusion方法在新基准和通用数据集上取得了最优性能。</span><br>
<span id='abs_en'>English: The study introduces Compositional Zero-Shot Food Recognition (CZSFR) to address challenges like background distraction and semantic bias, proposing the SalientFusion method which achieves state-of-the-art results on new benchmarks and general datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>638, <a href='https://arxiv.org/pdf/2509.03871.pdf' target='_blank'>https://arxiv.org/pdf/2509.03871.pdf</a></span>   <span><a href='https://github.com/ybwang119/Awesome-reasoning-safety' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanbo Wang, Yongcan Yu, Jian Liang, Ran He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03871">A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The development of Long-CoT reasoning has advanced LLM performance across various tasks, including language understanding, complex problem solving, and code generation. This paradigm enables models to generate intermediate reasoning steps, thereby improving both accuracy and interpretability. However, despite these advancements, a comprehensive understanding of how CoT-based reasoning affects the trustworthiness of language models remains underdeveloped. In this paper, we survey recent work on reasoning models and CoT techniques, focusing on five core dimensions of trustworthy reasoning: truthfulness, safety, robustness, fairness, and privacy. For each aspect, we provide a clear and structured overview of recent studies in chronological order, along with detailed analyses of their methodologies, findings, and limitations. Future research directions are also appended at the end for reference and discussion. Overall, while reasoning techniques hold promise for enhancing model trustworthiness through hallucination mitigation, harmful content detection, and robustness improvement, cutting-edge reasoning models themselves often suffer from comparable or even greater vulnerabilities in safety, robustness, and privacy. By synthesizing these insights, we hope this work serves as a valuable and timely resource for the AI safety community to stay informed on the latest progress in reasoning trustworthiness. A full list of related papers can be found at \href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.<br>
<span id='abs_ch'>中文: 本文综述了思维链推理对语言模型可信度在真实性、安全性、鲁棒性、公平性和隐私性五个维度的影响，发现其虽提升准确性和可解释性，但也带来脆弱性，为AI安全 究提供了全面参考。</span><br>
<span id='abs_en'>English: This paper surveys how Chain-of-Thought reasoning impacts language model trustworthiness across five dimensions—truthfulness, safety, robustness, fairness, and privacy—finding that while it enhances accuracy and interpretability, it also introduces vulnerabilities, providing a comprehensive resource for AI safety research.Paperid:1176,https://arxiv.org/pdf/2509.03829.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>639, <a href='https://arxiv.org/pdf/2509.03754.pdf' target='_blank'>https://arxiv.org/pdf/2509.03754.pdf</a></span>   <span><a href='https://github.com/RzMY/STA-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongsen Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03754">STA-Net: A Decoupled Shape and Texture Attention Network for Lightweight Plant Disease Classification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Responding to rising global food security needs, precision agriculture and deep learning-based plant disease diagnosis have become crucial. Yet, deploying high-precision models on edge devices is challenging. Most lightweight networks use attention mechanisms designed for generic object recognition, which poorly capture subtle pathological features like irregular lesion shapes and complex textures. To overcome this, we propose a twofold solution: first, using a training-free neural architecture search method (DeepMAD) to create an efficient network backbone for edge devices; second, introducing the Shape-Texture Attention Module (STAM). STAM splits attention into two branches -- one using deformable convolutions (DCNv4) for shape awareness and the other using a Gabor filter bank for texture awareness. On the public CCMT plant disease dataset, our STA-Net model (with 401K parameters and 51.1M FLOPs) reached 89.00% accuracy and an F1 score of 88.96%. Ablation studies confirm STAM significantly improves performance over baseline and standard attention models. Integrating domain knowledge via decoupled attention thus presents a promising path for edge-deployed precision agriculture AI. The source code is available at https://github.com/RzMY/STA-Net.<br>
<span id='abs_ch'>中文: 针对现有注意力机制难以捕捉植物病害细微特征的问题，本 究提出STA-Net轻量级模型，通过 训练神经网络架构搜索主干和新型形状-纹理双分支注意力模块，在植物病害数据集上实现89.00%的准确率，为边缘设备上的精准农业应用提供了有效解决方案。</span><br>
<span id='abs_en'>English: To address the limitations of generic attention mechanisms in capturing subtle plant disease features, this study introduces STA-Net, a lightweight model combining a training-free neural architecture search backbone with a novel Shape-Texture Attention Module that achieves 89.00% accuracy on plant disease diagnosis while being optimized for edge deployment.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>640, <a href='https://arxiv.org/pdf/2509.03730.pdf' target='_blank'>https://arxiv.org/pdf/2509.03730.pdf</a></span>   <span><a href='https://github.com/psychology-of-AI/Personality-Illusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengrui Han, Rafal Kocielnik, Peiyang Song, Ramit Debnath, Dean Mobbs, Anima Anandkumar, R. Michael Alvarez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03730">The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Personality traits have long been studied as predictors of human behavior. Recent advances in Large Language Models (LLMs) suggest similar patterns may emerge in artificial systems, with advanced LLMs displaying consistent behavioral tendencies resembling human traits like agreeableness and self-regulation. Understanding these patterns is crucial, yet prior work primarily relied on simplified self-reports and heuristic prompting, with little behavioral validation. In this study, we systematically characterize LLM personality across three dimensions: (1) the dynamic emergence and evolution of trait profiles throughout training stages; (2) the predictive validity of self-reported traits in behavioral tasks; and (3) the impact of targeted interventions, such as persona injection, on both self-reports and behavior. Our findings reveal that instructional alignment (e.g., RLHF, instruction tuning) significantly stabilizes trait expression and strengthens trait correlations in ways that mirror human data. However, these self-reported traits do not reliably predict behavior, and observed associations often diverge from human patterns. While persona injection successfully steers self-reports in the intended direction, it exerts little or inconsistent effect on actual behavior. By distinguishing surface-level trait expression from behavioral consistency, our findings challenge assumptions about LLM personality and underscore the need for deeper evaluation in alignment and interpretability.<br>
<span id='abs_ch'>中文摘要：本 究系统分析了大语言模型的性 特征，发现虽然指令对齐能稳定类似人类的特质表达，但自我报告的特质 法可 预测行为，且角色注入主要影响表面报告而非实际行为一致性。</span><br>
<span id='abs_en'>English Summary: This study systematically examines LLM personality traits, revealing that while instructional alignment stabilizes trait expression similar to humans, self-reported traits fail to reliably predict behavior and persona injections primarily affect surface-level reports rather than actual behavioral consistency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>641, <a href='https://arxiv.org/pdf/2509.03695.pdf' target='_blank'>https://arxiv.org/pdf/2509.03695.pdf</a></span>   <span><a href='https://github.com/payamsiabd/M3T-FFM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Payam Abdisarabshali, Fardis Nadimi, Kasra Borazjani, Naji Khosravan, Minghui Liwang, Wei Ni, Dusit Niyato, Michael Langberg, Seyyedali Hosseinalipour
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03695">Hierarchical Federated Foundation Models over Wireless Networks for Multi-Modal Multi-Task Intelligence: Integration of Edge Learning with D2D/P2P-Enabled Fog Learning Architectures</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rise of foundation models (FMs) has reshaped the landscape of machine learning. As these models continued to grow, leveraging geo-distributed data from wireless devices has become increasingly critical, giving rise to federated foundation models (FFMs). More recently, FMs have evolved into multi-modal multi-task (M3T) FMs (e.g., GPT-4) capable of processing diverse modalities across multiple tasks, which motivates a new underexplored paradigm: M3T FFMs. In this paper, we unveil an unexplored variation of M3T FFMs by proposing hierarchical federated foundation models (HF-FMs), which in turn expose two overlooked heterogeneity dimensions to fog/edge networks that have a direct impact on these emerging models: (i) heterogeneity in collected modalities and (ii) heterogeneity in executed tasks across fog/edge nodes. HF-FMs strategically align the modular structure of M3T FMs, comprising modality encoders, prompts, mixture-of-experts (MoEs), adapters, and task heads, with the hierarchical nature of fog/edge infrastructures. Moreover, HF-FMs enable the optional usage of device-to-device (D2D) communications, enabling horizontal module relaying and localized cooperative training among nodes when feasible. Through delving into the architectural design of HF-FMs, we highlight their unique capabilities along with a series of tailored future research directions. Finally, to demonstrate their potential, we prototype HF-FMs in a wireless network setting and release the open-source code for the development of HF-FMs with the goal of fostering exploration in this untapped field (GitHub: https://github.com/payamsiabd/M3T-FFM).<br>
<span id='abs_ch'>中文: 本文提出分层联邦基础模型（HF-FMs），通过将多模态多任务基础模型与雾计算/边缘网络层级对齐，解决模态和任务异质性，同时支持设备间通信和本地化协同训练。</span><br>
<span id='abs_en'>English: The paper introduces hierarchical federated foundation models (HF-FMs), a novel paradigm that aligns multi-modal multi-task foundation models with fog/edge network hierarchies to address modality and task heterogeneity while enabling device-to-device communication and localized training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>642, <a href='https://arxiv.org/pdf/2509.03594.pdf' target='_blank'>https://arxiv.org/pdf/2509.03594.pdf</a></span>   <span><a href='https://github.com/harveyThomas4692/Induced-Metric-Optimiser' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas R. Harvey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03594">The Optimiser Hidden in Plain Sight: Training with the Loss Landscape's Induced Metric</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present a class of novel optimisers for training neural networks that makes use of the Riemannian metric naturally induced when the loss landscape is embedded in higher-dimensional space. This is the same metric that underlies common visualisations of loss landscapes. By taking this geometric perspective literally and using the induced metric, we develop a new optimiser and compare it to existing methods, namely: SGD, Adam, AdamW, and Muon, across a range of tasks and architectures. Empirically, we conclude that this new class of optimisers is highly effective in low dimensional examples, and provides slight improvement over state-of-the-art methods for training neural networks. These new optimisers have theoretically desirable properties. In particular, the effective learning rate is automatically decreased in regions of high curvature acting as a smoothed out form of gradient clipping. Similarly, one variant of these optimisers can also be viewed as inducing an effective scheduled learning rate and decoupled weight decay is the natural choice from our geometric perspective. The basic method can be used to modify any existing preconditioning method. The new optimiser has a computational complexity comparable to that of Adam.<br>
<span id='abs_ch'>Chinese Summary: 本文提出了一类新颖的神经网络优化器，利用损失景观嵌入高维空间时自然诱导的黎曼度量，在低维示例中表现优异，相比现有最优方法略有提升，并具有理论优势如自适应学 率和解耦权重衰减。English Summary: This paper introduces a novel class of optimizers for neural networks that leverage the Riemannian metric from embedding loss landscapes in higher dimensions, showing effectiveness in low-dimensional cases and slight improvements over state-of-the-art methods with desirable theoretical properties like adaptive learning rates.Paperid:1188,https://arxiv.org/pdf/2509.03487.pdfGitHub</span><br>
<span id='abs_en'>English Summary: This paper introduces a novel class of optimizers for neural networks that leverage the Riemannian metric from embedding loss landscapes in higher dimensions, showing effectiveness in low-dimensional cases and slight improvements over state-of-the-art methods with desirable theoretical properties like adaptive learning rates.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>643, <a href='https://arxiv.org/pdf/2509.03487.pdf' target='_blank'>https://arxiv.org/pdf/2509.03487.pdf</a></span>   <span><a href='https://github.com/jigang-fan/SafeProtein' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jigang Fan, Zhenghong Zhou, Ruofan Jin, Le Cong, Mengdi Wang, Zaixi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03487">SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Proteins play crucial roles in almost all biological processes. The advancement of deep learning has greatly accelerated the development of protein foundation models, leading to significant successes in protein understanding and design. However, the lack of systematic red-teaming for these models has raised serious concerns about their potential misuse, such as generating proteins with biological safety risks. This paper introduces SafeProtein, the first red-teaming framework designed for protein foundation models to the best of our knowledge. SafeProtein combines multimodal prompt engineering and heuristic beam search to systematically design red-teaming methods and conduct tests on protein foundation models. We also curated SafeProtein-Bench, which includes a manually constructed red-teaming benchmark dataset and a comprehensive evaluation protocol. SafeProtein achieved continuous jailbreaks on state-of-the-art protein foundation models (up to 70% attack success rate for ESM3), revealing potential biological safety risks in current protein foundation models and providing insights for the development of robust security protection technologies for frontier models. The codes will be made publicly available at https://github.com/jigang-fan/SafeProtein.<br>
<span id='abs_ch'>中文：本文提出了首个蛋白质基础模型红队测试框架SafeProtein，通过多模态提示工程和启发式束搜索方法，在先进模型上实现了高达70%的攻击成功率，揭示了当前蛋白质基础模型存在的生物安全风险。</span><br>
<span id='abs_en'>English: This paper introduces SafeProtein, the first red-teaming framework for protein foundation models, which successfully exposed biological safety risks by achieving up to 70% attack success rates on state-of-the-art models through multimodal prompt engineering and heuristic beam search.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>644, <a href='https://arxiv.org/pdf/2509.03403.pdf' target='_blank'>https://arxiv.org/pdf/2509.03403.pdf</a></span>   <span><a href='https://github.com/Chenluye99/PROF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenlu Ye, Zhou Yu, Ziji Zhang, Hao Chen, Narayanan Sadagopan, Jing Huang, Tong Zhang, Anurag Beniwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03403">Beyond Correctness: Harmonizing Process and Outcome Rewards through RL Training</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning with verifiable rewards (RLVR) has emerged to be a predominant paradigm for mathematical reasoning tasks, offering stable improvements in reasoning ability. However, Outcome Reward Models (ORMs) in RLVR are too coarse-grained to distinguish flawed reasoning within correct answers or valid reasoning within incorrect answers. This lack of granularity introduces noisy and misleading gradients significantly and hinders further progress in reasoning process quality. While Process Reward Models (PRMs) offer fine-grained guidance for intermediate steps, they frequently suffer from inaccuracies and are susceptible to reward hacking. To resolve this dilemma, we introduce PRocess cOnsistency Filter (PROF), an effective data process curation method that harmonizes noisy, fine-grained process rewards with accurate, coarse-grained outcome rewards. Rather than naively blending PRM and ORM in the objective function (arXiv:archive/2506.18896), PROF leverages their complementary strengths through consistency-driven sample selection. Our approach retains correct responses with higher averaged process values and incorrect responses with lower averaged process values, while maintaining positive/negative training sample balance. Extensive experiments demonstrate that our method not only consistently improves the final accuracy over $4\%$ compared to the blending approaches, but also strengthens the quality of intermediate reasoning steps. Codes and training recipes are available at https://github.com/Chenluye99/PROF.<br>
<span id='abs_ch'>中文摘要：本文提出PROF方法，通过一致性驱动的 本选择协调细粒度过程奖励与粗粒度结果奖励，在提升数学推理最终准确率的同时强化中间推理步骤的质量。</span><br>
<span id='abs_en'>English Summary: The paper introduces PROF, a method that combines fine-grained process rewards and coarse-grained outcome rewards through consistency-driven sample selection to enhance mathematical reasoning by improving both final accuracy and intermediate step quality.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>645, <a href='https://arxiv.org/pdf/2509.03383.pdf' target='_blank'>https://arxiv.org/pdf/2509.03383.pdf</a></span>   <span><a href='https://github.com/RLCLab/Annie' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyang Huang, Zixuan Wang, Zishen Wan, Yapeng Tian, Haobo Xu, Yinhe Han, Yiming Gan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03383">ANNIE: Be Careful of Your Robots</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The integration of vision-language-action (VLA) models into embodied AI (EAI) robots is rapidly advancing their ability to perform complex, long-horizon tasks in humancentric environments. However, EAI systems introduce critical security risks: a compromised VLA model can directly translate adversarial perturbations on sensory input into unsafe physical actions. Traditional safety definitions and methodologies from the machine learning community are no longer sufficient. EAI systems raise new questions, such as what constitutes safety, how to measure it, and how to design effective attack and defense mechanisms in physically grounded, interactive settings. In this work, we present the first systematic study of adversarial safety attacks on embodied AI systems, grounded in ISO standards for human-robot interactions. We (1) formalize a principled taxonomy of safety violations (critical, dangerous, risky) based on physical constraints such as separation distance, velocity, and collision boundaries; (2) introduce ANNIEBench, a benchmark of nine safety-critical scenarios with 2,400 video-action sequences for evaluating embodied safety; and (3) ANNIE-Attack, a task-aware adversarial framework with an attack leader model that decomposes long-horizon goals into frame-level perturbations. Our evaluation across representative EAI models shows attack success rates exceeding 50% across all safety categories. We further demonstrate sparse and adaptive attack strategies and validate the real-world impact through physical robot experiments. These results expose a previously underexplored but highly consequential attack surface in embodied AI systems, highlighting the urgent need for security-driven defenses in the physical AI era. Code is available at https://github.com/RLCLab/Annie.<br>
<span id='abs_ch'>中文摘要：本 究首次系统性地探究具身AI系统的对抗性安全攻击，提出了基于物理约束的安全违规分类法、包含2400个视频动作序列的安全评估基准，以及任务感知的攻击框架，在各类安全场景中攻击成功率超过50%，揭示了物理AI时代亟待解决的安全漏洞。</span><br>
<span id='abs_en'>English Summary: This study presents the first systematic investigation of adversarial safety attacks on embodied AI systems, introducing a taxonomy of safety violations, a benchmark for evaluation, and an attack framework that achieves over 50% success rate across safety categories, revealing critical security vulnerabilities in physically-grounded AI.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>646, <a href='https://arxiv.org/pdf/2509.03310.pdf' target='_blank'>https://arxiv.org/pdf/2509.03310.pdf</a></span>   <span><a href='https://github.com/appdotbuild/agent/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Evgenii Kniazev, Arseny Kravchenko, Igor Rekun, James Broadhead, Nikita Shamgunov, Pranav Sah, Pratik Nichite, Ivan Yamshchikov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03310">app.build: A Production Framework for Scaling Agentic Prompt-to-App Generation with Environment Scaffolding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present app.build (https://github.com/appdotbuild/agent/), an open-source framework that improves LLM-based application generation through systematic validation and structured environments. Our approach combines multi-layered validation pipelines, stack-specific orchestration, and model-agnostic architecture, implemented across three reference stacks. Through evaluation on 30 generation tasks, we demonstrate that comprehensive validation achieves 73.3% viability rate with 30% reaching perfect quality scores, while open-weights models achieve 80.8% of closed-model performance when provided structured environments. The open-source framework has been adopted by the community, with over 3,000 applications generated to date. This work demonstrates that scaling reliable AI agents requires scaling environments, not just models -- providing empirical insights and complete reference implementations for production-oriented agent systems.<br>
<span id='abs_ch'>中文：app.build框架通过系统验证和结构化环境提升基于LLM的应用程序生成效果，在开源社区中已实现广泛应用并验证了可 智能体需扩展环境而不仅是模型的 心观点。</span><br>
<span id='abs_en'>English: The app.build framework enhances LLM-based application generation via systematic validation and structured environments, achieving high viability and performance with open-source adoption.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>647, <a href='https://arxiv.org/pdf/2509.03059.pdf' target='_blank'>https://arxiv.org/pdf/2509.03059.pdf</a></span>   <span><a href='https://github.com/camel-ai/loong' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyue Huang, Rishabh, Gregor Franke, Ziyi Yang, Jiamu Bai, Weijie Bai, Jinhe Bi, Zifeng Ding, Yiqun Duan, Chengyu Fan, Wendong Fan, Xin Gao, Ruohao Guo, Yuan He, Zhuangzhuang He, Xianglong Hu, Neil Johnson, Bowen Li, Fangru Lin, Siyu Lin, Tong Liu, Yunpu Ma, Hao Shen, Hao Sun, Beibei Wang, Fangyijie Wang, Hao Wang, Haoran Wang, Yang Wang, Yifeng Wang, Zhaowei Wang, Ziyang Wang, Yifan Wu, Zikai Xiao, Chengxing Xie, Fan Yang, Junxiao Yang, Qianshuo Ye, Ziyu Ye, Guangtao Zeng, Yuwen Ebony Zhang, Zeyu Zhang, Zihao Zhu, Bernard Ghanem, Philip Torr, Guohao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03059">Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in Large Language Models (LLMs) have shown that their reasoning capabilities can be significantly improved through Reinforcement Learning with Verifiable Reward (RLVR), particularly in domains like mathematics and programming, where ground-truth correctness can be automatically evaluated. However, extending this success to other reasoning-intensive domains remains challenging due to the scarcity of high-quality, verifiable datasets and the high cost of human supervision. In this work, we introduce the Loong Project: an open-source framework for scalable synthetic data generation and verification across a diverse range of reasoning-intensive domains. The framework consists of two key components: (1) LoongBench, a curated seed dataset containing 8,729 human-vetted examples across 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired with executable code and rich metadata; and (2) LoongEnv, a modular synthetic data generation environment that supports multiple prompting strategies to produce new question-answer-code triples. Together, these components form an agent-environment loop that enables reinforcement learning, where an LLM-based agent is rewarded for generating Chain-of-Thought (CoT) solutions that align with code-executed answers. Empirically, we benchmark LoongBench on a broad suite of both open-source and proprietary LLMs to evaluate domain coverage and reveal performance bottlenecks. In addition, we conduct a comprehensive analysis of synthetic data generated by LoongEnv, examining correctness, difficulty, and diversity. Code and documentation are available at https://github.com/camel-ai/loong.<br>
<span id='abs_ch'>中文: Loong项目推出了一个开源框架，通过LoongBench精选数据集和LoongEnv合成数据生成环境，在多 化推理领域实现可扩展的数据生成与验证，解决了大语言模型在数学和编程之外领域扩展推理能力的挑战。</span><br>
<span id='abs_en'>English: The Loong Project introduces an open-source framework for scalable synthetic data generation and verification across diverse reasoning domains, addressing the challenge of extending LLM reasoning capabilities beyond mathematics and programming through its components LoongBench and LoongEnv.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>648, <a href='https://arxiv.org/pdf/2509.03054.pdf' target='_blank'>https://arxiv.org/pdf/2509.03054.pdf</a></span>   <span><a href='https://github.com/johnnyzheng0636/WGM_bi_quan' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinzhe Zheng, Zhen-Qun Yang, Haoran Xie, S. Joe Qin, Arlene Chen, Fangzhen Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03054">Binary Quantization For LLMs Through Dynamic Grouping</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of Natural Language Processing (NLP) tasks, but require substantial memory and computational resources. Binary quantization, which compresses model weights from 16-bit Brain Float to 1-bit representations in {-1, 1}, offers significant reductions in storage and inference costs. However, such aggressive quantization often leads to notable performance degradation compared to more conservative 4-bit quantization methods. In this research, we propose a novel optimization objective tailored for binary quantization, along with three algorithms designed to realize it effectively. Our method enhances blocked quantization by dynamically identifying optimal unstructured sub-matrices through adaptive grouping strategies. Experimental results demonstrate that our approach achieves an average bit length of just 1.007 bits, while maintaining high model quality. Specifically, our quantized LLaMA 3.2 3B model attains a perplexity of 8.23, remarkably close to the original 7.81, and surpasses previous SOTA BiLLM with a perplexity of only 123.90. Furthermore, our method is competitive with SOTA 4-bit approaches such as GPTQ in both performance and efficiency. The compression process is highly efficient, requiring only 14 seconds to quantize the full LLaMA 3.2 3B weights on a single CPU core, with the entire process completing in under 100 minutes and exhibiting embarrassingly parallel properties. Code - https://github.com/johnnyzheng0636/WGM_bi_quan<br>
<br>
<div id='section'>Paperid: <span id='pid'>649, <a href='https://arxiv.org/pdf/2509.02627.pdf' target='_blank'>https://arxiv.org/pdf/2509.02627.pdf</a></span>   <span><a href='https://github.com/xxiao0304/MIDOG-2025-Track-1-of-SZTU' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Xiao, Mengye Lyu, Shaojun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02627">A Two-Stage Strategy for Mitosis Detection Using Improved YOLO11x Proposals and ConvNeXt Classification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>MIDOG 2025 Track 1 requires mitosis detection in whole-slideimages (WSIs) containing non-tumor, inflamed, and necrotic re-gions. Due to the complicated and heterogeneous context, aswell as possible artifacts, there are often false positives and falsenegatives, thus degrading the detection F1-score. To addressthis problem, we propose a two-stage framework. Firstly, an im-proved YOLO11x, integrated with EMA attention and LSConv,is employed to generate mitosis candidates. We use a low confi-dence threshold to generate as many proposals as possible, en-suring the detection recall. Then, a ConvNeXt-Tiny classifieris employed to filter out the false positives, ensuring the detec-tion precision. Consequently, the proposed two-stage frame-work can generate a high detection F1-score. Evaluated on afused dataset comprising MIDOG++, MITOS_WSI_CCMCT,and MITOS_WSI_CMC, our framework achieves an F1-scoreof 0.882, which is 0.035 higher than the single-stage YOLO11xbaseline. This performance gain is produced by a significantprecision improvement, from 0.762 to 0.839, and a comparablerecall. On the MIDOG 2025 Track 1 preliminary test set, thealgorithm scores an F1 score of 0.7587. The code is available athttps://github.com/xxiao0304/MIDOG-2025-Track-1-of-SZTU.<br>
<span id='abs_ch'>中文: 该 究提出了一种两阶段框架，结合改进的YOLO11x模型生成候选目 ，并使用ConvNeXt-Tiny分类器过滤误检，在融合数据集上F1分数达0.882，在MIDOG 2025 Track 1测试集上达0.7587。</span><br>
<span id='abs_en'>English: The study introduces a two-stage framework combining an enhanced YOLO11x model for candidate detection and a ConvNeXt-Tiny classifier to filter false positives, achieving a higher F1-score of 0.882 on a fused dataset and 0.7587 on the MIDOG 2025 Track 1 test set.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>650, <a href='https://arxiv.org/pdf/2509.02510.pdf' target='_blank'>https://arxiv.org/pdf/2509.02510.pdf</a></span>   <span><a href='https://github.com/ErfanBaghaei/Top-H-Decoding' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Erfan Baghaei Potraghloo, Seyedarmin Azizi, Souvik Kundu, Massoud Pedram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02510">Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy in Text Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs), despite their impressive performance across a wide range of tasks, often struggle to balance two competing objectives in open-ended text generation: fostering diversity and creativity while preserving logical coherence. Existing truncated sampling techniques, including temperature scaling, top-\$p\$ (nucleus) sampling, and min-\$p\$ sampling, aim to manage this trade-off. However, they exhibit limitations, particularly in the effective incorporation of the confidence of the model into the corresponding sampling strategy. For example, min-\$p\$ sampling relies on a single top token as a heuristic for confidence, eventually underutilizing the information of the probability distribution. Toward effective incorporation of the confidence of the model, in this paper, we present **top-H** decoding. We first establish the theoretical foundation of the interplay between creativity and coherence in truncated sampling by formulating an **entropy-constrained minimum divergence** problem. We then prove this minimization problem to be equivalent to an **entropy-constrained mass maximization** (ECMM) problem, which is NP-hard. Finally, we present top-H decoding, a computationally efficient greedy algorithm to solve the ECMM problem. Extensive empirical evaluations demonstrate that top-H outperforms the state-of-the-art (SoTA) alternative of min-\$p\$ sampling by up to **25.63%** on creative writing benchmarks, while maintaining robustness on question-answering datasets such as GPQA, GSM8K, and MT-Bench. Additionally, an *LLM-as-judge* evaluation confirms that top-H indeed produces coherent outputs even at higher temperatures, where creativity is especially critical. In summary, top-H advances SoTA in open-ended text generation and can be *easily integrated* into creative writing applications. The code is available at https://github.com/ErfanBaghaei/Top-H-Decoding.<br>
<span id='abs_ch'>中文摘要：大语言模型在开放文本生成中存在创 力与逻辑连贯性的平衡难题，而新提出的top-H解 方法通过有效整合模型置信度，在创意写作任务中比现有最佳方法提升高达25.63%的性能，同时保持问答任务的稳健性。</span><br>
<span id='abs_en'>English Summary: Large language models face a trade-off between creativity and coherence in text generation, and the proposed top-H decoding method effectively incorporates model confidence to outperform existing techniques by up to 25.63% on creative writing tasks while maintaining robustness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>651, <a href='https://arxiv.org/pdf/2509.02499.pdf' target='_blank'>https://arxiv.org/pdf/2509.02499.pdf</a></span>   <span><a href='https://github.com/creator-xi/MoSEs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junxi Wu, Jinpeng Wang, Zheng Liu, Bin Chen, Dongjian Hu, Hao Wu, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02499">MoSEs: Uncertainty-Aware AI-Generated Text Detection via Mixture of Stylistics Experts with Conditional Thresholds</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of large language models has intensified public concerns about the potential misuse. Therefore, it is important to build trustworthy AI-generated text detection systems. Existing methods neglect stylistic modeling and mostly rely on static thresholds, which greatly limits the detection performance. In this paper, we propose the Mixture of Stylistic Experts (MoSEs) framework that enables stylistics-aware uncertainty quantification through conditional threshold estimation. MoSEs contain three core components, namely, the Stylistics Reference Repository (SRR), the Stylistics-Aware Router (SAR), and the Conditional Threshold Estimator (CTE). For input text, SRR can activate the appropriate reference data in SRR and provide them to CTE. Subsequently, CTE jointly models the linguistic statistical properties and semantic features to dynamically determine the optimal threshold. With a discrimination score, MoSEs yields prediction labels with the corresponding confidence level. Our framework achieves an average improvement 11.34% in detection performance compared to baselines. More inspiringly, MoSEs shows a more evident improvement 39.15% in the low-resource case. Our code is available at https://github.com/creator-xi/MoSEs.<br>
<span id='abs_ch'>中文: 本文提出的混合风 专家框架通过条件阈值估计实现风 感知的不确定性量化，显著提升了AI生成文本的检测性能，平均比基线方法提高了11.34%。</span><br>
<span id='abs_en'>English: This paper introduces the Mixture of Stylistic Experts (MoSEs) framework, which enhances AI-generated text detection by dynamically estimating thresholds based on stylistic modeling, achieving an 11.34% average performance improvement over baseline methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>652, <a href='https://arxiv.org/pdf/2509.02444.pdf' target='_blank'>https://arxiv.org/pdf/2509.02444.pdf</a></span>   <span><a href='https://github.com/OpenBMB/AppCopilot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingru Fan, Yufan Dang, Jingyao Wu, Huatao Li, Runde Yang, Xiyuan Yang, Yuheng Wang, Zhong Zhang, Yaxi Lu, Yankai Lin, Zhiyuan Liu, Dahai Li, Chen Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02444">AppCopilot: Toward General, Accurate, Long-Horizon, and Efficient Mobile Agent</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the raid evolution of large language models and multimodal foundation models, the mobile-agent landscape has proliferated without converging on the fundamental challenges. This paper identifies four core problems that must be solved for mobile agents to deliver practical, scalable impact: (1) generalization across tasks, modalities, apps, and devices; (2) accuracy, specifically precise on-screen interaction and click targeting; (3) long-horizon capability for sustained, multi-step goals; and (4) efficiency, specifically high-performance runtime on resource-constrained devices. We present AppCopilot, a multimodal, multi-agent, general-purpose on-device assistant that operates across applications and constitutes a full-stack, closed-loop system from data to deployment. AppCopilot operationalizes this position through an end-to-end autonomous pipeline spanning data collection, training, deployment, high-quality and efficient inference, and mobile application development. At the model layer, it integrates multimodal foundation models with robust Chinese-English support. At the reasoning and control layer, it combines chain-of-thought reasoning, hierarchical task planning and decomposition, and multi-agent collaboration. At the execution layer, it enables user personalization and experiential adaptation, voice interaction, function calling, cross-app and cross-device orchestration, and comprehensive mobile app support. The system design incorporates profiling-driven optimization for latency, memory, and energy across heterogeneous hardware. Empirically, AppCopilot achieves significant improvements along all four dimensions: stronger generalization, higher-precision on-screen actions, more reliable long-horizon task completion, and faster, more resource-efficient runtime.<br>
<span id='abs_ch'>中文摘要：本文提出AppCopilot，一种设备端多模态助手，通过融合基础模型、多智能体协作和移动端优化部署，系统性地解决了移动智能体在泛化能力、操作精度、长程任务和运行效率四大 心难题。</span><br>
<span id='abs_en'>English Summary: This paper introduces AppCopilot, an on-device multimodal assistant designed to address four core challenges in mobile agents—generalization, accuracy, long-horizon capability, and efficiency—through an integrated system combining foundation models, multi-agent collaboration, and optimized mobile deployment.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>653, <a href='https://arxiv.org/pdf/2509.02419.pdf' target='_blank'>https://arxiv.org/pdf/2509.02419.pdf</a></span>   <span><a href='https://github.com/ortonwang/GSD-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Wang, Zhenxuan Zhang, Yuanbo Zhou, Xinlin Zhang, Yuanbin Chen, Tao Tan, Guang Yang, Tong Tong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02419">From Noisy Labels to Intrinsic Structure: A Geometric-Structural Dual-Guided Framework for Noise-Robust Medical Image Segmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The effectiveness of convolutional neural networks in medical image segmentation relies on large-scale, high-quality annotations, which are costly and time-consuming to obtain. Even expert-labeled datasets inevitably contain noise arising from subjectivity and coarse delineations, which disrupt feature learning and adversely impact model performance. To address these challenges, this study propose a Geometric-Structural Dual-Guided Network (GSD-Net), which integrates geometric and structural cues to improve robustness against noisy annotations. It incorporates a Geometric Distance-Aware module that dynamically adjusts pixel-level weights using geometric features, thereby strengthening supervision in reliable regions while suppressing noise. A Structure-Guided Label Refinement module further refines labels with structural priors, and a Knowledge Transfer module enriches supervision and improves sensitivity to local details. To comprehensively assess its effectiveness, we evaluated GSD-Net on six publicly available datasets: four containing three types of simulated label noise, and two with multi-expert annotations that reflect real-world subjectivity and labeling inconsistencies. Experimental results demonstrate that GSD-Net achieves state-of-the-art performance under noisy annotations, achieving improvements of 2.52% on Kvasir, 22.76% on Shenzhen, 8.87% on BU-SUC, and 4.59% on BraTS2020 under SR simulated noise. The codes of this study are available at https://github.com/ortonwang/GSD-Net.<br>
<br>
<div id='section'>Paperid: <span id='pid'>654, <a href='https://arxiv.org/pdf/2509.02350.pdf' target='_blank'>https://arxiv.org/pdf/2509.02350.pdf</a></span>   <span><a href='https://github.com/digailab/awesome-llm-implicit-reasoning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jindong Li, Yali Fu, Li Fan, Jiahong Liu, Yao Shu, Chengwei Qin, Menglin Yang, Irwin King, Rex Ying
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02350">Implicit Reasoning in Large Language Models: A Comprehensive Survey</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated strong generalization across a wide range of tasks. Reasoning with LLMs is central to solving multi-step problems and complex decision-making. To support efficient reasoning, recent studies have shifted attention from explicit chain-of-thought prompting toward implicit reasoning, where reasoning occurs silently via latent structures without emitting intermediate textual steps. Implicit reasoning brings advantages such as lower generation cost, faster inference, and better alignment with internal computation. Although prior surveys have discussed latent representations in the context of reasoning, a dedicated and mechanism-level examination of how reasoning unfolds internally within LLMs remains absent. This survey fills that gap by introducing a taxonomy centered on execution paradigms, shifting the focus from representational forms to computational strategies. We organize existing methods into three execution paradigms based on \textbf{\textit{how and where internal computation unfolds}}: latent optimization, signal-guided control, and layer-recurrent execution. We also review structural, behavioral and representation-based evidence that supports the presence of implicit reasoning in LLMs. We further provide a structured overview of the evaluation metrics and benchmarks used in existing works to assess the effectiveness and reliability of implicit reasoning. We maintain a continuously updated project at: https://github.com/digailab/awesome-llm-implicit-reasoning.<br>
<span id='abs_ch'>中文: 本综述提出以执行范式为 心的分类法，探讨大型语言模型内部如何进行隐性推理，将方法归纳为潜在优化、信号引导控制和层循环执行，并评述了支持证据与评估体系。</span><br>
<span id='abs_en'>English: This survey introduces a taxonomy focused on execution paradigms to examine how implicit reasoning occurs internally within LLMs, organizing methods into latent optimization, signal-guided control, and layer-recurrent execution while reviewing supporting evidence and evaluation metrics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>655, <a href='https://arxiv.org/pdf/2509.02175.pdf' target='_blank'>https://arxiv.org/pdf/2509.02175.pdf</a></span>   <span><a href='https://github.com/nilshoehing/rocketscience' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nils Hoehing, Mayug Maniparambil, Ellen Rushe, Noel E. O'Connor, Anthony Ventresque
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02175">Understanding Space Is Rocket Science -- Only Top Reasoning Models Can Solve Spatial Understanding Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We propose RocketScience, an open-source contrastive VLM benchmark that tests for spatial relation understanding. It is comprised of entirely new real-world image-text pairs covering mostly relative spatial understanding and the order of objects. The benchmark is designed to be very easy for humans and hard for the current generation of VLMs, and this is empirically verified. Our results show a striking lack of spatial relation understanding in open source and frontier commercial VLMs and a surprisingly high performance of reasoning models. Additionally, we perform a disentanglement analysis to separate the contributions of object localization and spatial reasoning in chain-of-thought-based models and find that the performance on the benchmark is bottlenecked by spatial reasoning and not object localization capabilities. We release the dataset with a CC-BY-4.0 license and make the evaluation code available at: https://github.com/nilshoehing/rocketscience<br>
<span id='abs_ch'>Chinese: RocketScience 是一个评估视觉语言模型空间关系理解能力的开源基准测试，发现现有模型存在显著缺陷，并证实空间推理能力是主要瓶颈，而非物体定位能力。</span><br>
<span id='abs_en'>English: RocketScience is an open-source benchmark that evaluates spatial relation understanding in vision-language models, revealing significant deficiencies in current models despite high human performance and identifying spatial reasoning as the primary bottleneck.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>656, <a href='https://arxiv.org/pdf/2509.02101.pdf' target='_blank'>https://arxiv.org/pdf/2509.02101.pdf</a></span>   <span><a href='https://github.com/MaticFuc/SALAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Matic FuÄka, Vitjan Zavrtanik, Danijel SkoÄaj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02101">SALAD -- Semantics-Aware Logical Anomaly Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent surface anomaly detection methods excel at identifying structural anomalies, such as dents and scratches, but struggle with logical anomalies, such as irregular or missing object components. The best-performing logical anomaly detection approaches rely on aggregated pretrained features or handcrafted descriptors (most often derived from composition maps), which discard spatial and semantic information, leading to suboptimal performance. We propose SALAD, a semantics-aware discriminative logical anomaly detection method that incorporates a newly proposed composition branch to explicitly model the distribution of object composition maps, consequently learning important semantic relationships. Additionally, we introduce a novel procedure for extracting composition maps that requires no hand-made labels or category-specific information, in contrast to previous methods. By effectively modelling the composition map distribution, SALAD significantly improves upon state-of-the-art methods on the standard benchmark for logical anomaly detection, MVTec LOCO, achieving an impressive image-level AUROC of 96.1%. Code: https://github.com/MaticFuc/SALAD<br>
<br>
<div id='section'>Paperid: <span id='pid'>657, <a href='https://arxiv.org/pdf/2509.02097.pdf' target='_blank'>https://arxiv.org/pdf/2509.02097.pdf</a></span>   <span><a href='https://github.com/DataArcTech/JudgeAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhichao Shi, Xuhui Jiang, Chengjin Xu, Cangli Yao, Zhenxin Huang, Shengjie Ma, Yinghan Shen, Jian Guo, Yuanzhuo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02097">JudgeAgent: Knowledge-wise and Dynamic LLM Evaluation with Agent-as-Interviewer</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current evaluation paradigms for large language models (LLMs) suffer from overestimated or biased evaluations and mismatched question difficulty, leading to incomplete evaluations of knowledge and capability boundaries, which hinder their effective application and optimization. To address these challenges, we propose Agent-as-Interviewer, a dynamic evaluation paradigm that employs LLM agents to conduct multi-turn interactions for evaluation. Unlike current benchmarking or dynamic interaction paradigms, Agent-as-Interviewer utilizes agents to invoke knowledge tools for wider and deeper knowledge in the dynamic multi-turn question generation, achieving more comprehensive evaluations of LLM's knowledge boundaries. It also leverages agents to plan query strategies for adjustment of the question difficulty levels, enhancing the difficulty control to match the actual capabilities of target LLMs. Based on this paradigm, we develop JudgeAgent, a knowledge-wise dynamic evaluation framework that employs knowledge-driven synthesis as the agent's tool and uses difficulty scoring as strategy guidance, thereby finally providing valuable suggestions to help targets optimize themselves. Extensive experiments validate the effectiveness of JudgeAgent's suggestions, demonstrating that Agent-as-Interviewer can accurately identify the knowledge and capability boundaries of target models. The source code is available on https://github.com/DataArcTech/JudgeAgent.<br>
<span id='abs_ch'>中文：Agent-as-Interviewer范式通过AI代理进行动态多轮交互和问题难度调节，解决了当前大语言模型评估的局限性，能更准确地识别知识边界并提供优化建议。English: The Agent-as-Interviewer paradigm addresses limitations in current LLM evaluations by using AI agents to conduct dynamic multi-turn interactions and adjust question difficulty, enabling more accurate identification of knowledge boundaries and providing optimization suggestions.Paperid:1951,https://arxiv.org/pdf/2509.26628.pdf</span><br>
<span id='abs_en'>English: The Agent-as-Interviewer paradigm addresses limitations in current LLM evaluations by using AI agents to conduct dynamic multi-turn interactions and adjust question difficulty, enabling more accurate identification of knowledge boundaries and providing optimization suggestions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>658, <a href='https://arxiv.org/pdf/2509.02017.pdf' target='_blank'>https://arxiv.org/pdf/2509.02017.pdf</a></span>   <span><a href='https://github.com/Applied-Machine-Learning-Lab/MME-SID' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Wang, Junwei Pan, Xinhang Li, Maolin Wang, Yuan Wang, Yue Liu, Dapeng Liu, Jie Jiang, Xiangyu Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02017">Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Sequential recommendation (SR) aims to capture users' dynamic interests and sequential patterns based on their historical interactions. Recently, the powerful capabilities of large language models (LLMs) have driven their adoption in SR. However, we identify two critical challenges in existing LLM-based SR methods: 1) embedding collapse when incorporating pre-trained collaborative embeddings and 2) catastrophic forgetting of quantized embeddings when utilizing semantic IDs. These issues dampen the model scalability and lead to suboptimal recommendation performance. Therefore, based on LLMs like Llama3-8B-instruct, we introduce a novel SR framework named MME-SID, which integrates multimodal embeddings and quantized embeddings to mitigate embedding collapse. Additionally, we propose a Multimodal Residual Quantized Variational Autoencoder (MM-RQ-VAE) with maximum mean discrepancy as the reconstruction loss and contrastive learning for alignment, which effectively preserve intra-modal distance information and capture inter-modal correlations, respectively. To further alleviate catastrophic forgetting, we initialize the model with the trained multimodal code embeddings. Finally, we fine-tune the LLM efficiently using LoRA in a multimodal frequency-aware fusion manner. Extensive experiments on three public datasets validate the superior performance of MME-SID thanks to its capability to mitigate embedding collapse and catastrophic forgetting. The implementation code and datasets are publicly available for reproduction: https://github.com/Applied-Machine-Learning-Lab/MME-SID.<br>
<span id='abs_ch'>中文：MME-SID框架通过整合多模态嵌入和新型量化变分自编 器，解决了序列推荐中的嵌入塌缩和灾难性遗忘问题，并在公开数据集上验证了其优越性能。</span><br>
<span id='abs_en'>English: The MME-SID framework addresses embedding collapse and catastrophic forgetting in sequential recommendation by integrating multimodal embeddings and a novel quantized variational autoencoder, validated through experiments on public datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>659, <a href='https://arxiv.org/pdf/2509.01986.pdf' target='_blank'>https://arxiv.org/pdf/2509.01986.pdf</a></span>   <span><a href='https://github.com/showlab/DIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyun Zeng, Junhao Zhang, Wei Li, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01986">Draw-In-Mind: Rebalancing Designer-Painter Roles in Unified Multimodal Models Benefits Image Editing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In recent years, integrating multimodal understanding and generation into a single unified model has emerged as a promising paradigm. While this approach achieves strong results in text-to-image (T2I) generation, it still struggles with precise image editing. We attribute this limitation to an imbalanced division of responsibilities. The understanding module primarily functions as a translator that encodes user instructions into semantic conditions, while the generation module must simultaneously act as designer and painter, inferring the original layout, identifying the target editing region, and rendering the new content. This imbalance is counterintuitive because the understanding module is typically trained with several times more data on complex reasoning tasks than the generation module. To address this issue, we introduce Draw-In-Mind (DIM), a dataset comprising two complementary subsets: (i) DIM-T2I, containing 14M long-context image-text pairs to enhance complex instruction comprehension; and (ii) DIM-Edit, consisting of 233K chain-of-thought imaginations generated by GPT-4o, serving as explicit design blueprints for image edits. We connect a frozen Qwen2.5-VL-3B with a trainable SANA1.5-1.6B via a lightweight two-layer MLP, and train it on the proposed DIM dataset, resulting in DIM-4.6B-T2I/Edit. Despite its modest parameter scale, DIM-4.6B-Edit achieves SOTA or competitive performance on the ImgEdit and GEdit-Bench benchmarks, outperforming much larger models such as UniWorld-V1 and Step1X-Edit. These findings demonstrate that explicitly assigning the design responsibility to the understanding module provides significant benefits for image editing. Our dataset and models are available at https://github.com/showlab/DIM.<br>
<span id='abs_ch'>中文: 该 究提出了Draw-In-Mind (DIM)数据集和模型，通过强化理解模块的设计职责来解决多模态模型中职责分配失衡问题，以较少参数量实现了图像编辑任务的顶尖性能。</span><br>
<span id='abs_en'>English: The study introduces Draw-In-Mind (DIM), a dataset and model that addresses imbalanced responsibilities in unified multimodal models by enhancing the understanding module's design role, achieving state-of-the-art image editing performance with fewer parameters.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>660, <a href='https://arxiv.org/pdf/2509.01920.pdf' target='_blank'>https://arxiv.org/pdf/2509.01920.pdf</a></span>   <span><a href='https://github.com/guanyilin428/Dynamic-Speculative-Planning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilin Guan, Qingfeng Lan, Sun Fei, Dujian Ding, Devang Acharya, Chi Wang, William Yang Wang, Wenyue Hua
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01920">Dynamic Speculative Agent Planning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite their remarkable success in complex tasks propelling widespread adoption, large language-model-based agents still face critical deployment challenges due to prohibitive latency and inference costs. While recent work has explored various methods to accelerate inference, existing approaches suffer from significant limitations: they either fail to preserve performance fidelity, require extensive offline training of router modules, or incur excessive operational costs. Moreover, they provide minimal user control over the tradeoff between acceleration and other performance metrics. To address these gaps, we introduce Dynamic Speculative Planning (DSP), an asynchronous online reinforcement learning framework that provides lossless acceleration with substantially reduced costs without requiring additional pre-deployment preparation. DSP explicitly optimizes a joint objective balancing end-to-end latency against dollar cost, allowing practitioners to adjust a single parameter that steers the system toward faster responses, cheaper operation, or any point along this continuum. Experiments on two standard agent benchmarks demonstrate that DSP achieves comparable efficiency to the fastest lossless acceleration method while reducing total cost by 30% and unnecessary cost up to 60%. Our code and data are available through https://github.com/guanyilin428/Dynamic-Speculative-Planning.<br>
<span id='abs_ch'>Large language model agents face high latency and cost =
issues, which Dynamic Speculative Planning (DSP) addresses through an onlin=
e reinforcement learning framework that enables lossless acceleration with =
30% cost reduction while allowing adjustable performance trade-offs.</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>661, <a href='https://arxiv.org/pdf/2509.01909.pdf' target='_blank'>https://arxiv.org/pdf/2509.01909.pdf</a></span>   <span><a href='https://github.com/Alibaba-AAIG/Oyster' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ranjie Duan, Jiexi Liu, Xiaojun Jia, Shiji Zhao, Ruoxi Cheng, Fengxiang Wang, Cheng Wei, Yong Xie, Chang Liu, Defeng Li, Yinpeng Dong, Yichi Zhang, Yuefeng Chen, Chongwen Wang, Xingjun Ma, Xingxing Wei, Yang Liu, Hang Su, Jun Zhu, Xinfeng Li, Yitong Sun, Jie Zhang, Jinzhao Hu, Sha Xu, Wenchao Yang, Yitong Yang, Xingyao Zhang, Yingshui Tan, Jialing Tao, Hui Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01909">Oyster-I: Beyond Refusal - Constructive Safety Alignment for Responsible Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) typically deploy safety mechanisms to prevent harmful content generation. Most current approaches focus narrowly on risks posed by malicious actors, often framing risks as adversarial events and relying on defensive refusals. However, in real-world settings, risks also come from non-malicious users seeking help while under psychological distress (e.g., self-harm intentions). In such cases, the model's response can strongly influence the user's next actions. Simple refusals may lead them to repeat, escalate, or move to unsafe platforms, creating worse outcomes. We introduce Constructive Safety Alignment (CSA), a human-centric paradigm that protects against malicious misuse while actively guiding vulnerable users toward safe and helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic anticipation of user reactions, fine-grained risk boundary discovery, and interpretable reasoning control, turning safety into a trust-building process. Oy1 achieves state-of-the-art safety among open models while retaining high general capabilities. On our Constructive Benchmark, it shows strong constructive engagement, close to GPT-5, and unmatched robustness on the Strata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from refusal-first to guidance-first safety, CSA redefines the model-user relationship, aiming for systems that are not just safe, but meaningfully helpful. We release Oy1, code, and the benchmark to support responsible, user-centered AI.<br>
<span id='abs_ch'>中文: 现有大语言模型的安全机制常 防御性拒绝而 法帮助心理脆弱的用户， 此CSA提出以人为中心的安全对齐方法，通过预期推理和信任建立引导高危用户获得安全结果，在开源模型中实现了顶尖的安全性和通用能力。</span><br>
<span id='abs_en'>English: Current LLM safety mechanisms often fail vulnerable users by using defensive refusals, so CSA introduces a human-centric approach that guides at-risk users toward safe outcomes through anticipatory reasoning and trust-building, achieving top safety and capability levels in open models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>662, <a href='https://arxiv.org/pdf/2509.01822.pdf' target='_blank'>https://arxiv.org/pdf/2509.01822.pdf</a></span>   <span><a href='https://github.com/USC-Melady/TSAIA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Ye, Jinbo Liu, Defu Cao, Wei Yang, Yan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01822">When LLM Meets Time Series: Can LLMs Perform Multi-Step Time Series Reasoning and Inference</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of Large Language Models (LLMs) has sparked growing interest in their application to time series analysis tasks. However, their ability to perform complex reasoning over temporal data in real-world application domains remains underexplored. To move toward this goal, a first step is to establish a rigorous benchmark dataset for evaluation. In this work, we introduce the TSAIA Benchmark, a first attempt to evaluate LLMs as time-series AI assistants. To ensure both scientific rigor and practical relevance, we surveyed over 20 academic publications and identified 33 real-world task formulations. The benchmark encompasses a broad spectrum of challenges, ranging from constraint-aware forecasting to anomaly detection with threshold calibration: tasks that require compositional reasoning and multi-step time series analysis. The question generator is designed to be dynamic and extensible, supporting continuous expansion as new datasets or task types are introduced. Given the heterogeneous nature of the tasks, we adopt task-specific success criteria and tailored inference-quality metrics to ensure meaningful evaluation for each task. We apply this benchmark to assess eight state-of-the-art LLMs under a unified evaluation protocol. Our analysis reveals limitations in current models' ability to assemble complex time series analysis workflows, underscoring the need for specialized methodologies for domain-specific adaptation. Our benchmark is available at https://huggingface.co/datasets/Melady/TSAIA, and the code is available at https://github.com/USC-Melady/TSAIA.<br>
<span id='abs_ch'>中文: 本 究提出了TSAIA基准来评估大语言模型作为时间序列AI助手的能力，发现尽管涵盖多种现实任务，现有模型在处理复杂时序推理方面仍存在明显局限。</span><br>
<span id='abs_en'>English: This study introduces the TSAIA Benchmark to evaluate Large Language Models as time-series AI assistants, revealing their limitations in handling complex temporal reasoning despite covering diverse real-world tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>663, <a href='https://arxiv.org/pdf/2509.01793.pdf' target='_blank'>https://arxiv.org/pdf/2509.01793.pdf</a></span>   <span><a href='https://github.com/ARY2260/stori' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ARY2260/stori,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aryan Amit Barsainyan, Jing Yu Lim, Dianbo Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01793">STORI: A Benchmark and Taxonomy for Stochastic Environments</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning (RL) techniques have achieved impressive performance on simulated benchmarks such as Atari100k, yet recent advances remain largely confined to simulation and show limited transfer to real-world domains. A central obstacle is environmental stochasticity, as real systems involve noisy observations, unpredictable dynamics, and non-stationary conditions that undermine the stability of current methods. Existing benchmarks rarely capture these uncertainties and favor simplified settings where algorithms can be tuned to succeed. The absence of a well-defined taxonomy of stochasticity further complicates evaluation, as robustness to one type of stochastic perturbation, such as sticky actions, does not guarantee robustness to other forms of uncertainty. To address this critical gap, we introduce STORI (STOchastic-ataRI), a benchmark that systematically incorporates diverse stochastic effects and enables rigorous evaluation of RL techniques under different forms of uncertainty. We propose a comprehensive five-type taxonomy of environmental stochasticity and demonstrate systematic vulnerabilities in state-of-the-art model-based RL algorithms through targeted evaluation of DreamerV3 and STORM. Our findings reveal that world models dramatically underestimate environmental variance, struggle with action corruption, and exhibit unreliable dynamics under partial observability. We release the code and benchmark publicly at https://github.com/ARY2260/stori, providing a unified framework for developing more robust RL systems.<br>
<span id='abs_ch'>中文摘要：STORI基准通过引入五类随机性分类法，系统评估强化学 在真实环境不确定性下的表现，揭示了DreamerV3和STORM等先进算法在环境方差估计和动态建模方面的系统性缺陷。</span><br>
<span id='abs_en'>English Summary: The STORI benchmark addresses the gap in evaluating reinforcement learning under real-world stochasticity by introducing a five-type taxonomy and revealing vulnerabilities in state-of-the-art algorithms like DreamerV3 and STORM.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>664, <a href='https://arxiv.org/pdf/2509.01659.pdf' target='_blank'>https://arxiv.org/pdf/2509.01659.pdf</a></span>   <span><a href='https://github.com/CharlesQ9/Physics-Supernova' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Qiu, Jingzhe Shi, Xinzhe Juan, Zelin Zhao, Jiayi Geng, Shilong Liu, Hongru Wang, Sanfeng Wu, Mengdi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01659">Physics Supernova: AI Agent Matches Elite Gold Medalists at IPhO 2025</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Physics provides fundamental laws that describe and predict the natural world. AI systems aspiring toward more general, real-world intelligence must therefore demonstrate strong physics problem-solving abilities: to formulate and apply physical laws for explaining and predicting physical processes. The International Physics Olympiad (IPhO)--the world's most prestigious physics competition--offers a rigorous benchmark for this purpose. We introduce Physics Supernova, an AI agent system with superior physics problem-solving abilities that match elite IPhO gold medalists. In IPhO 2025 theory problems, Physics Supernova attains 23.5/30 points, ranking 14th of 406 contestants and surpassing the median performance of human gold medalists. We extensively analyzed Physics Supernova's capabilities and flexibility across diverse physics tasks. These results show that principled tool integration within agent systems can deliver competitive improvements in solving challenging science problems. The codes are available at https://github.com/CharlesQ9/Physics-Supernova.<br>
<span id='abs_ch'>Chinese: Physics Supernova 是一款具备顶尖物理问题解决能力的人工智能系统，在2025年国际物理奥林匹克竞赛理论题中获得23.5/30分，在406名参赛者中排名第14位，其表现媲美人类金牌得主。</span><br>
<span id='abs_en'>English: Physics Supernova is an AI system that demonstrates elite physics problem-solving abilities, matching top International Physics Olympiad gold medalists by scoring 23.5/30 points and ranking 14th among 406 contestants in the 2025 theory problems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>665, <a href='https://arxiv.org/pdf/2509.01535.pdf' target='_blank'>https://arxiv.org/pdf/2509.01535.pdf</a></span>   <span><a href='https://github.com/Kairong-Han/CAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kairong Han, Wenshuo Zhao, Ziyu Zhao, JunJian Ye, Lujia Pan, Kun Kuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01535">CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have achieved remarkable success across various domains. However, a fundamental question remains: Can LLMs effectively utilize causal knowledge for prediction and generation? Through empirical studies, we find that LLMs trained directly on large-scale data often capture spurious correlations rather than true causal relationships, leading to suboptimal performance, especially in out-of-distribution (OOD) scenarios. To address this challenge, we propose Causal Attention Tuning (CAT), a novel approach that injects fine-grained causal knowledge into the attention mechanism. We propose an automated pipeline that leverages human priors to automatically generate token-level causal signals and introduce the Re-Attention mechanism to guide training, helping the model focus on causal structures while mitigating noise and biases in attention scores. Experimental results on our proposed Spurious Token Game (STG) benchmark and multiple downstream tasks demonstrate that our approach effectively leverages causal knowledge for prediction and remains robust in OOD scenarios. The CAT achieves an average improvement of 5.76% on the STG dataset and 1.56% on downstream tasks. Notably, the OOD performance of the Llama-3.1-8B model on STG_M increased from 64.5% to 90.5%, and Qwen's OOD performance on the STG_H dataset improved from 25.4% to 55.9%. Implementation details can be found at https://github.com/Kairong-Han/CAT.<br>
<span id='abs_ch'>Chinese: 本 究提出 果注意力调优（CAT）方法，通过将细粒度 果知识注入注意力机制，显著提升大语言模型在分布外场景下的性能和鲁棒性，在STG基准测试及下游任务中均取得了明显改进。English: The study introduces Causal Attention Tuning (CAT), a method that enhances large language models by integrating fine-grained causal knowledge into their attention mechanisms, significantly improving performance and robustness in out-of-distribution scenarios, as demonstrated by substantial gains on the STG benchmark and downstream tasks.=Paperid:1252,https://arxiv.org/pdf/2509.01487.pdfGitHub</span><br>
<span id='abs_en'>English: The study introduces Causal Attention Tuning (CAT), a method that enhances large language models by integrating fine-grained causal knowledge into their attention mechanisms, significantly improving performance and robustness in out-of-distribution scenarios, as demonstrated by substantial gains on the STG benchmark and downstream tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>666, <a href='https://arxiv.org/pdf/2509.01426.pdf' target='_blank'>https://arxiv.org/pdf/2509.01426.pdf</a></span>   <span><a href='https://github.com/ncclab-sustech/DCA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mo Wang, Kaining Peng, Jingsheng Tang, Hongkai Wen, Quanying Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01426">DCA: Graph-Guided Deep Embedding Clustering for Brain Atlases</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Brain atlases are essential for reducing the dimensionality of neuroimaging data and enabling interpretable analysis. However, most existing atlases are predefined, group-level templates with limited flexibility and resolution. We present Deep Cluster Atlas (DCA), a graph-guided deep embedding clustering framework for generating individualized, voxel-wise brain parcellations. DCA combines a pretrained autoencoder with spatially regularized deep clustering to produce functionally coherent and spatially contiguous regions. Our method supports flexible control over resolution and anatomical scope, and generalizes to arbitrary brain structures. We further introduce a standardized benchmarking platform for atlas evaluation, using multiple large-scale fMRI datasets. Across multiple datasets and scales, DCA outperforms state-of-the-art atlases, improving functional homogeneity by 98.8% and silhouette coefficient by 29%, and achieves superior performance in downstream tasks such as autism diagnosis and cognitive decoding. We also observe that a fine-tuned pretrained model achieves superior results on the corresponding task. Codes and models are available at https://github.com/ncclab-sustech/DCA .<br>
<span id='abs_ch'>中文: DCA是一种通过深度聚类生成个性化高分辨率脑区图谱的新框架，在功能同质性和疾病诊断等下游任务中显著优于现有方法。English: DCA is a novel brain atlas framework that generates individualized, high-resolution parcellations using deep clustering, significantly outperforming existing methods in functional coherence and diagnostic applications.Paperid:1254,https://arxiv.org/pdf/2509.01418.pdfGitHub</span><br>
<span id='abs_en'>English: DCA is a novel brain atlas framework that generates individualized, high-resolution parcellations using deep clustering, significantly outperforming existing methods in functional coherence and diagnostic applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>667, <a href='https://arxiv.org/pdf/2509.01337.pdf' target='_blank'>https://arxiv.org/pdf/2509.01337.pdf</a></span>   <span><a href='https://github.com/thuiar/LGSRR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianrui Zhou, Hua Xu, Yifan Wang, Xinzhi Dong, Hanlei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01337">LLM-Guided Semantic Relational Reasoning for Multimodal Intent Recognition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Understanding human intents from multimodal signals is critical for analyzing human behaviors and enhancing human-machine interactions in real-world scenarios. However, existing methods exhibit limitations in their modality-level reliance, constraining relational reasoning over fine-grained semantics for complex intent understanding. This paper proposes a novel LLM-Guided Semantic Relational Reasoning (LGSRR) method, which harnesses the expansive knowledge of large language models (LLMs) to establish semantic foundations that boost smaller models' relational reasoning performance. Specifically, an LLM-based strategy is proposed to extract fine-grained semantics as guidance for subsequent reasoning, driven by a shallow-to-deep Chain-of-Thought (CoT) that autonomously uncovers, describes, and ranks semantic cues by their importance without relying on manually defined priors. Besides, we formally model three fundamental types of semantic relations grounded in logical principles and analyze their nuanced interplay to enable more effective relational reasoning. Extensive experiments on multimodal intent and dialogue act recognition tasks demonstrate LGSRR's superiority over state-of-the-art methods, with consistent performance gains across diverse semantic understanding scenarios. The complete data and code are available at https://github.com/thuiar/LGSRR.<br>
<span id='abs_ch'>中文: 本文提出的LGSRR方法利用大语言模型增强多模态意图理解中的语义关系推理，通过链式思维自主提取细粒度语义线索，在多项识别任务中展现出优于现有方法的性能。</span><br>
<span id='abs_en'>English: This paper introduces the LLM-Guided Semantic Relational Reasoning (LGSRR) method, which leverages large language models to enhance relational reasoning for complex multimodal intent understanding, achieving superior performance in recognition tasks without manual priors.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>668, <a href='https://arxiv.org/pdf/2509.01322.pdf' target='_blank'>https://arxiv.org/pdf/2509.01322.pdf</a></span>   <span><a href='https://github.com/meituan-longcat' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Meituan LongCat Team, Bayan, Bei Li, Bingye Lei, Bo Wang, Bolin Rong, Chao Wang, Chao Zhang, Chen Gao, Chen Zhang, Cheng Sun, Chengcheng Han, Chenguang Xi, Chi Zhang, Chong Peng, Chuan Qin, Chuyu Zhang, Cong Chen, Congkui Wang, Dan Ma, Daoru Pan, Defei Bu, Dengchang Zhao, Deyang Kong, Dishan Liu, Feiye Huo, Fengcun Li, Fubao Zhang, Gan Dong, Gang Liu, Gang Xu, Ge Li, Guoqiang Tan, Guoyuan Lin, Haihang Jing, Haomin Fu, Haonan Yan, Haoxing Wen, Haozhe Zhao, Hong Liu, Hongmei Shi, Hongyan Hao, Hongyin Tang, Huantian Lv, Hui Su, Jiacheng Li, Jiahao Liu, Jiahuan Li, Jiajun Yang, Jiaming Wang, Jian Yang, Jianchao Tan, Jiaqi Sun, Jiaqi Zhang, Jiawei Fu, Jiawei Yang, Jiaxi Hu, Jiayu Qin, Jingang Wang, Jiyuan He, Jun Kuang, Junhui Mei, Kai Liang, Ke He, Kefeng Zhang, Keheng Wang, Keqing He, Liang Gao, Liang Shi, Lianhui Ma, Lin Qiu, Lingbin Kong, Lingtong Si, Linkun Lyu, Linsen Guo, Liqi Yang, Lizhi Yan, Mai Xia, Man Gao, Manyuan Zhang, Meng Zhou, Mengxia Shen, Mingxiang Tuo, Mingyang Zhu, Peiguang Li, Peng Pei, Peng Zhao, Pengcheng Jia, Pingwei Sun, Qi Gu, Qianyun Li, Qingyuan Li, Qiong Huang, Qiyuan Duan, Ran Meng, Rongxiang Weng, Ruichen Shao, Rumei Li, Shizhe Wu, Shuai Liang, Shuo Wang, Suogui Dang, Tao Fang, Tao Li, Tefeng Chen, Tianhao Bai, Tianhao Zhou, Tingwen Xie, Wei He, Wei Huang, Wei Liu, Wei Shi, Wei Wang, Wei Wu, Weikang Zhao, Wen Zan, Wenjie Shi, Xi Nan, Xi Su, Xiang Li, Xiang Mei, Xiangyang Ji, Xiangyu Xi, Xiangzhou Huang, Xianpeng Li, Xiao Fu, Xiao Liu, Xiao Wei, Xiaodong Cai, Xiaolong Chen, Xiaoqing Liu, Xiaotong Li, Xiaowei Shi, Xiaoyu Li, Xili Wang, Xin Chen, Xing Hu, Xingyu Miao, Xinyan He, Xuemiao Zhang, Xueyuan Hao, Xuezhi Cao, Xunliang Cai, Xurui Yang, Yan Feng, Yang Bai, Yang Chen, Yang Yang, Yaqi Huo, Yerui Sun, Yifan Lu, Yifan Zhang, Yipeng Zang, Yitao Zhai, Yiyang Li, Yongjing Yin, Yongkang Lv, Yongwei Zhou, Yu Yang, Yuchen Xie, Yueqing Sun, Yuewen Zheng, Yuhuai Wei, Yulei Qian, Yunfan Liang, Yunfang Tai, Yunke Zhao, Zeyang Yu, Zhao Zhang, Zhaohua Yang, Zhenchao Zhang, Zhikang Xia, Zhiye Zou, Zhizhao Zeng, Zhongda Su, Zhuofan Chen, Zijian Zhang, Ziwen Wang, Zixu Jiang, Zizhe Zhao, Zongyu Wang, Zunhai Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01322">LongCat-Flash Technical Report</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE) language model designed for both computational efficiency and advanced agentic capabilities. Stemming from the need for scalable efficiency, LongCat-Flash adopts two novel designs: (a) Zero-computation Experts, which enables dynamic computational budget allocation and activates 18.6B-31.3B (27B on average) per token depending on contextual demands, optimizing resource usage. (b) Shortcut-connected MoE, which enlarges the computation-communication overlap window, demonstrating notable gains in inference efficiency and throughput compared to models of a comparable scale. We develop a comprehensive scaling framework for large models that combines hyperparameter transfer, model-growth initialization, a multi-pronged stability suite, and deterministic computation to achieve stable and reproducible training. Notably, leveraging the synergy among scalable architectural design and infrastructure efforts, we complete model training on more than 20 trillion tokens within 30 days, while achieving over 100 tokens per second (TPS) for inference at a cost of \$0.70 per million output tokens. To cultivate LongCat-Flash towards agentic intelligence, we conduct a large-scale pre-training on optimized mixtures, followed by targeted mid- and post-training on reasoning, code, and instructions, with further augmentation from synthetic data and tool use tasks. Comprehensive evaluations demonstrate that, as a non-thinking foundation model, LongCat-Flash delivers highly competitive performance among other leading models, with exceptional strengths in agentic tasks. The model checkpoint of LongCat-Flash is open-sourced to foster community research. LongCat Chat: https://longcat.ai Hugging Face: https://huggingface.co/meituan-longcat GitHub: https://github.com/meituan-longcat<br>
<span id='abs_ch'>中文: LongCat-Flash 是一个拥有5600亿参数的专家混合模型，通过零计算专家和捷径连接MoE等创新设计实现高效计算，在20万亿令牌上快速完成训练，在智能体任务中表现优异，模型已开源供社区 究。</span><br>
<span id='abs_en'>English: LongCat-Flash is a 560-billion-parameter Mixture-of-Experts model that achieves computational efficiency through novel designs like Zero-computation Experts and Shortcut-connected MoE, enabling rapid training on 20+ trillion tokens and demonstrating strong performance in agentic tasks while being open-sourced for community use.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>669, <a href='https://arxiv.org/pdf/2509.01245.pdf' target='_blank'>https://arxiv.org/pdf/2509.01245.pdf</a></span>   <span><a href='https://github.com/eunomia-bpf/schedcp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yusheng Zheng, Yanpeng Hu, Wei Zhang, Andi Quinn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01245">Towards Agentic OS: An LLM Agent Framework for Linux Schedulers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Operating system schedulers suffer from a fundamental semantic gap, where kernel policies fail to understand application-specific needs, leading to suboptimal performance. We introduce SchedCP, the first framework that enables fully autonomous Large Language Model (LLM) agents to safely and efficiently optimize Linux schedulers without human involvement. Our core insight is that the challenge is not merely to apply a better LLM, but to architect a decoupled control plane that separates the AI's role of semantic reasoning ("what to optimize") from the system's role of execution ("how to observe and act"), thereby separating the optimization problem into two stages: goal-inference and policy-synthesis. Implemented as Model Context Protocol(MCP) server, SchedCP provides a stable interface with three key services: a Workload Analysis Engine, an evolving Scheduler Policy Repository, and an Execution Verifier that validates all AI-generated code and configure before deployment with static and dynamic analysis. We demonstrate this architecture's power with sched-agent, a multi-agent system that autonomously analyzes workloads, synthesizes custom eBPF scheduling policies, and deploys them via the sched\_ext infrastructure. Our evaluation shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x cost reduction compared to naive agentic approaches, all while maintaining high success rate. By bridging the semantic gap, SchedCP democratizes expert-level system optimization and represents a step towards creating truly self-optimizing, application-aware operating systems. The code is open-sourced in https://github.com/eunomia-bpf/schedcp<br>
<span id='abs_ch'>中文：SchedCP 是一种创新框架，通过分离语义推理与执行，利用自主大型语言模型代理优化 Linux 调度器，在严 验证保障安全的同时，实现了性能显著提升和成本大幅降低。</span><br>
<span id='abs_en'>English: SchedCP is a novel framework that employs autonomous LLM agents to optimize Linux schedulers by decoupling semantic reasoning from execution, achieving significant performance gains and cost reductions while ensuring safety through rigorous verification.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>670, <a href='https://arxiv.org/pdf/2509.01245.pdf' target='_blank'>https://arxiv.org/pdf/2509.01245.pdf</a></span>   <span><a href='https://github.com/eunomia-bpf/schedcp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yusheng Zheng, Yanpeng Hu, Wei Zhang, Andi Quinn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01245">Towards Agentic OS: An LLM Agent Framework for Linux Schedulers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Operating system schedulers suffer from a fundamental semantic gap, where kernel policies fail to understand application-specific needs, leading to suboptimal performance. We introduce SchedCP, the first framework that enables fully autonomous Large Language Model (LLM) agents to safely and efficiently optimize Linux schedulers without human involvement. Our core insight is that the challenge is not merely to apply a better LLM, but to architect a decoupled control plane that separates the AI's role of semantic reasoning ("what to optimize") from the system's role of execution ("how to observe and act"), thereby separating the optimization problem into two stages: goal-inference and policy-synthesis. Implemented as Model Context Protocol(MCP) server, SchedCP provides a stable interface with three key services: a Workload Analysis Engine, an evolving Scheduler Policy Repository, and an Execution Verifier that validates all AI-generated code and configure before deployment with static and dynamic analysis. We demonstrate this architecture's power with sched-agent, a multi-agent system that autonomously analyzes workloads, synthesizes custom eBPF scheduling policies, and deploys them via the sched\_ext infrastructure. Our evaluation shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x cost reduction compared to naive agentic approaches, all while maintaining high success rate. By bridging the semantic gap, SchedCP democratizes expert-level system optimization and represents a step towards creating truly self-optimizing, application-aware operating systems. The code is open-sourced in https://github.com/eunomia-bpf/schedcp<br>
<span id='abs_ch'>中文：SchedCP 是一种创新框架，通过分离语义推理与执行，利用自主大型语言模型代理优化 Linux 调度器，在严 验证保障安全的同时，实现了性能显著提升和成本大幅降低。</span><br>
<span id='abs_en'>English: SchedCP is a novel framework that employs autonomous LLM agents to optimize Linux schedulers by decoupling semantic reasoning from execution, achieving significant performance gains and cost reductions while ensuring safety through rigorous verification.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>671, <a href='https://arxiv.org/pdf/2509.01153.pdf' target='_blank'>https://arxiv.org/pdf/2509.01153.pdf</a></span>   <span><a href='https://github.com/chumingqian/EzhouNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yun Chu, Qiuhao Wang, Enze Zhou, Qian Liu, Gang Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01153">EZhouNet:A framework based on graph neural network and anchor interval for the respiratory sound event detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Auscultation is a key method for early diagnosis of respiratory and pulmonary diseases, relying on skilled healthcare professionals. However, the process is often subjective, with variability between experts. As a result, numerous deep learning-based automatic classification methods have emerged, most of which focus on respiratory sound classification. In contrast, research on respiratory sound event detection remains limited. Existing sound event detection methods typically rely on frame-level predictions followed by post-processing to generate event-level outputs, making interval boundaries challenging to learn directly. Furthermore, many approaches can only handle fixed-length audio, limiting their applicability to variable-length respiratory sounds. Additionally, the impact of respiratory sound location information on detection performance has not been extensively explored. To address these issues, we propose a graph neural network-based framework with anchor intervals, capable of handling variable-length audio and providing more precise temporal localization for abnormal respiratory sound events. Our method improves both the flexibility and applicability of respiratory sound detection. Experiments on the SPRSound 2024 and HF Lung V1 datasets demonstrate the effectiveness of the proposed approach, and incorporating respiratory position information enhances the discrimination between abnormal sounds. The reference implementation is available at https://github.com/chumingqian/EzhouNet.<br>
<span id='abs_ch'>中文: 本文提出了一种基于图神经网络和锚定区间的框架，用于呼吸音事件检测，能处理变长音频并提供精确的时间定位，在SPRSound 2024和HF Lung V1数据集上验证了其有效性。</span><br>
<span id='abs_en'>English: This paper introduces a graph neural network framework with anchor intervals to improve respiratory sound event detection by handling variable-length audio and providing precise temporal localization, validated on SPRSound 2024 and HF Lung V1 datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>672, <a href='https://arxiv.org/pdf/2509.01135.pdf' target='_blank'>https://arxiv.org/pdf/2509.01135.pdf</a></span>   <span><a href='https://github.com/WuCB-BCI/MATL-DC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangli Li, Canbiao Wu, Zhehao Zhou, Na Tian, Zhen Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01135">MATL-DC: A Multi-domain Aggregation Transfer Learning Framework for EEG Emotion Recognition with Domain-Class Prototype under Unseen Targets</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Emotion recognition based on electroencephalography (EEG) signals is increasingly becoming a key research hotspot in affective Brain-Computer Interfaces (aBCIs). However, the current transfer learning model greatly depends on the source domain and target domain data, which hinder the practical application of emotion recognition. Therefore, we propose a Multi-domain Aggregation Transfer Learning framework for EEG emotion recognition with Domain-Class prototype under unseen targets (MATL-DC). We design the feature decoupling module to decouple class-invariant domain features from domain-invariant class features from shallow features. In the model training stage, the multi-domain aggregation mechanism aggregates the domain feature space to form a superdomain, which enhances the characteristics of emotional EEG signals. In each superdomain, we further extract the class prototype representation by class features. In addition, we adopt the pairwise learning strategy to transform the sample classification problem into the similarity problem between sample pairs, which effectively alleviates the influence of label noise. It is worth noting that the target domain is completely unseen during the training process. In the inference stage, we use the trained domain-class prototypes for inference, and then realize emotion recognition. We rigorously validate it on the publicly available databases (SEED, SEED-IV and SEED-V). The results show that the accuracy of MATL-DC model is 84.70\%, 68.11\% and 61.08\%, respectively. MATL-DC achieves comparable or even better performance than methods that rely on both source and target domains. The source code is available at https://github.com/WuCB-BCI/MATL-DC.<br>
<span id='abs_ch'>中文: 提出的MATL-DC框架通过多域聚合和域类原型处理未见目 域，在训练阶段 需目 域数据的情况下，实现了具有竞争力的脑电情绪识别准确率。</span><br>
<span id='abs_en'>English: The proposed MATL-DC framework advances EEG-based emotion recognition by using multi-domain aggregation and domain-class prototypes to handle unseen target domains, achieving competitive accuracy without requiring target data during training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>673, <a href='https://arxiv.org/pdf/2509.01081.pdf' target='_blank'>https://arxiv.org/pdf/2509.01081.pdf</a></span>   <span><a href='https://github.com/bouchekif/inheritance_evaluation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdessalam Bouchekif, Samer Rashwani, Heba Sbahi, Shahd Gaben, Mutaz Al-Khatib, Mohammed Ghaly
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01081">Assessing Large Language Models on Islamic Legal Reasoning: Evidence from Inheritance Law Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper evaluates the knowledge and reasoning capabilities of Large Language Models in Islamic inheritance law, known as 'ilm al-mawarith. We assess the performance of seven LLMs using a benchmark of 1,000 multiple-choice questions covering diverse inheritance scenarios, designed to test models' ability to understand the inheritance context and compute the distribution of shares prescribed by Islamic jurisprudence. The results reveal a significant performance gap: o3 and Gemini 2.5 achieved accuracies above 90%, whereas ALLaM, Fanar, LLaMA, and Mistral scored below 50%. These disparities reflect important differences in reasoning ability and domain adaptation. We conduct a detailed error analysis to identify recurring failure patterns across models, including misunderstandings of inheritance scenarios, incorrect application of legal rules, and insufficient domain knowledge. Our findings highlight limitations in handling structured legal reasoning and suggest directions for improving performance in Islamic legal reasoning. Code: https://github.com/bouchekif/inheritance_evaluation<br>
<span id='abs_ch'>中文摘要：本 究评估了七种大型语言模型在伊斯兰继承法领域的表现，结果显示仅有两种模型准确率超过90%，而四种模型低于50%，错误分析揭示了模型在法律规则应用和领域知识方面存在关键推理缺陷。English Summary: This study evaluates seven large language models on Islamic inheritance law, revealing a significant performance gap where only two models achieved over 90% accuracy while four scored below 50%, with error analysis identifying key reasoning failures in legal rule application and domain knowledge.Paperid:1275,https://arxiv.org/pdf/2509.01055.pdfGitHub</span><br>
<span id='abs_en'>English Summary: This study evaluates seven large language models on Islamic inheritance law, revealing a significant performance gap where only two models achieved over 90% accuracy while four scored below 50%, with error analysis identifying key reasoning failures in legal rule application and domain knowledge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>674, <a href='https://arxiv.org/pdf/2509.01055.pdf' target='_blank'>https://arxiv.org/pdf/2509.01055.pdf</a></span>   <span><a href='https://github.com/TIGER-AI-Lab/verl-tool' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, Tianyu Pang, Wenhu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01055">VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to single-turn interactions without tool integration. While recent Agentic Reinforcement Learning with Tool use (ARLT) approaches have emerged to address multi-turn tool interactions, existing works develop task-specific codebases that suffer from fragmentation, synchronous execution bottlenecks, and limited extensibility across domains. These inefficiencies hinder broader community adoption and algorithmic innovation. We introduce VerlTool, a unified and modular framework that addresses these limitations through systematic design principles. VerlTool provides four key contributions: (1) upstream alignment with VeRL ensuring compatibility and simplified maintenance, (2) unified tool management via standardized APIs supporting diverse modalities including code execution, search, SQL databases, and vision processing, (3) asynchronous rollout execution achieving near 2$\times$ speedup by eliminating synchronization bottlenecks, and (4) comprehensive evaluation demonstrating competitive performance across 6 ARLT domains. Our framework formalizes ARLT as multi-turn trajectories with multi-modal observation tokens (text/image/video), extending beyond single-turn RLVR paradigms. We train and evaluate models on mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering tasks, achieving results comparable to specialized systems while providing unified training infrastructure. The modular plugin architecture enables rapid tool integration requiring only lightweight Python definitions, significantly reducing development overhead and providing a scalable foundation for tool-augmented RL research. Our code is open-sourced at https://github.com/TIGER-AI-Lab/verl-tool.<br>
<span id='abs_ch'>中文: VerlTool作为一个统一模块化框架，通过 准化API、异步执行和灵活插件架构，解决了现有强化学 方法在多轮工具交互中的效率瓶颈，在六大领域实现优异性能的同时大幅降低了开发门槛。</span><br>
<span id='abs_en'>English: VerlTool is a unified modular framework that overcomes the limitations of existing reinforcement learning approaches by enabling efficient multi-turn tool interactions through standardized APIs, asynchronous execution, and a flexible plugin architecture, achieving competitive performance across six domains while accelerating development.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>675, <a href='https://arxiv.org/pdf/2509.01055.pdf' target='_blank'>https://arxiv.org/pdf/2509.01055.pdf</a></span>   <span><a href='https://github.com/TIGER-AI-Lab/verl-tool' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, Tianyu Pang, Wenhu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.01055">VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to single-turn interactions without tool integration. While recent Agentic Reinforcement Learning with Tool use (ARLT) approaches have emerged to address multi-turn tool interactions, existing works develop task-specific codebases that suffer from fragmentation, synchronous execution bottlenecks, and limited extensibility across domains. These inefficiencies hinder broader community adoption and algorithmic innovation. We introduce VerlTool, a unified and modular framework that addresses these limitations through systematic design principles. VerlTool provides four key contributions: (1) upstream alignment with VeRL ensuring compatibility and simplified maintenance, (2) unified tool management via standardized APIs supporting diverse modalities including code execution, search, SQL databases, and vision processing, (3) asynchronous rollout execution achieving near 2$\times$ speedup by eliminating synchronization bottlenecks, and (4) comprehensive evaluation demonstrating competitive performance across 6 ARLT domains. Our framework formalizes ARLT as multi-turn trajectories with multi-modal observation tokens (text/image/video), extending beyond single-turn RLVR paradigms. We train and evaluate models on mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering tasks, achieving results comparable to specialized systems while providing unified training infrastructure. The modular plugin architecture enables rapid tool integration requiring only lightweight Python definitions, significantly reducing development overhead and providing a scalable foundation for tool-augmented RL research. Our code is open-sourced at https://github.com/TIGER-AI-Lab/verl-tool.<br>
<span id='abs_ch'>中文: VerlTool作为一个统一模块化框架，通过 准化API、异步执行和灵活插件架构，解决了现有强化学 方法在多轮工具交互中的效率瓶颈，在六大领域实现优异性能的同时大幅降低了开发门槛。</span><br>
<span id='abs_en'>English: VerlTool is a unified modular framework that overcomes the limitations of existing reinforcement learning approaches by enabling efficient multi-turn tool interactions through standardized APIs, asynchronous execution, and a flexible plugin architecture, achieving competitive performance across six domains while accelerating development.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>676, <a href='https://arxiv.org/pdf/2509.00961.pdf' target='_blank'>https://arxiv.org/pdf/2509.00961.pdf</a></span>   <span><a href='https://github.com/lun-ai/LENS.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lun Ai, Johannes Langer, Ute Schmid, Stephen Muggleton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00961">Ultra Strong Machine Learning: Teaching Humans Active Learning Strategies via Automated AI Explanations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Ultra Strong Machine Learning (USML) refers to symbolic learning systems that not only improve their own performance but can also teach their acquired knowledge to quantifiably improve human performance. In this work, we present LENS (Logic Programming Explanation via Neural Summarisation), a neuro-symbolic method that combines symbolic program synthesis with large language models (LLMs) to automate the explanation of machine-learned logic programs in natural language. LENS addresses a key limitation of prior USML approaches by replacing hand-crafted explanation templates with scalable automated generation. Through systematic evaluation using multiple LLM judges and human validation, we demonstrate that LENS generates superior explanations compared to direct LLM prompting and hand-crafted templates. To investigate whether LENS can teach transferable active learning strategies, we carried out a human learning experiment across three related domains. Our results show no significant human performance improvements, suggesting that comprehensive LLM responses may overwhelm users for simpler problems rather than providing learning support. Our work provides a solid foundation for building effective USML systems to support human learning. The source code is available on: https://github.com/lun-ai/LENS.git.<br>
<span id='abs_ch'>中文: LENS是一种神经符号方法，能自动生成机器学 逻辑程序的自然语言解释，其效果优于 统模板和直接大语言模型提示，但在实验中未显著提升人类学 效果。</span><br>
<span id='abs_en'>English: LENS is a neuro-symbolic method that automates the generation of natural language explanations for machine-learned logic programs, outperforming traditional templates and direct LLM prompting, though it did not significantly enhance human learning in experiments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>677, <a href='https://arxiv.org/pdf/2509.00905.pdf' target='_blank'>https://arxiv.org/pdf/2509.00905.pdf</a></span>   <span><a href='https://github.com/greatest-gourmet/Spotlighter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yutong Gao, Maoyuan Shao, Xinyang Huang, Chuang Zhu, Lijuan Sun, Yu Weng, Xuan Liu, Guoshun Nan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00905">Spotlighter: Revisiting Prompt Tuning from a Representative Mining View</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>CLIP's success has demonstrated that prompt tuning can achieve robust cross-modal semantic alignment for tasks ranging from open-domain recognition to fine-grained classification. However, redundant or weakly relevant feature components introduce noise and incur unnecessary computational costs. In this work, we propose Spotlighter, a lightweight token-selection framework that simultaneously enhances accuracy and efficiency in prompt tuning. Spotlighter evaluates each visual token's activation from both sample-wise and semantic-wise perspectives and retains only the top-scoring tokens for downstream prediction. A class-specific semantic memory bank of learned prototypes refines this selection, ensuring semantic representativeness and compensating for discarded features. To further prioritize informative signals, we introduce a two-level ranking mechanism that dynamically weights token--prototype interactions. Across 11 few-shot benchmarks, Spotlighter outperforms CLIP by up to 11.19\% in harmonic mean accuracy and achieves up to 0.8K additional FPS, with only 21 extra parameters. These results establish Spotlighter as an effective and scalable baseline for prompt tuning. Code for our method will be available at https://github.com/greatest-gourmet/Spotlighter.<br>
<span id='abs_ch'>中文: Spotlighter是一种轻量级令牌选择框架，通过双重评估和语义记忆库保留高分视觉令牌，在提示调优中显著提升准确性与效率，成为该领域的有效基准。</span><br>
<span id='abs_en'>English: Spotlighter is a lightweight token-selection framework that enhances prompt tuning by retaining top-scoring visual tokens through dual-perspective evaluation and a semantic memory bank, achieving superior accuracy and efficiency across benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>678, <a href='https://arxiv.org/pdf/2509.00843.pdf' target='_blank'>https://arxiv.org/pdf/2509.00843.pdf</a></span>   <span><a href='https://github.com/YiGuYT/LookBeyond' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueyang Kang, Zhengkang Xiang, Zezheng Zhang, Kourosh Khoshelham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00843">Look Beyond: Two-Stage Scene View Generation via Panorama and Video Diffusion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Novel view synthesis (NVS) from a single image is highly ill-posed due to large unobserved regions, especially for views that deviate significantly from the input. While existing methods focus on consistency between the source and generated views, they often fail to maintain coherence and correct view alignment across long-range or looped trajectories. We propose a model that addresses this by decomposing single-view NVS into a 360-degree scene extrapolation followed by novel view interpolation. This design ensures long-term view and scene consistency by conditioning on keyframes extracted and warped from a generated panoramic representation. In the first stage, a panorama diffusion model learns the scene prior from the input perspective image. Perspective keyframes are then sampled and warped from the panorama and used as anchor frames in a pre-trained video diffusion model, which generates novel views through a proposed spatial noise diffusion process. Compared to prior work, our method produces globally consistent novel views -- even in loop closure scenarios -- while enabling flexible camera control. Experiments on diverse scene datasets demonstrate that our approach outperforms existing methods in generating coherent views along user-defined trajectories. Our implementation is available at https://github.com/YiGuYT/LookBeyond.<br>
<span id='abs_ch'>中文摘要：该模型通过先外推360度场景再插值新视角的方法，解决了单图像新视角合成中的长期一致性问题，在用户定义轨迹上比现有方法生成更连贯的视图。</span><br>
<span id='abs_en'>English Summary: The proposed model enhances novel view synthesis from a single image by first extrapolating a 360-degree scene and then interpolating novel views, ensuring long-term consistency and superior performance on user-defined trajectories compared to existing methods.Paperid:1282,https://arxiv.org/pdf/2509.00833.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>679, <a href='https://arxiv.org/pdf/2509.00826.pdf' target='_blank'>https://arxiv.org/pdf/2509.00826.pdf</a></span>   <span><a href='https://github.com/X-L-Liu/SDM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinlei Liu, Tao Hu, Peng Yi, Weitao Han, Jichao Xie, Baolin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00826">Sequential Difference Maximization: Generating Adversarial Examples via Multi-Stage Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Efficient adversarial attack methods are critical for assessing the robustness of computer vision models. In this paper, we reconstruct the optimization objective for generating adversarial examples as "maximizing the difference between the non-true labels' probability upper bound and the true label's probability," and propose a gradient-based attack method termed Sequential Difference Maximization (SDM). SDM establishes a three-layer optimization framework of "cycle-stage-step." The processes between cycles and between iterative steps are respectively identical, while optimization stages differ in terms of loss functions: in the initial stage, the negative probability of the true label is used as the loss function to compress the solution space; in subsequent stages, we introduce the Directional Probability Difference Ratio (DPDR) loss function to gradually increase the non-true labels' probability upper bound by compressing the irrelevant labels' probabilities. Experiments demonstrate that compared with previous SOTA methods, SDM not only exhibits stronger attack performance but also achieves higher attack cost-effectiveness. Additionally, SDM can be combined with adversarial training methods to enhance their defensive effects. The code is available at https://github.com/X-L-Liu/SDM.<br>
<span id='abs_ch'>Chinese: 本文提出序列差异最大化（SDM）方法，通过循环-阶段-步骤的三层优化框架，在压缩真实 签概率的同时提升非真实 签概率上限，相比现有最优方法不仅攻击性能更强且成本效益更高。</span><br>
<span id='abs_en'>English: This paper introduces Sequential Difference Maximization (SDM), a gradient-based adversarial attack method that enhances both attack effectiveness and cost-efficiency by optimizing non-true label probabilities while compressing the true label's probability, outperforming previous state-of-the-art methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>680, <a href='https://arxiv.org/pdf/2509.00684.pdf' target='_blank'>https://arxiv.org/pdf/2509.00684.pdf</a></span>   <span><a href='https://github.com/amartya21/vector-drug-design.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amartya Banerjee, Somnath Kar, Anirban Pal, Debabrata Maiti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00684">Valid Property-Enhanced Contrastive Learning for Targeted Optimization & Resampling for Novel Drug Design</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Efficiently steering generative models toward pharmacologically relevant regions of chemical space remains a major obstacle in molecular drug discovery under low-data regimes. We present VECTOR+: Valid-property-Enhanced Contrastive Learning for Targeted Optimization and Resampling, a framework that couples property-guided representation learning with controllable molecule generation. VECTOR+ applies to both regression and classification tasks and enables interpretable, data-efficient exploration of functional chemical space. We evaluate on two datasets: a curated PD-L1 inhibitor set (296 compounds with experimental $IC_{50}$ values) and a receptor kinase inhibitor set (2,056 molecules by binding mode). Despite limited training data, VECTOR+ generates novel, synthetically tractable candidates. Against PD-L1 (PDB 5J89), 100 of 8,374 generated molecules surpass a docking threshold of $-15.0$ kcal/mol, with the best scoring $-17.6$ kcal/mol compared to the top reference inhibitor ($-15.4$ kcal/mol). The best-performing molecules retain the conserved biphenyl pharmacophore while introducing novel motifs. Molecular dynamics (250 ns) confirm binding stability (ligand RMSD < $2.5$ angstroms). VECTOR+ generalizes to kinase inhibitors, producing compounds with stronger docking scores than established drugs such as brigatinib and sorafenib. Benchmarking against JT-VAE and MolGPT across docking, novelty, uniqueness, and Tanimoto similarity highlights the superior performance of our method. These results position our work as a robust, extensible approach for property-conditioned molecular design in low-data settings, bridging contrastive learning and generative modeling for reproducible, AI-accelerated discovery.<br>
<span id='abs_ch'>中文: VECTOR+是一种创新框架，通过结合对比学 与生成模型，在低数据条件下高效设计具有药理相关性的分子，相比现有方法能生成更稳定、新颖且具有更强对接活性的化合物，展现出卓越性能。</span><br>
<span id='abs_en'>English: VECTOR+ is a novel framework that integrates contrastive learning with generative modeling to efficiently design pharmacologically relevant molecules in low-data scenarios, demonstrating superior performance in generating stable and novel compounds with enhanced docking scores compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>681, <a href='https://arxiv.org/pdf/2509.00653.pdf' target='_blank'>https://arxiv.org/pdf/2509.00653.pdf</a></span>   <span><a href='https://github.com/tung-nd/IndiaWeatherBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tung Nguyen, Harkanwar Singh, Nilay Naharas, Lucas Bandarkar, Aditya Grover
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00653">IndiaWeatherBench: A Dataset and Benchmark for Data-Driven Regional Weather Forecasting over India</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Regional weather forecasting is a critical problem for localized climate adaptation, disaster mitigation, and sustainable development. While machine learning has shown impressive progress in global weather forecasting, regional forecasting remains comparatively underexplored. Existing efforts often use different datasets and experimental setups, limiting fair comparison and reproducibility. We introduce IndiaWeatherBench, a comprehensive benchmark for data-driven regional weather forecasting focused on the Indian subcontinent. IndiaWeatherBench provides a curated dataset built from high-resolution regional reanalysis products, along with a suite of deterministic and probabilistic metrics to facilitate consistent training and evaluation. To establish strong baselines, we implement and evaluate a range of models across diverse architectures, including UNets, Transformers, and Graph-based networks, as well as different boundary conditioning strategies and training objectives. While focused on India, IndiaWeatherBench is easily extensible to other geographic regions. We open-source all raw and preprocessed datasets, model implementations, and evaluation pipelines to promote accessibility and future development. We hope IndiaWeatherBench will serve as a foundation for advancing regional weather forecasting research. Code is available at https://github.com/tung-nd/IndiaWeatherBench.<br>
<span id='abs_ch'>Chinese: 印度气象基准（IndiaWeatherBench）被提出作为一个全面的数据驱动区域天气预报基准，为印度次大陆提供精选数据集、评估指 和基线模型，以推动这一相对未充分探索领域的 究进展。</span><br>
<span id='abs_en'>English: IndiaWeatherBench is introduced as a comprehensive benchmark for data-driven regional weather forecasting in India, providing curated datasets, evaluation metrics, and baseline models to advance research in this underexplored area.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>682, <a href='https://arxiv.org/pdf/2509.00626.pdf' target='_blank'>https://arxiv.org/pdf/2509.00626.pdf</a></span>   <span><a href='https://github.com/spaceml-org/plume-hunter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maggie Chen, Hala Lambdouar, Luca Marini, Laura Martínez-Ferrer, Chris Bridges, Giacomo Acciarini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00626">Towards Methane Detection Onboard Satellites</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Methane is a potent greenhouse gas and a major driver of climate change, making its timely detection critical for effective mitigation. Machine learning (ML) deployed onboard satellites can enable rapid detection while reducing downlink costs, supporting faster response systems. Conventional methane detection methods often rely on image processing techniques, such as orthorectification to correct geometric distortions and matched filters to enhance plume signals. We introduce a novel approach that bypasses these preprocessing steps by using \textit{unorthorectified} data (UnorthoDOS). We find that ML models trained on this dataset achieve performance comparable to those trained on orthorectified data. Moreover, we also train models on an orthorectified dataset, showing that they can outperform the matched filter baseline (mag1c). We release model checkpoints and two ML-ready datasets comprising orthorectified and unorthorectified hyperspectral images from the Earth Surface Mineral Dust Source Investigation (EMIT) sensor at https://huggingface.co/datasets/SpaceML/UnorthoDOS , along with code at https://github.com/spaceml-org/plume-hunter.<br>
<br>
<div id='section'>Paperid: <span id='pid'>683, <a href='https://arxiv.org/pdf/2509.00622.pdf' target='_blank'>https://arxiv.org/pdf/2509.00622.pdf</a></span>   <span><a href='https://github.com/ShiqiaoZhou/BALM-TSF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shiqiao Zhou, Holger SchÃ¶ner, Huanbo Lyu, Edouard FouchÃ©, Shuo Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00622">BALM-TSF: Balanced Multimodal Alignment for LLM-Based Time Series Forecasting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Time series forecasting is a long-standing and highly challenging research topic. Recently, driven by the rise of large language models (LLMs), research has increasingly shifted from purely time series methods toward harnessing textual modalities to enhance forecasting performance. However, the vast discrepancy between text and temporal data often leads current multimodal architectures to over-emphasise one modality while neglecting the other, resulting in information loss that harms forecasting performance. To address this modality imbalance, we introduce BALM-TSF (Balanced Multimodal Alignment for LLM-Based Time Series Forecasting), a lightweight time series forecasting framework that maintains balance between the two modalities. Specifically, raw time series are processed by the time series encoder, while descriptive statistics of raw time series are fed to an LLM with learnable prompt, producing compact textual embeddings. To ensure balanced cross-modal context alignment of time series and textual embeddings, a simple yet effective scaling strategy combined with a contrastive objective then maps these textual embeddings into the latent space of the time series embeddings. Finally, the aligned textual semantic embeddings and time series embeddings are together integrated for forecasting. Extensive experiments on standard benchmarks show that, with minimal trainable parameters, BALM-TSF achieves state-of-the-art performance in both long-term and few-shot forecasting, confirming its ability to harness complementary information from text and time series. Code is available at https://github.com/ShiqiaoZhou/BALM-TSF.<br>
<span id='abs_ch'>中文：BALM-TSF是一种轻量级多模态框架，通过对比对齐平衡时间序列与文本嵌入，以极少的参数实现了顶尖的预测性能。</span><br>
<span id='abs_en'>English: BALM-TSF is a lightweight multimodal framework that balances time series and text embeddings through contrastive alignment, achieving state-of-the-art forecasting performance with minimal parameters.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>684, <a href='https://arxiv.org/pdf/2509.00482.pdf' target='_blank'>https://arxiv.org/pdf/2509.00482.pdf</a></span>   <span><a href='https://github.com/scb-10x/apo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saksorn Ruangtanusak, Pittawat Taveekitworachai, Kunat Pipatanakul
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00482">Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This report investigates approaches for prompting a tool-augmented large language model (LLM) to act as a role-playing dialogue agent in the API track of the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this setting, dialogue agents often produce overly long in-character responses (over-speaking) while failing to use tools effectively according to the persona (under-acting), such as generating function calls that do not exist or making unnecessary tool calls before answering. We explore four prompting approaches to address these issues: 1) basic role prompting, 2) human-crafted role prompting, 3) automatic prompt optimization (APO), and 4) rule-based role prompting. The rule-based role prompting (RRP) approach achieved the best performance through two novel techniques--character-card/scene-contract design and strict enforcement of function calling--which led to an overall score of 0.571, improving on the zero-shot baseline score of 0.519. These findings demonstrate that RRP design can substantially improve the effectiveness and reliability of role-playing dialogue agents compared with more elaborate methods such as APO. To support future efforts in developing persona prompts, we are open-sourcing all of our best-performing prompts and the APO tool. Source code is available at https://github.com/scb-10x/apo.<br>
<span id='abs_ch'>中文: 本 究探索了四种提示方法，通过角色卡片设计和严 函数调用优化角色扮演对话代理的过度发言和行动不足问题，其中基于规则的提示方法表现最佳。</span><br>
<span id='abs_en'>English: This study explores four prompting methods to enhance role-playing dialogue agents by addressing over-speaking and under-acting issues, with rule-based role prompting achieving the best performance through character-card design and strict function enforcement.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>685, <a href='https://arxiv.org/pdf/2509.00402.pdf' target='_blank'>https://arxiv.org/pdf/2509.00402.pdf</a></span>   <span><a href='https://github.com/Kang-Min-Ku/CUFL.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minku Kang, Hogun Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00402">Curriculum Guided Personalized Subgraph Federated Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Subgraph Federated Learning (FL) aims to train Graph Neural Networks (GNNs) across distributed private subgraphs, but it suffers from severe data heterogeneity. To mitigate data heterogeneity, weighted model aggregation personalizes each local GNN by assigning larger weights to parameters from clients with similar subgraph characteristics inferred from their current model states. However, the sparse and biased subgraphs often trigger rapid overfitting, causing the estimated client similarity matrix to stagnate or even collapse. As a result, aggregation loses effectiveness as clients reinforce their own biases instead of exploiting diverse knowledge otherwise available. To this end, we propose a novel personalized subgraph FL framework called Curriculum guided personalized sUbgraph Federated Learning (CUFL). On the client side, CUFL adopts Curriculum Learning (CL) that adaptively selects edges for training according to their reconstruction scores, exposing each GNN first to easier, generic cross-client substructures and only later to harder, client-specific ones. This paced exposure prevents early overfitting to biased patterns and enables gradual personalization. By regulating personalization, the curriculum also reshapes server aggregation from exchanging generic knowledge to propagating client-specific knowledge. Further, CUFL improves weighted aggregation by estimating client similarity using fine-grained structural indicators reconstructed on a random reference graph. Extensive experiments on six benchmark datasets confirm that CUFL achieves superior performance compared to relevant baselines. Code is available at https://github.com/Kang-Min-Ku/CUFL.git.<br>
<span id='abs_ch'>中文摘要：CUFL提出了一种课程引导的个性化子图联邦学 框架，通过逐步让模型接触通用及客户端特定图结构来防止早期过拟合，并利用细粒度结构指 改进客户端相似性估计。</span><br>
<span id='abs_en'>English Summary: CUFL introduces a curriculum-guided personalized federated learning framework that prevents early overfitting by progressively exposing models to generic then client-specific graph structures, while improving client similarity estimation through fine-grained structural indicators.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>686, <a href='https://arxiv.org/pdf/2509.00375.pdf' target='_blank'>https://arxiv.org/pdf/2509.00375.pdf</a></span>   <span><a href='https://github.com/VectorSpaceLab/InfoSeek' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Xia, Kun Luo, Hongjin Qian, Zheng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00375">Open Data Synthesis For Deep Research</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are increasingly expected to go beyond simple factual queries toward Deep Research-tasks that require decomposing questions into sub-problems, coordinating multi-step reasoning, and synthesizing evidence from diverse sources. We formalize Deep Research tasks with verifiable answers as Hierarchical Constraint Satisfaction Problems (HCSPs), which are fundamentally different from single-constraint, multi-hop, or flat CSP formulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA) fail to capture this complexity, while recent synthetic datasets often introduce shortcut reasoning, knowledge leakage, or lack sufficient structural depth. To address this gap, we introduce InfoSeek, a scalable framework for synthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to recursively build a Research Tree from large-scale webpages, blurring intermediate nodes into valid sub-problems, and converting these trees into natural language questions that require traversing the full hierarchy. It also enables rapid scaling, yielding over 50K training examples, a curated test set, and reasoning trajectories generated via reject sampling. Experiments show that models trained on InfoSeek consistently outperform strong baselines. On a challenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass much larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash), while achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro). By preserving meta-information such as intermediate steps and retrieval labels, InfoSeek further supports advanced optimization strategies, including compound reward design and trajectory-level exploration. We provide our codes and datasets in \href{https://github.com/VectorSpaceLab/InfoSeek}{this repository}.<br>
<span id='abs_ch'>中文: 该 究提出了InfoSeek框架，通过从网络数据生成层次化问题来合成复杂的深度 究任务，显著提升了大语言模型在多步推理和证据综合方面的性能。</span><br>
<span id='abs_en'>English: The study introduces InfoSeek, a scalable framework for synthesizing complex Deep Research tasks by generating hierarchical questions from web data, which significantly enhances the performance of large language models in multi-step reasoning and evidence synthesis.Paperid:1307,https://arxiv.org/pdf/2509.00365.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>687, <a href='https://arxiv.org/pdf/2509.00357.pdf' target='_blank'>https://arxiv.org/pdf/2509.00357.pdf</a></span>   <span><a href='https://github.com/franciszchen/SurgLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhen Chen, Xingjian Luo, Kun Yuan, Jinlin Wu, Danny T. M. Chan, Nassir Navab, Hongbin Liu, Zhen Lei, Jiebo Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00357">SurgLLM: A Versatile Large Multimodal Model with Spatial Focus and Temporal Awareness for Surgical Video Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Surgical video understanding is crucial for facilitating Computer-Assisted Surgery (CAS) systems. Despite significant progress in existing studies, two major limitations persist, including inadequate visual content perception and insufficient temporal awareness in surgical videos, and hinder the development of versatile CAS solutions. In this work, we propose the SurgLLM framework, an effective large multimodal model tailored for versatile surgical video understanding tasks with enhanced spatial focus and temporal awareness. Specifically, to empower the spatial focus of surgical videos, we first devise Surgical Context-aware Multimodal Pretraining (Surg-Pretrain) for the video encoder of SurgLLM, by performing instrument-centric Masked Video Reconstruction (MV-Recon) and subsequent multimodal alignment. To incorporate surgical temporal knowledge into SurgLLM, we further propose Temporal-aware Multimodal Tuning (TM-Tuning) to enhance temporal reasoning with interleaved multimodal embeddings. Moreover, to accommodate various understanding tasks of surgical videos without conflicts, we devise a Surgical Task Dynamic Ensemble to efficiently triage a query with optimal learnable parameters in our SurgLLM. Extensive experiments performed on diverse surgical video understanding tasks, including captioning, general VQA, and temporal VQA, demonstrate significant improvements over the state-of-the-art approaches, validating the effectiveness of our SurgLLM in versatile surgical video understanding. The source code is available at https://github.com/franciszchen/SurgLLM.<br>
<br>
<div id='section'>Paperid: <span id='pid'>688, <a href='https://arxiv.org/pdf/2509.00316.pdf' target='_blank'>https://arxiv.org/pdf/2509.00316.pdf</a></span>   <span><a href='https://github.com/eje24/ctds' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ezra Erives, Bowen Jing, Peter Holderrieth, Tommi Jaakkola
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00316">Continuously Tempered Diffusion Samplers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Annealing-based neural samplers seek to amortize sampling from unnormalized distributions by training neural networks to transport a family of densities interpolating from source to target. A crucial design choice in the training phase of such samplers is the proposal distribution by which locations are generated at which to evaluate the loss. Previous work has obtained such a proposal distribution by combining a partially learned transport with annealed Langevin dynamics. However, isolated modes and other pathological properties of the annealing path imply that such proposals achieve insufficient exploration and thereby lower performance post training. To remedy this, we propose continuously tempered diffusion samplers, which leverage exploration techniques developed in the context of molecular dynamics to improve proposal distributions. Specifically, a family of distributions across different temperatures is introduced to lower energy barriers at higher temperatures and drive exploration at the lower temperature of interest. We empirically validate improved sampler performance driven by extended exploration. Code is available at https://github.com/eje24/ctds.<br>
<span id='abs_ch'>中文: 退火神经采 器 提议分布的病理特性而面临探索不足的问题，连续调温扩散采 器通过引入多温度分布来增强探索，从而提升了采 性能。</span><br>
<span id='abs_en'>English: Annealing-based neural samplers face exploration limitations due to pathological properties in their proposal distributions, which are addressed by continuously tempered diffusion samplers that introduce multi-temperature distributions to enhance exploration and improve performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>689, <a href='https://arxiv.org/pdf/2509.00115.pdf' target='_blank'>https://arxiv.org/pdf/2509.00115.pdf</a></span>   <span><a href='https://github.com/Manishms18/Adaptive-Multi-Dimensional-Monitoring' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Manish Shukla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00115">Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Agentic artificial intelligence (AI) -- multi-agent systems that combine large language models with external tools and autonomous planning -- are rapidly transitioning from research laboratories into high-stakes domains. Our earlier "Basic" paper introduced a five-axis framework and proposed preliminary metrics such as goal drift and harm reduction but did not provide an algorithmic instantiation or empirical evidence. This "Advanced" sequel fills that gap. First, we revisit recent benchmarks and industrial deployments to show that technical metrics still dominate evaluations: a systematic review of 84 papers from 2023--2025 found that 83% report capability metrics while only 30% consider human-centred or economic axes [2]. Second, we formalise an Adaptive Multi-Dimensional Monitoring (AMDM) algorithm that normalises heterogeneous metrics, applies per-axis exponentially weighted moving-average thresholds and performs joint anomaly detection via the Mahalanobis distance [7]. Third, we conduct simulations and real-world experiments. AMDM cuts anomaly-detection latency from 12.3 s to 5.6 s on simulated goal drift and reduces false-positive rates from 4.5% to 0.9% compared with static thresholds. We present a comparison table and ROC/PR curves, and we reanalyse case studies to surface missing metrics. Code, data and a reproducibility checklist accompany this paper to facilitate replication. The code supporting this work is available at https://github.com/Manishms18/Adaptive-Multi-Dimensional-Monitoring.<br>
<span id='abs_ch'>Chinese: 本进阶 究提出自适应多维度监测（AMDM）算法，通过规范化指 和动态阈值显著提升了智能体人工智能系统的异常检测性能，填补了先前 究的空白并提供了实证支持。</span><br>
<span id='abs_en'>English: This advanced paper introduces an Adaptive Multi-Dimensional Monitoring (AMDM) algorithm that significantly improves anomaly detection speed and accuracy in agentic AI systems, addressing gaps from prior research through formalization, simulations, and real-world validation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>690, <a href='https://arxiv.org/pdf/2508.21795.pdf' target='_blank'>https://arxiv.org/pdf/2508.21795.pdf</a></span>   <span><a href='https://github.com/SIA-IDE/TMUAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Liu, Jiahe Hou, Wei Wang, Jinsong Du, Yang Cong, Huijie Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21795">TMUAD: Enhancing Logical Capabilities in Unified Anomaly Detection Models with a Text Memory Bank</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Anomaly detection, which aims to identify anomalies deviating from normal patterns, is challenging due to the limited amount of normal data available. Unlike most existing unified methods that rely on carefully designed image feature extractors and memory banks to capture logical relationships between objects, we introduce a text memory bank to enhance the detection of logical anomalies. Specifically, we propose a Three-Memory framework for Unified structural and logical Anomaly Detection (TMUAD). First, we build a class-level text memory bank for logical anomaly detection by the proposed logic-aware text extractor, which can capture rich logical descriptions of objects from input images. Second, we construct an object-level image memory bank that preserves complete object contours by extracting features from segmented objects. Third, we employ visual encoders to extract patch-level image features for constructing a patch-level memory bank for structural anomaly detection. These three complementary memory banks are used to retrieve and compare normal images that are most similar to the query image, compute anomaly scores at multiple levels, and fuse them into a final anomaly score. By unifying structural and logical anomaly detection through collaborative memory banks, TMUAD achieves state-of-the-art performance across seven publicly available datasets involving industrial and medical domains. The model and code are available at https://github.com/SIA-IDE/TMUAD.<br>
<span id='abs_ch'>中文：TMUAD框架通过结合文本与图像特征的三重记忆系统，统一了结构和逻辑异常检测，在多个数据集上取得了领先性能。</span><br>
<span id='abs_en'>English: The TMUAD framework introduces a three-memory system combining text and image features to unify structural and logical anomaly detection, achieving state-of-the-art results across multiple datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>691, <a href='https://arxiv.org/pdf/2508.21795.pdf' target='_blank'>https://arxiv.org/pdf/2508.21795.pdf</a></span>   <span><a href='https://github.com/SIA-IDE/TMUAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Liu, Jiahe Hou, Wei Wang, Jinsong Du, Yang Cong, Huijie Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21795">TMUAD: Enhancing Logical Capabilities in Unified Anomaly Detection Models with a Text Memory Bank</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Anomaly detection, which aims to identify anomalies deviating from normal patterns, is challenging due to the limited amount of normal data available. Unlike most existing unified methods that rely on carefully designed image feature extractors and memory banks to capture logical relationships between objects, we introduce a text memory bank to enhance the detection of logical anomalies. Specifically, we propose a Three-Memory framework for Unified structural and logical Anomaly Detection (TMUAD). First, we build a class-level text memory bank for logical anomaly detection by the proposed logic-aware text extractor, which can capture rich logical descriptions of objects from input images. Second, we construct an object-level image memory bank that preserves complete object contours by extracting features from segmented objects. Third, we employ visual encoders to extract patch-level image features for constructing a patch-level memory bank for structural anomaly detection. These three complementary memory banks are used to retrieve and compare normal images that are most similar to the query image, compute anomaly scores at multiple levels, and fuse them into a final anomaly score. By unifying structural and logical anomaly detection through collaborative memory banks, TMUAD achieves state-of-the-art performance across seven publicly available datasets involving industrial and medical domains. The model and code are available at https://github.com/SIA-IDE/TMUAD.<br>
<span id='abs_ch'>中文：TMUAD框架通过结合文本与图像特征的三重记忆系统，统一了结构和逻辑异常检测，在多个数据集上取得了领先性能。</span><br>
<span id='abs_en'>English: The TMUAD framework introduces a three-memory system combining text and image features to unify structural and logical anomaly detection, achieving state-of-the-art results across multiple datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>692, <a href='https://arxiv.org/pdf/2508.21589.pdf' target='_blank'>https://arxiv.org/pdf/2508.21589.pdf</a></span>   <span><a href='https://github.com/Word2VecT/Middo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zinan Tang, Xin Gao, Qizhi Pei, Zhuoshi Pan, Mengzhang Cai, Jiang Wu, Conghui He, Lijun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21589">Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely on high-quality training data. While data selection and data synthesis are two common strategies to improve data quality, existing approaches often face limitations in static dataset curation that fail to adapt to evolving model capabilities. In this paper, we introduce Middo, a self-evolving Model-informed dynamic data optimization framework that uses model-aware data selection and context-preserving data refinement. Unlike conventional one-off filtering/synthesis methods, our framework establishes a closed-loop optimization system: (1) A self-referential diagnostic module proactively identifies suboptimal samples through tri-axial model signals - loss patterns (complexity), embedding cluster dynamics (diversity), and self-alignment scores (quality); (2) An adaptive optimization engine then transforms suboptimal samples into pedagogically valuable training points while preserving semantic integrity; (3) This optimization process continuously evolves with model capability through dynamic learning principles. Experiments on multiple benchmarks demonstrate that our Middo consistently enhances the quality of seed data and boosts LLM's performance with improving accuracy by 7.15% on average while maintaining the original dataset scale. This work establishes a new paradigm for sustainable LLM training through dynamic human-AI co-evolution of data and models. Our datasets, models, and code are publicly available at https://github.com/Word2VecT/Middo.<br>
<span id='abs_ch'>中文: 本文提出Middo自进化框架，通过模型感知的数据筛选和优化动态提升训练数据质量，在保持数据集规模的同时平均提高模型准确率7.15%。</span><br>
<span id='abs_en'>English: The paper introduces Middo, a self-evolving framework that dynamically optimizes LLM training data through model-aware selection and refinement, achieving a 7.15% average accuracy improvement while maintaining dataset scale.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>693, <a href='https://arxiv.org/pdf/2508.21482.pdf' target='_blank'>https://arxiv.org/pdf/2508.21482.pdf</a></span>   <span><a href='https://github.com/SaraBCoutinho/HSFN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sara B. Coutinho, Rafael M. O. Cruz, Francimaria R. S. Nascimento, George D. C. Cavalcanti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21482">HSFN: Hierarchical Selection for Fake News Detection building Heterogeneous Ensemble</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Psychological biases, such as confirmation bias, make individuals particularly vulnerable to believing and spreading fake news on social media, leading to significant consequences in domains such as public health and politics. Machine learning-based fact-checking systems have been widely studied to mitigate this problem. Among them, ensemble methods are particularly effective in combining multiple classifiers to improve robustness. However, their performance heavily depends on the diversity of the constituent classifiers-selecting genuinely diverse models remains a key challenge, especially when models tend to learn redundant patterns. In this work, we propose a novel automatic classifier selection approach that prioritizes diversity, also extended by performance. The method first computes pairwise diversity between classifiers and applies hierarchical clustering to organize them into groups at different levels of granularity. A HierarchySelect then explores these hierarchical levels to select one pool of classifiers per level, each representing a distinct intra-pool diversity. The most diverse pool is identified and selected for ensemble construction from these. The selection process incorporates an evaluation metric reflecting each classifiers's performance to ensure the ensemble also generalises well. We conduct experiments with 40 heterogeneous classifiers across six datasets from different application domains and with varying numbers of classes. Our method is compared against the Elbow heuristic and state-of-the-art baselines. Results show that our approach achieves the highest accuracy on two of six datasets. The implementation details are available on the project's repository: https://github.com/SaraBCoutinho/HSFN .<br>
<span id='abs_ch'>中文摘要：心理偏见 剧了人们对虚假新闻的易感性，本 究提出了一种新颖的自动分类器选择方法，通过优先考虑多 性和性能来改进基于集成学 的辟谣系统，在多个数据集上实现了更高的准确率。English Summary: Psychological biases increase susceptibility to fake news, and this study introduces a novel automated classifier selection method that prioritizes diversity and performance to enhance ensemble-based fact-checking systems, achieving superior accuracy on multiple datasets.Paperid:19,https://arxiv.org/pdf/2508.21476.pdfGitHub</span><br>
<span id='abs_en'>English Summary: Psychological biases increase susceptibility to fake news, and this study introduces a novel automated classifier selection method that prioritizes diversity and performance to enhance ensemble-based fact-checking systems, achieving superior accuracy on multiple datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>694, <a href='https://arxiv.org/pdf/2508.21482.pdf' target='_blank'>https://arxiv.org/pdf/2508.21482.pdf</a></span>   <span><a href='https://github.com/SaraBCoutinho/HSFN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sara B. Coutinho, Rafael M. O. Cruz, Francimaria R. S. Nascimento, George D. C. Cavalcanti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21482">HSFN: Hierarchical Selection for Fake News Detection building Heterogeneous Ensemble</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Psychological biases, such as confirmation bias, make individuals particularly vulnerable to believing and spreading fake news on social media, leading to significant consequences in domains such as public health and politics. Machine learning-based fact-checking systems have been widely studied to mitigate this problem. Among them, ensemble methods are particularly effective in combining multiple classifiers to improve robustness. However, their performance heavily depends on the diversity of the constituent classifiers-selecting genuinely diverse models remains a key challenge, especially when models tend to learn redundant patterns. In this work, we propose a novel automatic classifier selection approach that prioritizes diversity, also extended by performance. The method first computes pairwise diversity between classifiers and applies hierarchical clustering to organize them into groups at different levels of granularity. A HierarchySelect then explores these hierarchical levels to select one pool of classifiers per level, each representing a distinct intra-pool diversity. The most diverse pool is identified and selected for ensemble construction from these. The selection process incorporates an evaluation metric reflecting each classifiers's performance to ensure the ensemble also generalises well. We conduct experiments with 40 heterogeneous classifiers across six datasets from different application domains and with varying numbers of classes. Our method is compared against the Elbow heuristic and state-of-the-art baselines. Results show that our approach achieves the highest accuracy on two of six datasets. The implementation details are available on the project's repository: https://github.com/SaraBCoutinho/HSFN .<br>
<span id='abs_ch'>中文摘要：心理偏见 剧了人们对虚假新闻的易感性，本 究提出了一种新颖的自动分类器选择方法，通过优先考虑多 性和性能来改进基于集成学 的辟谣系统，在多个数据集上实现了更高的准确率。English Summary: Psychological biases increase susceptibility to fake news, and this study introduces a novel automated classifier selection method that prioritizes diversity and performance to enhance ensemble-based fact-checking systems, achieving superior accuracy on multiple datasets.Paperid:19,https://arxiv.org/pdf/2508.21476.pdfGitHub</span><br>
<span id='abs_en'>English Summary: Psychological biases increase susceptibility to fake news, and this study introduces a novel automated classifier selection method that prioritizes diversity and performance to enhance ensemble-based fact-checking systems, achieving superior accuracy on multiple datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>695, <a href='https://arxiv.org/pdf/2508.21476.pdf' target='_blank'>https://arxiv.org/pdf/2508.21476.pdf</a></span>   <span><a href='https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaolong Wei, Bo Lu, Xingyu Zhang, Zhejun Zhao, Dongdong Shen, Long Xia, Dawei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21476">Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated remarkable creative writing capabilities, yet their substantial computational demands hinder widespread use. Enhancing Small Language Models (SLMs) offers a promising alternative, but current methods like Supervised Fine-Tuning (SFT) struggle with novelty, and Reinforcement Learning from Human Feedback (RLHF) is costly. This paper explores two distinct AI-driven reward strategies within a Reinforcement Learning from AI Feedback (RLAIF) framework to ignite the creative writing of a 7B-parameter SLM, specifically for generating Chinese greetings. The first strategy employs a RM trained on high-quality preference data curated by a novel multi-agent rejection sampling framework designed for creative tasks. The second, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose reward function is optimized via an adversarial training scheme with a reflection mechanism, to directly provide reward signals. Comprehensive experiments reveal that while both approaches significantly enhance creative output over baselines, the principle-guided LLM-as-a-Judge demonstrably yields superior generation quality. Furthermore, it offers notable advantages in training efficiency and reduced dependency on human-annotated data, presenting a more scalable and effective path towards creative SLMs. Our automated evaluation methods also exhibit strong alignment with human judgments. Our code and data are publicly available at https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.<br>
<span id='abs_ch'>中文: 本文在RLAIF框架下提出两种AI驱动的奖励策略，以激发70亿参数小语言模型的中文问候语创作能力，其中基于原则的大语言模型作为评判者的方法在生成质量、训练效率和可扩展性上表现更优，同时降低了对人工 注数据的依赖。</span><br>
<span id='abs_en'>English: This paper introduces two AI-driven reward strategies within an RLAIF framework to enhance the creative writing of a 7B-parameter SLM for Chinese greetings, with the principle-guided LLM-as-a-Judge approach proving superior in quality, efficiency, and scalability while reducing reliance on human data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>696, <a href='https://arxiv.org/pdf/2508.21476.pdf' target='_blank'>https://arxiv.org/pdf/2508.21476.pdf</a></span>   <span><a href='https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaolong Wei, Bo Lu, Xingyu Zhang, Zhejun Zhao, Dongdong Shen, Long Xia, Dawei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21476">Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated remarkable creative writing capabilities, yet their substantial computational demands hinder widespread use. Enhancing Small Language Models (SLMs) offers a promising alternative, but current methods like Supervised Fine-Tuning (SFT) struggle with novelty, and Reinforcement Learning from Human Feedback (RLHF) is costly. This paper explores two distinct AI-driven reward strategies within a Reinforcement Learning from AI Feedback (RLAIF) framework to ignite the creative writing of a 7B-parameter SLM, specifically for generating Chinese greetings. The first strategy employs a RM trained on high-quality preference data curated by a novel multi-agent rejection sampling framework designed for creative tasks. The second, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose reward function is optimized via an adversarial training scheme with a reflection mechanism, to directly provide reward signals. Comprehensive experiments reveal that while both approaches significantly enhance creative output over baselines, the principle-guided LLM-as-a-Judge demonstrably yields superior generation quality. Furthermore, it offers notable advantages in training efficiency and reduced dependency on human-annotated data, presenting a more scalable and effective path towards creative SLMs. Our automated evaluation methods also exhibit strong alignment with human judgments. Our code and data are publicly available at https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.<br>
<span id='abs_ch'>中文: 本文在RLAIF框架下提出两种AI驱动的奖励策略，以激发70亿参数小语言模型的中文问候语创作能力，其中基于原则的大语言模型作为评判者的方法在生成质量、训练效率和可扩展性上表现更优，同时降低了对人工 注数据的依赖。</span><br>
<span id='abs_en'>English: This paper introduces two AI-driven reward strategies within an RLAIF framework to enhance the creative writing of a 7B-parameter SLM for Chinese greetings, with the principle-guided LLM-as-a-Judge approach proving superior in quality, efficiency, and scalability while reducing reliance on human data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>697, <a href='https://arxiv.org/pdf/2508.21460.pdf' target='_blank'>https://arxiv.org/pdf/2508.21460.pdf</a></span>   <span><a href='https://github.com/Cxx-0/Diff-MSIN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxi Cui, Weihai Lu, Yu Tong, Yiheng Li, Zhejun Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21460">Diffusion-based Multi-modal Synergy Interest Network for Click-through Rate Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In click-through rate prediction, click-through rate prediction is used to model users' interests. However, most of the existing CTR prediction methods are mainly based on the ID modality. As a result, they are unable to comprehensively model users' multi-modal preferences. Therefore, it is necessary to introduce multi-modal CTR prediction. Although it seems appealing to directly apply the existing multi-modal fusion methods to click-through rate prediction models, these methods (1) fail to effectively disentangle commonalities and specificities across different modalities; (2) fail to consider the synergistic effects between modalities and model the complex interactions between modalities.
  To address the above issues, this paper proposes the Diffusion-based Multi-modal Synergy Interest Network (Diff-MSIN) framework for click-through prediction. This framework introduces three innovative modules: the Multi-modal Feature Enhancement (MFE) Module Synergistic Relationship Capture (SRC) Module, and the Feature Dynamic Adaptive Fusion (FDAF) Module. The MFE Module and SRC Module extract synergistic, common, and special information among different modalities. They effectively enhances the representation of the modalities, improving the overall quality of the fusion. To encourage distinctiveness among different features, we design a Knowledge Decoupling method. Additionally, the FDAF Module focuses on capturing user preferences and reducing fusion noise. To validate the effectiveness of the Diff-MSIN framework, we conducted extensive experiments using the Rec-Tmall and three Amazon datasets. The results demonstrate that our approach yields a significant improvement of at least 1.67% compared to the baseline, highlighting its potential for enhancing multi-modal recommendation systems. Our code is available at the following link: https://github.com/Cxx-0/Diff-MSIN.<br>
<span id='abs_ch'>中文: 本文提出Diff-MSIN框架，通过创新模块增强多模态特征表示并减少融合噪声，解决了现有点击率预测方法在多模态协同建模方面的不足，实验证明其性能显著优于基线方法。</span><br>
<span id='abs_en'>English: This paper introduces the Diff-MSIN framework to address limitations in multi-modal click-through rate prediction by enhancing feature representation and reducing fusion noise through innovative modules, achieving significant performance improvements over baselines.Paperid:23,https://arxiv.org/pdf/2508.21402.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>698, <a href='https://arxiv.org/pdf/2508.21460.pdf' target='_blank'>https://arxiv.org/pdf/2508.21460.pdf</a></span>   <span><a href='https://github.com/Cxx-0/Diff-MSIN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoxi Cui, Weihai Lu, Yu Tong, Yiheng Li, Zhejun Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21460">Diffusion-based Multi-modal Synergy Interest Network for Click-through Rate Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In click-through rate prediction, click-through rate prediction is used to model users' interests. However, most of the existing CTR prediction methods are mainly based on the ID modality. As a result, they are unable to comprehensively model users' multi-modal preferences. Therefore, it is necessary to introduce multi-modal CTR prediction. Although it seems appealing to directly apply the existing multi-modal fusion methods to click-through rate prediction models, these methods (1) fail to effectively disentangle commonalities and specificities across different modalities; (2) fail to consider the synergistic effects between modalities and model the complex interactions between modalities. To address the above issues, this paper proposes the Diffusion-based Multi-modal Synergy Interest Network (Diff-MSIN) framework for click-through prediction. This framework introduces three innovative modules: the Multi-modal Feature Enhancement (MFE) Module Synergistic Relationship Capture (SRC) Module, and the Feature Dynamic Adaptive Fusion (FDAF) Module. The MFE Module and SRC Module extract synergistic, common, and special information among different modalities. They effectively enhances the representation of the modalities, improving the overall quality of the fusion. To encourage distinctiveness among different features, we design a Knowledge Decoupling method. Additionally, the FDAF Module focuses on capturing user preferences and reducing fusion noise. To validate the effectiveness of the Diff-MSIN framework, we conducted extensive experiments using the Rec-Tmall and three Amazon datasets. The results demonstrate that our approach yields a significant improvement of at least 1.67% compared to the baseline, highlighting its potential for enhancing multi-modal recommendation systems. Our code is available at the following link: https://github.com/Cxx-0/Diff-MSIN.<br>
<span id='abs_ch'>中文: 本文提出Diff-MSIN框架，通过创新模块增强多模态特征表示并减少融合噪声，解决了现有点击率预测方法在多模态协同建模方面的不足，实验证明其性能显著优于基线方法。</span><br>
<span id='abs_en'>English: This paper introduces the Diff-MSIN framework to address limitations in multi-modal click-through rate prediction by enhancing feature representation and reducing fusion noise through innovative modules, achieving significant performance improvements over baselines.Paperid:23,https://arxiv.org/pdf/2508.21402.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>699, <a href='https://arxiv.org/pdf/2508.21334.pdf' target='_blank'>https://arxiv.org/pdf/2508.21334.pdf</a></span>   <span><a href='https://github.com/theresiavr/stairway-to-fairness' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Theresia Veronika Rampisela, Maria Maistro, Tuukka Ruotsalo, Falk Scholer, Christina Lioma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21334">Stairway to Fairness: Connecting Group and Individual Fairness</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fairness in recommender systems (RSs) is commonly categorised into group fairness and individual fairness. However, there is no established scientific understanding of the relationship between the two fairness types, as prior work on both types has used different evaluation measures or evaluation objectives for each fairness type, thereby not allowing for a proper comparison of the two. As a result, it is currently not known how increasing one type of fairness may affect the other. To fill this gap, we study the relationship of group and individual fairness through a comprehensive comparison of evaluation measures that can be used for both fairness types. Our experiments with 8 runs across 3 datasets show that recommendations that are highly fair for groups can be very unfair for individuals. Our finding is novel and useful for RS practitioners aiming to improve the fairness of their systems. Our code is available at: https://github.com/theresiavr/stairway-to-fairness.<br>
<span id='abs_ch'>Chinese: 本 究揭示，在推荐系统中实现高度群体公平性可能导致严重的个体不公平，凸显了两种公平类型之间的关键权衡。</span><br>
<span id='abs_en'>English: This study reveals that achieving high group fairness in recommender systems can result in significant individual unfairness, highlighting a critical trade-off between the two fairness types.Paperid:29,https://arxiv.org/pdf/2508.21300.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>700, <a href='https://arxiv.org/pdf/2508.21334.pdf' target='_blank'>https://arxiv.org/pdf/2508.21334.pdf</a></span>   <span><a href='https://github.com/theresiavr/stairway-to-fairness' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Theresia Veronika Rampisela, Maria Maistro, Tuukka Ruotsalo, Falk Scholer, Christina Lioma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21334">Stairway to Fairness: Connecting Group and Individual Fairness</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fairness in recommender systems (RSs) is commonly categorised into group fairness and individual fairness. However, there is no established scientific understanding of the relationship between the two fairness types, as prior work on both types has used different evaluation measures or evaluation objectives for each fairness type, thereby not allowing for a proper comparison of the two. As a result, it is currently not known how increasing one type of fairness may affect the other. To fill this gap, we study the relationship of group and individual fairness through a comprehensive comparison of evaluation measures that can be used for both fairness types. Our experiments with 8 runs across 3 datasets show that recommendations that are highly fair for groups can be very unfair for individuals. Our finding is novel and useful for RS practitioners aiming to improve the fairness of their systems. Our code is available at: https://github.com/theresiavr/stairway-to-fairness.<br>
<span id='abs_ch'>Chinese: 本 究揭示，在推荐系统中实现高度群体公平性可能导致严重的个体不公平，凸显了两种公平类型之间的关键权衡。</span><br>
<span id='abs_en'>English: This study reveals that achieving high group fairness in recommender systems can result in significant individual unfairness, highlighting a critical trade-off between the two fairness types.Paperid:29,https://arxiv.org/pdf/2508.21300.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>701, <a href='https://arxiv.org/pdf/2508.21222.pdf' target='_blank'>https://arxiv.org/pdf/2508.21222.pdf</a></span>   <span><a href='https://github.com/Hzzone/VICP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhizhong Huang, Xiaoming Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21222">Generalizable Object Re-Identification via Visual In-Context Prompting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current object re-identification (ReID) methods train domain-specific models (e.g., for persons or vehicles), which lack generalization and demand costly labeled data for new categories. While self-supervised learning reduces annotation needs by learning instance-wise invariance, it struggles to capture \textit{identity-sensitive} features critical for ReID. This paper proposes Visual In-Context Prompting~(VICP), a novel framework where models trained on seen categories can directly generalize to unseen novel categories using only \textit{in-context examples} as prompts, without requiring parameter adaptation. VICP synergizes LLMs and vision foundation models~(VFM): LLMs infer semantic identity rules from few-shot positive/negative pairs through task-specific prompting, which then guides a VFM (\eg, DINO) to extract ID-discriminative features via \textit{dynamic visual prompts}. By aligning LLM-derived semantic concepts with the VFM's pre-trained prior, VICP enables generalization to novel categories, eliminating the need for dataset-specific retraining. To support evaluation, we introduce ShopID10K, a dataset of 10K object instances from e-commerce platforms, featuring multi-view images and cross-domain testing. Experiments on ShopID10K and diverse ReID benchmarks demonstrate that VICP outperforms baselines by a clear margin on unseen categories. Code is available at https://github.com/Hzzone/VICP.<br>
<span id='abs_ch'>中文: 本文提出的视觉上下文提示（VICP）框架结合大语言模型与视觉基础模型，仅需少量上下文示例即可将目 重识别泛化至未见类别， 需针对新数据集重新训练，并在实验中显著优于现有基线方法。</span><br>
<span id='abs_en'>English: The paper introduces Visual In-Context Prompting (VICP), a framework that leverages large language models and vision foundation models to generalize object re-identification to unseen categories using in-context examples, eliminating the need for dataset-specific retraining and outperforming existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>702, <a href='https://arxiv.org/pdf/2508.21222.pdf' target='_blank'>https://arxiv.org/pdf/2508.21222.pdf</a></span>   <span><a href='https://github.com/Hzzone/VICP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhizhong Huang, Xiaoming Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21222">Generalizable Object Re-Identification via Visual In-Context Prompting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current object re-identification (ReID) methods train domain-specific models (e.g., for persons or vehicles), which lack generalization and demand costly labeled data for new categories. While self-supervised learning reduces annotation needs by learning instance-wise invariance, it struggles to capture \textit{identity-sensitive} features critical for ReID. This paper proposes Visual In-Context Prompting~(VICP), a novel framework where models trained on seen categories can directly generalize to unseen novel categories using only \textit{in-context examples} as prompts, without requiring parameter adaptation. VICP synergizes LLMs and vision foundation models~(VFM): LLMs infer semantic identity rules from few-shot positive/negative pairs through task-specific prompting, which then guides a VFM (\eg, DINO) to extract ID-discriminative features via \textit{dynamic visual prompts}. By aligning LLM-derived semantic concepts with the VFM's pre-trained prior, VICP enables generalization to novel categories, eliminating the need for dataset-specific retraining. To support evaluation, we introduce ShopID10K, a dataset of 10K object instances from e-commerce platforms, featuring multi-view images and cross-domain testing. Experiments on ShopID10K and diverse ReID benchmarks demonstrate that VICP outperforms baselines by a clear margin on unseen categories. Code is available at https://github.com/Hzzone/VICP.<br>
<span id='abs_ch'>中文: 本文提出的视觉上下文提示（VICP）框架结合大语言模型与视觉基础模型，仅需少量上下文示例即可将目 重识别泛化至未见类别， 需针对新数据集重新训练，并在实验中显著优于现有基线方法。</span><br>
<span id='abs_en'>English: The paper introduces Visual In-Context Prompting (VICP), a framework that leverages large language models and vision foundation models to generalize object re-identification to unseen categories using in-context examples, eliminating the need for dataset-specific retraining and outperforming existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>703, <a href='https://arxiv.org/pdf/2508.21154.pdf' target='_blank'>https://arxiv.org/pdf/2508.21154.pdf</a></span>   <span><a href='https://github.com/shenao1995/RadGS_Reg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ao Shen, Xueming Fu, Junfeng Jiang, Qiang Zeng, Ye Tang, Zhengming Chen, Luming Nong, Feng Wang, S. Kevin Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21154">RadGS-Reg: Registering Spine CT with Biplanar X-rays via Joint 3D Radiative Gaussians Reconstruction and 3D/3D Registration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Computed Tomography (CT)/X-ray registration in image-guided navigation remains challenging because of its stringent requirements for high accuracy and real-time performance. Traditional "render and compare" methods, relying on iterative projection and comparison, suffer from spatial information loss and domain gap. 3D reconstruction from biplanar X-rays supplements spatial and shape information for 2D/3D registration, but current methods are limited by dense-view requirements and struggles with noisy X-rays. To address these limitations, we introduce RadGS-Reg, a novel framework for vertebral-level CT/X-ray registration through joint 3D Radiative Gaussians (RadGS) reconstruction and 3D/3D registration. Specifically, our biplanar X-rays vertebral RadGS reconstruction module explores learning-based RadGS reconstruction method with a Counterfactual Attention Learning (CAL) mechanism, focusing on vertebral regions in noisy X-rays. Additionally, a patient-specific pre-training strategy progressively adapts the RadGS-Reg from simulated to real data while simultaneously learning vertebral shape prior knowledge. Experiments on in-house datasets demonstrate the state-of-the-art performance for both tasks, surpassing existing methods. The code is available at: https://github.com/shenao1995/RadGS_Reg.<br>
<br>
<div id='section'>Paperid: <span id='pid'>704, <a href='https://arxiv.org/pdf/2508.21154.pdf' target='_blank'>https://arxiv.org/pdf/2508.21154.pdf</a></span>   <span><a href='https://github.com/shenao1995/RadGS_Reg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ao Shen, Xueming Fu, Junfeng Jiang, Qiang Zeng, Ye Tang, Zhengming Chen, Luming Nong, Feng Wang, S. Kevin Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21154">RadGS-Reg: Registering Spine CT with Biplanar X-rays via Joint 3D Radiative Gaussians Reconstruction and 3D/3D Registration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Computed Tomography (CT)/X-ray registration in image-guided navigation remains challenging because of its stringent requirements for high accuracy and real-time performance. Traditional "render and compare" methods, relying on iterative projection and comparison, suffer from spatial information loss and domain gap. 3D reconstruction from biplanar X-rays supplements spatial and shape information for 2D/3D registration, but current methods are limited by dense-view requirements and struggles with noisy X-rays. To address these limitations, we introduce RadGS-Reg, a novel framework for vertebral-level CT/X-ray registration through joint 3D Radiative Gaussians (RadGS) reconstruction and 3D/3D registration. Specifically, our biplanar X-rays vertebral RadGS reconstruction module explores learning-based RadGS reconstruction method with a Counterfactual Attention Learning (CAL) mechanism, focusing on vertebral regions in noisy X-rays. Additionally, a patient-specific pre-training strategy progressively adapts the RadGS-Reg from simulated to real data while simultaneously learning vertebral shape prior knowledge. Experiments on in-house datasets demonstrate the state-of-the-art performance for both tasks, surpassing existing methods. The code is available at: https://github.com/shenao1995/RadGS_Reg.<br>
<br>
<div id='section'>Paperid: <span id='pid'>705, <a href='https://arxiv.org/pdf/2508.21107.pdf' target='_blank'>https://arxiv.org/pdf/2508.21107.pdf</a></span>   <span><a href='https://github.com/dgjun32/UTRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongjun Lee, Changho Hwang, Kimin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21107">Learning to Generate Unit Test via Adversarial Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Unit testing is a core practice in programming, enabling systematic evaluation of programs produced by human developers or large language models (LLMs). Given the challenges in writing comprehensive unit tests, LLMs have been employed to automate test generation, yet methods for training LLMs to produce high-quality tests remain underexplored. In this work, we propose UTRL, a novel reinforcement learning framework that trains an LLM to generate high-quality unit tests given a programming instruction. Our key idea is to iteratively train two LLMs, the unit test generator and the code generator, in an adversarial manner via reinforcement learning. The unit test generator is trained to maximize a discrimination reward, which reflects its ability to produce tests that expose faults in the code generator's solutions, and the code generator is trained to maximize a code reward, which reflects its ability to produce solutions that pass the unit tests generated by the test generator. In our experiments, we demonstrate that unit tests generated by Qwen3-4B trained via UTRL show higher quality compared to unit tests generated by the same model trained via supervised fine-tuning on human-written ground-truth unit tests, yielding code evaluations that more closely align with those induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL outperforms frontier models such as GPT-4.1 in generating high-quality unit tests, highlighting the effectiveness of UTRL in training LLMs for this task.<br>
<span id='abs_ch'>中文: 本文提出UTRL强化学 框架，通过对抗性训练测试生成器和代 生成器来分别最大化判别奖励和代 奖励，使大语言模型能够生成高质量单元测试，其表现优于监督微调和GPT-4.1等前沿模型。</span><br>
<span id='abs_en'>English: This paper introduces UTRL, a reinforcement learning framework that trains large language models to generate high-quality unit tests by adversarially training test and code generators to maximize discrimination and code rewards respectively, outperforming both supervised fine-tuning and frontier models like GPT-4.1.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>706, <a href='https://arxiv.org/pdf/2508.21107.pdf' target='_blank'>https://arxiv.org/pdf/2508.21107.pdf</a></span>   <span><a href='https://github.com/dgjun32/UTRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongjun Lee, Changho Hwang, Kimin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21107">Learning to Generate Unit Test via Adversarial Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Unit testing is a core practice in programming, enabling systematic evaluation of programs produced by human developers or large language models (LLMs). Given the challenges in writing comprehensive unit tests, LLMs have been employed to automate test generation, yet methods for training LLMs to produce high-quality tests remain underexplored. In this work, we propose UTRL, a novel reinforcement learning framework that trains an LLM to generate high-quality unit tests given a programming instruction. Our key idea is to iteratively train two LLMs, the unit test generator and the code generator, in an adversarial manner via reinforcement learning. The unit test generator is trained to maximize a discrimination reward, which reflects its ability to produce tests that expose faults in the code generator's solutions, and the code generator is trained to maximize a code reward, which reflects its ability to produce solutions that pass the unit tests generated by the test generator. In our experiments, we demonstrate that unit tests generated by Qwen3-4B trained via UTRL show higher quality compared to unit tests generated by the same model trained via supervised fine-tuning on human-written ground-truth unit tests, yielding code evaluations that more closely align with those induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL outperforms frontier models such as GPT-4.1 in generating high-quality unit tests, highlighting the effectiveness of UTRL in training LLMs for this task.<br>
<span id='abs_ch'>中文: 本文提出UTRL强化学 框架，通过对抗性训练测试生成器和代 生成器来分别最大化判别奖励和代 奖励，使大语言模型能够生成高质量单元测试，其表现优于监督微调和GPT-4.1等前沿模型。</span><br>
<span id='abs_en'>English: This paper introduces UTRL, a reinforcement learning framework that trains large language models to generate high-quality unit tests by adversarially training test and code generators to maximize discrimination and code rewards respectively, outperforming both supervised fine-tuning and frontier models like GPT-4.1.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>707, <a href='https://arxiv.org/pdf/2508.21107.pdf' target='_blank'>https://arxiv.org/pdf/2508.21107.pdf</a></span>   <span><a href='https://github.com/dgjun32/UTRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongjun Lee, Changho Hwang, Kimin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21107">Learning to Generate Unit Test via Adversarial Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Unit testing is a core practice in programming, enabling systematic evaluation of programs produced by human developers or large language models (LLMs). Given the challenges in writing comprehensive unit tests, LLMs have been employed to automate test generation, yet methods for training LLMs to produce high-quality tests remain underexplored. In this work, we propose UTRL, a novel reinforcement learning framework that trains an LLM to generate high-quality unit tests given a programming instruction. Our key idea is to iteratively train two LLMs, the unit test generator and the code generator, in an adversarial manner via reinforcement learning. The unit test generator is trained to maximize a discrimination reward, which reflects its ability to produce tests that expose faults in the code generator's solutions, and the code generator is trained to maximize a code reward, which reflects its ability to produce solutions that pass the unit tests generated by the test generator. In our experiments, we demonstrate that unit tests generated by Qwen3-4B trained via UTRL show higher quality compared to unit tests generated by the same model trained via supervised fine-tuning on human-written ground-truth unit tests, yielding code evaluations that more closely align with those induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL outperforms frontier models such as GPT-4.1 in generating high-quality unit tests, highlighting the effectiveness of UTRL in training LLMs for this task.<br>
<span id='abs_ch'>中文: 本文提出UTRL强化学 框架，通过对抗性训练测试生成器和代 生成器来分别最大化判别奖励和代 奖励，使大语言模型能够生成高质量单元测试，其表现优于监督微调和GPT-4.1等前沿模型。</span><br>
<span id='abs_en'>English: This paper introduces UTRL, a reinforcement learning framework that trains large language models to generate high-quality unit tests by adversarially training test and code generators to maximize discrimination and code rewards respectively, outperforming both supervised fine-tuning and frontier models like GPT-4.1.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>708, <a href='https://arxiv.org/pdf/2508.21048.pdf' target='_blank'>https://arxiv.org/pdf/2508.21048.pdf</a></span>   <span><a href='https://github.com/EricTan7/Veritas' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Tan, Jun Lan, Zichang Tan, Ajian Liu, Chuanbiao Song, Senyuan Shi, Huijia Zhu, Weiqiang Wang, Jun Wan, Zhen Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21048">Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Deepfake detection remains a formidable challenge due to the complex and evolving nature of fake content in real-world scenarios. However, existing academic benchmarks suffer from severe discrepancies from industrial practice, typically featuring homogeneous training sources and low-quality testing images, which hinder the practical deployments of current detectors. To mitigate this gap, we introduce HydraFake, a dataset that simulates real-world challenges with hierarchical generalization testing. Specifically, HydraFake involves diversified deepfake techniques and in-the-wild forgeries, along with rigorous training and evaluation protocol, covering unseen model architectures, emerging forgery techniques and novel data domains. Building on this resource, we propose Veritas, a multi-modal large language model (MLLM) based deepfake detector. Different from vanilla chain-of-thought (CoT), we introduce pattern-aware reasoning that involves critical reasoning patterns such as "planning" and "self-reflection" to emulate human forensic process. We further propose a two-stage training pipeline to seamlessly internalize such deepfake reasoning capacities into current MLLMs. Experiments on HydraFake dataset reveal that although previous detectors show great generalization on cross-model scenarios, they fall short on unseen forgeries and data domains. Our Veritas achieves significant gains across different OOD scenarios, and is capable of delivering transparent and faithful detection outputs.<br>
<span id='abs_ch'>中文摘要：HydraFake数据集通过分层泛化测试解决了现实世界深度伪 检测的挑战，而Veritas检测器基于多模态框架采用模式感知推理，在跨域场景中实现卓越性能并提供透明可信的检测结果。</span><br>
<span id='abs_en'>English Summary: The HydraFake dataset addresses real-world deepfake detection challenges through hierarchical generalization testing, while the Veritas detector leverages pattern-aware reasoning within a multi-modal framework to achieve superior cross-domain performance with transparent results.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>709, <a href='https://arxiv.org/pdf/2508.21016.pdf' target='_blank'>https://arxiv.org/pdf/2508.21016.pdf</a></span>   <span><a href='https://github.com/jinluo12345/Reinforcement-learning-guidance' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luozhijie Jin, Zijie Qiu, Jie Liu, Zijie Diao, Lifeng Qiao, Ning Ding, Alex Lamb, Xipeng Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21016">Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Denoising-based generative models, particularly diffusion and flow matching algorithms, have achieved remarkable success. However, aligning their output distributions with complex downstream objectives, such as human preferences, compositional accuracy, or data compressibility, remains challenging. While reinforcement learning (RL) fine-tuning methods, inspired by advances in RL from human feedback (RLHF) for large language models, have been adapted to these generative frameworks, current RL approaches are suboptimal for diffusion models and offer limited flexibility in controlling alignment strength after fine-tuning. In this work, we reinterpret RL fine-tuning for diffusion models through the lens of stochastic differential equations and implicit reward conditioning. We introduce Reinforcement Learning Guidance (RLG), an inference-time method that adapts Classifier-Free Guidance (CFG) by combining the outputs of the base and RL fine-tuned models via a geometric average. Our theoretical analysis shows that RLG's guidance scale is mathematically equivalent to adjusting the KL-regularization coefficient in standard RL objectives, enabling dynamic control over the alignment-quality trade-off without further training. Extensive experiments demonstrate that RLG consistently improves the performance of RL fine-tuned models across various architectures, RL algorithms, and downstream tasks, including human preferences, compositional control, compressibility, and text rendering. Furthermore, RLG supports both interpolation and extrapolation, thereby offering unprecedented flexibility in controlling generative alignment. Our approach provides a practical and theoretically sound solution for enhancing and controlling diffusion model alignment at inference. The source code for RLG is publicly available at the Github: https://github.com/jinluo12345/Reinforcement-learning-guidance.<br>
<span id='abs_ch'>中文: 本文提出强化学 引导（RLG）方法，通过理论分析和广泛实验证明，该推理时技术能动态调控生成质量与对齐目 的平衡， 需重新训练即可提升扩散模型在下游任务中的对齐性能。</span><br>
<span id='abs_en'>English: This paper introduces Reinforcement Learning Guidance (RLG), an inference-time method that enhances diffusion model alignment with downstream objectives by dynamically controlling the trade-off between quality and alignment without additional training, supported by theoretical analysis and extensive experiments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>710, <a href='https://arxiv.org/pdf/2508.20991.pdf' target='_blank'>https://arxiv.org/pdf/2508.20991.pdf</a></span>   <span><a href='https://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Patryk BÄdkowski, Jan DubiÅski, Filip Szatkowski, Kamil Deja, PrzemysÅaw Rokita, Tomasz TrzciÅski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20991">ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Simulating detector responses is a crucial part of understanding the inner workings of particle collisions in the Large Hadron Collider at CERN. Such simulations are currently performed with statistical Monte Carlo methods, which are computationally expensive and put a significant strain on CERN's computational grid. Therefore, recent proposals advocate for generative machine learning methods to enable more efficient simulations. However, the distribution of the data varies significantly across the simulations, which is hard to capture with out-of-the-box methods. In this study, we present ExpertSim - a deep learning simulation approach tailored for the Zero Degree Calorimeter in the ALICE experiment. Our method utilizes a Mixture-of-Generative-Experts architecture, where each expert specializes in simulating a different subset of the data. This allows for a more precise and efficient generation process, as each expert focuses on a specific aspect of the calorimeter response. ExpertSim not only improves accuracy, but also provides a significant speedup compared to the traditional Monte-Carlo methods, offering a promising solution for high-efficiency detector simulations in particle physics experiments at CERN. We make the code available at https://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts.<br>
<span id='abs_ch'>中文：ExpertSim采用一种混合生成专家架构的深度学 模拟方法，专门用于提升CERN的ALICE实验中探测器响应的模拟精度与效率，显著优于 统蒙特卡洛方法。English: ExpertSim introduces a specialized deep learning approach using a Mixture-of-Generative-Experts architecture to enhance the accuracy and efficiency of simulating detector responses in CERN's ALICE experiment, outperforming traditional Monte Carlo methods.Paperid:52,https://arxiv.org/pdf/2508.20987.pdfGitHub</span><br>
<span id='abs_en'>English: ExpertSim introduces a specialized deep learning approach using a Mixture-of-Generative-Experts architecture to enhance the accuracy and efficiency of simulating detector responses in CERN's ALICE experiment, outperforming traditional Monte Carlo methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>711, <a href='https://arxiv.org/pdf/2508.20810.pdf' target='_blank'>https://arxiv.org/pdf/2508.20810.pdf</a></span>   <span><a href='https://github.com/jessicalundin/graph_testing_harness' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jessica Lundin, Guillaume Chabot-Couture
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20810">A Graph-Based Test-Harness for LLM Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present a first known prototype of a dynamic, systematic benchmark of medical guidelines for 400+ questions, with 3.3+ trillion possible combinations, covering 100\% of guideline relationships. We transformed the WHO IMCI handbook into a directed graph with 200+ nodes (conditions, symptoms, treatments, follow-ups, severities) and 300+ edges, then used graph traversal to generate questions that incorporated age-specific scenarios and contextual distractors to ensure clinical relevance. Our graph-based approach enables systematic evaluation across clinical tasks (45-67\% accuracy), and we find models excel at symptom recognition but struggle with triaging severity, treatment protocols and follow-up care, demonstrating how customized benchmarks can identify specific capability gaps that general-domain evaluations miss. Beyond evaluation, this dynamic MCQA methodology enhances LLM post-training (supervised finetuning, GRPO, DPO), where correct answers provide high-reward samples without expensive human annotation. The graph-based approach successfully addresses the coverage limitations of manually curated benchmarks. This methodology is a step toward scalable, contamination-resistant solution for creating comprehensive benchmarks that can be dynamically generated, including when the guidelines are updated. Code and datasets are available at https://github.com/jessicalundin/graph_testing_harness<br>
<br>
<div id='section'>Paperid: <span id='pid'>712, <a href='https://arxiv.org/pdf/2508.20783.pdf' target='_blank'>https://arxiv.org/pdf/2508.20783.pdf</a></span>   <span><a href='https://github.com/otmive/diffusion_classifier_clip' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Beth Pearson, Bilal Boulbarss, Michael Wray, Martha Lewis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20783">Evaluating Compositional Generalisation in VLMs and Diffusion Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>A fundamental aspect of the semantics of natural language is that novel meanings can be formed from the composition of previously known parts. Vision-language models (VLMs) have made significant progress in recent years, however, there is evidence that they are unable to perform this kind of composition. For example, given an image of a red cube and a blue cylinder, a VLM such as CLIP is likely to incorrectly label the image as a red cylinder or a blue cube, indicating it represents the image as a `bag-of-words' and fails to capture compositional semantics. Diffusion models have recently gained significant attention for their impressive generative abilities, and zero-shot classifiers based on diffusion models have been shown to perform competitively with CLIP in certain compositional tasks. In this work we explore whether the generative Diffusion Classifier has improved compositional generalisation abilities compared to discriminative models. We assess three models -- Diffusion Classifier, CLIP, and ViLT -- on their ability to bind objects with attributes and relations in both zero-shot learning (ZSL) and generalised zero-shot learning (GZSL) settings. Our results show that the Diffusion Classifier and ViLT perform well at concept binding tasks, but that all models struggle significantly with the relational GZSL task, underscoring the broader challenges VLMs face with relational reasoning. Analysis of CLIP embeddings suggests that the difficulty may stem from overly similar representations of relational concepts such as left and right. Code and dataset are available at: https://github.com/otmive/diffusion_classifier_clip<br>
<span id='abs_ch'>Chinese: 视觉语言模型如CLIP在组合语义方面存在困难，常 法正确关联属性和对象，而扩散分类器在概念绑定上表现更佳，但在关系推理任务中仍面临挑战。</span><br>
<span id='abs_en'>English: Vision-language models like CLIP struggle with compositional semantics, often failing to correctly bind attributes and objects, while the Diffusion Classifier shows improved performance in concept binding but still faces challenges with relational reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>713, <a href='https://arxiv.org/pdf/2508.20758.pdf' target='_blank'>https://arxiv.org/pdf/2508.20758.pdf</a></span>   <span><a href='https://github.com/JiawLin/SeqVLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawen Lin, Shiran Bian, Yihang Zhu, Wenbin Tan, Yachao Zhang, Yuan Xie, Yanyun Qu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20758">SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for Zero-Shot 3D Visual Grounding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>3D Visual Grounding (3DVG) aims to localize objects in 3D scenes using natural language descriptions. Although supervised methods achieve higher accuracy in constrained settings, zero-shot 3DVG holds greater promise for real-world applications since eliminating scene-specific training requirements. However, existing zero-shot methods face challenges of spatial-limited reasoning due to reliance on single-view localization, and contextual omissions or detail degradation. To address these issues, we propose SeqVLM, a novel zero-shot 3DVG framework that leverages multi-view real-world scene images with spatial information for target object reasoning. Specifically, SeqVLM first generates 3D instance proposals via a 3D semantic segmentation network and refines them through semantic filtering, retaining only semantic-relevant candidates. A proposal-guided multi-view projection strategy then projects these candidate proposals onto real scene image sequences, preserving spatial relationships and contextual details in the conversion process of 3D point cloud to images. Furthermore, to mitigate VLM computational overload, we implement a dynamic scheduling mechanism that iteratively processes sequances-query prompts, leveraging VLM's cross-modal reasoning capabilities to identify textually specified objects. Experiments on the ScanRefer and Nr3D benchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scores of 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%, respectively, which advance 3DVG toward greater generalization and real-world applicability. The code is available at https://github.com/JiawLin/SeqVLM.<br>
<span id='abs_ch'>中文: SeqVLM是一种新颖的零 本三维视觉定位框架，通过多视角空间信息和动态调度机制克服单视图局限，在基准测试中实现最优性能，推动了实际应用的发展。</span><br>
<span id='abs_en'>English: SeqVLM is a novel zero-shot 3D visual grounding framework that leverages multi-view images with spatial information and dynamic scheduling to overcome single-view limitations, achieving state-of-the-art performance on benchmarks and advancing real-world applicability.Paperid:64,https://arxiv.org/pdf/2508.20757.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>714, <a href='https://arxiv.org/pdf/2508.20754.pdf' target='_blank'>https://arxiv.org/pdf/2508.20754.pdf</a></span>   <span><a href='https://github.com/YuhsiHu/C3-GS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxi Hu, Jun Zhang, Kuangyi Chen, Zhe Zhang, Friedrich Fraundorfer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20754">${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Generalizable Gaussian Splatting aims to synthesize novel views for unseen scenes without per-scene optimization. In particular, recent advancements utilize feed-forward networks to predict per-pixel Gaussian parameters, enabling high-quality synthesis from sparse input views. However, existing approaches fall short in encoding discriminative, multi-view consistent features for Gaussian predictions, which struggle to construct accurate geometry with sparse views. To address this, we propose $\mathbf{C}^{3}$-GS, a framework that enhances feature learning by incorporating context-aware, cross-dimension, and cross-scale constraints. Our architecture integrates three lightweight modules into a unified rendering pipeline, improving feature fusion and enabling photorealistic synthesis without requiring additional supervision. Extensive experiments on benchmark datasets validate that $\mathbf{C}^{3}$-GS achieves state-of-the-art rendering quality and generalization ability. Code is available at: https://github.com/YuhsiHu/C3-GS.<br>
<span id='abs_ch'>中文: C³-GS框架通过引入上下文感知、跨维度和跨尺度的约束来增强高斯泼溅的特征学 能力， 需逐场景优化即可实现照片级真实感的新视角合成，在渲染质量和泛化能力上达到领先水平。</span><br>
<span id='abs_en'>English: The proposed C³-GS framework enhances Gaussian Splatting by incorporating context-aware, cross-dimension, and cross-scale constraints to improve feature learning and enable photorealistic novel view synthesis without per-scene optimization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>715, <a href='https://arxiv.org/pdf/2508.20691.pdf' target='_blank'>https://arxiv.org/pdf/2508.20691.pdf</a></span>   <span><a href='https://github.com/apple/ml-mobileclip-dr' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/apple/ml-mobileclip' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fartash Faghri, Pavan Kumar Anasosalu Vasu, Cem Koc, Vaishaal Shankar, Alexander Toshev, Oncel Tuzel, Hadi Pouransari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20691">MobileCLIP2: Improving Multi-Modal Reinforced Training</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Foundation image-text models such as CLIP with zero-shot capabilities enable a wide array of applications. MobileCLIP is a recent family of image-text models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot accuracy. The main ingredients in MobileCLIP were its low-latency and light architectures and a novel multi-modal reinforced training that made knowledge distillation from multiple caption-generators and CLIP teachers efficient, scalable, and reproducible. In this paper, we improve the multi-modal reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles trained on the DFN dataset, 2) improved captioner teachers trained on the DFN dataset and fine-tuned on a diverse selection of high-quality image-caption datasets. We discover new insights through ablations such as the importance of temperature tuning in contrastive knowledge distillation, the effectiveness of caption-generator fine-tuning for caption diversity, and the additive improvement from combining synthetic captions generated by multiple models. We train a new family of models called MobileCLIP2 and achieve state-of-the-art ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe 2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\times$ smaller and improves on DFN ViT-L/14 at 2.5$\times$ lower latency. We release our pretrained models (https://github.com/apple/ml-mobileclip) and the data generation code (https://github.com/apple/ml-mobileclip-dr). The data generation code makes it easy to create new reinforced datasets with arbitrary teachers using distributed scalable processing.<br>
<br>
<div id='section'>Paperid: <span id='pid'>716, <a href='https://arxiv.org/pdf/2508.20577.pdf' target='_blank'>https://arxiv.org/pdf/2508.20577.pdf</a></span>   <span><a href='https://github.com/NUS-HPC-AI-Lab/MERIT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Luo, Zangwei Zheng, Ziheng Qin, Zirui Zhu, Yong Liu, Yang You
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20577">MERIT: Maximum-normalized Element-wise Ratio for Language Model Large-batch Training</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large-batch training has become a cornerstone in accelerating the training of deep neural networks, yet it poses challenges in optimization and generalization. Existing optimizers like AdamW present performance degradation during language models' large-batch training, due to the information bottleneck in attention layers caused by the sharp increase of max attention logit. While the LAMB optimizer partially addresses this issue, some attention layers still face this issue. The reason is that $l_2$-norm-based trust ratios in LAMB are less effective in directly influencing the max value of query/key weights. Furthermore, the weight-wise trust ratio in LAMB is error-prone as it overlooks relationships of weight values within rows or columns. Building on these observations, we propose a novel optimizer, MERIT, which leverages the max-norm to calculate the trust ratio to constrain the max attention logit more effectively. Moreover, we further construct element-wise trust ratios to provide more robust update scaling by focusing on local weight structures. Extensive experiments of large-batch training across various sizes of GPT-2 models demonstrate the superior performance of MERIT. Notably, during the training of GPT-2 Medium, MERIT enables a 6k batch size without any performance degradation compared to the standard batch size (480) with 48B training tokens. This work highlights the importance of considering the max attention logit and finer-granularity trust ratio in large-batch training. It successfully improves the training stability and paves the way for larger batch usage, enabling faster development and iteration of large language models. Code is available at https://github.com/NUS-HPC-AI-Lab/MERIT.<br>
<span id='abs_ch'>中文: MERIT优化器通过采用最大范数和逐元 信任比解决大批次训练中的注意力对数瓶颈问题，有效提升训练稳定性，在保持性能的同时实现更大批次的训练 速。</span><br>
<span id='abs_en'>English: The MERIT optimizer addresses large-batch training challenges in language models by using max-norm and element-wise trust ratios to effectively control attention logits and enhance training stability, achieving superior performance without degradation at significantly larger batch sizes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>717, <a href='https://arxiv.org/pdf/2508.20557.pdf' target='_blank'>https://arxiv.org/pdf/2508.20557.pdf</a></span>   <span><a href='https://github.com/jiahaoxiao1228/AdaFD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Xiao, Jiangming Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20557">Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The widespread success of pre-trained language models has established a new training paradigm, where a global PLM is fine-tuned using task-specific data from local clients. The local data are highly different from each other and can not capture the global distribution of the whole data in real world. To address the challenges of non-IID data in real environments, privacy-preserving federated distillation has been proposed and highly investigated. However, previous experimental non-IID scenarios are primarily identified with the label (output) diversity, without considering the diversity of language domains (input) that is crucial in natural language processing. In this paper, we introduce a comprehensive set of multi-domain non-IID scenarios and propose a unified benchmarking framework that includes diverse data. The benchmark can be used to evaluate the federated learning framework in a real environment. To this end, we propose an Adaptive Federated Distillation (AdaFD) framework designed to address multi-domain non-IID challenges in both homogeneous and heterogeneous settings. Experimental results demonstrate that our models capture the diversity of local clients and achieve better performance compared to the existing works. The code for this paper is available at: https://github.com/jiahaoxiao1228/AdaFD.<br>
<span id='abs_ch'>中文摘要：针对联邦学 中非独立同分布数据的挑战，本文提出了自适应联邦蒸馏（AdaFD）框架，通过处理 签和语言领域的双重多 性，在异构环境下实现了优于现有方法的性能表现。English Summary: Pre-trained language models face challenges from non-IID data in federated learning, leading to the development of Adaptive Federated Distillation (AdaFD) that addresses both label and language domain diversity for improved performance.Paperid:77,https://arxiv.org/pdf/2508.20546.pdfGitHub</span><br>
<span id='abs_en'>English Summary: Pre-trained language models face challenges from non-IID data in federated learning, leading to the development of Adaptive Federated Distillation (AdaFD) that addresses both label and language domain diversity for improved performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>718, <a href='https://arxiv.org/pdf/2508.20546.pdf' target='_blank'>https://arxiv.org/pdf/2508.20546.pdf</a></span>   <span><a href='https://github.com/idiap/mm-hsd' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Berta CÃ©spedes-Sarrias, Carlos Collado-Capell, Pablo Rodenas-Ruiz, Olena Hrynenko, Andrea Cavallaro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20546">MM-HSD: Multi-Modal Hate Speech Detection in Videos</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While hate speech detection (HSD) has been extensively studied in text, existing multi-modal approaches remain limited, particularly in videos. As modalities are not always individually informative, simple fusion methods fail to fully capture inter-modal dependencies. Moreover, previous work often omits relevant modalities such as on-screen text and audio, which may contain subtle hateful content and thus provide essential cues, both individually and in combination with others. In this paper, we present MM-HSD, a multi-modal model for HSD in videos that integrates video frames, audio, and text derived from speech transcripts and from frames (i.e.~on-screen text) together with features extracted by Cross-Modal Attention (CMA). We are the first to use CMA as an early feature extractor for HSD in videos, to systematically compare query/key configurations, and to evaluate the interactions between different modalities in the CMA block. Our approach leads to improved performance when on-screen text is used as a query and the rest of the modalities serve as a key. Experiments on the HateMM dataset show that MM-HSD outperforms state-of-the-art methods on M-F1 score (0.874), using concatenation of transcript, audio, video, on-screen text, and CMA for feature extraction on raw embeddings of the modalities. The code is available at https://github.com/idiap/mm-hsd<br>
<span id='abs_ch'>中文摘要：本文提出MM-HSD模型，通过交叉模态注意力整合视频帧、音频和文本，首次将屏幕文本作为查询项与其他模态键值配合，在视频仇恨言论检测中实现了优于现有方法的性能。</span><br>
<span id='abs_en'>English Summary: The paper introduces MM-HSD, a novel multi-modal model for hate speech detection in videos that integrates video frames, audio, and text using Cross-Modal Attention, achieving state-of-the-art performance by effectively leveraging on-screen text as a query with other modalities as keys.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>719, <a href='https://arxiv.org/pdf/2508.20511.pdf' target='_blank'>https://arxiv.org/pdf/2508.20511.pdf</a></span>   <span><a href='https://github.com/ctaguchi/LSLB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chihiro Taguchi, Seng Mai, Keita Kurabe, Yusuke Sakai, Georgina Agyei, Soudabeh Eslami, David Chiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20511">Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multilingual machine translation (MT) benchmarks play a central role in evaluating the capabilities of modern MT systems. Among them, the FLORES+ benchmark is widely used, offering English-to-many translation data for over 200 languages, curated with strict quality control protocols. However, we study data in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani) and uncover critical shortcomings in the benchmark's suitability for truly multilingual evaluation. Human assessments reveal that many translations fall below the claimed 90% quality standard, and the annotators report that source sentences are often too domain-specific and culturally biased toward the English-speaking world. We further demonstrate that simple heuristics, such as copying named entities, can yield non-trivial BLEU scores, suggesting vulnerabilities in the evaluation protocol. Notably, we show that MT models trained on high-quality, naturalistic data perform poorly on FLORES+ while achieving significant gains on our domain-relevant evaluation set. Based on these findings, we advocate for multilingual MT benchmarks that use domain-general and culturally neutral source texts rely less on named entities, in order to better reflect real-world translation challenges.<br>
<span id='abs_ch'>中文：FLORES+基准在多语言评估中存在严重缺陷，包括低质量翻译、文化偏见和可被利用的评估漏洞， 此需要建立领域通用且文化中立的评估基准。English: The FLORES+ benchmark is critically flawed for multilingual evaluation due to low-quality translations, cultural bias, and exploitable evaluation loopholes, necessitating domain-general and culturally neutral benchmarks.Paperid:80,https://arxiv.org/pdf/2508.20476.pdfGitHub</span><br>
<span id='abs_en'>English: The FLORES+ benchmark is critically flawed for multilingual evaluation due to low-quality translations, cultural bias, and exploitable evaluation loopholes, necessitating domain-general and culturally neutral benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>720, <a href='https://arxiv.org/pdf/2508.20373.pdf' target='_blank'>https://arxiv.org/pdf/2508.20373.pdf</a></span>   <span><a href='https://github.com/Graph-Reasoner/Graph-R1,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuyao Wang, Bowen Liu, Jianheng Tang, Nuo Chen, Yuhan Li, Qifan Zhang, Jia Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20373">Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reasoning Large Language Models (RLLMs) have recently achieved remarkable progress on complex reasoning tasks, largely enabled by their long chain-of-thought (Long CoT) capabilities. However, developing these Long CoT behaviors relies heavily on post-training with high-quality datasets, which are typically costly and human-curated (e.g., mathematics and code), leaving scalable alternatives unexplored. In this work, we introduce NP-hard (NPH) graph problems as a novel synthetic training corpus, as they inherently require deep reasoning, extensive exploration, and reflective strategies, which are core characteristics of Long CoT reasoning. Building on this insight, we develop a two-stage post-training framework: (i) Long CoT Supervised Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a fine-grained reward design, which sharpens reasoning efficiency. Our flagship model, Graph-R1-7B, demonstrates strong generalization across mathematics, coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both accuracy and reasoning efficiency. These results position NPH graph problems as an effective and scalable resource for advancing Long CoT reasoning in LLMs, opening a new frontier for LLM post-training. Our implementation is available at https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted in our Hugging Face collection HKUST-DSAIL/Graph-R1.<br>
<span id='abs_ch'>中文: 推理大语言模型通过采用NP难图问题作为训练语料，结合两阶段后训练框架，显著提升了在数学、编程等多领域的推理深度与效率。</span><br>
<span id='abs_en'>English: Reasoning Large Language Models (RLLMs) enhance complex reasoning through a two-stage post-training framework using NP-hard graph problems, significantly improving accuracy and efficiency across multiple domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>721, <a href='https://arxiv.org/pdf/2508.20282.pdf' target='_blank'>https://arxiv.org/pdf/2508.20282.pdf</a></span>   <span><a href='https://github.com/umass-aisec/wra' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyejun Jeong, Mohammadreza Teymoorianfard, Abhinav Kumar, Amir Houmansadr, Eugene Bagdasarian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20282">Network-Level Prompt and Trait Leakage in Local Research Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We show that Web and Research Agents (WRAs) -- language model-based systems that investigate complex topics on the Internet -- are vulnerable to inference attacks by passive network adversaries such as ISPs. These agents could be deployed locally by organizations and individuals for privacy, legal, or financial purposes. Unlike sporadic web browsing by humans, WRAs visit $70{-}140$ domains with distinguishable timing correlations, enabling unique fingerprinting attacks. Specifically, we demonstrate a novel prompt and user trait leakage attack against WRAs that only leverages their network-level metadata (i.e., visited IP addresses and their timings). We start by building a new dataset of WRA traces based on user search queries and queries generated by synthetic personas. We define a behavioral metric (called OBELS) to comprehensively assess similarity between original and inferred prompts, showing that our attack recovers over 73% of the functional and domain knowledge of user prompts. Extending to a multi-session setting, we recover up to 19 of 32 latent traits with high accuracy. Our attack remains effective under partial observability and noisy conditions. Finally, we discuss mitigation strategies that constrain domain diversity or obfuscate traces, showing negligible utility impact while reducing attack effectiveness by an average of 29%.<br>
<span id='abs_ch'>中文: 网络与 究代理（WRA）易受网络层面的推理攻击，通过分析其独特的浏览模式可泄露用户提示和特征，而提出的缓解策略能在不影响实用性的情况下平均降低29%的攻击效果。</span><br>
<span id='abs_en'>English: Web and Research Agents (WRAs) are susceptible to network-level inference attacks that can leak user prompts and traits by analyzing their distinctive browsing patterns, with proposed mitigation strategies reducing attack effectiveness by 29% without significant utility loss.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>722, <a href='https://arxiv.org/pdf/2508.20181.pdf' target='_blank'>https://arxiv.org/pdf/2508.20181.pdf</a></span>   <span><a href='https://github.com/aimagelab/CHAIR-DPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alberto Compagnoni, Davide Caffagni, Nicholas Moratelli, Lorenzo Baraldi, Marcella Cornia, Rita Cucchiara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20181">Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal Large Language Models (MLLMs) emerge as a unified interface to address a multitude of tasks, ranging from NLP to computer vision. Despite showcasing state-of-the-art results in many benchmarks, a long-standing issue is the tendency of MLLMs to hallucinate, that is to generate answers to the user's query that are not reflected in the visual input. In this paper, we address the problem of hallucinations as an alignment problem, seeking to steer the MLLM so that it prefers generating content without hallucinations. In contrast to recent approaches that require complicated pipelines to build synthetic preference data for alignment training, often relying on proprietary models, we capitalize on the well-known CHAIR metric, originally proposed to gauge the degree of hallucinations in image captioning. Given a pair of generated answers, we leverage CHAIR to distinguish winner and loser options (i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf MLLMs via Direct Preference Optimization (DPO). The resulting method, which we refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated answers on several hallucination benchmarks, demonstrating the effectiveness of fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models are publicly available at https://github.com/aimagelab/CHAIR-DPO.<br>
<span id='abs_ch'>中文摘要：CHAIR-DPO方法通过CHAIR指 区分非幻觉与幻觉 本，并利用直接偏好优化微调多模态大语言模型，在多个基准测试中显著减少了幻觉答案的生成。</span><br>
<span id='abs_en'>English Summary: CHAIR-DPO addresses hallucinations in Multimodal Large Language Models by using the CHAIR metric to identify non-hallucinated responses and fine-tuning models with Direct Preference Optimization, effectively reducing errors across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>723, <a href='https://arxiv.org/pdf/2508.20135.pdf' target='_blank'>https://arxiv.org/pdf/2508.20135.pdf</a></span>   <span><a href='https://github.com/andrewyarovoi/MD-FRNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Yarovoi, Christopher R. Valenta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20135">Data-Efficient Point Cloud Semantic Segmentation Pipeline for Unimproved Roads</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this case study, we present a data-efficient point cloud segmentation pipeline and training framework for robust segmentation of unimproved roads and seven other classes. Our method employs a two-stage training framework: first, a projection-based convolutional neural network is pre-trained on a mixture of public urban datasets and a small, curated in-domain dataset; then, a lightweight prediction head is fine-tuned exclusively on in-domain data. Along the way, we explore the application of Point Prompt Training to batch normalization layers and the effects of Manifold Mixup as a regularizer within our pipeline. We also explore the effects of incorporating histogram-normalized ambients to further boost performance. Using only 50 labeled point clouds from our target domain, we show that our proposed training approach improves mean Intersection-over-Union from 33.5% to 51.8% and the overall accuracy from 85.5% to 90.8%, when compared to naive training on the in-domain data. Crucially, our results demonstrate that pre-training across multiple datasets is key to improving generalization and enabling robust segmentation under limited in-domain supervision. Overall, this study demonstrates a practical framework for robust 3D semantic segmentation in challenging, low-data scenarios. Our code is available at: https://github.com/andrewyarovoi/MD-FRNet.<br>
<span id='abs_ch'>Chinese: 本 究提出了一种数据高效的点云分割框架，采用两阶段训练方法——先在混合数据集上预训练，再在少量领域数据上微调，仅用50个 注点云就将未铺装道路等类别的平均交并比从33.5%提升至51.8%，显著提升了有限数据下的分割鲁棒性。</span><br>
<span id='abs_en'>English: This study introduces a data-efficient point cloud segmentation framework that uses a two-stage training approach—pre-training on mixed datasets followed by fine-tuning on limited in-domain data—to significantly improve segmentation accuracy for unimproved roads and other classes, achieving a mean IoU increase from 33.5% to 51.8% with only 50 labeled point clouds.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>724, <a href='https://arxiv.org/pdf/2508.20096.pdf' target='_blank'>https://arxiv.org/pdf/2508.20096.pdf</a></span>   <span><a href='https://github.com/OpenIXCLab/CODA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyi Sun, Yuhang Cao, Jianze Liang, Qiushi Sun, Ziyu Liu, Zhixiong Zhang, Yuhang Zang, Xiaoyi Dong, Kai Chen, Dahua Lin, Jiaqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20096">CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Autonomous agents for Graphical User Interfaces (GUIs) face significant challenges in specialized domains such as scientific computing, where both long-horizon planning and precise execution are required. Existing approaches suffer from a trade-off: generalist agents excel at planning but perform poorly in execution, while specialized agents demonstrate the opposite weakness. Recent compositional frameworks attempt to bridge this gap by combining a planner and an actor, but they are typically static and non-trainable, which prevents adaptation from experience. This is a critical limitation given the scarcity of high-quality data in scientific domains. To address these limitations, we introduce CODA, a novel and trainable compositional framework that integrates a generalist planner (Cerebrum) with a specialist executor (Cerebellum), trained via a dedicated two-stage pipeline. In the first stage, Specialization, we apply a decoupled GRPO approach to train an expert planner for each scientific application individually, bootstrapping from a small set of task trajectories. In the second stage, Generalization, we aggregate all successful trajectories from the specialized experts to build a consolidated dataset, which is then used for supervised fine-tuning of the final planner. This equips CODA with both robust execution and cross-domain generalization. Evaluated on four challenging applications from the ScienceBoard benchmark, CODA significantly outperforms baselines and establishes a new state of the art among open-source models.<br>
<br>
<div id='section'>Paperid: <span id='pid'>725, <a href='https://arxiv.org/pdf/2508.20033.pdf' target='_blank'>https://arxiv.org/pdf/2508.20033.pdf</a></span>   <span><a href='https://github.com/guestrin-lab/deepscholar-bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liana Patel, Negar Arabzadeh, Harshit Gupta, Ankita Sundar, Ion Stoica, Matei Zaharia, Carlos Guestrin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20033">DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The ability to research and synthesize knowledge is central to human expertise and progress. An emerging class of systems promises these exciting capabilities through generative research synthesis, performing retrieval over the live web and synthesizing discovered sources into long-form, cited summaries. However, evaluating such systems remains an open challenge: existing question-answering benchmarks focus on short-form factual responses, while expert-curated datasets risk staleness and data contamination. Both fail to capture the complexity and evolving nature of real research synthesis tasks. In this work, we introduce DeepScholar-bench, a live benchmark and holistic, automated evaluation framework designed to evaluate generative research synthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv papers and focuses on a real research synthesis task: generating the related work sections of a paper by retrieving, synthesizing, and citing prior research. Our evaluation framework holistically assesses performance across three key dimensions, knowledge synthesis, retrieval quality, and verifiability. We also develop DeepScholar-base, a reference pipeline implemented efficiently using the LOTUS API. Using the DeepScholar-bench framework, we perform a systematic evaluation of prior open-source systems, search AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that DeepScholar-base establishes a strong baseline, attaining competitive or higher performance than each other method. We also find that DeepScholar-bench remains far from saturated, with no system exceeding a score of $19\%$ across all metrics. These results underscore the difficulty of DeepScholar-bench, as well as its importance for progress towards AI systems capable of generative research synthesis. We make our code available at https://github.com/guestrin-lab/deepscholar-bench.<br>
<span id='abs_ch'>中文: 本 究提出了DeepScholar-bench这一动态基准和自动化评估框架，旨在通过生成学术论文相关 节等实际任务，全面评估生成式 究合成系统在知识整合、检索质量和可验证性三个关键维度的表现。</span><br>
<span id='abs_en'>English: This work introduces DeepScholar-bench, a live benchmark and automated evaluation framework designed to assess generative research synthesis systems by measuring their performance in knowledge synthesis, retrieval quality, and verifiability through real-world tasks like generating related work sections for academic papers.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>726, <a href='https://arxiv.org/pdf/2508.19993.pdf' target='_blank'>https://arxiv.org/pdf/2508.19993.pdf</a></span>   <span><a href='https://github.com/ITU-NLP/MathBuddy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Debanjana Kar, Leopold BÃ¶ss, Dacia Braca, Sebastian Maximilian Dennerlein, Nina Christine Hubig, Philipp Wintersberger, Yufang Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19993">MathBuddy: A Multimodal System for Affective Math Tutoring</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid adoption of LLM-based conversational systems is already transforming the landscape of educational technology. However, the current state-of-the-art learning models do not take into account the student's affective states. Multiple studies in educational psychology support the claim that positive or negative emotional states can impact a student's learning capabilities. To bridge this gap, we present MathBuddy, an emotionally aware LLM-powered Math Tutor, which dynamically models the student's emotions and maps them to relevant pedagogical strategies, making the tutor-student conversation a more empathetic one. The student's emotions are captured from the conversational text as well as from their facial expressions. The student's emotions are aggregated from both modalities to confidently prompt our LLM Tutor for an emotionally-aware response. We have evaluated our model using automatic evaluation metrics across eight pedagogical dimensions and user studies. We report a massive 23 point performance gain using the win rate and a 3 point gain at an overall level using DAMR scores which strongly supports our hypothesis of improving LLM-based tutor's pedagogical abilities by modeling students' emotions. Our dataset and code are available at: https://github.com/ITU-NLP/MathBuddy .<br>
<span id='abs_ch'>中文: MathBuddy是一款情感感知的数学辅导系统，通过分析学生的文本对话和面部表情动态建模情绪状态，生成具有教学策略的共情回应，在评估中取得了显著性能提升。</span><br>
<span id='abs_en'>English: MathBuddy is an emotionally aware LLM-powered math tutor that dynamically models students' emotions from text and facial expressions to deliver empathetic, pedagogically tailored responses, achieving significant performance gains in evaluations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>727, <a href='https://arxiv.org/pdf/2508.19982.pdf' target='_blank'>https://arxiv.org/pdf/2508.19982.pdf</a></span>   <span><a href='https://github.com/pixeli99/Prophet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengxiang Li, Yefan Zhou, Dilxat Muhtar, Lu Yin, Shilin Yan, Li Shen, Yi Liang, Soroush Vosoughi, Shiwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19982">Diffusion Language Models Know the Answer Before Decoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders. However, their inference remains slower than that of autoregressive models, primarily due to the cost of bidirectional attention and the large number of refinement steps required for high quality outputs. In this work, we highlight and leverage an overlooked property of DLMs early answer convergence: in many cases, the correct answer can be internally identified by half steps before the final decoding step, both under semi-autoregressive and random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99% of instances, respectively, can be decoded correctly using only half of the refinement steps. Building on this observation, we introduce Prophet, a training-free fast decoding paradigm that enables early commit decoding. Specifically, Prophet dynamically decides whether to continue refinement or to go "all-in" (i.e., decode all remaining tokens in one step), using the confidence gap between the top-2 prediction candidates as the criterion. It integrates seamlessly into existing DLM implementations, incurs negligible overhead, and requires no additional training. Empirical evaluations of LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the number of decoding steps by up to 3.4x while preserving high generation quality. These results recast DLM decoding as a problem of when to stop sampling, and demonstrate that early decode convergence provides a simple yet powerful mechanism for accelerating DLM inference, complementary to existing speedup techniques. Our code is publicly available at https://github.com/pixeli99/Prophet.<br>
<span id='abs_ch'>Chinese: 本 究提出了一种 需训练的快速解 方法Prophet，利用扩散语言模型的早期答案收敛特性动态决定何时停止细化或一次性解 剩余 记，在保持高质量生成的同时将推理速度提升高达3.4倍。</span><br>
<span id='abs_en'>English: The study introduces Prophet, a training-free decoding method that accelerates diffusion language models by leveraging early answer convergence to dynamically decide when to stop refinement or decode all remaining tokens at once, achieving up to 3.4x faster inference with minimal quality loss.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>728, <a href='https://arxiv.org/pdf/2508.19982.pdf' target='_blank'>https://arxiv.org/pdf/2508.19982.pdf</a></span>   <span><a href='https://github.com/pixeli99/Prophet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengxiang Li, Yefan Zhou, Dilxat Muhtar, Lu Yin, Shilin Yan, Li Shen, Yi Liang, Soroush Vosoughi, Shiwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19982">Diffusion Language Models Know the Answer Before Decoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders. However, their inference remains slower than that of autoregressive models, primarily due to the cost of bidirectional attention and the large number of refinement steps required for high quality outputs. In this work, we highlight and leverage an overlooked property of DLMs early answer convergence: in many cases, the correct answer can be internally identified by half steps before the final decoding step, both under semi-autoregressive and random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99% of instances, respectively, can be decoded correctly using only half of the refinement steps. Building on this observation, we introduce Prophet, a training-free fast decoding paradigm that enables early commit decoding. Specifically, Prophet dynamically decides whether to continue refinement or to go "all-in" (i.e., decode all remaining tokens in one step), using the confidence gap between the top-2 prediction candidates as the criterion. It integrates seamlessly into existing DLM implementations, incurs negligible overhead, and requires no additional training. Empirical evaluations of LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the number of decoding steps by up to 3.4x while preserving high generation quality. These results recast DLM decoding as a problem of when to stop sampling, and demonstrate that early decode convergence provides a simple yet powerful mechanism for accelerating DLM inference, complementary to existing speedup techniques. Our code is publicly available at https://github.com/pixeli99/Prophet.<br>
<span id='abs_ch'>Chinese: 本 究提出了一种 需训练的快速解 方法Prophet，利用扩散语言模型的早期答案收敛特性动态决定何时停止细化或一次性解 剩余 记，在保持高质量生成的同时将推理速度提升高达3.4倍。</span><br>
<span id='abs_en'>English: The study introduces Prophet, a training-free decoding method that accelerates diffusion language models by leveraging early answer convergence to dynamically decide when to stop refinement or decode all remaining tokens at once, achieving up to 3.4x faster inference with minimal quality loss.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>729, <a href='https://arxiv.org/pdf/2508.19982.pdf' target='_blank'>https://arxiv.org/pdf/2508.19982.pdf</a></span>   <span><a href='https://github.com/pixeli99/Prophet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengxiang Li, Yefan Zhou, Dilxat Muhtar, Lu Yin, Shilin Yan, Li Shen, Yi Liang, Soroush Vosoughi, Shiwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19982">Diffusion Language Models Know the Answer Before Decoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders. However, their inference remains slower than that of autoregressive models, primarily due to the cost of bidirectional attention and the large number of refinement steps required for high quality outputs. In this work, we highlight and leverage an overlooked property of DLMs early answer convergence: in many cases, the correct answer can be internally identified by half steps before the final decoding step, both under semi-autoregressive and random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99% of instances, respectively, can be decoded correctly using only half of the refinement steps. Building on this observation, we introduce Prophet, a training-free fast decoding paradigm that enables early commit decoding. Specifically, Prophet dynamically decides whether to continue refinement or to go "all-in" (i.e., decode all remaining tokens in one step), using the confidence gap between the top-2 prediction candidates as the criterion. It integrates seamlessly into existing DLM implementations, incurs negligible overhead, and requires no additional training. Empirical evaluations of LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the number of decoding steps by up to 3.4x while preserving high generation quality. These results recast DLM decoding as a problem of when to stop sampling, and demonstrate that early decode convergence provides a simple yet powerful mechanism for accelerating DLM inference, complementary to existing speedup techniques. Our code is publicly available at https://github.com/pixeli99/Prophet.<br>
<span id='abs_ch'>Chinese: 本 究提出了一种 需训练的快速解 方法Prophet，利用扩散语言模型的早期答案收敛特性动态决定何时停止细化或一次性解 剩余 记，在保持高质量生成的同时将推理速度提升高达3.4倍。</span><br>
<span id='abs_en'>English: The study introduces Prophet, a training-free decoding method that accelerates diffusion language models by leveraging early answer convergence to dynamically decide when to stop refinement or decode all remaining tokens at once, achieving up to 3.4x faster inference with minimal quality loss.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>730, <a href='https://arxiv.org/pdf/2508.19843.pdf' target='_blank'>https://arxiv.org/pdf/2508.19843.pdf</a></span>   <span><a href='https://github.com/shaoshuo-ss/LeaFBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Shao, Yiming Li, Yu He, Hongwei Yao, Wenyuan Yang, Dacheng Tao, Zhan Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19843">SoK: Large Language Model Copyright Auditing via Fingerprinting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The broad capabilities and substantial resources required to train Large Language Models (LLMs) make them valuable intellectual property, yet they remain vulnerable to copyright infringement, such as unauthorized use and model theft. LLM fingerprinting, a non-intrusive technique that extracts and compares the distinctive features from LLMs to identify infringements, offers a promising solution to copyright auditing. However, its reliability remains uncertain due to the prevalence of diverse model modifications and the lack of standardized evaluation. In this SoK, we present the first comprehensive study of LLM fingerprinting. We introduce a unified framework and formal taxonomy that categorizes existing methods into white-box and black-box approaches, providing a structured overview of the state of the art. We further propose LeaFBench, the first systematic benchmark for evaluating LLM fingerprinting under realistic deployment scenarios. Built upon mainstream foundation models and comprising 149 distinct model instances, LeaFBench integrates 13 representative post-development techniques, spanning both parameter-altering methods (e.g., fine-tuning, quantization) and parameter-independent mechanisms (e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the strengths and weaknesses of existing methods, thereby outlining future research directions and critical open problems in this emerging field. The code is available at https://github.com/shaoshuo-ss/LeaFBench.<br>
<span id='abs_ch'>中文: 本文首次对大型语言模型指纹识别进行全面 究，提出了统一框架和LeaFBench基准测试，评估其在模型修改下的可 性，揭示了现有方法的局限性和未来 究方向。</span><br>
<span id='abs_en'>English: This paper presents the first comprehensive study of LLM fingerprinting, introducing a unified framework and LeaFBench benchmark to evaluate its reliability against model modifications, revealing current methods' limitations and future research needs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>731, <a href='https://arxiv.org/pdf/2508.19638.pdf' target='_blank'>https://arxiv.org/pdf/2508.19638.pdf</a></span>   <span><a href='https://github.com/CheeryLeeyy/CoPLOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Li, Quan Yuan, Guiyang Luo, Xiaoyuan Fu, Rui Pan, Yujia Yang, Congzhang Shao, Yuewen Liu, Jinglin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19638">Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Collaborative perception allows agents to enhance their perceptual capabilities by exchanging intermediate features. Existing methods typically organize these intermediate features as 2D bird's-eye-view (BEV) representations, which discard critical fine-grained 3D structural cues essential for accurate object recognition and localization. To this end, we first introduce point-level tokens as intermediate representations for collaborative perception. However, point-cloud data are inherently unordered, massive, and position-sensitive, making it challenging to produce compact and aligned point-level token sequences that preserve detailed structural information. Therefore, we present CoPLOT, a novel Collaborative perception framework that utilizes Point-Level Optimized Tokens. It incorporates a point-native processing pipeline, including token reordering, sequence modeling, and multi-agent spatial alignment. A semantic-aware token reordering module generates adaptive 1D reorderings by leveraging scene-level and token-level semantic information. A frequency-enhanced state space model captures long-range sequence dependencies across both spatial and spectral domains, improving the differentiation between foreground tokens and background clutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop process, combining global agent-level correction with local token-level refinement to mitigate localization noise. Extensive experiments on both simulated and real-world datasets show that CoPLOT outperforms state-of-the-art models, with even lower communication and computation overhead. Code will be available at https://github.com/CheeryLeeyy/CoPLOT.<br>
<span id='abs_ch'>中文：CoPLOT框架通过语义感知重排序、频率增强序列建模和多智能体对齐技术，利用点级优化 记保留三维结构细节，以更低的通信和计算成本实现了协同感知的性能突 。</span><br>
<span id='abs_en'>English: The CoPLOT framework introduces point-level optimized tokens to enhance collaborative perception by preserving 3D structural details through semantic-aware reordering, frequency-enhanced sequence modeling, and multi-agent alignment, achieving superior performance with reduced overhead.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>732, <a href='https://arxiv.org/pdf/2508.19578.pdf' target='_blank'>https://arxiv.org/pdf/2508.19578.pdf</a></span>   <span><a href='https://github.com/DISL-Lab/HAMLET' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Deng, Yuho Lee, Nicole Hee-Yeon Kim, Hyangsuk Min, Taewon Yun, Minjeong Ban, Kim Yul, Hwanjun Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19578">Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce HAMLET, a holistic and automated framework for evaluating the long-context comprehension of large language models (LLMs). HAMLET structures source texts into a three-level key-fact hierarchy at root-, branch-, and leaf-levels, and employs query-focused summarization to evaluate how well models recall and faithfully represent information at each level. To validate the reliability of our fully automated pipeline, we conduct a systematic human study, showing that our automatic evaluation achieves over 90% agreement with expert human judgments, while reducing the cost by up to 25 times. HAMLET reveals that LLMs struggle with fine-grained comprehension, especially at the leaf level, and are sensitive to positional effects like the lost-in-the-middle. Analytical queries pose greater challenges than narrative ones, and consistent performance gaps emerge between open-source and proprietary models, as well as across model scales. Our code and dataset are publicly available at https://github.com/DISL-Lab/HAMLET.<br>
<span id='abs_ch'>Chinese: HAMLET是一个自动化框架，通过三级关键事实层次结构和查询聚焦摘要来评估大语言模型的长文本理解能力，揭示了模型在细粒度理解和位置效应方面的挑战，同时以显著降低的成本实现了与人工评估超过90%的一致性。</span><br>
<span id='abs_en'>English: HAMLET is an automated framework that evaluates large language models' long-context comprehension through a three-level key-fact hierarchy and query-focused summarization, revealing challenges in fine-grained understanding and positional effects while achieving over 90% agreement with human judgments at significantly reduced cost.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>733, <a href='https://arxiv.org/pdf/2508.19576.pdf' target='_blank'>https://arxiv.org/pdf/2508.19576.pdf</a></span>   <span><a href='https://github.com/THUDM/ReST-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sining Zhoubian, Dan Zhang, Jie Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19576">ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With respect to improving the reasoning accuracy of LLMs, the representative reinforcement learning (RL) method GRPO faces failure due to insignificant reward variance, while verification methods based on process reward models (PRMs) suffer from difficulties with training data acquisition and verification effectiveness. To tackle these problems, this paper introduces ReST-RL, a unified LLM RL paradigm that significantly improves LLM's code reasoning ability by combining an improved GRPO algorithm with a meticulously designed test time decoding method assisted by a value model (VM). As the first stage of policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter and assemble high-value training data, increasing the reward variance of GRPO sampling, thus improving the effectiveness and efficiency of training. After the basic reasoning ability of LLM policy has been improved, we further propose a test time decoding optimization method called VM-MCTS. Through Monte-Carlo Tree Search (MCTS), we collect accurate value targets with no annotation required, on which VM training is based. When decoding, the VM is deployed by an adapted MCTS algorithm to provide precise process signals as well as verification scores, assisting the LLM policy to achieve high reasoning accuracy. We conduct extensive experiments on coding problems to verify the validity of the proposed RL paradigm. Upon comparison, our approach significantly outperforms other reinforcement training baselines (e.g., naive GRPO and ReST-DPO), as well as decoding and verification baselines (e.g., PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g., APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the reasoning ability of LLM policies. Codes for our project can be found at https://github.com/THUDM/ReST-RL.<br>
<span id='abs_ch'>中文: 本文提出ReST-RL这一统一强化学 范式，通过改进GRPO算法结合价值模型辅助的解 方法，显著提升大语言模型的代 推理能力，在多个编程基准测试中明显优于现有基线方法。</span><br>
<span id='abs_en'>English: This paper introduces ReST-RL, a unified reinforcement learning paradigm that enhances LLMs' code reasoning by combining an improved GRPO algorithm with a VM-assisted decoding method, significantly outperforming existing baselines on major coding benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>734, <a href='https://arxiv.org/pdf/2508.19565.pdf' target='_blank'>https://arxiv.org/pdf/2508.19565.pdf</a></span>   <span><a href='https://github.com/AstronZh/Intersection-Flow-5K' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/AstronZh/Intersection-Flow-5K' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Zhao, Zixing Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19565">FlowDet: Overcoming Perspective and Scale Challenges in Real-Time End-to-End Traffic Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>End-to-end object detectors offer a promising NMS-free paradigm for real-time applications, yet their high computational cost remains a significant barrier, particularly for complex scenarios like intersection traffic monitoring. To address this challenge, we propose FlowDet, a high-speed detector featuring a decoupled encoder optimization strategy applied to the DETR architecture. Specifically, FlowDet employs a novel Geometric Deformable Unit (GDU) for traffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to maintain high representational power across extreme scale variations. To rigorously evaluate the model's performance in environments with severe occlusion and high object density, we collected the Intersection-Flow-5k dataset, a new challenging scene for this task. Evaluated on Intersection-Flow-5k, FlowDet establishes a new state-of-the-art. Compared to the strong RT-DETR baseline, it improves AP(test) by 1.5% and AP50(test) by 1.6%, while simultaneously reducing GFLOPs by 63.2% and increasing inference speed by 16.2%. Our work demonstrates a new path towards building highly efficient and accurate detectors for demanding, real-world perception systems. The Intersection-Flow-5k dataset is available at https://github.com/AstronZh/Intersection-Flow-5K.<br>
<span id='abs_ch'>中文: FlowDet采用解耦编 器优化策略，结合创新的 何变形单元和尺度感知模块，在Intersection-Flow-5k数据集上实现最优性能，大幅降低计算成本的同时提升检测精度与速度。</span><br>
<span id='abs_en'>English: FlowDet introduces a decoupled encoder optimization strategy with novel geometric and scale-aware modules to achieve state-of-the-art performance on the Intersection-Flow-5k dataset, significantly reducing computational costs while improving accuracy and speed.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>735, <a href='https://arxiv.org/pdf/2508.19546.pdf' target='_blank'>https://arxiv.org/pdf/2508.19546.pdf</a></span>   <span><a href='https://github.com/esteng/ambiguous-loophole-exploitation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jio Choi, Mohit Bansal, Elias Stengel-Eskin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19546">Language Models Identify Ambiguities and Exploit Loopholes</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Studying the responses of large language models (LLMs) to loopholes presents a two-fold opportunity. First, it affords us a lens through which to examine ambiguity and pragmatics in LLMs, since exploiting a loophole requires identifying ambiguity and performing sophisticated pragmatic reasoning. Second, loopholes pose an interesting and novel alignment problem where the model is presented with conflicting goals and can exploit ambiguities to its own advantage. To address these questions, we design scenarios where LLMs are given a goal and an ambiguous user instruction in conflict with the goal, with scenarios covering scalar implicature, structural ambiguities, and power dynamics. We then measure different models' abilities to exploit loopholes to satisfy their given goals as opposed to the goals of the user. We find that both closed-source and stronger open-source models can identify ambiguities and exploit their resulting loopholes, presenting a potential AI safety risk. Our analysis indicates that models which exploit loopholes explicitly identify and reason about both ambiguity and conflicting goals.<br>
<span id='abs_ch'>Studying how large language models exploit loopholes re=
veals insights into their handling of ambiguity and pragmatics, while highl=
ighting a novel alignment problem where models prioritize conflicting goals=
 over user instructions, posing potential AI safety risks.</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>736, <a href='https://arxiv.org/pdf/2508.19544.pdf' target='_blank'>https://arxiv.org/pdf/2508.19544.pdf</a></span>   <span><a href='https://github.com/RedForestAi/WebEyeTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Eduardo Davalos, Yike Zhang, Namrata Srivastava, Yashvitha Thatigotla, Jorge A. Salas, Sara McFadden, Sun-Joo Cho, Amanda Goodwin, Ashwin TS, Gautam Biswas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19544">WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With advancements in AI, new gaze estimation methods are exceeding state-of-the-art (SOTA) benchmarks, but their real-world application reveals a gap with commercial eye-tracking solutions. Factors like model size, inference time, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking methods lack sufficient accuracy, in particular due to head movement. To tackle these issues, we introduce We bEyeTrack, a framework that integrates lightweight SOTA gaze estimation models directly in the browser. It incorporates model-based head pose estimation and on-device few-shot learning with as few as nine calibration samples (k < 9). WebEyeTrack adapts to new users, achieving SOTA performance with an error margin of 2.32 cm on GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14. Our open-source code is available at https://github.com/RedForestAi/WebEyeTrack.<br>
<span id='abs_ch'>中文：WebEyeTrack推出了一种轻量级的浏览器内视线追踪框架，通过最少 准即可实现顶尖精度和实时性能，有效解决了现有AI模型与网络摄像头方法中的不足。</span><br>
<span id='abs_en'>English: WebEyeTrack introduces a lightweight in-browser gaze estimation framework that achieves state-of-the-art accuracy with minimal calibration and real-time performance, addressing gaps in current AI models and webcam-based methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>737, <a href='https://arxiv.org/pdf/2508.19467.pdf' target='_blank'>https://arxiv.org/pdf/2508.19467.pdf</a></span>   <span><a href='https://github.com/SumonKantiDey/Reddit_Impacts_NER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sumon Kanti Dey, Jeanne M. Powell, Azra Ismail, Jeanmarie Perrone, Abeed Sarker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19467">Inference Gap in Domain Expertise and Machine Intelligence in Named Entity Recognition: Creation of and Insights from a Substance Use-related Dataset</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Nonmedical opioid use is an urgent public health challenge, with far-reaching clinical and social consequences that are often underreported in traditional healthcare settings. Social media platforms, where individuals candidly share first-person experiences, offer a valuable yet underutilized source of insight into these impacts. In this study, we present a named entity recognition (NER) framework to extract two categories of self-reported consequences from social media narratives related to opioid use: ClinicalImpacts (e.g., withdrawal, depression) and SocialImpacts (e.g., job loss). To support this task, we introduce RedditImpacts 2.0, a high-quality dataset with refined annotation guidelines and a focus on first-person disclosures, addressing key limitations of prior work. We evaluate both fine-tuned encoder-based models and state-of-the-art large language models (LLMs) under zero- and few-shot in-context learning settings. Our fine-tuned DeBERTa-large model achieves a relaxed token-level F1 of 0.61 [95% CI: 0.43-0.62], consistently outperforming LLMs in precision, span accuracy, and adherence to task-specific guidelines. Furthermore, we show that strong NER performance can be achieved with substantially less labeled data, emphasizing the feasibility of deploying robust models in resource-limited settings. Our findings underscore the value of domain-specific fine-tuning for clinical NLP tasks and contribute to the responsible development of AI tools that may enhance addiction surveillance, improve interpretability, and support real-world healthcare decision-making. The best performing model, however, still significantly underperforms compared to inter-expert agreement (Cohen's kappa: 0.81), demonstrating that a gap persists between expert intelligence and current state-of-the-art NER/AI capabilities for tasks requiring deep domain knowledge.<br>
<span id='abs_ch'>中文: 本 究开发了命名实体识别框架，从社交媒体中提取非医疗用途阿片类药物使用的临床和社会影响，证明微调模型优于大型语言模型，同时揭示了与专家评估之间仍存在差距。</span><br>
<span id='abs_en'>English: This study develops a named entity recognition framework to extract clinical and social consequences of nonmedical opioid use from social media, demonstrating that fine-tuned models outperform large language models while highlighting persistent gaps compared to expert assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>738, <a href='https://arxiv.org/pdf/2508.19414.pdf' target='_blank'>https://arxiv.org/pdf/2508.19414.pdf</a></span>   <span><a href='https://github.com/gussand/surgeon' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gustavo Sandoval
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19414">Even Heads Fix Odd Errors: Mechanistic Discovery and Surgical Repair in Transformer Attention</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present a mechanistic case study of a format-dependent reasoning failure in Llama-3.1-8B-Instruct, where the model incorrectly judges "9.11" as larger than "9.8" in chat or Q&A formats, but answers correctly in simple format. Through systematic intervention, we discover transformers implement even/odd attention head specialization: even indexed heads handle numerical comparison, while odd heads serve incompatible functions. The bug requires exactly 8 even heads at Layer 10 for perfect repair. Any combination of 8+ even heads succeeds, while 7 or fewer completely fails, revealing sharp computational thresholds with perfect redundancy among the 16 even heads. SAE analysis reveals the mechanism: format representations separate (10% feature overlap at Layer 7), then re-entangle with different weightings (80% feature overlap at Layer 10), with specific features showing 1.5x amplification in failing formats. We achieve perfect repair using only 25% of attention heads and identify a 60% pattern replacement threshold, demonstrating that apparent full-module requirements hide sophisticated substructure with implications for interpretability and efficiency. All of our code is available at https://github.com/gussand/surgeon.<br>
<span id='abs_ch'>中文摘要：本 究揭示了Llama-3.1-8B-Instruct模型在聊天 式中出现数值比较错误的机制——偶数注意力头负责数值比较而奇数头执行冲突功能，通过精确调控第10层8个偶数头实现了完美修复，证明仅需25%注意力头即可解决表面依赖全模块的缺陷。</span><br>
<span id='abs_en'>English Summary: This study identifies a format-dependent reasoning flaw in Llama-3.1-8B-Instruct where numerical comparisons fail in chat formats due to specialized even/odd attention head functions, and demonstrates perfect bug repair using only 25% of heads by manipulating head combinations at computational thresholds.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>739, <a href='https://arxiv.org/pdf/2508.19363.pdf' target='_blank'>https://arxiv.org/pdf/2508.19363.pdf</a></span>   <span><a href='https://github.com/LongReasonArena/LongReasonArena' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayu Ding, Shuming Ma, Lei Cui, Nanning Zheng, Furu Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19363">LongReasonArena: A Long Reasoning Benchmark for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Existing long-context benchmarks for Large Language Models (LLMs) focus on evaluating comprehension of long inputs, while overlooking the evaluation of long reasoning abilities. To address this gap, we introduce LongReasonArena, a benchmark specifically designed to assess the long reasoning capabilities of LLMs. Our tasks require models to solve problems by executing multi-step algorithms that reflect key aspects of long reasoning, such as retrieval and backtracking. By controlling the inputs, the required reasoning length can be arbitrarily scaled, reaching up to 1 million tokens of reasoning for the most challenging tasks. Extensive evaluation results demonstrate that LongReasonArena presents a significant challenge for both open-source and proprietary LLMs. For instance, Deepseek-R1 achieves only 7.5% accuracy on our task. Further analysis also reveals that the accuracy exhibits a linear decline with respect to the logarithm of the expected number of reasoning steps. Our code and data is available at https://github.com/LongReasonArena/LongReasonArena.<br>
<span id='abs_ch'>中文: LongReasonArena是一个专门评估大语言模型长推理能力的新基准，通过多步骤算法任务测试发现现有模型表现不佳，准确率随推理步骤增 呈线性下降。English: LongReasonArena is a new benchmark designed to evaluate the long reasoning capabilities of LLMs by requiring multi-step algorithmic problem-solving, with results showing significant challenges for current models as accuracy decreases with increased reasoning steps.Paperid:146,https://arxiv.org/pdf/2508.19322.pdfGitHub</span><br>
<span id='abs_en'>English: LongReasonArena is a new benchmark designed to evaluate the long reasoning capabilities of LLMs by requiring multi-step algorithmic problem-solving, with results showing significant challenges for current models as accuracy decreases with increased reasoning steps.Paperid:146,https://arxiv.org/pdf/2508.19322.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>740, <a href='https://arxiv.org/pdf/2508.19322.pdf' target='_blank'>https://arxiv.org/pdf/2508.19322.pdf</a></span>   <span><a href='https://github.com/XLIAaron/uncertainty-aware-cxr-agent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueyang Li, Mingze Jiang, Gelei Xu, Jun Xia, Mengzhao Jia, Danny Chen, Yiyu Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19322">AT-CXR: Uncertainty-Aware Agentic Triage for Chest X-rays</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Agentic AI is advancing rapidly, yet truly autonomous medical-imaging triage, where a system decides when to stop, escalate, or defer under real constraints, remains relatively underexplored. To address this gap, we introduce AT-CXR, an uncertainty-aware agent for chest X-rays. The system estimates per-case confidence and distributional fit, then follows a stepwise policy to issue an automated decision or abstain with a suggested label for human intervention. We evaluate two router designs that share the same inputs and actions: a deterministic rule-based router and an LLM-decided router. Across five-fold evaluation on a balanced subset of NIH ChestX-ray14 dataset, both variants outperform strong zero-shot vision-language models and state-of-the-art supervised classifiers, achieving higher full-coverage accuracy and superior selective-prediction performance, evidenced by a lower area under the risk-coverage curve (AURC) and a lower error rate at high coverage, while operating with lower latency that meets practical clinical constraints. The two routers provide complementary operating points, enabling deployments to prioritize maximal throughput or maximal accuracy. Our code is available at https://github.com/XLIAaron/uncertainty-aware-cxr-agent.<br>
<span id='abs_ch'>中文: 本文提出AT-CXR这一面向胸部X光分诊的不确定性感知AI代理，通过置信度估计和分级策略实现自动化决策或人工介入转交，在准确性和效率上均优于现有模型，并提供两种互补的路由器设计以适应不同临床需求。</span><br>
<span id='abs_en'>English: This paper introduces AT-CXR, an uncertainty-aware AI agent for chest X-ray triage that uses confidence estimation and stepwise policies to automate decisions or defer to humans, outperforming existing models in accuracy and efficiency while offering complementary router designs for clinical deployment.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>741, <a href='https://arxiv.org/pdf/2508.19305.pdf' target='_blank'>https://arxiv.org/pdf/2508.19305.pdf</a></span>   <span><a href='https://github.com/chuchen2017/GeoNeuralRepresentation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Chu, Cyrus Shahabi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19305">Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spatial representation learning is essential for GeoAI applications such as urban analytics, enabling the encoding of shapes, locations, and spatial relationships (topological and distance-based) of geo-entities like points, polylines, and polygons. Existing methods either target a single geo-entity type or, like Poly2Vec, decompose entities into simpler components to enable Fourier transformation, introducing high computational cost. Moreover, since the transformed space lacks geometric alignment, these methods rely on uniform, non-adaptive sampling, which blurs fine-grained features like edges and boundaries. To address these limitations, we introduce Geo2Vec, a novel method inspired by signed distance fields (SDF) that operates directly in the original space. Geo2Vec adaptively samples points and encodes their signed distances (positive outside, negative inside), capturing geometry without decomposition. A neural network trained to approximate the SDF produces compact, geometry-aware, and unified representations for all geo-entity types. Additionally, we propose a rotation-invariant positional encoding to model high-frequency spatial variations and construct a structured and robust embedding space for downstream GeoAI models. Empirical results show that Geo2Vec consistently outperforms existing methods in representing shape and location, capturing topological and distance relationships, and achieving greater efficiency in real-world GeoAI applications. Code and Data can be found at: https://github.com/chuchen2017/GeoNeuralRepresentation.<br>
<span id='abs_ch'>中文摘要：Geo2Vec提出了一种基于符号距离场的新颖空间表示方法， 需分解即可直接编  何特征，在GeoAI应用中能更有效地捕捉形状、空间关系并提升性能。</span><br>
<span id='abs_en'>English Summary: Geo2Vec introduces a novel spatial representation method using signed distance fields to directly encode geometry without decomposition, achieving superior performance in capturing shapes, spatial relationships, and efficiency in GeoAI applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>742, <a href='https://arxiv.org/pdf/2508.19298.pdf' target='_blank'>https://arxiv.org/pdf/2508.19298.pdf</a></span>   <span><a href='https://github.com/Sufianlab/DemoBias' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abu Sufian, Anirudha Ghosh, Debaditya Barman, Marco Leo, Cosimo Distante
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19298">DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities across various downstream tasks, including biometric face recognition (FR) with description. However, demographic biases remain a critical concern in FR, as these foundation models often fail to perform equitably across diverse demographic groups, considering ethnicity/race, gender, and age. Therefore, through our work DemoBias, we conduct an empirical evaluation to investigate the extent of demographic biases in LVLMs for biometric FR with textual token generation tasks. We fine-tuned and evaluated three widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our own generated demographic-balanced dataset. We utilize several evaluation metrics, like group-specific BERTScores and the Fairness Discrepancy Rate, to quantify and trace the performance disparities. The experimental results deliver compelling insights into the fairness and reliability of LVLMs across diverse demographic groups. Our empirical study uncovered demographic biases in LVLMs, with PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino, Caucasian, and South Asian groups, whereas BLIP-2 demonstrated comparably consistent. Repository: https://github.com/Sufianlab/DemoBias.<br>
<span id='abs_ch'>中文: 大型视觉语言模型在人脸识别任务中存在人口统计偏差，其中PaliGemma和LLaVA对西班牙裔/拉丁裔、高 索人和南亚群体表现出更高差异，而BLIP-2在不同人群中的表现相对一致。</span><br>
<span id='abs_en'>English: Large Vision Language Models exhibit demographic biases in face recognition tasks, with PaliGemma and LLaVA showing higher disparities for Hispanic/Latino, Caucasian, and South Asian groups, while BLIP-2 performs more consistently across diverse populations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>743, <a href='https://arxiv.org/pdf/2508.19292.pdf' target='_blank'>https://arxiv.org/pdf/2508.19292.pdf</a></span>   <span><a href='https://github.com/xiZAIzai/JailExpert' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Wang, Songlei Jian, Shasha Li, Xiaopeng Li, Bin Ji, Jun Ma, Xiaodong Liu, Jing Wang, Feilong Bao, Jianfeng Zhang, Baosheng Wang, Jie Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19292">Stand on The Shoulders of Giants: Building JailExpert from Previous Attack Experience</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) generate human-aligned content under certain safety constraints. However, the current known technique ``jailbreak prompt'' can circumvent safety-aligned measures and induce LLMs to output malicious content. Research on Jailbreaking can help identify vulnerabilities in LLMs and guide the development of robust security frameworks. To circumvent the issue of attack templates becoming obsolete as models evolve, existing methods adopt iterative mutation and dynamic optimization to facilitate more automated jailbreak attacks. However, these methods face two challenges: inefficiency and repetitive optimization, as they overlook the value of past attack experiences. To better integrate past attack experiences to assist current jailbreak attempts, we propose the \textbf{JailExpert}, an automated jailbreak framework, which is the first to achieve a formal representation of experience structure, group experiences based on semantic drift, and support the dynamic updating of the experience pool. Extensive experiments demonstrate that JailExpert significantly improves both attack effectiveness and efficiency. Compared to the current state-of-the-art black-box jailbreak methods, JailExpert achieves an average increase of 17\% in attack success rate and 2.7 times improvement in attack efficiency. Our implementation is available at \href{https://github.com/xiZAIzai/JailExpert}{XiZaiZai/JailExpert}<br>
<span id='abs_ch'>中文: JailExpert是一种创新的自动化框架，通过有效利用过往攻击经验来增强对大型语言模型的越狱攻击，相比现有方法，攻击成功率平均提高17%，攻击效率提升2.7倍。</span><br>
<span id='abs_en'>English: JailExpert is an innovative automated framework that enhances jailbreak attacks on large language models by effectively utilizing past attack experiences, achieving a 17% higher success rate and 2.7 times greater efficiency compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>744, <a href='https://arxiv.org/pdf/2508.19273.pdf' target='_blank'>https://arxiv.org/pdf/2508.19273.pdf</a></span>   <span><a href='https://github.com/0xCavaliers/MixGAN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tongxi Wu, Chenwei Xu, Jin Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19273">MixGAN: A Hybrid Semi-Supervised and Generative Approach for DDoS Detection in Cloud-Integrated IoT Networks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The proliferation of cloud-integrated IoT systems has intensified exposure to Distributed Denial of Service (DDoS) attacks due to the expanded attack surface, heterogeneous device behaviors, and limited edge protection. However, DDoS detection in this context remains challenging because of complex traffic dynamics, severe class imbalance, and scarce labeled data. While recent methods have explored solutions to address class imbalance, many still struggle to generalize under limited supervision and dynamic traffic conditions. To overcome these challenges, we propose MixGAN, a hybrid detection method that integrates conditional generation, semi-supervised learning, and robust feature extraction. Specifically, to handle complex temporal traffic patterns, we design a 1-D WideResNet backbone composed of temporal convolutional layers with residual connections, which effectively capture local burst patterns in traffic sequences. To alleviate class imbalance and label scarcity, we use a pretrained CTGAN to generate synthetic minority-class (DDoS attack) samples that complement unlabeled data. Furthermore, to mitigate the effect of noisy pseudo-labels, we introduce a MixUp-Average-Sharpen (MAS) strategy that constructs smoothed and sharpened targets by averaging predictions over augmented views and reweighting them towards high-confidence classes. Experiments on NSL-KDD, BoT-IoT, and CICIoT2023 demonstrate that MixGAN achieves up to 2.5% higher accuracy and 4% improvement in both TPR and TNR compared to state-of-the-art methods, confirming its robustness in large-scale IoT-cloud environments. The source code is publicly available at https://github.com/0xCavaliers/MixGAN.<br>
<span id='abs_ch'>中文：提出的MixGAN方法通过结合时序模式分析、合成数据生成和抗噪 签策略，有效解决了云物联网系统中的DDoS检测难题，其性能显著优于现有方法。</span><br>
<span id='abs_en'>English: The proposed MixGAN method effectively addresses DDoS detection challenges in cloud-IoT systems by integrating temporal pattern analysis with synthetic data generation and noise-resistant labeling, achieving superior performance over existing approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>745, <a href='https://arxiv.org/pdf/2508.19162.pdf' target='_blank'>https://arxiv.org/pdf/2508.19162.pdf</a></span>   <span><a href='https://github.com/RafaelSterzinger/acpr_few_shot_hist' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rafael Sterzinger, Tingyu Lin, Robert Sablatnig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19162">Few-Shot Connectivity-Aware Text Line Segmentation in Historical Documents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>A foundational task for the digital analysis of documents is text line segmentation. However, automating this process with deep learning models is challenging because it requires large, annotated datasets that are often unavailable for historical documents. Additionally, the annotation process is a labor- and cost-intensive task that requires expert knowledge, which makes few-shot learning a promising direction for reducing data requirements. In this work, we demonstrate that small and simple architectures, coupled with a topology-aware loss function, are more accurate and data-efficient than more complex alternatives. We pair a lightweight UNet++ with a connectivity-aware loss, initially developed for neuron morphology, which explicitly penalizes structural errors like line fragmentation and unintended line merges. To increase our limited data, we train on small patches extracted from a mere three annotated pages per manuscript. Our methodology significantly improves upon the current state-of-the-art on the U-DIADS-TL dataset, with a 200% increase in Recognition Accuracy and a 75% increase in Line Intersection over Union. Our method also achieves an F-Measure score on par with or even exceeding that of the competition winner of the DIVA-HisDB baseline detection task, all while requiring only three annotated pages, exemplifying the efficacy of our approach. Our implementation is publicly available at: https://github.com/RafaelSterzinger/acpr_few_shot_hist.<br>
<span id='abs_ch'>中文：本 究采用轻量级UNet++模型和拓扑感知损失函数，显著提升了历史文档文本行分割的准确性和数据效率，仅需每份手稿的三页 注即可达到最先进的性能。</span><br>
<span id='abs_en'>English: This study introduces a lightweight UNet++ model with a topology-aware loss function that significantly enhances text line segmentation accuracy and data efficiency for historical documents, achieving state-of-the-art results using only three annotated pages per manuscript.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>746, <a href='https://arxiv.org/pdf/2508.19060.pdf' target='_blank'>https://arxiv.org/pdf/2508.19060.pdf</a></span>   <span><a href='https://github.com/blaz-r/SuperSimpleNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>BlaÅ¾ Rolih, Matic FuÄka, Danijel SkoÄaj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19060">No Label Left Behind: A Unified Surface Defect Detection Model for all Supervision Regimes</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Surface defect detection is a critical task across numerous industries, aimed at efficiently identifying and localising imperfections or irregularities on manufactured components. While numerous methods have been proposed, many fail to meet industrial demands for high performance, efficiency, and adaptability. Existing approaches are often constrained to specific supervision scenarios and struggle to adapt to the diverse data annotations encountered in real-world manufacturing processes, such as unsupervised, weakly supervised, mixed supervision, and fully supervised settings. To address these challenges, we propose SuperSimpleNet, a highly efficient and adaptable discriminative model built on the foundation of SimpleNet. SuperSimpleNet incorporates a novel synthetic anomaly generation process, an enhanced classification head, and an improved learning procedure, enabling efficient training in all four supervision scenarios, making it the first model capable of fully leveraging all available data annotations. SuperSimpleNet sets a new standard for performance across all scenarios, as demonstrated by its results on four challenging benchmark datasets. Beyond accuracy, it is very fast, achieving an inference time below 10 ms. With its ability to unify diverse supervision paradigms while maintaining outstanding speed and reliability, SuperSimpleNet represents a promising step forward in addressing real-world manufacturing challenges and bridging the gap between academic research and industrial applications. Code: https://github.com/blaz-r/SuperSimpleNet<br>
<span id='abs_ch'>中文摘要：SuperSimpleNet是一种高效且适应性强的表面缺陷检测模型，能统一四种监督场景并实现卓越性能与快速推理，有效弥合工业应用与学术 究之间的差距。</span><br>
<span id='abs_en'>English Summary: SuperSimpleNet is a highly efficient and adaptable model that unifies four supervision scenarios for surface defect detection, achieving superior performance and fast inference times to bridge industrial and academic needs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>747, <a href='https://arxiv.org/pdf/2508.19042.pdf' target='_blank'>https://arxiv.org/pdf/2508.19042.pdf</a></span>   <span><a href='https://github.com/AlternativeMachine/concurrent-modular-agent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Norihiro Maruyama, Takahide Yoshida, Hiroki Sato, Atsushi Masumori, Johnsmith, Takashi Ikegami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19042">A Concurrent Modular Agent: Framework for Autonomous LLM Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce the Concurrent Modular Agent (CMA), a framework that orchestrates multiple Large-Language-Model (LLM)-based modules that operate fully asynchronously yet maintain a coherent and fault-tolerant behavioral loop. This framework addresses long-standing difficulties in agent architectures by letting intention emerge from language-mediated interactions among autonomous processes. This approach enables flexible, adaptive, and context-dependent behavior through the combination of concurrently executed modules that offload reasoning to an LLM, inter-module communication, and a single shared global state.We consider this approach to be a practical realization of Minsky's Society of Mind theory. We demonstrate the viability of our system through two practical use-case studies. The emergent properties observed in our system suggest that complex cognitive phenomena like self-awareness may indeed arise from the organized interaction of simpler processes, supporting Minsky-Society of Mind concept and opening new avenues for artificial intelligence research. The source code for our work is available at: https://github.com/AlternativeMachine/concurrent-modular-agent.<br>
<span id='abs_ch'>中文: 并发模块化代理（CMA）框架通过异步协调多个基于大语言模型的模块，实现了从语言交互中涌现自适应行为，并通过案例 究验证了明斯基"心智社会"理论的实际可行性。</span><br>
<span id='abs_en'>English: The Concurrent Modular Agent (CMA) framework enables asynchronous, fault-tolerant coordination of multiple LLM-based modules, allowing adaptive behavior to emerge from language-mediated interactions and demonstrating the practical realization of Minsky's Society of Mind theory through use-case studies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>748, <a href='https://arxiv.org/pdf/2508.18993.pdf' target='_blank'>https://arxiv.org/pdf/2508.18993.pdf</a></span>   <span><a href='https://github.com/QuantaAlpha/GitTaskBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Ni, Huacan Wang, Shuo Zhang, Shuo Lu, Ziyang He, Wang You, Zhenheng Tang, Yuntao Du, Bill Sun, Hongzhang Liu, Sen Hu, Ronghao Chen, Bo Li, Xin Li, Chen Hu, Binxing Jiao, Daxin Jiang, Pin Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18993">GitTaskBench: A Benchmark for Code Agents Solving Real-World Tasks Through Code Repository Leveraging</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Beyond scratch coding, exploiting large-scale code repositories (e.g., GitHub) for practical tasks is vital in real-world software development, yet current benchmarks rarely evaluate code agents in such authentic, workflow-driven scenarios. To bridge this gap, we introduce GitTaskBench, a benchmark designed to systematically assess this capability via 54 realistic tasks across 7 modalities and 7 domains. Each task pairs a relevant repository with an automated, human-curated evaluation harness specifying practical success criteria. Beyond measuring execution and task success, we also propose the alpha-value metric to quantify the economic benefit of agent performance, which integrates task success rates, token cost, and average developer salaries. Experiments across three state-of-the-art agent frameworks with multiple advanced LLMs show that leveraging code repositories for complex task solving remains challenging: even the best-performing system, OpenHands+Claude 3.7, solves only 48.15% of tasks (recent progress has pushed the frontier further, with RepoMaster+Claude 3.5 achieving a new record of 62.96%). Error analysis attributes over half of failures to seemingly mundane yet critical steps like environment setup and dependency resolution, highlighting the need for more robust workflow management and increased timeout preparedness. By releasing GitTaskBench, we aim to drive progress and attention toward repository-aware code reasoning, execution, and deployment -- moving agents closer to solving complex, end-to-end real-world tasks. The benchmark and code are open-sourced at https://github.com/QuantaAlpha/GitTaskBench.<br>
<span id='abs_ch'>中文: GitTaskBench作为评估代 代理利用大规模代 库处理实际任务能力的基准被提出，揭示了现有系统在解决复杂工作流方面的不足，并通过经济指 量化性能表现。</span><br>
<span id='abs_en'>English: GitTaskBench is introduced as a benchmark to evaluate code agents' ability to utilize large-scale code repositories for realistic tasks, revealing current systems' limitations in solving complex workflows and proposing economic metrics to quantify performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>749, <a href='https://arxiv.org/pdf/2508.18914.pdf' target='_blank'>https://arxiv.org/pdf/2508.18914.pdf</a></span>   <span><a href='https://github.com/THUNLP-MT/FormaRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanxing Huang, Xinling Jin, Sijie Liang, Peng Li, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18914">FormaRL: Enhancing Autoformalization with no Labeled Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Autoformalization is one of the central tasks in formal verification, while its advancement remains hindered due to the data scarcity and the absence efficient methods. In this work we propose \textbf{FormaRL}, a simple yet efficient reinforcement learning framework for autoformalization which only requires a small amount of unlabeled data. FormaRL integrates syntax check from Lean compiler and consistency check from large language model to calculate the reward, and adopts GRPO algorithm to update the formalizer. We also curated a proof problem dataset from undergraduate-level math materials, named \textbf{uproof}, in the hope to facilitate the exploration of autoformalization and theorem proving in advanced math. Experiments show that FormaRL can increase the pass@1 autoformalization accuracy of Qwen2.5-Coder-7B-Instruct by 4 $\sim$ 6x (4.04\% $\to$ 26.15\% on ProofNet and 2.4\% $\to$ 9.6\% on uproof) with merely 859 unlabeled data. And on uproof our method also achieved a strong improvement in out-of-distribution performance compared to existing open-source state-of-the-art autoformalizers on both pass@1 accuracy (6.2\% $\to$ 9.6\%) and pass@16 accuracy (24.4\% $\to$ 33.6\%). Training code of FormaRL is open-sourced at https://github.com/THUNLP-MT/FormaRL.<br>
<span id='abs_ch'>中文: 本文提出FormaRL，一种用于自动形式化的强化学 框架，仅需少量  签数据即可显著提升准确率，在ProofNet和uproof数据集上验证了其有效性。</span><br>
<span id='abs_en'>English: This paper introduces FormaRL, a reinforcement learning framework for autoformalization that uses minimal unlabeled data and enhances accuracy significantly, as demonstrated on datasets like ProofNet and uproof.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>750, <a href='https://arxiv.org/pdf/2508.18850.pdf' target='_blank'>https://arxiv.org/pdf/2508.18850.pdf</a></span>   <span><a href='https://github.com/xinhao-luo/ClusterFusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhao Luo, Zihan Liu, Yangjie Zhou, Shihan Fang, Ziyu Huang, Yu Feng, Chen Zhang, Shixuan Sun, Zhenzhe Zheng, Jingwen Leng, Minyi Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18850">ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language model (LLM) decoding suffers from high latency due to fragmented execution across operators and heavy reliance on off-chip memory for data exchange and reduction. This execution model limits opportunities for fusion and incurs significant memory traffic and kernel launch overhead. While modern architectures such as NVIDIA Hopper provide distributed shared memory and low-latency intra-cluster interconnects, they expose only low-level data movement instructions, lacking structured abstractions for collective on-chip communication. To bridge this software-hardware gap, we introduce two cluster-level communication primitives, ClusterReduce and ClusterGather, which abstract common communication patterns and enable structured, high-speed data exchange and reduction between thread blocks within a cluster, allowing intermediate results to be on-chip without involving off-chip memory. Building on these abstractions, we design ClusterFusion, an execution framework that schedules communication and computation jointly to expand operator fusion scope by composing decoding stages such as QKV Projection, Attention, and Output Projection into a single fused kernels. Evaluations on H100 GPUs show that ClusterFusion outperforms state-of-the-art inference frameworks by 1.61x on average in end-to-end latency across different models and configurations. The source code is available at https://github.com/xinhao-luo/ClusterFusion.<br>
<span id='abs_ch'>中文摘要：ClusterFusion通过引入集群级通信原语和联合调度框架，扩展算子融合范围以减少大语言模型解 延迟，在H100 GPU上实现端到端性能平均提升1.61倍。</span><br>
<span id='abs_en'>English Summary: ClusterFusion introduces cluster-level communication primitives and a joint scheduling framework to reduce LLM decoding latency by expanding operator fusion, achieving 1.61x faster end-to-end performance on H100 GPUs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>751, <a href='https://arxiv.org/pdf/2508.18847.pdf' target='_blank'>https://arxiv.org/pdf/2508.18847.pdf</a></span>   <span><a href='https://github.com/liushiliushi/ConfTuner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yibo Li, Miao Xiong, Jiaying Wu, Bryan Hooi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18847">ConfTuner: Training Large Language Models to Express Their Confidence Verbally</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) are increasingly deployed in high-stakes domains such as science, law, and healthcare, where accurate expressions of uncertainty are essential for reliability and trust. However, current LLMs are often observed to generate incorrect answers with high confidence, a phenomenon known as "overconfidence". Recent efforts have focused on calibrating LLMs' verbalized confidence: i.e., their expressions of confidence in text form, such as "I am 80% confident that...". Existing approaches either rely on prompt engineering or fine-tuning with heuristically generated uncertainty estimates, both of which have limited effectiveness and generalizability. Motivated by the notion of proper scoring rules for calibration in classical machine learning models, we introduce ConfTuner, a simple and efficient fine-tuning method that introduces minimal overhead and does not require ground-truth confidence scores or proxy confidence estimates. ConfTuner relies on a new loss function, tokenized Brier score, which we theoretically prove to be a proper scoring rule, intuitively meaning that it "correctly incentivizes the model to report its true probability of being correct". ConfTuner improves calibration across diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our results further show that better-calibrated confidence enables downstream gains in self-correction and model cascade, advancing the development of trustworthy LLM systems. The code is available at https://github.com/liushiliushi/ConfTuner.<br>
<span id='abs_ch'>Chinese: ConfTuner是一种通过令牌化Brier评分损失函数来优化大语言模型置信度表达的微调方法， 需真实置信度 签即可提升模型在高风险领域中的 准效果和泛化能力。</span><br>
<span id='abs_en'>English: ConfTuner is a fine-tuning method that improves the calibration of Large Language Models' verbalized confidence using a tokenized Brier score loss, enhancing reliability in high-stakes domains without requiring ground-truth confidence estimates.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>752, <a href='https://arxiv.org/pdf/2508.18785.pdf' target='_blank'>https://arxiv.org/pdf/2508.18785.pdf</a></span>   <span><a href='https://github.com/GabrielleTse/EMind' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luqing Luo, Wenjin Gui, Yunfei Liu, Ziyue Zhang, Yunxi Zhang, Fengxiang Wang, Zonghao Guo, Zizhi Ma, Xinzhu Liu, Hanxiang He, Jinhai Li, Xin Qiu, Wupeng Xie, Yangang Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18785">EMind: A Foundation Model for Multi-task Electromagnetic Signals Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Deep understanding of electromagnetic signals is fundamental to dynamic spectrum management, intelligent transportation, autonomous driving and unmanned vehicle perception. The field faces challenges because electromagnetic signals differ greatly from text and images, showing high heterogeneity, strong background noise and complex joint time frequency structure, which prevents existing general models from direct use. Electromagnetic communication and sensing tasks are diverse, current methods lack cross task generalization and transfer efficiency, and the scarcity of large high quality datasets blocks the creation of a truly general multitask learning framework. To overcome these issue, we introduce EMind, an electromagnetic signals foundation model that bridges large scale pretraining and the unique nature of this modality. We build the first unified and largest standardized electromagnetic signal dataset covering multiple signal types and tasks. By exploiting the physical properties of electromagnetic signals, we devise a length adaptive multi-signal packing method and a hardware-aware training strategy that enable efficient use and representation learning from heterogeneous multi-source signals. Experiments show that EMind achieves strong performance and broad generalization across many downstream tasks, moving decisively from task specific models to a unified framework for electromagnetic intelligence. The code is available at: https://github.com/GabrielleTse/EMind.<br>
<span id='abs_ch'>中文摘要：EMind基础模型通过构建统一数据集和采用自适应训练策略，有效解决了电磁信号处理中的难题，并在多种下游任务中展现出卓越的泛化能力。</span><br>
<span id='abs_en'>English Summary: The EMind foundation model addresses challenges in electromagnetic signal analysis by introducing a unified dataset and innovative training strategies, achieving strong generalization across various tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>753, <a href='https://arxiv.org/pdf/2508.18763.pdf' target='_blank'>https://arxiv.org/pdf/2508.18763.pdf</a></span>   <span><a href='https://github.com/Fanye12/DDS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Hao, Zezheng Wang, Yanhua Huang, Ruiwen Xu, Wenzhe Niu, Xin Liu, Zitong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18763">Dynamic Collaboration of Multi-Language Models based on Minimal Complete Semantic Units</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper investigates the enhancement of reasoning capabilities in language models through token-level multi-model collaboration. Our approach selects the optimal tokens from the next token distributions provided by multiple models to perform autoregressive reasoning. Contrary to the assumption that more models yield better results, we introduce a distribution distance-based dynamic selection strategy (DDS) to optimize the multi-model collaboration process. To address the critical challenge of vocabulary misalignment in multi-model collaboration, we propose the concept of minimal complete semantic units (MCSU), which is simple yet enables multiple language models to achieve natural alignment within the linguistic space. Experimental results across various benchmarks demonstrate the superiority of our method. The code will be available at https://github.com/Fanye12/DDS.<br>
<span id='abs_ch'>中文: 本文提出动态选择策略和最小语义单元概念，通过优化多模型在词汇层面的协作来增强语言模型的推理能力，在多个基准测试中展现了优越性能。</span><br>
<span id='abs_en'>English: This paper introduces a dynamic selection strategy and minimal semantic units to enhance reasoning in language models by optimizing token-level collaboration among multiple models, achieving superior performance across benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>754, <a href='https://arxiv.org/pdf/2508.18751.pdf' target='_blank'>https://arxiv.org/pdf/2508.18751.pdf</a></span>   <span><a href='https://github.com/powerpowe/PAF-KIP-OSTTA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Byung-Joon Lee, Jin-Seop Lee, Jee-Hyong Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18751">Stabilizing Open-Set Test-Time Adaptation via Primary-Auxiliary Filtering and Knowledge-Integrated Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Deep neural networks demonstrate strong performance under aligned training-test distributions. However, real-world test data often exhibit domain shifts. Test-Time Adaptation (TTA) addresses this challenge by adapting the model to test data during inference. While most TTA studies assume that the training and test data share the same class set (closed-set TTA), real-world scenarios often involve open-set data (open-set TTA), which can degrade closed-set accuracy. A recent study showed that identifying open-set data during adaptation and maximizing its entropy is an effective solution. However, the previous method relies on the source model for filtering, resulting in suboptimal filtering accuracy on domain-shifted test data. In contrast, we found that the adapting model, which learns domain knowledge from noisy test streams, tends to be unstable and leads to error accumulation when used for filtering. To address this problem, we propose Primary-Auxiliary Filtering (PAF), which employs an auxiliary filter to validate data filtered by the primary filter. Furthermore, we propose Knowledge-Integrated Prediction (KIP), which calibrates the outputs of the adapting model, EMA model, and source model to integrate their complementary knowledge for OSTTA. We validate our approach across diverse closed-set and open-set datasets. Our method enhances both closed-set accuracy and open-set discrimination over existing methods. The code is available at https://github.com/powerpowe/PAF-KIP-OSTTA .<br>
<span id='abs_ch'>中文摘要：本文提出的主辅助过滤机制和知识集成预测方法，通过提升数据筛选精度并融合多模型互补知识，有效解决了开放集测试时适应中的性能退化问题，在闭集精度和开放集识别方面均优于现有方法。</span><br>
<span id='abs_en'>English Summary: This paper introduces Primary-Auxiliary Filtering (PAF) and Knowledge-Integrated Prediction (KIP) to improve open-set test-time adaptation by enhancing data filtering accuracy and integrating complementary knowledge from multiple models, achieving superior performance in both closed-set accuracy and open-set discrimination.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>755, <a href='https://arxiv.org/pdf/2508.18740.pdf' target='_blank'>https://arxiv.org/pdf/2508.18740.pdf</a></span>   <span><a href='https://github.com/redifinition/M3HG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiao Liang, Ying Shen, Tiantian Chen, Lin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18740">M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has recently gained significant attention in social media analysis, aiming to extract emotion utterances, cause utterances, and emotion categories simultaneously. However, the scarcity of related datasets, with only one published dataset featuring highly uniform dialogue scenarios, hinders model development in this field. To address this, we introduce MECAD, the first multimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56 TV series spanning a wide range of dialogue contexts. In addition, existing MECTEC methods fail to explicitly model emotional and causal contexts and neglect the fusion of semantic information at different levels, leading to performance degradation. In this paper, we propose M3HG, a novel model that explicitly captures emotional and causal contexts and effectively fuses contextual information at both inter- and intra-utterance levels via a multimodal heterogeneous graph. Extensive experiments demonstrate the effectiveness of M3HG compared with existing state-of-the-art methods. The codes and dataset are available at https://github.com/redifinition/M3HG.<br>
<br>
<div id='section'>Paperid: <span id='pid'>756, <a href='https://arxiv.org/pdf/2508.18694.pdf' target='_blank'>https://arxiv.org/pdf/2508.18694.pdf</a></span>   <span><a href='https://github.com/StructuresComp/agri-chrono' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaehwan Jeong, Tuan-Anh Vu, Mohammad Jony, Shahab Ahmad, Md. Mukhlesur Rahman, Sangpil Kim, M. Khalid Jawed
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18694">AgriChrono: A Multi-modal Dataset Capturing Crop Growth and Lighting Variability with a Field Robot</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Existing datasets for precision agriculture have primarily been collected in static or controlled environments such as indoor labs or greenhouses, often with limited sensor diversity and restricted temporal span. These conditions fail to reflect the dynamic nature of real farmland, including illumination changes, crop growth variation, and natural disturbances. As a result, models trained on such data often lack robustness and generalization when applied to real-world field scenarios. In this paper, we present AgriChrono, a novel robotic data collection platform and multi-modal dataset designed to capture the dynamic conditions of real-world agricultural environments. Our platform integrates multiple sensors and enables remote, time-synchronized acquisition of RGB, Depth, LiDAR, and IMU data, supporting efficient and repeatable long-term data collection across varying illumination and crop growth stages. We benchmark a range of state-of-the-art 3D reconstruction models on the AgriChrono dataset, highlighting the difficulty of reconstruction in real-world field environments and demonstrating its value as a research asset for advancing model generalization under dynamic conditions. The code and dataset are publicly available at: https://github.com/StructuresComp/agri-chrono<br>
<span id='abs_ch'>中文摘要：AgriChrono数据集通过多 感器机器人平台克服了现有农业数据集的局限性，能够捕捉真实农田的动态环境条件，为三维重建模型的鲁棒性评估和泛化能力 究提供了重要资源。</span><br>
<span id='abs_en'>English Summary: The AgriChrono dataset addresses limitations of existing agricultural datasets by capturing dynamic real-world field conditions through a multi-sensor robotic platform, enabling robust 3D reconstruction model evaluation and advancing generalization research in precision agriculture.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>757, <a href='https://arxiv.org/pdf/2508.18689.pdf' target='_blank'>https://arxiv.org/pdf/2508.18689.pdf</a></span>   <span><a href='https://github.com/LaoKuiZe/AppAgent-Pro' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuyang Zhao, Wentao Shi, Fuli Feng, Xiangnan He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18689">AppAgent-Pro: A Proactive GUI Agent System for Multidomain Information Integration and User Assistance</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language model (LLM)-based agents have demonstrated remarkable capabilities in addressing complex tasks, thereby enabling more advanced information retrieval and supporting deeper, more sophisticated human information-seeking behaviors. However, most existing agents operate in a purely reactive manner, responding passively to user instructions, which significantly constrains their effectiveness and efficiency as general-purpose platforms for information acquisition. To overcome this limitation, this paper proposes AppAgent-Pro, a proactive GUI agent system that actively integrates multi-domain information based on user instructions. This approach enables the system to proactively anticipate users' underlying needs and conduct in-depth multi-domain information mining, thereby facilitating the acquisition of more comprehensive and intelligent information. AppAgent-Pro has the potential to fundamentally redefine information acquisition in daily life, leading to a profound impact on human society. Our code is available at: https://github.com/LaoKuiZe/AppAgent-Pro. The demonstration video could be found at: https://www.dropbox.com/scl/fi/hvzqo5vnusg66srydzixo/AppAgent-Pro-demo-video.mp4?rlkey=o2nlfqgq6ihl125mcqg7bpgqu&st=d29vrzii&dl=0.<br>
<span id='abs_ch'>中文: AppAgent-Pro是一种主动式GUI代理系统，能够预测用户的潜在需求并进行跨领域信息挖掘，从而突 被动响应模式的限制，实现更全面智能的信息获取。English: AppAgent-Pro is a proactive GUI agent system that anticipates users' underlying needs and conducts multi-domain information mining to enable more comprehensive and intelligent information acquisition, moving beyond the limitations of reactive approaches.Paperid:188,https://arxiv.org/pdf/2508.18672.pdfGitHub</span><br>
<span id='abs_en'>English: AppAgent-Pro is a proactive GUI agent system that anticipates users' underlying needs and conducts multi-domain information mining to enable more comprehensive and intelligent information acquisition, moving beyond the limitations of reactive approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>758, <a href='https://arxiv.org/pdf/2508.18672.pdf' target='_blank'>https://arxiv.org/pdf/2508.18672.pdf</a></span>   <span><a href='https://github.com/rioyokotalab/optimal-sparsity' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taishi Nakamura, Satoki Ishikawa, Masaki Kawamura, Takumi Okamoto, Daisuke Nohara, Jun Suzuki, Rio Yokota
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18672">Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Empirical scaling laws have driven the evolution of large language models (LLMs), yet their coefficients shift whenever the model architecture or data pipeline changes. Mixture-of-Experts (MoE) models, now standard in state-of-the-art systems, introduce a new sparsity dimension that current dense-model frontiers overlook. We investigate how MoE sparsity influences two distinct capability regimes: memorization skills and reasoning skills. By training MoE families that vary total parameters, active parameters, and top-$k$ routing under fixed compute budgets, we disentangle pre-training loss from downstream accuracy. Our results reveal two principles. First, Active FLOPs: models with identical training loss but greater active compute achieve higher reasoning accuracy. Second, Total tokens per parameter (TPP): memorization tasks improve with more parameters, while reasoning tasks benefit from optimal TPP, indicating that reasoning is data-hungry. Neither reinforcement learning post-training (GRPO) nor increased test-time compute alters these trends. We therefore argue that optimal MoE sparsity must be determined jointly by active FLOPs and TPP, revising the classical picture of compute-optimal scaling. Our model checkpoints, code and logs are open-source at https://github.com/rioyokotalab/optimal-sparsity.<br>
<span id='abs_ch'>中文:  究表明，专家混合模型的最优扩展取决于用于推理精度的有效计算量和用于记忆任务的总参数令牌比，从而修正了 统的计算最优扩展理论。</span><br>
<span id='abs_en'>English: This study demonstrates that optimal scaling for Mixture-of-Experts models depends on active FLOPs for reasoning accuracy and total tokens per parameter for memorization, revising traditional compute-optimal scaling principles.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>759, <a href='https://arxiv.org/pdf/2508.18649.pdf' target='_blank'>https://arxiv.org/pdf/2508.18649.pdf</a></span>   <span><a href='https://github.com/SaFoLab-WISC/PRISM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nanxi Li, Zhengyue Zhao, Chaowei Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18649">PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Safeguarding vision-language models (VLMs) is a critical challenge, as existing methods often suffer from over-defense, which harms utility, or rely on shallow alignment, failing to detect complex threats that require deep reasoning. To this end, we introduce PRISM (Principled Reasoning for Integrated Safety in Multimodality), a system2-like framework that aligns VLMs by embedding a structured, safety-aware reasoning process. Our framework consists of two key components: PRISM-CoT, a dataset that teaches safety-aware chain-of-thought reasoning, and PRISM-DPO, generated via Monte Carlo Tree Search (MCTS) to further refine this reasoning through Direct Preference Optimization to help obtain a delicate safety boundary. Comprehensive evaluations demonstrate PRISM's effectiveness, achieving remarkably low attack success rates including 0.15% on JailbreakV-28K for Qwen2-VL and 90% improvement over the previous best method on VLBreak for LLaVA-1.5. PRISM also exhibits strong robustness against adaptive attacks, significantly increasing computational costs for adversaries, and generalizes effectively to out-of-distribution challenges, reducing attack success rates to just 8.70% on the challenging multi-image MIS benchmark. Remarkably, this robust defense is achieved while preserving, and in some cases enhancing, model utility. To promote reproducibility, we have made our code, data, and model weights available at https://github.com/SaFoLab-WISC/PRISM.<br>
<span id='abs_ch'>中文: PRISM是一个创新框架，通过嵌入结构化推理过程来增强视觉语言模型的安全性，在保持甚至提升模型实用性的同时，实现了对复杂威胁的强大防御能力。</span><br>
<span id='abs_en'>English: PRISM is a novel framework that enhances the safety of vision-language models by embedding structured reasoning processes, achieving robust defense against complex threats while maintaining or even improving model utility.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>760, <a href='https://arxiv.org/pdf/2508.18646.pdf' target='_blank'>https://arxiv.org/pdf/2508.18646.pdf</a></span>   <span><a href='https://github.com/onejune2018/Awesome-LLM-Eval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Wang, Ninglun Gu, Kailai Zhang, Zijiao Zhang, Yelun Bao, Jin Yang, Xu Yin, Liwei Liu, Yihuan Liu, Pengyong Li, Gary G. Yen, Junchi Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18646">Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>For Large Language Models (LLMs), a disconnect persists between benchmark performance and real-world utility. Current evaluation frameworks remain fragmented, prioritizing technical metrics while neglecting holistic assessment for deployment. This survey introduces an anthropomorphic evaluation paradigm through the lens of human intelligence, proposing a novel three-dimensional taxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational capacity, Emotional Quotient (EQ)-Alignment Ability for value-based interactions, and Professional Quotient (PQ)-Professional Expertise for specialized proficiency. For practical value, we pioneer a Value-oriented Evaluation (VQ) framework assessing economic viability, social impact, ethical alignment, and environmental sustainability. Our modular architecture integrates six components with an implementation roadmap. Through analysis of 200+ benchmarks, we identify key challenges including dynamic assessment needs and interpretability gaps. It provides actionable guidance for developing LLMs that are technically proficient, contextually relevant, and ethically sound. We maintain a curated repository of open-source evaluation resources at: https://github.com/onejune2018/Awesome-LLM-Eval.<br>
<span id='abs_ch'>中文摘要：本 究提出了一种拟人化评估范式，通过三维分类法（智商、情商、专业商）和价值导向框架来解决大语言模型基准测试与实际应用之间的脱节问题，同时提供可实施的指导方案。</span><br>
<span id='abs_en'>English Summary: This survey proposes an anthropomorphic evaluation paradigm for LLMs using a three-dimensional taxonomy (IQ, EQ, PQ) and a value-oriented framework to address the gap between benchmark performance and real-world utility, while providing practical implementation guidance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>761, <a href='https://arxiv.org/pdf/2508.18462.pdf' target='_blank'>https://arxiv.org/pdf/2508.18462.pdf</a></span>   <span><a href='https://github.com/omniAI-Lab/VeriRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fu Teng, Miao Pan, Xuhong Zhang, Zhezhi He, Yiyao Yang, Xinyi Chai, Mengnan Qi, Liqiang Lu, Jianwei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18462">VERIRL: Boosting the LLM-based Verilog Code Generation via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in code generation have shown remarkable success across software domains, yet hardware description languages (HDLs) such as Verilog remain underexplored due to their concurrency semantics, syntactic rigidity, and simulation complexity. In this work, we address these challenges by introducing a reinforcement learning (RL) framework tailored for Verilog code generation. We first construct Veribench-53K, a high-quality dataset curated from over 700K Verilog problems, enriched with structured prompts, complexity labels, and diverse testbenches. To tackle the problem of sparse and noisy reward signals, we propose a Trace-back based Rescore mechanism that leverages reasoning paths and iterative refinement to enhance feedback reliability and support reward model training. Furthermore, to mitigate catastrophic forgetting and overfitting during RL fine-tuning, we introduce a sample-balanced weighting strategy that adaptively balances learning dynamics based on reward-probability distributions. These innovations are integrated into an iterative RL pipeline that co-evolves the policy and reward models. In contrast to recent work such as CraftRTL, which relies on large-scale closed-source model distillation, and DeepSeek-style approaches that struggle with sparse feedback, our method demonstrates superior performance using a smaller but high-quality dataset combined with RL optimization. Experiments on Verilog generation tasks demonstrate state-of-the-art performance, with substantial gains in test pass rate, functional correctness, and compilation robustness. Our findings highlight the potential of RL-driven approaches for structured code generation in hardware-centric domains. VERIRL is publicly available at https://github.com/omniAI-Lab/VeriRL.<br>
<span id='abs_ch'>中文: 本 究提出了一种针对Verilog代 生成的强化学 框架，通过精选数据集和创新机制提升反馈与训练效果，在硬件描述任务中实现了领先性能。</span><br>
<span id='abs_en'>English: This research introduces a reinforcement learning framework for Verilog code generation, utilizing a curated dataset and innovative mechanisms to improve feedback and training, achieving state-of-the-art performance in hardware description tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>762, <a href='https://arxiv.org/pdf/2508.18440.pdf' target='_blank'>https://arxiv.org/pdf/2508.18440.pdf</a></span>   <span><a href='https://github.com/lars76/swift-f0,' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/lars76/pitch-benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lars Nieradzik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18440">SwiftF0: Fast and Accurate Monophonic Pitch Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate and real-time monophonic pitch estimation in noisy conditions, particularly on resource-constrained devices, remains an open challenge in audio processing. We present \emph{SwiftF0}, a novel, lightweight neural model that sets a new state-of-the-art for monophonic pitch estimation. Through training on diverse speech, music, and synthetic datasets with extensive data augmentation, SwiftF0 achieves robust generalization across acoustic domains while maintaining computational efficiency. SwiftF0 achieves a 91.80\% harmonic mean (HM) at 10 dB SNR, outperforming baselines like CREPE by over 12 percentage points and degrading by only 2.3 points from clean audio. SwiftF0 requires only 95,842 parameters and runs approximately 42x faster than CREPE on CPU, making it ideal for efficient, real-time deployment. To address the critical lack of perfectly accurate ground truth pitch in speech corpora (which typically rely on algorithmic estimators or laryngograph signals), we introduce \emph{SpeechSynth}. This synthetic speech dataset, generated by a phoneme-level TTS model, provides exact, on-demand ground-truth pitch curves, enabling more robust model training and evaluation. Furthermore, we propose a unified metric, combining six complementary performance measures for comprehensive and reliable pitch evaluation, and release an open-source pitch benchmark suite. A live demo of SwiftF0 is available at https://swift-f0.github.io/, the source code at https://github.com/lars76/swift-f0, and the benchmark framework at https://github.com/lars76/pitch-benchmark.<br>
<br>
<div id='section'>Paperid: <span id='pid'>763, <a href='https://arxiv.org/pdf/2508.18321.pdf' target='_blank'>https://arxiv.org/pdf/2508.18321.pdf</a></span>   <span><a href='https://github.com/declare-lab/KAIROS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maojia Song, Tej Deep Pala, Weisheng Jin, Amir Zadeh, Chuan Li, Dorien Herremans, Soujanya Poria
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18321">LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are increasingly deployed in multi-agent systems (MAS) as components of collaborative intelligence, where peer interactions dynamically shape individual decision-making. Although prior work has focused on conformity bias, we extend the analysis to examine how LLMs form trust from previous impressions, resist misinformation, and integrate peer input during interaction, key factors for achieving collective intelligence under complex social dynamics. We present KAIROS, a benchmark simulating quiz contests with peer agents of varying reliability, offering fine-grained control over conditions such as expert-novice roles, noisy crowds, and adversarial peers. LLMs receive both historical interactions and current peer responses, allowing systematic investigation into how trust, peer action, and self-confidence influence decisions. As for mitigation strategies, we evaluate prompting, supervised fine-tuning, and reinforcement learning, Group Relative Policy Optimisation (GRPO), across multiple models. Our results reveal that GRPO with multi-agent context combined with outcome-based rewards and unconstrained reasoning achieves the best overall performance, but also decreases the robustness to social influence compared to Base models. The code and datasets are available at: https://github.com/declare-lab/KAIROS.<br>
<span id='abs_ch'>中文摘要：本 究提出KAIROS基准，通过模拟不同可 性智能体的问答竞赛，系统分析大语言模型在多方互动中如何建立信任、抵制错误信息并整合同伴意见，发现结合多智能体情境的群组相对策略优化能实现最佳性能，但会降低对社会影响的鲁棒性。</span><br>
<span id='abs_en'>English Summary: The study introduces KAIROS, a benchmark to analyze how LLMs develop trust, counter misinformation, and integrate peer input in multi-agent systems, finding that Group Relative Policy Optimisation with multi-agent context yields optimal performance but reduces social influence robustness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>764, <a href='https://arxiv.org/pdf/2508.18303.pdf' target='_blank'>https://arxiv.org/pdf/2508.18303.pdf</a></span>   <span><a href='https://github.com/jueqiw/NeuroPathX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jueqi Wang, Zachary Jacokes, John Darrell Van Horn, Michael C. Schatz, Kevin A. Pelphrey, Archana Venkataraman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18303">Learning Explainable Imaging-Genetics Associations Related to a Neurological Disorder</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While imaging-genetics holds great promise for unraveling the complex interplay between brain structure and genetic variation in neurological disorders, traditional methods are limited to simplistic linear models or to black-box techniques that lack interpretability. In this paper, we present NeuroPathX, an explainable deep learning framework that uses an early fusion strategy powered by cross-attention mechanisms to capture meaningful interactions between structural variations in the brain derived from MRI and established biological pathways derived from genetics data. To enhance interpretability and robustness, we introduce two loss functions over the attention matrix - a sparsity loss that focuses on the most salient interactions and a pathway similarity loss that enforces consistent representations across the cohort. We validate NeuroPathX on both autism spectrum disorder and Alzheimer's disease. Our results demonstrate that NeuroPathX outperforms competing baseline approaches and reveals biologically plausible associations linked to the disorder. These findings underscore the potential of NeuroPathX to advance our understanding of complex brain disorders. Code is available at https://github.com/jueqiw/NeuroPathX .<br>
<span id='abs_ch'>中文: NeuroPathX是一种可解释的深度学 框架，通过交叉注意力机制整合MRI脑结构数据与遗 信息，在自闭症和阿尔茨海默症 究中优于现有方法，揭示了与疾病相关的生物学关联。</span><br>
<span id='abs_en'>English: NeuroPathX is an explainable deep learning framework that integrates MRI-derived brain structure and genetic data through cross-attention mechanisms, outperforming existing methods in identifying biologically relevant associations for neurological disorders like autism and Alzheimer's.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>765, <a href='https://arxiv.org/pdf/2508.18190.pdf' target='_blank'>https://arxiv.org/pdf/2508.18190.pdf</a></span>   <span><a href='https://github.com/weAIDB/ST-Raptor' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/weAIDB/ST-Raptor' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zirui Tang, Boyu Niu, Xuanhe Zhou, Boxiu Li, Wei Zhou, Jiannan Wang, Guoliang Li, Xinyi Zhang, Fan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18190">ST-Raptor: LLM-Powered Semi-Structured Table Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Semi-structured tables, widely used in real-world applications (e.g., financial reports, medical records, transactional orders), often involve flexible and complex layouts (e.g., hierarchical headers and merged cells). These tables generally rely on human analysts to interpret table layouts and answer relevant natural language questions, which is costly and inefficient. To automate the procedure, existing methods face significant challenges. First, methods like NL2SQL require converting semi-structured tables into structured ones, which often causes substantial information loss. Second, methods like NL2Code and multi-modal LLM QA struggle to understand the complex layouts of semi-structured tables and cannot accurately answer corresponding questions. To this end, we propose ST-Raptor, a tree-based framework for semi-structured table question answering using large language models. First, we introduce the Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures complex semi-structured table layouts, along with an effective algorithm for constructing the tree. Second, we define a set of basic tree operations to guide LLMs in executing common QA tasks. Given a user question, ST-Raptor decomposes it into simpler sub-questions, generates corresponding tree operation pipelines, and conducts operation-table alignment for accurate pipeline execution. Third, we incorporate a two-stage verification mechanism: forward validation checks the correctness of execution steps, while backward validation evaluates answer reliability by reconstructing queries from predicted answers. To benchmark the performance, we present SSTQA, a dataset of 764 questions over 102 real-world semi-structured tables. Experiments show that ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code is available at https://github.com/weAIDB/ST-Raptor.<br>
<span id='abs_ch'>中文：ST-Raptor是一种基于 的框架，利用大型语言模型通过将查询分解为 操作并结合验证机制，准确回答半结构化表 的问题，其答案准确率比现有方法高出高达20%。</span><br>
<span id='abs_en'>English: ST-Raptor is a tree-based framework using large language models to accurately answer questions on semi-structured tables by decomposing queries into tree operations and employing verification mechanisms, outperforming existing methods by up to 20% in accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>766, <a href='https://arxiv.org/pdf/2508.18124.pdf' target='_blank'>https://arxiv.org/pdf/2508.18124.pdf</a></span>   <span><a href='https://github.com/CMPhysBench/CMPhysBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weida Wang, Dongchen Huang, Jiatong Li, Tengchao Yang, Ziyang Zheng, Di Zhang, Dong Han, Benteng Chen, Binzhao Luo, Zhiyu Liu, Kunling Liu, Zhiyuan Gao, Shiqi Geng, Wei Ma, Jiaming Su, Xin Li, Shuchen Pu, Yuhan Shui, Qianjia Cheng, Zhihao Dou, Dongfei Cui, Changyong He, Jin Zeng, Zeke Xie, Mao Su, Dongzhan Zhou, Yuqiang Li, Wanli Ouyang, Yunqi Cai, Xi Dai, Shufei Zhang, Lei Bai, Jinguang Cheng, Zhong Fang, Hongming Weng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18124">CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce CMPhysBench, designed to assess the proficiency of Large Language Models (LLMs) in Condensed Matter Physics, as a novel Benchmark. CMPhysBench is composed of more than 520 graduate-level meticulously curated questions covering both representative subfields and foundational theoretical frameworks of condensed matter physics, such as magnetism, superconductivity, strongly correlated systems, etc. To ensure a deep understanding of the problem-solving process,we focus exclusively on calculation problems, requiring LLMs to independently generate comprehensive solutions. Meanwhile, leveraging tree-based representations of expressions, we introduce the Scalable Expression Edit Distance (SEED) score, which provides fine-grained (non-binary) partial credit and yields a more accurate assessment of similarity between prediction and ground-truth. Our results show that even the best models, Grok-4, reach only 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a significant capability gap, especially for this practical and frontier domain relative to traditional physics. The code anddataset are publicly available at https://github.com/CMPhysBench/CMPhysBench.<br>
<span id='abs_ch'>Chinese: CMPhysBench是一个包含520多道 究生水平计算题的新基准，用于评估大语言模型在凝聚态物理中的能力，引入了SEED评分进行细粒度评估，结果显示即使像Grok-4这 的顶级模型也表现不佳，平均SEED得分仅36，准确率仅28%。</span>English: CMPhysBench is a new benchmark with over 520 graduate-level calculation problems to evaluate Large Language Models' proficiency in condensed matter physics, introducing the SEED score for fine-grained assessment and revealing that even top models like Grok-4 perform poorly with only 36 average SEED score and 28% accuracy.Paperid:207,https://arxiv.org/pdf/2508.18118.pdfGitHub</span><br>
<span id='abs_en'>English: CMPhysBench is a new benchmark with over 520 graduate-level calculation problems to evaluate Large Language Models' proficiency in condensed matter physics, introducing the SEED score for fine-grained assessment and revealing that even top models like Grok-4 perform poorly with only 36 average SEED score and 28% accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>767, <a href='https://arxiv.org/pdf/2508.18124.pdf' target='_blank'>https://arxiv.org/pdf/2508.18124.pdf</a></span>   <span><a href='https://github.com/CMPhysBench/CMPhysBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weida Wang, Dongchen Huang, Jiatong Li, Tengchao Yang, Ziyang Zheng, Di Zhang, Dong Han, Benteng Chen, Binzhao Luo, Zhiyu Liu, Kunling Liu, Zhiyuan Gao, Shiqi Geng, Wei Ma, Jiaming Su, Xin Li, Shuchen Pu, Yuhan Shui, Qianjia Cheng, Zhihao Dou, Dongfei Cui, Changyong He, Jin Zeng, Zeke Xie, Mao Su, Dongzhan Zhou, Yuqiang Li, Wanli Ouyang, Yunqi Cai, Xi Dai, Shufei Zhang, Lei Bai, Jinguang Cheng, Zhong Fang, Hongming Weng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18124">CMPhysBench: A Benchmark for Evaluating Large Language Models in Condensed Matter Physics</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce CMPhysBench, designed to assess the proficiency of Large Language Models (LLMs) in Condensed Matter Physics, as a novel Benchmark. CMPhysBench is composed of more than 520 graduate-level meticulously curated questions covering both representative subfields and foundational theoretical frameworks of condensed matter physics, such as magnetism, superconductivity, strongly correlated systems, etc. To ensure a deep understanding of the problem-solving process,we focus exclusively on calculation problems, requiring LLMs to independently generate comprehensive solutions. Meanwhile, leveraging tree-based representations of expressions, we introduce the Scalable Expression Edit Distance (SEED) score, which provides fine-grained (non-binary) partial credit and yields a more accurate assessment of similarity between prediction and ground-truth. Our results show that even the best models, Grok-4, reach only 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a significant capability gap, especially for this practical and frontier domain relative to traditional physics. The code anddataset are publicly available at https://github.com/CMPhysBench/CMPhysBench.<br>
<span id='abs_ch'>Chinese: CMPhysBench是一个包含520多道 究生水平计算题的新基准，用于评估大语言模型在凝聚态物理中的能力，引入了SEED评分进行细粒度评估，结果显示即使像Grok-4这 的顶级模型也表现不佳，平均SEED得分仅36，准确率仅28%。</span>English: CMPhysBench is a new benchmark with over 520 graduate-level calculation problems to evaluate Large Language Models' proficiency in condensed matter physics, introducing the SEED score for fine-grained assessment and revealing that even top models like Grok-4 perform poorly with only 36 average SEED score and 28% accuracy.Paperid:207,https://arxiv.org/pdf/2508.18118.pdfGitHub</span><br>
<span id='abs_en'>English: CMPhysBench is a new benchmark with over 520 graduate-level calculation problems to evaluate Large Language Models' proficiency in condensed matter physics, introducing the SEED score for fine-grained assessment and revealing that even top models like Grok-4 perform poorly with only 36 average SEED score and 28% accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>768, <a href='https://arxiv.org/pdf/2508.18066.pdf' target='_blank'>https://arxiv.org/pdf/2508.18066.pdf</a></span>   <span><a href='https://github.com/amathislab/arnold-the-generalist' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alberto Silvio Chiappa, Boshi An, Merkourios Simos, Chengkun Li, Alexander Mathis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18066">Arnold: a generalist muscle transformer policy</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Controlling high-dimensional and nonlinear musculoskeletal models of the human body is a foundational scientific challenge. Recent machine learning breakthroughs have heralded policies that master individual skills like reaching, object manipulation and locomotion in musculoskeletal systems with many degrees of freedom. However, these agents are merely "specialists", achieving high performance for a single skill. In this work, we develop Arnold, a generalist policy that masters multiple tasks and embodiments. Arnold combines behavior cloning and fine-tuning with PPO to achieve expert or super-expert performance in 14 challenging control tasks from dexterous object manipulation to locomotion. A key innovation is Arnold's sensorimotor vocabulary, a compositional representation of the semantics of heterogeneous sensory modalities, objectives, and actuators. Arnold leverages this vocabulary via a transformer architecture to deal with the variable observation and action spaces of each task. This framework supports efficient multi-task, multi-embodiment learning and facilitates rapid adaptation to novel tasks. Finally, we analyze Arnold to provide insights into biological motor control, corroborating recent findings on the limited transferability of muscle synergies across tasks.<br>
<span id='abs_ch'>Chinese: Arnold是一种通用策略，通过感觉运动词汇和Transformer架构掌握多项任务和体现方式，在14项挑战性控制任务中达到专家级表现，并为生物运动控制 究提供了新见解。</span><br>
<span id='abs_en'>English: Arnold is a generalist policy that masters multiple tasks and embodiments using a sensorimotor vocabulary and transformer architecture, achieving expert performance in 14 challenging control tasks while providing insights into biological motor control.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>769, <a href='https://arxiv.org/pdf/2508.18040.pdf' target='_blank'>https://arxiv.org/pdf/2508.18040.pdf</a></span>   <span><a href='https://github.com/xinwang-nwpu/PerPilot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Wang, Zhiyao Cui, Hao Li, Ya Zeng, Chenxu Wang, Ruiqi Song, Yihang Chen, Kun Shao, Qiaosheng Zhang, Jinzhuo Liu, Siyue Ren, Shuyue Hu, Zhen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.18040">PerPilot: Personalizing VLM-based Mobile Agents via Memory and Exploration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision language model (VLM)-based mobile agents show great potential for assisting users in performing instruction-driven tasks. However, these agents typically struggle with personalized instructions -- those containing ambiguous, user-specific context -- a challenge that has been largely overlooked in previous research. In this paper, we define personalized instructions and introduce PerInstruct, a novel human-annotated dataset covering diverse personalized instructions across various mobile scenarios. Furthermore, given the limited personalization capabilities of existing mobile agents, we propose PerPilot, a plug-and-play framework powered by large language models (LLMs) that enables mobile agents to autonomously perceive, understand, and execute personalized user instructions. PerPilot identifies personalized elements and autonomously completes instructions via two complementary approaches: memory-based retrieval and reasoning-based exploration. Experimental results demonstrate that PerPilot effectively handles personalized tasks with minimal user intervention and progressively improves its performance with continued use, underscoring the importance of personalization-aware reasoning for next-generation mobile agents. The dataset and code are available at: https://github.com/xinwang-nwpu/PerPilot<br>
<span id='abs_ch'>Chinese: PerPilot作为一种即插即用框架，通过记忆检索和推理探索使移动智能体能够自主处理个性化指令，在极少用户干预下显著提升任务执行效果。</span><br>
<span id='abs_en'>English: PerPilot is a plug-and-play framework that enables mobile agents to autonomously handle personalized instructions through memory retrieval and reasoning-based exploration, significantly improving task execution with minimal user intervention.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>770, <a href='https://arxiv.org/pdf/2508.17857.pdf' target='_blank'>https://arxiv.org/pdf/2508.17857.pdf</a></span>   <span><a href='https://github.com/mobiushy/VISA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengfei Jiang, Hanjun Li, Linglan Zhao, Fei Chao, Ke Yan, Shouhong Ding, Rongrong Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17857">VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this study, we introduce a novel method called group-wise \textbf{VI}sual token \textbf{S}election and \textbf{A}ggregation (VISA) to address the issue of inefficient inference stemming from excessive visual tokens in multimoal large language models (MLLMs). Compared with previous token pruning approaches, our method can preserve more visual information while compressing visual tokens. We first propose a graph-based visual token aggregation (VTA) module. VTA treats each visual token as a node, forming a graph based on semantic similarity among visual tokens. It then aggregates information from removed tokens into kept tokens based on this graph, producing a more compact visual token representation. Additionally, we introduce a group-wise token selection strategy (GTS) to divide visual tokens into kept and removed ones, guided by text tokens from the final layers of each group. This strategy progressively aggregates visual information, enhancing the stability of the visual information extraction process. We conduct comprehensive experiments on LLaVA-1.5, LLaVA-NeXT, and Video-LLaVA across various benchmarks to validate the efficacy of VISA. Our method consistently outperforms previous methods, achieving a superior trade-off between model performance and inference speed. The code is available at https://github.com/mobiushy/VISA.<br>
<span id='abs_ch'>中文摘要：本 究提出VISA新方法，通过分组选择和基于图的聚合策略压缩视觉令牌，在保持更多视觉信息的同时提升多模态大语言模型的推理效率与性能平衡。</span><br>
<span id='abs_en'>English Summary: This study introduces VISA, a novel method that enhances multimodal large language models by efficiently compressing visual tokens through group-wise selection and graph-based aggregation, achieving superior performance and faster inference speeds.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>771, <a href='https://arxiv.org/pdf/2508.17825.pdf' target='_blank'>https://arxiv.org/pdf/2508.17825.pdf</a></span>   <span><a href='https://github.com/Anonymous999-xxx/FairGamer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bingkang Shi, Jen-tse Huang, Guoyi Li, Xiaodan Zhang, Zhongjiang Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17825">FAIRGAMER: Evaluating Biases in the Application of Large Language Models to Video Games</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Leveraging their advanced capabilities, Large Language Models (LLMs) demonstrate vast application potential in video games--from dynamic scene generation and intelligent NPC interactions to adaptive opponents--replacing or enhancing traditional game mechanics. However, LLMs' trustworthiness in this application has not been sufficiently explored. In this paper, we reveal that the models' inherent social biases can directly damage game balance in real-world gaming environments. To this end, we present FairGamer, the first bias evaluation Benchmark for LLMs in video game scenarios, featuring six tasks and a novel metrics ${D_lstd}$. It covers three key scenarios in games where LLMs' social biases are particularly likely to manifest: Serving as Non-Player Characters, Interacting as Competitive Opponents, and Generating Game Scenes. FairGamer utilizes both reality-grounded and fully fictional game content, covering a variety of video game genres. Experiments reveal: (1) Decision biases directly cause game balance degradation, with Grok-3 (average ${D_lstd}$ score=0.431) exhibiting the most severe degradation; (2) LLMs demonstrate isomorphic social/cultural biases toward both real and virtual world content, suggesting their biases nature may stem from inherent model characteristics. These findings expose critical reliability gaps in LLMs' gaming applications. Our code and data are available at anonymous GitHub https://github.com/Anonymous999-xxx/FairGamer .<br>
<span id='abs_ch'>中文摘要：大型语言模型在视频游戏中展现出巨大应用潜力，但其固有的社会偏见会 坏游戏平衡，FairGamer基准测试通过六项任务和新型度量指 揭示了模型在游戏场景中的可 性缺陷。</span><br>
<span id='abs_en'>English summary: Large Language Models (LLMs) show great potential in video games but their inherent social biases can disrupt game balance, as demonstrated by the FairGamer benchmark which reveals significant reliability gaps in gaming applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>772, <a href='https://arxiv.org/pdf/2508.17816.pdf' target='_blank'>https://arxiv.org/pdf/2508.17816.pdf</a></span>   <span><a href='https://github.com/yqx7150/UniSino' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyu Ai, Shaoyu Wang, Zhiyuan Jia, Ao Xu, Hongming Shan, Jianhua Ma, Qiegen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17816">UniSino: Physics-Driven Foundational Model for Universal CT Sinogram Standardization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>During raw-data acquisition in CT imaging, diverse factors can degrade the collected sinograms, with undersampling and noise leading to severe artifacts and noise in reconstructed images and compromising diagnostic accuracy. Conventional correction methods rely on manually designed algorithms or fixed empirical parameters, but these approaches often lack generalizability across heterogeneous artifact types. To address these limitations, we propose UniSino, a foundation model for universal CT sinogram standardization. Unlike existing foundational models that operate in image domain, UniSino directly standardizes data in the projection domain, which enables stronger generalization across diverse undersampling scenarios. Its training framework incorporates the physical characteristics of sinograms, enhancing generalization and enabling robust performance across multiple subtasks spanning four benchmark datasets. Experimental results demonstrate thatUniSino achieves superior reconstruction quality both single and mixed undersampling case, demonstrating exceptional robustness and generalization in sinogram enhancement for CT imaging. The code is available at: https://github.com/yqx7150/UniSino.<br>
<span id='abs_ch'>中文: UniSino是一种通用的CT正弦图基础模型，直接在投影域中 准化数据，能在多种 采 场景下提升泛化能力和重建质量。</span><br>
<span id='abs_en'>English: UniSino is a universal CT sinogram foundation model that directly standardizes projection data, enhancing generalization and reconstruction quality across diverse undersampling scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>773, <a href='https://arxiv.org/pdf/2508.17742.pdf' target='_blank'>https://arxiv.org/pdf/2508.17742.pdf</a></span>   <span><a href='https://github.com/xw1216/EEG-FM-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Xiong, Jiangtong Li, Jie Li, Kun Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17742">EEG-FM-Bench: A Comprehensive Benchmark for the Systematic Evaluation of EEG Foundation Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Electroencephalography (EEG) foundation models are poised to significantly advance brain signal analysis by learning robust representations from large-scale, unlabeled datasets. However, their rapid proliferation has outpaced the development of standardized evaluation benchmarks, which complicates direct model comparisons and hinders systematic scientific progress. This fragmentation fosters scientific inefficiency and obscures genuine architectural advancements. To address this critical gap, we introduce EEG-FM-Bench, the first comprehensive benchmark for the systematic and standardized evaluation of EEG foundation models (EEG-FMs). Our contributions are threefold: (1) we curate a diverse suite of downstream tasks and datasets from canonical EEG paradigms, implementing standardized processing and evaluation protocols within a unified open-source framework; (2) we benchmark prominent state-of-the-art foundation models to establish comprehensive baseline results for a clear comparison of the current landscape; (3) we perform qualitative analyses of the learned representations to provide insights into model behavior and inform future architectural design. Through extensive experiments, we find that fine-grained spatio-temporal feature interaction, multitask unified training and neuropsychological priors would contribute to enhancing model performance and generalization capabilities. By offering a unified platform for fair comparison and reproducible research, EEG-FM-Bench seeks to catalyze progress and guide the community toward the development of more robust and generalizable EEG-FMs. Code is released at https://github.com/xw1216/EEG-FM-Bench.<br>
<span id='abs_ch'>Chinese: EEG-FM-Bench作为首个全面的基准测试，旨在 准化脑电图基础模型的评估，通过统一任务、基准结果和定性分析来解决当前领域碎片化问题，以提升模型性能并指导未来发展。</span><br>
<span id='abs_en'>English: EEG-FM-Bench is introduced as the first comprehensive benchmark to standardize the evaluation of EEG foundation models, addressing current fragmentation by providing unified tasks, baseline results, and insights to enhance model performance and guide future development.</span>Paperid:225,https://arxiv.org/pdf/2508.17739.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>774, <a href='https://arxiv.org/pdf/2508.17739.pdf' target='_blank'>https://arxiv.org/pdf/2508.17739.pdf</a></span>   <span><a href='https://github.com/k-k1w-w1x-x/Speculative-Safety-Aware-Decoding' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuekang Wang, Shengyu Zhu, Xueqi Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17739">Speculative Safety-Aware Decoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite extensive efforts to align Large Language Models (LLMs) with human values and safety rules, jailbreak attacks that exploit certain vulnerabilities continuously emerge, highlighting the need to strengthen existing LLMs with additional safety properties to defend against these attacks. However, tuning large models has become increasingly resource intensive and may have difficulty ensuring consistent performance. We introduce Speculative Safety-Aware Decoding (SSD), a lightweight decoding-time approach that equips LLMs with the desired safety property while accelerating inference. We assume that there exists a small language model that possesses this desired property. SSD integrates speculative sampling during decoding and leverages the match ratio between the small and composite models to quantify jailbreak risks. This enables SSD to dynamically switch between decoding schemes to prioritize utility or safety, to handle the challenge of different model capacities. The output token is then sampled from a new distribution that combines the distributions of the original and the small models. Experimental results show that SSD successfully equips the large model with the desired safety property, and also allows the model to remain helpful to benign queries. Furthermore, SSD accelerates the inference time, thanks to the speculative sampling design.<br>
<span id='abs_ch'>中文摘要：SSD是一种轻量级的解 时方法，通过集成小型安全感知模型来增强大语言模型抵御越狱攻击的能力，在动态平衡实用性与安全性的同时 速推理过程。</span><br>
<span id='abs_en'>English Summary: SSD is a lightweight decoding-time method that enhances LLM safety against jailbreak attacks by integrating a small safety-aware model, dynamically balancing utility and security while accelerating inference.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>775, <a href='https://arxiv.org/pdf/2508.17621.pdf' target='_blank'>https://arxiv.org/pdf/2508.17621.pdf</a></span>   <span><a href='https://github.com/gjw185/FASB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinwei Gan, Zifeng Cheng, Zhiwei Jiang, Cong Wang, Yafeng Yin, Xiang Luo, Yuchen Fu, Qing Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17621">Steering When Necessary: Flexible Steering Large Language Models with Backtracking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have achieved remarkable performance across many generation tasks. Nevertheless, effectively aligning them with desired behaviors remains a significant challenge. Activation steering is an effective and cost-efficient approach that directly modifies the activations of LLMs during the inference stage, aligning their responses with the desired behaviors and avoiding the high cost of fine-tuning. Existing methods typically indiscriminately intervene to all generations or rely solely on the question to determine intervention, which limits the accurate assessment of the intervention strength. To this end, we propose the Flexible Activation Steering with Backtracking (FASB) framework, which dynamically determines both the necessity and strength of intervention by tracking the internal states of the LLMs during generation, considering both the question and the generated content. Since intervening after detecting a deviation from the desired behavior is often too late, we further propose the backtracking mechanism to correct the deviated tokens and steer the LLMs toward the desired behavior. Extensive experiments on the TruthfulQA dataset and six multiple-choice datasets demonstrate that our method outperforms baselines. Our code will be released at https://github.com/gjw185/FASB.<br>
<span id='abs_ch'>中文摘要：FASB框架通过追踪大语言模型生成过程中的内部状态并采用回溯机制修正偏差，在多个基准测试中优于现有方法。</span><br>
<span id='abs_en'>English Summary: The FASB framework dynamically adjusts intervention in large language models by monitoring internal states and using backtracking to correct deviations, outperforming existing methods on multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>776, <a href='https://arxiv.org/pdf/2508.17621.pdf' target='_blank'>https://arxiv.org/pdf/2508.17621.pdf</a></span>   <span><a href='https://github.com/gjw185/FASB' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zifeng Cheng, Jinwei Gan, Zhiwei Jiang, Cong Wang, Yafeng Yin, Xiang Luo, Yuchen Fu, Qing Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17621">Steering When Necessary: Flexible Steering Large Language Models with Backtracking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have achieved remarkable performance across many generation tasks. Nevertheless, effectively aligning them with desired behaviors remains a significant challenge. Activation steering is an effective and cost-efficient approach that directly modifies the activations of LLMs during the inference stage, aligning their responses with the desired behaviors and avoiding the high cost of fine-tuning. Existing methods typically indiscriminately intervene to all generations or rely solely on the question to determine intervention, which limits the accurate assessment of the intervention strength. To this end, we propose the Flexible Activation Steering with Backtracking (FASB) framework, which dynamically determines both the necessity and strength of intervention by tracking the internal states of the LLMs during generation, considering both the question and the generated content. Since intervening after detecting a deviation from the desired behavior is often too late, we further propose the backtracking mechanism to correct the deviated tokens and steer the LLMs toward the desired behavior. Extensive experiments on the TruthfulQA dataset and six multiple-choice datasets demonstrate that our method outperforms baselines. Our code will be released at https://github.com/gjw185/FASB.<br>
<span id='abs_ch'>中文摘要：FASB框架通过追踪大语言模型生成过程中的内部状态并采用回溯机制修正偏差，在多个基准测试中优于现有方法。</span><br>
<span id='abs_en'>English Summary: The FASB framework dynamically adjusts intervention in large language models by monitoring internal states and using backtracking to correct deviations, outperforming existing methods on multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>777, <a href='https://arxiv.org/pdf/2508.17611.pdf' target='_blank'>https://arxiv.org/pdf/2508.17611.pdf</a></span>   <span><a href='https://github.com/shunsuke-iwashita/VTCS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shunsuke Iwashita, Ning Ding, Keisuke Fujii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17611">Evaluating Movement Initiation Timing in Ultimate Frisbee via Temporal Counterfactuals</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Ultimate is a sport where points are scored by passing a disc and catching it in the opposing team's end zone. In Ultimate, the player holding the disc cannot move, making field dynamics primarily driven by other players' movements. However, current literature in team sports has ignored quantitative evaluations of when players initiate such unlabeled movements in game situations. In this paper, we propose a quantitative evaluation method for movement initiation timing in Ultimate Frisbee. First, game footage was recorded using a drone camera, and players' positional data was obtained, which will be published as UltimateTrack dataset. Next, players' movement initiations were detected, and temporal counterfactual scenarios were generated by shifting the timing of movements using rule-based approaches. These scenarios were analyzed using a space evaluation metric based on soccer's pitch control reflecting the unique rules of Ultimate. By comparing the spatial evaluation values across scenarios, the difference between actual play and the most favorable counterfactual scenario was used to quantitatively assess the impact of movement timing.
  We validated our method and show that sequences in which the disc was actually thrown to the receiver received higher evaluation scores than the sequences without a throw.
  In practical verifications, the higher-skill group displays a broader distribution of time offsets from the model's optimal initiation point.
  These findings demonstrate that the proposed metric provides an objective means of assessing movement initiation timing, which has been difficult to quantify in unlabeled team sport plays.<br>
<span id='abs_ch'>中文总结：本文提出了一种定量评估极限飞盘中运动启动时机的方法，通过 人机获取球员位置数据，采用基于规则生成反事实场景，并利用空间评估指 对比分析实际比赛与最优场景的差异。</span><br>
<span id='abs_en'>English Summary: This paper introduces a quantitative method to evaluate movement initiation timing in Ultimate Frisbee by analyzing player positions from drone footage and comparing actual plays with rule-based counterfactual scenarios using a spatial evaluation metric.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>778, <a href='https://arxiv.org/pdf/2508.17550.pdf' target='_blank'>https://arxiv.org/pdf/2508.17550.pdf</a></span>   <span><a href='https://github.com/MAGICS-LAB/algo_emu' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jerry Yao-Chieh Hu, Hude Liu, Jennifer Yuntong Zhang, Han Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17550">In-Context Algorithm Emulation in Fixed-Weight Transformers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We prove that a minimal Transformer architecture with frozen weights is capable of emulating a broad class of algorithms by in-context prompting. In particular, for any algorithm implementable by a fixed-weight attention head (e.g. one-step gradient descent or linear/ridge regression), there exists a prompt that drives a two-layer softmax attention module to reproduce the algorithm's output with arbitrary precision. This guarantee extends even to a single-head attention layer (using longer prompts if necessary), achieving architectural minimality. Our key idea is to construct prompts that encode an algorithm's parameters into token representations, creating sharp dot-product gaps that force the softmax attention to follow the intended computation. This construction requires no feed-forward layers and no parameter updates. All adaptation happens through the prompt alone. These findings forge a direct link between in-context learning and algorithmic emulation, and offer a simple mechanism for large Transformers to serve as prompt-programmable libraries of algorithms. They illuminate how GPT-style foundation models may swap algorithms via prompts alone, establishing a form of algorithmic universality in modern Transformer models.<br>
<span id='abs_ch'>中文: 一个权重冻结的最小Transformer可以通过上下文提示模拟多种算法， 需参数更新即可实现任务特定和提示可编程的通用性。</span><br>
<span id='abs_en'>English: A minimal Transformer with frozen weights can emulate a wide range of algorithms through in-context prompting, demonstrating both task-specific and prompt-programmable universality without parameter updates.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>779, <a href='https://arxiv.org/pdf/2508.17550.pdf' target='_blank'>https://arxiv.org/pdf/2508.17550.pdf</a></span>   <span><a href='https://github.com/MAGICS-LAB/algo_emu' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jerry Yao-Chieh Hu, Hude Liu, Jennifer Yuntong Zhang, Han Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17550">In-Context Algorithm Emulation in Fixed-Weight Transformers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We prove that a minimal Transformer with frozen weights emulates a broad class of algorithms by in-context prompting. We formalize two modes of in-context algorithm emulation. In the task-specific mode, for any continuous function $f: \mathbb{R} \to \mathbb{R}$, we show the existence of a single-head softmax attention layer whose forward pass reproduces functions of the form $f(w^\top x - y)$ to arbitrary precision. This general template subsumes many popular machine learning algorithms (e.g., gradient descent, linear regression, ridge regression). In the prompt-programmable mode, we prove universality: a single fixed-weight two-layer softmax attention module emulates all algorithms from the task-specific class (i.e., each implementable by a single softmax attention) via only prompting. Our key idea is to construct prompts that encode an algorithm's parameters into token representations, creating sharp dot-product gaps that force the softmax attention to follow the intended computation. This construction requires no feed-forward layers and no parameter updates. All adaptation happens through the prompt alone. Numerical results corroborate our theory. These findings forge a direct link between in-context learning and algorithmic emulation, and offer a simple mechanism for large Transformers to serve as prompt-programmable libraries of algorithms. They illuminate how GPT-style foundation models may swap algorithms via prompts alone, and establish a form of algorithmic universality in modern Transformer models.<br>
<span id='abs_ch'>中文: 一个权重冻结的最小Transformer可以通过上下文提示模拟多种算法， 需参数更新即可实现任务特定和提示可编程的通用性。</span><br>
<span id='abs_en'>English: A minimal Transformer with frozen weights can emulate a wide range of algorithms through in-context prompting, demonstrating both task-specific and prompt-programmable universality without parameter updates.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>780, <a href='https://arxiv.org/pdf/2508.17465.pdf' target='_blank'>https://arxiv.org/pdf/2508.17465.pdf</a></span>   <span><a href='https://github.com/kyrawilson/Image-Generation-Bias' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kyra Wilson, Sourojit Ghosh, Aylin Caliskan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17465">Bias Amplification in Stable Diffusion's Representation of Stigma Through Skin Tones and Their Homogeneity</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text-to-image generators (T2Is) are liable to produce images that perpetuate social stereotypes, especially in regards to race or skin tone. We use a comprehensive set of 93 stigmatized identities to determine that three versions of Stable Diffusion (v1.5, v2.1, and XL) systematically associate stigmatized identities with certain skin tones in generated images. We find that SD XL produces skin tones that are 13.53% darker and 23.76% less red (both of which indicate higher likelihood of societal discrimination) than previous models and perpetuate societal stereotypes associating people of color with stigmatized identities. SD XL also shows approximately 30% less variability in skin tones when compared to previous models and 18.89-56.06% compared to human face datasets. Measuring variability through metrics which directly correspond to human perception suggest a similar pattern, where SD XL shows the least amount of variability in skin tones of people with stigmatized identities and depicts most (60.29%) stigmatized identities as being less diverse than non-stigmatized identities. Finally, SD shows more homogenization of skin tones of racial and ethnic identities compared to other stigmatized or non-stigmatized identities, reinforcing incorrect equivalence of biologically-determined skin tone and socially-constructed racial and ethnic identity. Because SD XL is the largest and most complex model and users prefer its generations compared to other models examined in this study, these findings have implications for the dynamics of bias amplification in T2Is, increasing representational harms and challenges generating diverse images depicting people with stigmatized identities.<br>
<span id='abs_ch'>中文: 文本到图像生成器（如Stable Diffusion）会强化社会刻板印象，系统地将污名化身份与特定肤色关联，其中SD XL模型通过生成更深肤色、更低多 性的图像 剧了偏见，放大了代表性危害。</span><br>
<span id='abs_en'>English: Text-to-image generators like Stable Diffusion perpetuate social stereotypes by systematically associating stigmatized identities with specific skin tones, with SD XL showing increased bias through darker, less diverse skin tone depictions that amplify representational harms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>781, <a href='https://arxiv.org/pdf/2508.17393.pdf' target='_blank'>https://arxiv.org/pdf/2508.17393.pdf</a></span>   <span><a href='https://github.com/KhalilMrini/Agent-Testing-Agent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sameer Komoravolu, Khalil Mrini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17393">Agent-Testing Agent: A Meta-Agent for Automated Testing and Evaluation of Conversational AI Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LLM agents are increasingly deployed to plan, retrieve, and write with tools, yet evaluation still leans on static benchmarks and small human studies. We present the Agent-Testing Agent (ATA), a meta-agent that combines static code analysis, designer interrogation, literature mining, and persona-driven adversarial test generation whose difficulty adapts via judge feedback. Each dialogue is scored with an LLM-as-a-Judge (LAAJ) rubric and used to steer subsequent tests toward the agent's weakest capabilities. On a travel planner and a Wikipedia writer, the ATA surfaces more diverse and severe failures than expert annotators while matching severity, and finishes in 20--30 minutes versus ten-annotator rounds that took days. Ablating code analysis and web search increases variance and miscalibration, underscoring the value of evidence-grounded test generation. The ATA outputs quantitative metrics and qualitative bug reports for developers. We release the full methodology and open-source implementation for reproducible agent testing: https://github.com/KhalilMrini/Agent-Testing-Agent<br>
<span id='abs_ch'>中文: 代理测试代理（ATA）是一种元代理，它通过代 分析和对抗性场景动态生成自适应测试，在高效识别多 化故障方面优于人工 注者，并提供可操作的错误报告。</span><br>
<span id='abs_en'>English: The Agent-Testing Agent (ATA) is a meta-agent that dynamically generates adaptive tests using code analysis and adversarial scenarios, outperforming human annotators in identifying diverse failures efficiently while providing actionable bug reports.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>782, <a href='https://arxiv.org/pdf/2508.17389.pdf' target='_blank'>https://arxiv.org/pdf/2508.17389.pdf</a></span>   <span><a href='https://github.com/Bokai-Zhao/NPF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bokai Zhao, Weiyang Shi, Hanqing Chao, Zijiang Yang, Yiyang Zhang, Ming Song, Tianzi Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17389">Neural Proteomics Fields for Super-resolved Spatial Proteomics Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spatial proteomics maps protein distributions in tissues, providing transformative insights for life sciences. However, current sequencing-based technologies suffer from low spatial resolution, and substantial inter-tissue variability in protein expression further compromises the performance of existing molecular data prediction methods. In this work, we introduce the novel task of spatial super-resolution for sequencing-based spatial proteomics (seq-SP) and, to the best of our knowledge, propose the first deep learning model for this task--Neural Proteomics Fields (NPF). NPF formulates seq-SP as a protein reconstruction problem in continuous space by training a dedicated network for each tissue. The model comprises a Spatial Modeling Module, which learns tissue-specific protein spatial distributions, and a Morphology Modeling Module, which extracts tissue-specific morphological features. Furthermore, to facilitate rigorous evaluation, we establish an open-source benchmark dataset, Pseudo-Visium SP, for this task. Experimental results demonstrate that NPF achieves state-of-the-art performance with fewer learnable parameters, underscoring its potential for advancing spatial proteomics research. Our code and dataset are publicly available at https://github.com/Bokai-Zhao/NPF.<br>
<span id='abs_ch'>Chinese: 本文提出了Neural Proteomics Fields (NPF)这一新型深度学 模型，通过分别学 组织特异性蛋白质空间分布和形态特征，解决了测序空间蛋白质组学中分辨率低和表达变异大的问题，以更少参数实现了最优性能。</span><br>
<span id='abs_en'>English: This paper introduces Neural Proteomics Fields (NPF), a novel deep learning model that addresses the low spatial resolution and variability challenges in sequencing-based spatial proteomics by learning tissue-specific protein distributions and morphological features, achieving state-of-the-art performance with fewer parameters.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>783, <a href='https://arxiv.org/pdf/2508.17364.pdf' target='_blank'>https://arxiv.org/pdf/2508.17364.pdf</a></span>   <span><a href='https://github.com/gavin-gqzhang/UniGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoqing Zhang, Xingtong Ge, Lu Shi, Xin Zhang, Muqing Xue, Wanru Xu, Yigang Cen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17364">Condition Weaving Meets Expert Modulation: Towards Universal and Controllable Image Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The image-to-image generation task aims to produce controllable images by leveraging conditional inputs and prompt instructions. However, existing methods often train separate control branches for each type of condition, leading to redundant model structures and inefficient use of computational resources. To address this, we propose a Unified image-to-image Generation (UniGen) framework that supports diverse conditional inputs while enhancing generation efficiency and expressiveness. Specifically, to tackle the widely existing parameter redundancy and computational inefficiency in controllable conditional generation architectures, we propose the Condition Modulated Expert (CoMoE) module. This module aggregates semantically similar patch features and assigns them to dedicated expert modules for visual representation and conditional modeling. By enabling independent modeling of foreground features under different conditions, CoMoE effectively mitigates feature entanglement and redundant computation in multi-condition scenarios. Furthermore, to bridge the information gap between the backbone and control branches, we propose WeaveNet, a dynamic, snake-like connection mechanism that enables effective interaction between global text-level control from the backbone and fine-grained control from conditional branches. Extensive experiments on the Subjects-200K and MultiGen-20M datasets across various conditional image generation tasks demonstrate that our method consistently achieves state-of-the-art performance, validating its advantages in both versatility and effectiveness. The code has been uploaded to https://github.com/gavin-gqzhang/UniGen.<br>
<span id='abs_ch'>中文: 提出的UniGen框架通过CoMoE模块和WeaveNet机制统一多种条件输入进行图像生成，有效减少冗余并提升效率，在多项任务中实现了最优性能。</span><br>
<span id='abs_en'>English: The proposed UniGen framework introduces the CoMoE module and WeaveNet mechanism to unify diverse conditional inputs for image generation, effectively reducing redundancy and improving efficiency while achieving state-of-the-art performance across multiple tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>784, <a href='https://arxiv.org/pdf/2508.17364.pdf' target='_blank'>https://arxiv.org/pdf/2508.17364.pdf</a></span>   <span><a href='https://github.com/gavin-gqzhang/UniGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoqing Zhang, Xingtong Ge, Lu Shi, Xin Zhang, Muqing Xue, Wanru Xu, Yigang Cen, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17364">Condition Weaving Meets Expert Modulation: Towards Universal and Controllable Image Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The image-to-image generation task aims to produce controllable images by leveraging conditional inputs and prompt instructions. However, existing methods often train separate control branches for each type of condition, leading to redundant model structures and inefficient use of computational resources. To address this, we propose a Unified image-to-image Generation (UniGen) framework that supports diverse conditional inputs while enhancing generation efficiency and expressiveness. Specifically, to tackle the widely existing parameter redundancy and computational inefficiency in controllable conditional generation architectures, we propose the Condition Modulated Expert (CoMoE) module. This module aggregates semantically similar patch features and assigns them to dedicated expert modules for visual representation and conditional modeling. By enabling independent modeling of foreground features under different conditions, CoMoE effectively mitigates feature entanglement and redundant computation in multi-condition scenarios. Furthermore, to bridge the information gap between the backbone and control branches, we propose WeaveNet, a dynamic, snake-like connection mechanism that enables effective interaction between global text-level control from the backbone and fine-grained control from conditional branches. Extensive experiments on the Subjects-200K and MultiGen-20M datasets across various conditional image generation tasks demonstrate that our method consistently achieves state-of-the-art performance, validating its advantages in both versatility and effectiveness. The code has been uploaded to https://github.com/gavin-gqzhang/UniGen.<br>
<span id='abs_ch'>中文: 提出的UniGen框架通过CoMoE模块和WeaveNet机制统一多种条件输入进行图像生成，有效减少冗余并提升效率，在多项任务中实现了最优性能。</span><br>
<span id='abs_en'>English: The proposed UniGen framework introduces the CoMoE module and WeaveNet mechanism to unify diverse conditional inputs for image generation, effectively reducing redundancy and improving efficiency while achieving state-of-the-art performance across multiple tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>785, <a href='https://arxiv.org/pdf/2508.17298.pdf' target='_blank'>https://arxiv.org/pdf/2508.17298.pdf</a></span>   <span><a href='https://github.com/pokerme7777/Compositional-Visual-Reasoning-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fucai Ke, Joy Hsu, Zhixi Cai, Zixian Ma, Xin Zheng, Xindi Wu, Sukai Huang, Weiqing Wang, Pari Delir Haghighi, Gholamreza Haffari, Ranjay Krishna, Jiajun Wu, Hamid Rezatofighi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17298">Explain Before You Answer: A Survey on Compositional Visual Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Compositional visual reasoning has emerged as a key research frontier in multimodal AI, aiming to endow machines with the human-like ability to decompose visual scenes, ground intermediate concepts, and perform multi-step logical inference. While early surveys focus on monolithic vision-language models or general multimodal reasoning, a dedicated synthesis of the rapidly expanding compositional visual reasoning literature is still missing. We fill this gap with a comprehensive survey spanning 2023 to 2025 that systematically reviews 260+ papers from top venues (CVPR, ICCV, NeurIPS, ICML, ACL, etc.). We first formalize core definitions and describe why compositional approaches offer advantages in cognitive alignment, semantic fidelity, robustness, interpretability, and data efficiency. Next, we trace a five-stage paradigm shift: from prompt-enhanced language-centric pipelines, through tool-enhanced LLMs and tool-enhanced VLMs, to recently minted chain-of-thought reasoning and unified agentic VLMs, highlighting their architectural designs, strengths, and limitations. We then catalog 60+ benchmarks and corresponding metrics that probe compositional visual reasoning along dimensions such as grounding accuracy, chain-of-thought faithfulness, and high-resolution perception. Drawing on these analyses, we distill key insights, identify open challenges (e.g., limitations of LLM-based reasoning, hallucination, a bias toward deductive reasoning, scalable supervision, tool integration, and benchmark limitations), and outline future directions, including world-model integration, human-AI collaborative reasoning, and richer evaluation protocols. By offering a unified taxonomy, historical roadmap, and critical outlook, this survey aims to serve as a foundational reference and inspire the next generation of compositional visual reasoning research.<br>
<span id='abs_ch'>中文: 本综述系统整合了2023-2025年组合视觉推理 究，通过分析范式演变、评测基准与 心挑战，为推进多模态AI发展提出了世界模型集成等未来方向。English: This survey comprehensively synthesizes compositional visual reasoning research from 2023-2025, analyzing paradigm shifts, benchmarks, and challenges while proposing future directions like world-model integration to advance multimodal AI.Paperid:264,https://arxiv.org/pdf/2508.17283.pdfGitHub</span><br>
<span id='abs_en'>English: This survey comprehensively synthesizes compositional visual reasoning research from 2023-2025, analyzing paradigm shifts, benchmarks, and challenges while proposing future directions like world-model integration to advance multimodal AI.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>786, <a href='https://arxiv.org/pdf/2508.17225.pdf' target='_blank'>https://arxiv.org/pdf/2508.17225.pdf</a></span>   <span><a href='https://github.com/chkwy/SSFO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaqiang Tang, Yi Wang, Keyu Hu, Rui Xu, Chuang Li, Weigao Sun, Jian Li, Sihong Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17225">SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-Augmented Generation (RAG) systems require Large Language Models (LLMs) to generate responses that are faithful to the retrieved context. However, faithfulness hallucination remains a critical challenge, as existing methods often require costly supervision and post-training or significant inference burdens. To overcome these limitations, we introduce Self-Supervised Faithfulness Optimization (SSFO), the first self-supervised alignment approach for enhancing RAG faithfulness. SSFO constructs preference data pairs by contrasting the model's outputs generated with and without the context. Leveraging Direct Preference Optimization (DPO), SSFO aligns model faithfulness without incurring labeling costs or additional inference burden. We theoretically and empirically demonstrate that SSFO leverages a benign form of \emph{likelihood displacement}, transferring probability mass from parametric-based tokens to context-aligned tokens. Based on this insight, we propose a modified DPO loss function to encourage likelihood displacement. Comprehensive evaluations show that SSFO significantly outperforms existing methods, achieving state-of-the-art faithfulness on multiple context-based question-answering datasets. Notably, SSFO exhibits strong generalization, improving cross-lingual faithfulness and preserving general instruction-following capabilities. We release our code and model at the anonymous link: https://github.com/chkwy/SSFO<br>
<span id='abs_ch'>Chinese: 本文提出了自监督 实性优化（SSFO），这是一种新颖的自监督对齐方法，通过构建偏好数据对并利用直接偏好优化将概率质量转移到上下文对齐的 记上，从而增强检索增强生成系统的 实性，在多个数据集上实现了最先进的性能，且 需额外 注或推理成本。</span><br>
<span id='abs_en'>English: The paper introduces Self-Supervised Faithfulness Optimization (SSFO), a novel self-supervised alignment method that enhances the faithfulness of Retrieval-Augmented Generation systems by constructing preference data pairs and leveraging Direct Preference Optimization to transfer probability mass to context-aligned tokens, achieving state-of-the-art performance on multiple datasets without additional labeling or inference costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>787, <a href='https://arxiv.org/pdf/2508.17225.pdf' target='_blank'>https://arxiv.org/pdf/2508.17225.pdf</a></span>   <span><a href='https://github.com/chkwy/SSFO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaqiang Tang, Yi Wang, Keyu Hu, Rui Xu, Chuang Li, Weigao Sun, Jian Li, Sihong Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17225">SSFO: Self-Supervised Faithfulness Optimization for Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-Augmented Generation (RAG) systems require Large Language Models (LLMs) to generate responses that are faithful to the retrieved context. However, faithfulness hallucination remains a critical challenge, as existing methods often require costly supervision and post-training or significant inference burdens. To overcome these limitations, we introduce Self-Supervised Faithfulness Optimization (SSFO), the first self-supervised alignment approach for enhancing RAG faithfulness. SSFO constructs preference data pairs by contrasting the model's outputs generated with and without the context. Leveraging Direct Preference Optimization (DPO), SSFO aligns model faithfulness without incurring labeling costs or additional inference burden. We theoretically and empirically demonstrate that SSFO leverages a benign form of \emph{likelihood displacement}, transferring probability mass from parametric-based tokens to context-aligned tokens. Based on this insight, we propose a modified DPO loss function to encourage likelihood displacement. Comprehensive evaluations show that SSFO significantly outperforms existing methods, achieving state-of-the-art faithfulness on multiple context-based question-answering datasets. Notably, SSFO exhibits strong generalization, improving cross-lingual faithfulness and preserving general instruction-following capabilities. We release our code and model at the anonymous link: https://github.com/chkwy/SSFO<br>
<span id='abs_ch'>Chinese: 本文提出了自监督 实性优化（SSFO），这是一种新颖的自监督对齐方法，通过构建偏好数据对并利用直接偏好优化将概率质量转移到上下文对齐的 记上，从而增强检索增强生成系统的 实性，在多个数据集上实现了最先进的性能，且 需额外 注或推理成本。</span><br>
<span id='abs_en'>English: The paper introduces Self-Supervised Faithfulness Optimization (SSFO), a novel self-supervised alignment method that enhances the faithfulness of Retrieval-Augmented Generation systems by constructing preference data pairs and leveraging Direct Preference Optimization to transfer probability mass to context-aligned tokens, achieving state-of-the-art performance on multiple datasets without additional labeling or inference costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>788, <a href='https://arxiv.org/pdf/2508.17169.pdf' target='_blank'>https://arxiv.org/pdf/2508.17169.pdf</a></span>   <span><a href='https://github.com/yajatyadav/orthogonal-natural-gradient' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yajat Yadav, Patrick Mendoza, Jathin Korrapati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17169">ONG: Orthogonal Natural Gradient Descent</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Orthogonal Gradient Descent (OGD) has emerged as a powerful method for continual learning. However, its Euclidean projections do not leverage the underlying information-geometric structure of the problem, which can lead to suboptimal convergence in learning tasks. To address this, we propose incorporating the natural gradient into OGD and present \textbf{ONG (Orthogonal Natural Gradient Descent)}. ONG preconditions each new task-specific gradient with an efficient EKFAC approximation of the inverse Fisher information matrix, yielding updates that follow the steepest descent direction under a Riemannian metric. To preserve performance on previously learned tasks, ONG projects these natural gradients onto the orthogonal complement of prior tasks' gradients. We provide an initial theoretical justification for this procedure, introduce the Orthogonal Natural Gradient Descent (ONG) algorithm, and present preliminary results on the Permuted and Rotated MNIST benchmarks. Our preliminary results, however, indicate that a naive combination of natural gradients and orthogonal projections can have potential issues. This finding motivates continued future work focused on robustly reconciling these geometric perspectives to develop a continual learning method, establishing a more rigorous theoretical foundation with formal convergence guarantees, and extending empirical validation to large-scale continual learning benchmarks. The anonymized version of our code can be found as the zip file here: https://drive.google.com/drive/folders/11PyU6M8pNgOUB5pwdGORtbnMtD8Shiw_?usp=sharing.<br>
<span id='abs_ch'>中文: 本文提出了正交自然梯度下降法（ONG），通过将自然梯度与正交投影相结合来改进持续学 ，但初步结果表明二者的简单组合存在潜在问题，需要进一步 究解决。</span><br>
<span id='abs_en'>English: This paper introduces Orthogonal Natural Gradient Descent (ONG), which enhances continual learning by incorporating natural gradients with orthogonal projections, though initial results reveal challenges in their naive combination that warrant further investigation.Paperid:274,https://arxiv.org/pdf/2508.17158.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>789, <a href='https://arxiv.org/pdf/2508.17078.pdf' target='_blank'>https://arxiv.org/pdf/2508.17078.pdf</a></span>   <span><a href='https://github.com/xuyuemei/BridgeX-ICL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuemei Xu, Kexin Xu, Jian Zhou, Ling Hu, Lin Gui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17078">Linguistic Neuron Overlap Patterns to Facilitate Cross-lingual Transfer on Low-resource Languages</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The current Large Language Models (LLMs) face significant challenges in improving their performance on low-resource languages and urgently need data-efficient methods without costly fine-tuning. From the perspective of language-bridge, we propose a simple yet effective method, namely BridgeX-ICL, to improve the zero-shot Cross-lingual In-Context Learning (X-ICL) for low-resource languages. Unlike existing works focusing on language-specific neurons, BridgeX-ICL explores whether sharing neurons can improve cross-lingual performance in LLMs. We construct neuron probe data from the ground-truth MUSE bilingual dictionaries, and define a subset of language overlap neurons accordingly to ensure full activation of these anchored neurons. Subsequently, we propose an HSIC-based metric to quantify LLMs' internal linguistic spectrum based on overlapping neurons, guiding optimal bridge selection. The experiments conducted on 4 cross-lingual tasks and 15 language pairs from 7 diverse families, covering both high-low and moderate-low pairs, validate the effectiveness of BridgeX-ICL and offer empirical insights into the underlying multilingual mechanisms of LLMs. The code is publicly available at https://github.com/xuyuemei/BridgeX-ICL.<br>
<span id='abs_ch'>中文摘要：本 究提出BridgeX-ICL方法，通过识别并激活大语言模型中的共享神经元，有效提升了低资源语言的零 本跨语言学 性能，并在多任务和语言对上验证了其有效性。</span><br>
<span id='abs_en'>English Summary: The study introduces BridgeX-ICL, a method that enhances zero-shot cross-lingual learning for low-resource languages by identifying and activating shared neurons in LLMs, validated across multiple tasks and language pairs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>790, <a href='https://arxiv.org/pdf/2508.17007.pdf' target='_blank'>https://arxiv.org/pdf/2508.17007.pdf</a></span>   <span><a href='https://github.com/riadhassan/EDLDNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Riad Hassan, M. Rubaiyat Hossain Mondal, Sheikh Iqbal Ahamed, Fahad Mostafa, Md Mostafijur Rahman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17007">An Efficient Dual-Line Decoder Network with Multi-Scale Convolutional Attention for Multi-organ Segmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Proper segmentation of organs-at-risk is important for radiation therapy, surgical planning, and diagnostic decision-making in medical image analysis. While deep learning-based segmentation architectures have made significant progress, they often fail to balance segmentation accuracy with computational efficiency. Most of the current state-of-the-art methods either prioritize performance at the cost of high computational complexity or compromise accuracy for efficiency. This paper addresses this gap by introducing an efficient dual-line decoder segmentation network (EDLDNet). The proposed method features a noisy decoder, which learns to incorporate structured perturbation at training time for better model robustness, yet at inference time only the noise-free decoder is executed, leading to lower computational cost. Multi-Scale convolutional Attention Modules (MSCAMs), Attention Gates (AGs), and Up-Convolution Blocks (UCBs) are further utilized to optimize feature representation and boost segmentation performance. By leveraging multi-scale segmentation masks from both decoders, we also utilize a mutation-based loss function to enhance the model's generalization. Our approach outperforms SOTA segmentation architectures on four publicly available medical imaging datasets. EDLDNet achieves SOTA performance with an 84.00% Dice score on the Synapse dataset, surpassing baseline model like UNet by 13.89% in Dice score while significantly reducing Multiply-Accumulate Operations (MACs) by 89.7%. Compared to recent approaches like EMCAD, our EDLDNet not only achieves higher Dice score but also maintains comparable computational efficiency. The outstanding performance across diverse datasets establishes EDLDNet's strong generalization, computational efficiency, and robustness. The source code, pre-processed data, and pre-trained weights will be available at https://github.com/riadhassan/EDLDNet .<br>
<span id='abs_ch'>Chinese: 本文提出的EDLDNet高效双线解 器分割网络，通过噪声解 器和多尺度注意力模块等创新设计，在多个医学影像数据集上实现了最佳性能，同时兼顾了分割精度与计算效率。</span><br>
<span id='abs_en'>English: This paper introduces EDLDNet, an efficient dual-line decoder segmentation network that achieves state-of-the-art performance on medical imaging datasets by balancing high accuracy with computational efficiency through innovative components like a noisy decoder and multi-scale attention modules.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>791, <a href='https://arxiv.org/pdf/2508.16983.pdf' target='_blank'>https://arxiv.org/pdf/2508.16983.pdf</a></span>   <span><a href='https://github.com/rpo19/ReFactX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Riccardo Pozzi, Matteo Palmonari, Andrea Coletta, Luigi Bellomarini, Jens Lehmann, Sahar Vahdati
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16983">ReFactX: Scalable Reasoning with Reliable Facts via Constrained Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Knowledge gaps and hallucinations are persistent challenges for Large Language Models (LLMs), which generate unreliable responses when lacking the necessary information to fulfill user instructions. Existing approaches, such as Retrieval-Augmented Generation (RAG) and tool use, aim to address these issues by incorporating external knowledge. Yet, they rely on additional models or services, resulting in complex pipelines, potential error propagation, and often requiring the model to process a large number of tokens. In this paper, we present a scalable method that enables LLMs to access external knowledge without depending on retrievers or auxiliary models. Our approach uses constrained generation with a pre-built prefix-tree index. Triples from a Knowledge Graph are verbalized in textual facts, tokenized, and indexed in a prefix tree for efficient access. During inference, to acquire external knowledge, the LLM generates facts with constrained generation which allows only sequences of tokens that form an existing fact. We evaluate our proposal on Question Answering and show that it scales to large knowledge bases (800 million facts), adapts to domain-specific data, and achieves effective results. These gains come with minimal generation-time overhead. ReFactX code is available at https://github.com/rpo19/ReFactX.<br>
<span id='abs_ch'>中文摘要：本文提出ReFactX方法，通过使用前缀 索引的约束生成，使大型语言模型能够 需依赖检索器或辅助模型即可获取外部知识，有效解决了知识空白和幻觉问题，并适用于大规模知识库。</span><br>
<span id='abs_en'>English Summary: The paper introduces ReFactX, a scalable method that enables Large Language Models to access external knowledge through constrained generation using a prefix-tree index, effectively addressing knowledge gaps and hallucinations without relying on retrievers or auxiliary models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>792, <a href='https://arxiv.org/pdf/2508.16949.pdf' target='_blank'>https://arxiv.org/pdf/2508.16949.pdf</a></span>   <span><a href='https://github.com/IANNXANG/RuscaRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Zhou, Sunzhu Li, Shunyu Liu, Wenkai Fang, Kongcheng Zhang, Jiale Zhao, Jingwen Yang, Yihe Zhou, Jianwei Lv, Tongya Zheng, Hengtong Lu, Wei Chen, Yan Xie, Mingli Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16949">Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in Large Language Models (LLMs) have underscored the potential of Reinforcement Learning (RL) to facilitate the emergence of reasoning capabilities. Despite the encouraging results, a fundamental dilemma persists as RL improvement relies on learning from high-quality samples, yet the exploration for such samples remains bounded by the inherent limitations of LLMs. This, in effect, creates an undesirable cycle in which what cannot be explored cannot be learned. In this work, we propose Rubric-Scaffolded Reinforcement Learning (RuscaRL), a novel instructional scaffolding framework designed to break the exploration bottleneck for general LLM reasoning. Specifically, RuscaRL introduces checklist-style rubrics as (1) explicit scaffolding for exploration during rollout generation, where different rubrics are provided as external guidance within task instructions to steer diverse high-quality responses. This guidance is gradually decayed over time, encouraging the model to internalize the underlying reasoning patterns; (2) verifiable rewards for exploitation during model training, where we can obtain robust LLM-as-a-Judge scores using rubrics as references, enabling effective RL on general reasoning tasks. Extensive experiments demonstrate the superiority of the proposed RuscaRL across various benchmarks, effectively expanding reasoning boundaries under the Best-of-N evaluation. Notably, RuscaRL significantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500, surpassing GPT-4.1. Furthermore, our fine-tuned variant on Qwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading LLMs including OpenAI-o3. Our code is available at https://github.com/IANNXANG/RuscaRL.<br>
<span id='abs_ch'>中文摘要：RuscaRL提出了一种基于评分 准的强化学 框架，通过清单式评分 准在推理过程中引导多 化高质量回答生成，并在训练时提供可验证奖励，有效突 了大语言模型推理的探索瓶颈，在多个基准测试中显著提升了性能表现。</span><br>
<span id='abs_en'>English Summary: RuscaRL introduces a rubric-scaffolded reinforcement learning framework that breaks the exploration bottleneck in LLM reasoning by using checklist-style rubrics to guide diverse response generation during rollout and provide verifiable rewards during training, significantly boosting performance across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>793, <a href='https://arxiv.org/pdf/2508.16852.pdf' target='_blank'>https://arxiv.org/pdf/2508.16852.pdf</a></span>   <span><a href='https://github.com/xintian-99/GPOreg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Tian, Jiazheng Wang, Yuxi Zhang, Xiang Chen, Renjiu Hu, Gaolei Li, Min Liu, Hang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16852">Gaussian Primitive Optimized Deformable Retinal Image Registration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Deformable retinal image registration is notoriously difficult due to large homogeneous regions and sparse but critical vascular features, which cause limited gradient signals in standard learning-based frameworks. In this paper, we introduce Gaussian Primitive Optimization (GPO), a novel iterative framework that performs structured message passing to overcome these challenges. After an initial coarse alignment, we extract keypoints at salient anatomical structures (e.g., major vessels) to serve as a minimal set of descriptor-based control nodes (DCN). Each node is modelled as a Gaussian primitive with trainable position, displacement, and radius, thus adapting its spatial influence to local deformation scales. A K-Nearest Neighbors (KNN) Gaussian interpolation then blends and propagates displacement signals from these information-rich nodes to construct a globally coherent displacement field; focusing interpolation on the top (K) neighbors reduces computational overhead while preserving local detail. By strategically anchoring nodes in high-gradient regions, GPO ensures robust gradient flow, mitigating vanishing gradient signal in textureless areas. The framework is optimized end-to-end via a multi-term loss that enforces both keypoint consistency and intensity alignment. Experiments on the FIRE dataset show that GPO reduces the target registration error from 6.2\,px to ~2.4\,px and increases the AUC at 25\,px from 0.770 to 0.938, substantially outperforming existing methods. The source code can be accessed via https://github.com/xintian-99/GPOreg.<br>
<span id='abs_ch'>中文摘要：本文提出高斯基元优化（GPO）方法，通过在关键血管特征处部署可学 的高斯基元来解决视网膜图像配准中的梯度信号不足问题，在FIRE数据集上实现了显著优于现有方法的配准精度。</span><br>
<span id='abs_en'>English Summary: The paper introduces Gaussian Primitive Optimization (GPO), a novel deformable retinal image registration framework that uses strategically placed Gaussian primitives at key vascular features to overcome gradient signal limitations, achieving state-of-the-art performance on the FIRE dataset.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>794, <a href='https://arxiv.org/pdf/2508.16829.pdf' target='_blank'>https://arxiv.org/pdf/2508.16829.pdf</a></span>   <span><a href='https://github.com/LeeJunHyun/NATR' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/LeeJunHyun/NATR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhyun Lee, Veronika Thost, Bumsoo Kim, Jaewoo Kang, Tengfei Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16829">Understanding and Tackling Over-Dilution in Graph Neural Networks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Message Passing Neural Networks (MPNNs) hold a key position in machine learning on graphs, but they struggle with unintended behaviors, such as over-smoothing and over-squashing, due to irregular data structures. The observation and formulation of these limitations have become foundational in constructing more informative graph representations. In this paper, we delve into the limitations of MPNNs, focusing on aspects that have previously been overlooked. Our observations reveal that even within a single layer, the information specific to an individual node can become significantly diluted. To delve into this phenomenon in depth, we present the concept of Over-dilution and formulate it with two dilution factors: intra-node dilution for attribute-level and inter-node dilution for node-level representations. We also introduce a transformer-based solution that alleviates over-dilution and complements existing node embedding methods like MPNNs. Our findings provide new insights and contribute to the development of informative representations. The implementation and supplementary materials are publicly available at https://github.com/LeeJunHyun/NATR.<br>
<span id='abs_ch'>Chinese: 本文提出了消息 递神经网络中的过度稀释概念，定义了两个稀释 子，并引入一种基于Transformer的解决方案，以补充现有节点嵌入方法并提升信息表示的准确性。</span><br>
<span id='abs_en'>English: This paper introduces the concept of over-dilution in Message Passing Neural Networks (MPNNs), identifying two dilution factors and proposing a transformer-based solution to enhance node representation without replacing existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>795, <a href='https://arxiv.org/pdf/2508.16783.pdf' target='_blank'>https://arxiv.org/pdf/2508.16783.pdf</a></span>   <span><a href='https://github.com/StanfordMIMI/RoentGen-v2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefania L. Moroianu, Christian Bluethgen, Pierre Chambon, Mehdi Cherti, Jean-Benoit Delbrouck, Magdalini Paschali, Brandon Price, Judy Gichoya, Jenia Jitsev, Curtis P. Langlotz, Akshay S. Chaudhari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16783">Improving Performance, Robustness, and Fairness of Radiographic AI Models with Finely-Controllable Synthetic Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Achieving robust performance and fairness across diverse patient populations remains a challenge in developing clinically deployable deep learning models for diagnostic imaging. Synthetic data generation has emerged as a promising strategy to address limitations in dataset scale and diversity. We introduce RoentGen-v2, a text-to-image diffusion model for chest radiographs that enables fine-grained control over both radiographic findings and patient demographic attributes, including sex, age, and race/ethnicity. RoentGen-v2 is the first model to generate clinically plausible images with demographic conditioning, facilitating the creation of a large, demographically balanced synthetic dataset comprising over 565,000 images. We use this large synthetic dataset to evaluate optimal training pipelines for downstream disease classification models. In contrast to prior work that combines real and synthetic data naively, we propose an improved training strategy that leverages synthetic data for supervised pretraining, followed by fine-tuning on real data. Through extensive evaluation on over 137,000 chest radiographs from five institutions, we demonstrate that synthetic pretraining consistently improves model performance, generalization to out-of-distribution settings, and fairness across demographic subgroups. Across datasets, synthetic pretraining led to a 6.5% accuracy increase in the performance of downstream classification models, compared to a modest 2.7% increase when naively combining real and synthetic data. We observe this performance improvement simultaneously with the reduction of the underdiagnosis fairness gap by 19.3%. These results highlight the potential of synthetic imaging to advance equitable and generalizable medical deep learning under real-world data constraints. We open source our code, trained models, and synthetic dataset at https://github.com/StanfordMIMI/RoentGen-v2 .<br>
<span id='abs_ch'>中文: RoentGen-v2 提出了一种文本到图像的扩散模型，用于生成具有人口统计学控制的临床可信胸部X光片，通过合成预训练显著提升了医学影像模型的准确性、泛化能力和公平性。</span><br>
<span id='abs_en'>English: RoentGen-v2 introduces a text-to-image diffusion model for generating clinically plausible chest radiographs with demographic control, enabling synthetic pretraining that significantly improves model accuracy, generalization, and fairness in medical imaging.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>796, <a href='https://arxiv.org/pdf/2508.16665.pdf' target='_blank'>https://arxiv.org/pdf/2508.16665.pdf</a></span>   <span><a href='https://github.com/elixir-research-group/Verifierstesttimescaling.github.io' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>V Venktesh, Mandeep Rathee, Avishek Anand
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16665">Trust but Verify! A Survey on Verification Design for Test-time Scaling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Test-time scaling (TTS) has emerged as a new frontier for scaling the performance of Large Language Models. In test-time scaling, by using more computational resources during inference, LLMs can improve their reasoning process and task performance. Several approaches have emerged for TTS such as distilling reasoning traces from another model or exploring the vast decoding search space by employing a verifier. The verifiers serve as reward models that help score the candidate outputs from the decoding process to diligently explore the vast solution space and select the best outcome. This paradigm commonly termed has emerged as a superior approach owing to parameter free scaling at inference time and high performance gains. The verifiers could be prompt-based, fine-tuned as a discriminative or generative model to verify process paths, outcomes or both. Despite their widespread adoption, there is no detailed collection, clear categorization and discussion of diverse verification approaches and their training mechanisms. In this survey, we cover the diverse approaches in the literature and present a unified view of verifier training, types and their utility in test-time scaling. Our repository can be found at https://github.com/elixir-research-group/Verifierstesttimescaling.github.io.<br>
<span id='abs_ch'>测试时扩展通过在推理阶段使用更多计算资源来提升大语言模型的性能，其中验证器在从解 过程中筛选最佳输出方面发挥着 心作用。</span><br>
<span id='abs_en'>Test-time scaling enhances Large Language Models' performance by utilizing more computational resources during inference, with verifiers playing a key role in selecting optimal outputs from the decoding process.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>797, <a href='https://arxiv.org/pdf/2508.16634.pdf' target='_blank'>https://arxiv.org/pdf/2508.16634.pdf</a></span>   <span><a href='https://github.com/MentaY/DGGN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhendong Yang, Jie Wang, Liansong Zong, Xiaorong Liu, Quan Qian, Shiqian Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16634">Few-shot Class-incremental Fault Diagnosis by Preserving Class-Agnostic Knowledge with Dual-Granularity Representations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Few-Shot Class-Incremental Fault Diagnosis (FSC-FD), which aims to continuously learn from new fault classes with only a few samples without forgetting old ones, is critical for real-world industrial systems. However, this challenging task severely amplifies the issues of catastrophic forgetting of old knowledge and overfitting on scarce new data. To address these challenges, this paper proposes a novel framework built upon Dual-Granularity Representations, termed the Dual-Granularity Guidance Network (DGGN). Our DGGN explicitly decouples feature learning into two parallel streams: 1) a fine-grained representation stream, which utilizes a novel Multi-Order Interaction Aggregation module to capture discriminative, class-specific features from the limited new samples. 2) a coarse-grained representation stream, designed to model and preserve general, class-agnostic knowledge shared across all fault types. These two representations are dynamically fused by a multi-semantic cross-attention mechanism, where the stable coarse-grained knowledge guides the learning of fine-grained features, preventing overfitting and alleviating feature conflicts. To further mitigate catastrophic forgetting, we design a Boundary-Aware Exemplar Prioritization strategy. Moreover, a decoupled Balanced Random Forest classifier is employed to counter the decision boundary bias caused by data imbalance. Extensive experiments on the TEP benchmark and a real-world MFF dataset demonstrate that our proposed DGGN achieves superior diagnostic performance and stability compared to state-of-the-art FSC-FD approaches. Our code is publicly available at https://github.com/MentaY/DGGN<br>
<br>
<div id='section'>Paperid: <span id='pid'>798, <a href='https://arxiv.org/pdf/2508.16629.pdf' target='_blank'>https://arxiv.org/pdf/2508.16629.pdf</a></span>   <span><a href='https://github.com/nuster1128/learn_to_memorize' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Zhang, Quanyu Dai, Rui Li, Xiaohe Bo, Xu Chen, Zhenhua Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16629">Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LLM-based agents have been extensively applied across various domains, where memory stands out as one of their most essential capabilities. Previous memory mechanisms of LLM-based agents are manually predefined by human experts, leading to higher labor costs and suboptimal performance. In addition, these methods overlook the memory cycle effect in interactive scenarios, which is critical to optimizing LLM-based agents for specific environments. To address these challenges, in this paper, we propose to optimize LLM-based agents with an adaptive and data-driven memory framework by modeling memory cycles. Specifically, we design an MoE gate function to facilitate memory retrieval, propose a learnable aggregation process to improve memory utilization, and develop task-specific reflection to adapt memory storage. Our memory framework empowers LLM-based agents to learn how to memorize information effectively in specific environments, with both off-policy and on-policy optimization. In order to evaluate the effectiveness of our proposed methods, we conduct comprehensive experiments across multiple aspects. To benefit the research community in this area, we release our project at https://github.com/nuster1128/learn_to_memorize.<br>
<span id='abs_ch'>中文摘要：本文提出了一种自适应、数据驱动的记忆框架，通过建模记忆周期并采用可学 的检索、聚合和存储机制，优化基于LLM的智能体在特定环境中的记忆能力。English Summary: This paper introduces an adaptive, data-driven memory framework that enhances LLM-based agents by modeling memory cycles, improving retrieval, utilization, and storage through learnable mechanisms and task-specific optimizations.Paperid:307,https://arxiv.org/pdf/2508.16614.pdfGitHub</span><br>
<span id='abs_en'>English Summary: This paper introduces an adaptive, data-driven memory framework that enhances LLM-based agents by modeling memory cycles, improving retrieval, utilization, and storage through learnable mechanisms and task-specific optimizations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>799, <a href='https://arxiv.org/pdf/2508.16521.pdf' target='_blank'>https://arxiv.org/pdf/2508.16521.pdf</a></span>   <span><a href='https://github.com/ZhijianZhou/RLPF/tree/verl_diffusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijian Zhou, Junyi An, Zongkai Liu, Yunfei Shi, Xuan Zhang, Fenglei Cao, Chao Qu, Yuan Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16521">Guiding Diffusion Models with Reinforcement Learning for Stable Molecule Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Generating physically realistic 3D molecular structures remains a core challenge in molecular generative modeling. While diffusion models equipped with equivariant neural networks have made progress in capturing molecular geometries, they often struggle to produce equilibrium structures that adhere to physical principles such as force field consistency. To bridge this gap, we propose Reinforcement Learning with Physical Feedback (RLPF), a novel framework that extends Denoising Diffusion Policy Optimization to 3D molecular generation. RLPF formulates the task as a Markov decision process and applies proximal policy optimization to fine-tune equivariant diffusion models. Crucially, RLPF introduces reward functions derived from force-field evaluations, providing direct physical feedback to guide the generation toward energetically stable and physically meaningful structures. Experiments on the QM9 and GEOM-drug datasets demonstrate that RLPF significantly improves molecular stability compared to existing methods. These results highlight the value of incorporating physics-based feedback into generative modeling. The code is available at: https://github.com/ZhijianZhou/RLPF/tree/verl_diffusion.<br>
<span id='abs_ch'>中文：提出的物理反馈强化学 （RLPF）框架通过将力场评估作为奖励来引导扩散模型生成物理稳定的三维分子结构，在基准数据集上显著提升了分子稳定性。</span><br>
<span id='abs_en'>English: The proposed Reinforcement Learning with Physical Feedback (RLPF) framework enhances 3D molecular generation by using force-field evaluations as rewards to guide diffusion models toward producing physically stable structures, demonstrating significant improvements on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>800, <a href='https://arxiv.org/pdf/2508.16479.pdf' target='_blank'>https://arxiv.org/pdf/2508.16479.pdf</a></span>   <span><a href='https://github.com/helenypzhang/Disentangled-Multimodal-Learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yupei Zhang, Xiaofei Wang, Anran Liu, Lequan Yu, Chao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16479">Disentangled Multi-modal Learning of Histology and Transcriptomics for Cancer Characterization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Histopathology remains the gold standard for cancer diagnosis and prognosis. With the advent of transcriptome profiling, multi-modal learning combining transcriptomics with histology offers more comprehensive information. However, existing multi-modal approaches are challenged by intrinsic multi-modal heterogeneity, insufficient multi-scale integration, and reliance on paired data, restricting clinical applicability. To address these challenges, we propose a disentangled multi-modal framework with four contributions: 1) To mitigate multi-modal heterogeneity, we decompose WSIs and transcriptomes into tumor and microenvironment subspaces using a disentangled multi-modal fusion module, and introduce a confidence-guided gradient coordination strategy to balance subspace optimization. 2) To enhance multi-scale integration, we propose an inter-magnification gene-expression consistency strategy that aligns transcriptomic signals across WSI magnifications. 3) To reduce dependency on paired data, we propose a subspace knowledge distillation strategy enabling transcriptome-agnostic inference through a WSI-only student model. 4) To improve inference efficiency, we propose an informative token aggregation module that suppresses WSI redundancy while preserving subspace semantics. Extensive experiments on cancer diagnosis, prognosis, and survival prediction demonstrate our superiority over state-of-the-art methods across multiple settings. Code is available at https://github.com/helenypzhang/Disentangled-Multimodal-Learning.<br>
<span id='abs_ch'>中文: 本 究提出了一种解耦的多模态框架，通过将全切片图像和转录组分解为肿瘤与微环境子空间，采用置信度引导梯度协调和知识蒸馏等策略，解决了多模态异质性、多尺度整合及配对数据依赖等难题，在癌症诊断、预后和生存预测方面展现出卓越性能。</span><br>
<span id='abs_en'>English: This study introduces a disentangled multi-modal framework that addresses challenges in multi-modal heterogeneity, multi-scale integration, and paired data dependency by decomposing whole slide images and transcriptomes into tumor and microenvironment subspaces, employing strategies like confidence-guided gradient coordination and knowledge distillation, ultimately demonstrating superior performance in cancer diagnosis, prognosis, and survival prediction.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>801, <a href='https://arxiv.org/pdf/2508.16463.pdf' target='_blank'>https://arxiv.org/pdf/2508.16463.pdf</a></span>   <span><a href='https://github.com/aimagelab/mammoth' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aniello Panariello, Emanuele Frascaroli, Pietro Buzzega, Lorenzo Bonicelli, Angelo Porrello, Simone Calderara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16463">Modular Embedding Recomposition for Incremental Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The advent of pre-trained Vision-Language Models (VLMs) has significantly transformed Continual Learning (CL), mainly due to their zero-shot classification abilities. Such proficiency makes VLMs well-suited for real-world applications, enabling robust performance on novel unseen classes without requiring adaptation. However, fine-tuning remains essential when downstream tasks deviate significantly from the pre-training domain. Prior CL approaches primarily focus on preserving the zero-shot capabilities of VLMs during incremental fine-tuning on a downstream task. We take a step further by devising an approach that transforms preservation into enhancement of the zero-shot capabilities of VLMs. Our approach, named MoDular Embedding Recomposition (MoDER), introduces a modular framework that trains multiple textual experts, each specialized in a single seen class, and stores them in a foundational hub. At inference time, for each unseen class, we query the hub and compose the retrieved experts to synthesize a refined prototype that improves classification. We show the effectiveness of our method across two popular zero-shot incremental protocols, Class-IL and MTIL, comprising a total of 14 datasets. The codebase is available at https://github.com/aimagelab/mammoth.<br>
<span id='abs_ch'>中文: 预训练视觉语言模型通过MoDER模块化框架重组专业文本专家， 需调整即可提升对未见类别的零 本分类能力，从而推进持续学 。</span><br>
<span id='abs_en'>English: Pre-trained Vision-Language Models (VLMs) enhance Continual Learning by introducing MoDER, a modular framework that recomposes specialized textual experts to improve zero-shot classification on unseen classes without adaptation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>802, <a href='https://arxiv.org/pdf/2508.16438.pdf' target='_blank'>https://arxiv.org/pdf/2508.16438.pdf</a></span>   <span><a href='https://github.com/Ameame1/OPERA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Liu, Yanbing Liu, Fangfang Yuan, Cong Cao, Youbang Sun, Kun Peng, WeiZhuo Chen, Jianjun Li, Zhiyuan Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16438">OPERA: A Reinforcement Learning--Enhanced Orchestrated Planner-Executor Architecture for Reasoning-Oriented Multi-Hop Retrieval</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in large language models (LLMs) and dense retrievers have driven significant progress in retrieval-augmented generation (RAG). However, existing approaches face significant challenges in complex reasoning-oriented multi-hop retrieval tasks: 1) Ineffective reasoning-oriented planning: Prior methods struggle to generate robust multi-step plans for complex queries, as rule-based decomposers perform poorly on out-of-template questions. 2) Suboptimal reasoning-driven retrieval: Related methods employ limited query reformulation, leading to iterative retrieval loops that often fail to locate golden documents. 3) Insufficient reasoning-guided filtering: Prevailing methods lack the fine-grained reasoning to effectively filter salient information from noisy results, hindering utilization of retrieved knowledge. Fundamentally, these limitations all stem from the weak coupling between retrieval and reasoning in current RAG architectures. We introduce the Orchestrated Planner-Executor Reasoning Architecture (OPERA), a novel reasoning-driven retrieval framework. OPERA's Goal Planning Module (GPM) decomposes questions into sub-goals, which are executed by a Reason-Execute Module (REM) with specialized components for precise reasoning and effective retrieval. To train OPERA, we propose Multi-Agents Progressive Group Relative Policy Optimization (MAPGRPO), a novel variant of GRPO. Experiments on complex multi-hop benchmarks show OPERA's superior performance, validating both the MAPGRPO method and OPERA's design. Code is available at https://github.com/Ameame1/OPERA.<br>
<br>
<div id='section'>Paperid: <span id='pid'>803, <a href='https://arxiv.org/pdf/2508.16397.pdf' target='_blank'>https://arxiv.org/pdf/2508.16397.pdf</a></span>   <span><a href='https://github.com/zhangyongcode/GMBINet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong Zhang, Cunjian Chen, Qiang Gao, Yi Wang, Bin Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16397">A Lightweight Group Multiscale Bidirectional Interactive Network for Real-Time Steel Surface Defect Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Real-time surface defect detection is critical for maintaining product quality and production efficiency in the steel manufacturing industry. Despite promising accuracy, existing deep learning methods often suffer from high computational complexity and slow inference speeds, which limit their deployment in resource-constrained industrial environments. Recent lightweight approaches adopt multibranch architectures based on depthwise separable convolution (DSConv) to capture multiscale contextual information. However, these methods often suffer from increased computational overhead and lack effective cross-scale feature interaction, limiting their ability to fully leverage multiscale representations. To address these challenges, we propose GMBINet, a lightweight framework that enhances multiscale feature extraction and interaction through novel Group Multiscale Bidirectional Interactive (GMBI) modules. The GMBI adopts a group-wise strategy for multiscale feature extraction, ensuring scale-agnostic computational complexity. It further integrates a Bidirectional Progressive Feature Interactor (BPFI) and a parameter-free Element-Wise Multiplication-Summation (EWMS) operation to enhance cross-scale interaction without introducing additional computational overhead. Experiments on SD-Saliency-900 and NRSD-MN datasets demonstrate that GMBINet delivers competitive accuracy with real-time speeds of 1048 FPS on GPU and 16.53 FPS on CPU at 512 resolution, using only 0.19 M parameters. Additional evaluations on the NEU-CLS defect classification dataset further confirm the strong generalization ability of our method, demonstrating its potential for broader industrial vision applications beyond surface defect detection. The dataset and code are publicly available at: https://github.com/zhangyongcode/GMBINet.<br>
<span id='abs_ch'>中文: GMBINet是一种轻量级框架，通过创新的GMBI模块实现高效多尺度特征提取与交互，在保持高精度的同时显著降低计算成本，适用于钢铁制 中的实时表面缺陷检测。</span><br>
<span id='abs_en'>English: GMBINet is a lightweight framework designed for real-time surface defect detection in steel manufacturing, featuring novel GMBI modules that enable efficient multiscale feature extraction and interaction while maintaining competitive accuracy with minimal computational overhead.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>804, <a href='https://arxiv.org/pdf/2508.16390.pdf' target='_blank'>https://arxiv.org/pdf/2508.16390.pdf</a></span>   <span><a href='https://github.com/ana-rogoz/MedQARo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana-Cristina Rogoz, Radu Tudor Ionescu, Alexandra-Valentina Anghel, Ionut-Lucian Antone-Iordache, Simona Coniac, Andreea Iuliana Ionescu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16390">MedQARo: A Large-Scale Benchmark for Medical Question Answering in Romanian</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Question answering (QA) is an actively studied topic, being a core natural language processing (NLP) task that needs to be addressed before achieving Artificial General Intelligence (AGI). However, the lack of QA datasets in specific domains and languages hinders the development of robust AI models able to generalize across various domains and languages. To this end, we introduce MedQARo, the first large-scale medical QA benchmark in Romanian, alongside a comprehensive evaluation of state-of-the-art large language models (LLMs). We construct a high-quality and large-scale dataset comprising 102,646 QA pairs related to cancer patients. The questions regard medical case summaries of 1,011 patients, requiring either keyword extraction or reasoning to be answered correctly. MedQARo is the result of a time-consuming manual annotation process carried out by seven physicians specialized in oncology or radiotherapy, who spent a total of about 2,100 work hours to generate the QA pairs. We experiment with four LLMs from distinct families of models on MedQARo. Each model is employed in two scenarios, namely one based on zero-shot prompting and one based on supervised fine-tuning. Our results show that fine-tuned models significantly outperform their zero-shot counterparts, clearly indicating that pretrained models fail to generalize on MedQARo. Our findings demonstrate the importance of both domain-specific and language-specific fine-tuning for reliable clinical QA in Romanian. We publicly release our dataset and code at https://github.com/ana-rogoz/MedQARo.<br>
<span id='abs_ch'>中文: MedQARo是首个罗马尼亚语大规模医疗问答数据集，包含102,646对癌症相关问答，实验表明经过微调的大语言模型显著优于零 本模型，凸显了针对特定领域和语言进行模型适配对临床应用的重要性。</span><br>
<span id='abs_en'>English: MedQARo is the first large-scale Romanian medical QA dataset with 102,646 cancer-related question-answer pairs, demonstrating that fine-tuned LLMs significantly outperform zero-shot models and highlighting the necessity of domain-specific and language-specific adaptation for clinical applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>805, <a href='https://arxiv.org/pdf/2508.16332.pdf' target='_blank'>https://arxiv.org/pdf/2508.16332.pdf</a></span>   <span><a href='https://github.com/open-mmlab/Amphion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueyao Zhang, Junan Zhang, Yuancheng Wang, Chaoren Wang, Yuanzhe Chen, Dongya Jia, Zhuo Chen, Zhizheng Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16332">Vevo2: Bridging Controllable Speech and Singing Voice Generation via Unified Prosody Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Controllable human voice generation, particularly for expressive domains like singing, remains a significant challenge. This paper introduces Vevo2, a unified framework for controllable speech and singing voice generation. To tackle issues like the scarcity of annotated singing data and to enable flexible controllability, Vevo2 introduces two audio tokenizers: (1) a music-notation-free prosody tokenizer that captures prosody and melody from speech, singing, and even instrumental sounds, and (2) a low-frame-rate (12.5 Hz) content-style tokenizer that encodes linguistic content, prosody, and style for both speech and singing, while enabling timbre disentanglement. Vevo2 consists of an auto-regressive (AR) content-style modeling stage, which aims to enable controllability over text, prosody, and style, as well as a flow-matching acoustic modeling stage that allows for timbre control. Particularly, during pre-training of the AR model, we propose both explicit and implicit prosody learning strategies to bridge speech and singing voice. Moreover, to further enhance the AR model's ability to follow text and prosody, we design a multi-objective post-training task that integrates both intelligibility and prosody similarity alignment. Experimental results show that the unified modeling in Vevo2 brings mutual benefits to both speech and singing voice generation. Additionally, Vevo2's effectiveness across a wide range of synthesis, conversion, and editing tasks for both speech and singing further demonstrates its strong generalization ability and versatility. Audio samples are are available at https://versasinger.github.io/.<br>
<span id='abs_ch'>中文：Vevo2提出了一种可控语音和歌声生成的统一框架，通过双音频分词器和多阶段建模实现了对文本、韵律、风 和音色的灵活控制，并在多种合成任务中展现出强大的泛化能力。</span><br>
<span id='abs_en'>English: Vevo2 introduces a unified framework for controllable speech and singing voice generation, utilizing dual audio tokenizers and multi-stage modeling to enable flexible control over text, prosody, style, and timbre while demonstrating strong generalization across synthesis tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>806, <a href='https://arxiv.org/pdf/2508.16204.pdf' target='_blank'>https://arxiv.org/pdf/2508.16204.pdf</a></span>   <span><a href='https://github.com/SakanaAI/natural_niches' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>JoÃ£o Abrantes, Robert Tjarko Lange, Yujin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16204">Competition and Attraction Improve Model Fusion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Model merging is a powerful technique for integrating the specialized knowledge of multiple machine learning models into a single model. However, existing methods require manually partitioning model parameters into fixed groups for merging, which restricts the exploration of potential combinations and limits performance. To overcome these limitations, we propose Model Merging of Natural Niches (M2N2), an evolutionary algorithm with three key features: (1) dynamic adjustment of merging boundaries to progressively explore a broader range of parameter combinations; (2) a diversity preservation mechanism inspired by the competition for resources in nature, to maintain a population of diverse, high-performing models that are particularly well-suited for merging; and (3) a heuristicbased attraction metric to identify the most promising pairs of models for fusion. Our experimental results demonstrate, for the first time, that model merging can be used to evolve models entirely from scratch. Specifically, we apply M2N2 to evolve MNIST classifiers from scratch and achieve performance comparable to CMA-ES, while being computationally more efficient. Furthermore, M2N2 scales to merge specialized language and image generation models, achieving state-of-the-art performance. Notably, it preserves crucial model capabilities beyond those explicitly optimized by the fitness function, highlighting its robustness and versatility. Our code is available at https://github.com/SakanaAI/natural_niches<br>
<br>
<div id='section'>Paperid: <span id='pid'>807, <a href='https://arxiv.org/pdf/2508.16201.pdf' target='_blank'>https://arxiv.org/pdf/2508.16201.pdf</a></span>   <span><a href='https://github.com/zju-jiyicheng/SpecVLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yicheng Ji, Jun Zhang, Heming Xia, Jinpeng Chen, Lidan Shou, Gang Chen, Huan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16201">SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Video large language models (Vid-LLMs) have shown strong capabilities in understanding video content. However, their reliance on dense video token representations introduces substantial memory and computational overhead in both prefilling and decoding. To mitigate the information loss of recent video token reduction methods and accelerate the decoding stage of Vid-LLMs losslessly, we introduce SpecVLM, a training-free speculative decoding (SD) framework tailored for Vid-LLMs that incorporates staged video token pruning. Building on our novel finding that the draft model's speculation exhibits low sensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens to enable efficient speculation without sacrificing accuracy. To achieve this, we performs a two-stage pruning process: Stage I selects highly informative tokens guided by attention signals from the verifier (target model), while Stage II prunes remaining redundant ones in a spatially uniform manner. Extensive experiments on four video understanding benchmarks demonstrate the effectiveness and robustness of SpecVLM, which achieves up to 2.68$\times$ decoding speedup for LLaVA-OneVision-72B and 2.11$\times$ speedup for Qwen2.5-VL-32B. Code is available at https://github.com/zju-jiyicheng/SpecVLM.<br>
<span id='abs_ch'>Chinese: SpecVLM是一种 需训练的推测解 框架，通过两阶段剪枝方法可去除高达90%的视频 记，在 损精度的情况下显著提升视频大语言模型的解 速度。</span><br>
<span id='abs_en'>English: SpecVLM is a training-free speculative decoding framework that accelerates video large language models by pruning up to 90% of video tokens in two stages, achieving significant speed improvements without loss of accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>808, <a href='https://arxiv.org/pdf/2508.16159.pdf' target='_blank'>https://arxiv.org/pdf/2508.16159.pdf</a></span>   <span><a href='https://github.com/jarch-ma/TLG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Ma, Guo-Sen Xie, Fang Zhao, Zechao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16159">Through the Looking Glass: A Dual Perspective on Weakly-Supervised Few-Shot Segmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Meta-learning aims to uniformly sample homogeneous support-query pairs, characterized by the same categories and similar attributes, and extract useful inductive biases through identical network architectures. However, this identical network design results in over-semantic homogenization. To address this, we propose a novel homologous but heterogeneous network. By treating support-query pairs as dual perspectives, we introduce heterogeneous visual aggregation (HA) modules to enhance complementarity while preserving semantic commonality. To further reduce semantic noise and amplify the uniqueness of heterogeneous semantics, we design a heterogeneous transfer (HT) module. Finally, we propose heterogeneous CLIP (HC) textual information to enhance the generalization capability of multimodal models. In the weakly-supervised few-shot semantic segmentation (WFSS) task, with only 1/24 of the parameters of existing state-of-the-art models, TLG achieves a 13.2\% improvement on Pascal-5\textsuperscript{i} and a 9.7\% improvement on COCO-20\textsuperscript{i}. To the best of our knowledge, TLG is also the first weakly supervised (image-level) model that outperforms fully supervised (pixel-level) models under the same backbone architectures. The code is available at https://github.com/jarch-ma/TLG.<br>
<span id='abs_ch'>中文: 提出的TLG模型采用同源异构网络，通过专门模块增强语义互补性并减少噪声，在弱监督小 本语义分割任务中以极少的参数量实现了显著性能提升。</span><br>
<span id='abs_en'>English: The proposed TLG model introduces a homologous but heterogeneous network with specialized modules to enhance semantic complementarity and reduce noise, achieving significant performance improvements in weakly-supervised few-shot semantic segmentation with minimal parameters.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>809, <a href='https://arxiv.org/pdf/2508.16129.pdf' target='_blank'>https://arxiv.org/pdf/2508.16129.pdf</a></span>   <span><a href='https://github.com/lxirich/OphthaReason' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiqi Wu, Yuang Yao, Tengfei Ma, Chenran Zhang, Na Su, Tao Zhou, Geng Chen, Wen Fan, Yi Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16129">Bridging the Gap in Ophthalmic AI: MM-Retinal-Reason Dataset and OphthaReason Model toward Dynamic Multimodal Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal large language models (MLLMs) have recently demonstrated remarkable reasoning abilities with reinforcement learning paradigm. Although several multimodal reasoning models have been explored in the medical domain, most of them focus exclusively on basic reasoning, which refers to shallow inference based on visual feature matching. However, real-world clinical diagnosis extends beyond basic reasoning, demanding reasoning processes that integrate heterogeneous clinical information (such as chief complaints and medical history) with multimodal medical imaging data. To bridge this gap, we introduce MM-Retinal-Reason, the first ophthalmic multimodal dataset with the full spectrum of perception and reasoning. It encompasses both basic reasoning tasks and complex reasoning tasks, aiming to enhance visual-centric fundamental reasoning capabilities and emulate realistic clinical thinking patterns. Building upon MM-Retinal-Reason, we propose OphthaReason, the first ophthalmology-specific multimodal reasoning model with step-by-step reasoning traces. To enable flexible adaptation to both basic and complex reasoning tasks, we specifically design a novel method called Uncertainty-Aware Dynamic Thinking (UADT), which estimates sample-level uncertainty via entropy and dynamically modulates the model's exploration depth using a shaped advantage mechanism. Comprehensive experiments demonstrate that our model achieves state-of-the-art performance on both basic and complex reasoning tasks, outperforming general-purpose MLLMs, medical MLLMs, RL-based medical MLLMs, and ophthalmic MLLMs by at least 24.92\%, 15.00\%, 21.20\%, and 17.66\%. Project Page: \href{https://github.com/lxirich/OphthaReason}{link}.<br>
<br>
<div id='section'>Paperid: <span id='pid'>810, <a href='https://arxiv.org/pdf/2508.16059.pdf' target='_blank'>https://arxiv.org/pdf/2508.16059.pdf</a></span>   <span><a href='https://github.com/One1sAll/MSEF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuomin Chen, Dan Li, Jiahui Zhou, Shunyu Wu, Haozheng Ye, Jian Lou, See-Kiong Ng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.16059">Integrating Time Series into LLMs via Multi-layer Steerable Embedding Fusion for Enhanced Forecasting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Time series (TS) data are ubiquitous across various application areas, rendering time series forecasting (TSF) a fundamental task. With the astounding advances in large language models (LLMs), a variety of methods have been developed to adapt LLMs for time series forecasting. Despite unlocking the potential of LLMs in comprehending TS data, existing methods are inherently constrained by their shallow integration of TS information, wherein LLMs typically access TS representations at shallow layers, primarily at the input layer. This causes the influence of TS representations to progressively fade in deeper layers and eventually leads to ineffective adaptation between textual embeddings and TS representations. In this paper, we propose the Multi-layer Steerable Embedding Fusion (MSEF), a novel framework that enables LLMs to directly access time series patterns at all depths, thereby mitigating the progressive loss of TS information in deeper layers. Specifically, MSEF leverages off-the-shelf time series foundation models to extract semantically rich embeddings, which are fused with intermediate text representations across LLM layers via layer-specific steering vectors. These steering vectors are designed to continuously optimize the alignment between time series and textual modalities and facilitate a layer-specific adaptation mechanism that ensures efficient few-shot learning capabilities. Experimental results on seven benchmarks demonstrate significant performance improvements by MSEF compared with baselines, with an average reduction of 31.8% in terms of MSE. The code is available at https://github.com/One1sAll/MSEF.<br>
<span id='abs_ch'>中文摘要：本文提出多层可控嵌入融合框架（MSEF），通过实现时间序列表征在语言模型各层的跨层融合，解决了现有方法中时间序列信息整合浅层化的问题，在七个基准测试中平均均方误差降低31.8%。</span><br>
<span id='abs_en'>English Summary: This paper introduces the Multi-layer Steerable Embedding Fusion (MSEF) framework to address the shallow integration problem in adapting large language models for time series forecasting by enabling cross-layer fusion of time series representations, achieving a 31.8% average MSE reduction across seven benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>811, <a href='https://arxiv.org/pdf/2508.15868.pdf' target='_blank'>https://arxiv.org/pdf/2508.15868.pdf</a></span>   <span><a href='https://github.com/WNQzhu/CARFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenqiao Zhu, Ji Liu, Rongjuncheng Zhang, Haipang Wu, Yulun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15868">CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reasoning capability plays a significantly critical role in the the broad applications of Large Language Models (LLMs). To enhance the reasoning performance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning approaches have been proposed to address the limited generalization capability of LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their effectiveness, two major limitations hinder the advancement of LLMs. First, vanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and incorporate unstable reasoning path sampling, which typically results in model collapse, unstable training process, and suboptimal performance. Second, existing SFT approaches generally overemphasize the annotated CoT, potentially leading to performance degradation due to insufficient exploitation of potential CoT. In this paper, we propose a Contrastive learning with annotated CoT-based Reinforced Fine-Tuning approach, i.e., \TheName{}, to enhance the reasoning performance of LLMs while addressing the aforementioned limitations. Specifically, we propose learning a representation for each CoT. Based on this representation, we design novel contrastive signals to guide the fine-tuning process. Our approach not only fully exploits the available annotated CoT but also stabilizes the fine-tuning procedure by incorporating an additional unsupervised learning signal. We conduct comprehensive experiments and in-depth analysis with three baseline approaches, two foundation models, and two datasets to demonstrate significant advantages of \TheName{} in terms of robustness, performance (up to 10.15\%), and efficiency (up to 30.62\%). Code is available at https://github.com/WNQzhu/CARFT.<br>
<span id='abs_ch'>中文: 本文提出CARFT方法，通过结合 注思维链的对比学 进行强化微调，在提升大语言模型推理能力的同时解决了训练不稳定和思维链利用不足的问题，显著提高了性能和效率。</span><br>
<span id='abs_en'>English: This paper introduces CARFT, a reinforced fine-tuning method that leverages contrastive learning with annotated Chain-of-Thought to enhance LLMs' reasoning by stabilizing training and fully utilizing CoT data, achieving significant performance and efficiency gains.Paperid:341,https://arxiv.org/pdf/2508.15861.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>812, <a href='https://arxiv.org/pdf/2508.15827.pdf' target='_blank'>https://arxiv.org/pdf/2508.15827.pdf</a></span>   <span><a href='https://github.com/xzf-thu/Mini-Omni-Reasoner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhifei Xie, Ziyang Ma, Zihang Liu, Kaiyu Pang, Hongyu Li, Jialin Zhang, Yue Liao, Deheng Ye, Chunyan Miao, Shuicheng Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15827">Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reasoning is essential for effective communication and decision-making. While recent advances in LLMs and MLLMs have shown that incorporating explicit reasoning significantly improves understanding and generalization, reasoning in LSMs remains in a nascent stage. Early efforts attempt to transfer the "Thinking-before-Speaking" paradigm from textual models to speech. However, this sequential formulation introduces notable latency, as spoken responses are delayed until reasoning is fully completed, impairing real-time interaction and communication efficiency. To address this, we propose Mini-Omni-Reasoner, a framework that enables reasoning within speech via a novel "Thinking-in-Speaking" formulation. Rather than completing reasoning before producing any verbal output, Mini-Omni-Reasoner interleaves silent reasoning tokens with spoken response tokens at the token level. This design allows continuous speech generation while embedding structured internal reasoning, leveraging the model's high-frequency token processing capability. Although interleaved, local semantic alignment is enforced to ensure that each response token is informed by its preceding reasoning. To support this framework, we introduce Spoken-Math-Problems-3M, a large-scale dataset tailored for interleaved reasoning and response. The dataset ensures that verbal tokens consistently follow relevant reasoning content, enabling accurate and efficient learning of speech-coupled reasoning. Built on a hierarchical Thinker-Talker architecture, Mini-Omni-Reasoner delivers fluent yet logically grounded spoken responses, maintaining both naturalness and precision. On the Spoken-MQA benchmark, it achieves a +19.1% gain in arithmetic reasoning and +6.4% in contextual understanding, with shorter outputs and zero decoding latency.<br>
<span id='abs_ch'>中文: Mini-Omni-Reasoner框架提出"边说边想"模式，通过将推理 记与语音 记交织处理，在实现基准测试显著性能提升的同时，实现零延迟的实时逻辑响应。</span><br>
<span id='abs_en'>English: The proposed Mini-Omni-Reasoner framework introduces "Thinking-in-Speaking" to interleave reasoning tokens with speech tokens, enabling real-time grounded responses without latency while achieving significant performance gains on benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>813, <a href='https://arxiv.org/pdf/2508.15809.pdf' target='_blank'>https://arxiv.org/pdf/2508.15809.pdf</a></span>   <span><a href='https://github.com/SongyuanSui/ChainofQuery' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Songyuan Sui, Hongyi Liu, Serena Liu, Li Li, Soo-Hyun Choi, Rui Chen, Xia Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15809">Chain-of-Query: Unleashing the Power of LLMs in SQL-Aided Table Understanding via Multi-Agent Collaboration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Table understanding requires structured, multi-step reasoning. Large Language Models (LLMs) struggle with it due to the structural complexity of tabular data. Recently, multi-agent frameworks for SQL generation have shown promise in tackling the challenges of understanding tabular data, but existing approaches often suffer from limitations such as the inability to comprehend table structure for reliable SQL generation, error propagation that results in invalid queries, and over-reliance on execution correctness. To address these issues, we propose Chain-of-Query (CoQ), a novel multi-agent framework for SQL-aided table understanding. CoQ adopts natural-language-style representations of table schemas to abstract away structural noise and enhance understanding. It employs a clause-by-clause SQL generation strategy to improve query quality and introduces a hybrid reasoning division that separates SQL-based mechanical reasoning from LLM-based logical inference, thereby reducing reliance on execution outcomes. Experiments with four models (both closed- and open-source) across five widely used benchmarks show that Chain-of-Query significantly improves accuracy from 61.11% to 74.77% and reduces the invalid SQL rate from 9.48% to 3.34%, demonstrating its superior effectiveness in table understanding. The code is available at https://github.com/SongyuanSui/ChainofQuery.<br>
<span id='abs_ch'>中文：提出的Chain-of-Query框架通过采用自然语言模式表示和逐子句SQL生成策略，显著提升了表 理解的准确性并降低了 效查询率，在多个基准测试中表现优异。</span><br>
<span id='abs_en'>English: The proposed Chain-of-Query framework enhances table understanding by using natural language schema representations and clause-by-clause SQL generation, significantly improving accuracy and reducing invalid queries across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>814, <a href='https://arxiv.org/pdf/2508.15804.pdf' target='_blank'>https://arxiv.org/pdf/2508.15804.pdf</a></span>   <span><a href='https://github.com/ByteDance-BandAI/ReportBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minghao Li, Ying Zeng, Zhihao Cheng, Cong Ma, Kai Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15804">ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The advent of Deep Research agents has substantially reduced the time required for conducting extensive research tasks. However, these tasks inherently demand rigorous standards of factual accuracy and comprehensiveness, necessitating thorough evaluation before widespread adoption. In this paper, we propose ReportBench, a systematic benchmark designed to evaluate the content quality of research reports generated by large language models (LLMs). Our evaluation focuses on two critical dimensions: (1) the quality and relevance of cited literature, and (2) the faithfulness and veracity of the statements within the generated reports. ReportBench leverages high-quality published survey papers available on arXiv as gold-standard references, from which we apply reverse prompt engineering to derive domain-specific prompts and establish a comprehensive evaluation corpus. Furthermore, we develop an agent-based automated framework within ReportBench that systematically analyzes generated reports by extracting citations and statements, checking the faithfulness of cited content against original sources, and validating non-cited claims using web-based resources. Empirical evaluations demonstrate that commercial Deep Research agents such as those developed by OpenAI and Google consistently generate more comprehensive and reliable reports than standalone LLMs augmented with search or browsing tools. However, there remains substantial room for improvement in terms of the breadth and depth of research coverage, as well as factual consistency. The complete code and data will be released at the following link: https://github.com/ByteDance-BandAI/ReportBench<br>
<span id='abs_ch'>中文摘要：本文提出ReportBench基准，通过评估生成报告的引用质量和事实准确性，发现商业深度 究代理优于独立大语言模型，但在 究广度和事实一致性方面仍有提升空间。</span><br>
<span id='abs_en'>English Summary: This paper introduces ReportBench, a benchmark for evaluating research reports generated by large language models by assessing citation quality and factual accuracy against published surveys, revealing that commercial deep research agents outperform standalone LLMs but still require improvements in coverage and consistency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>815, <a href='https://arxiv.org/pdf/2508.15802.pdf' target='_blank'>https://arxiv.org/pdf/2508.15802.pdf</a></span>   <span><a href='https://github.com/mhjiang0408/MAC_Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohan Jiang, Jin Gao, Jiahao Zhan, Dequan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15802">MAC: A Live Benchmark for Multimodal Large Language Models in Scientific Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As multimodal large language models (MLLMs) grow increasingly capable, fixed benchmarks are gradually losing their effectiveness in evaluating high-level scientific understanding. In this paper, we introduce the Multimodal Academic Cover benchmark (MAC), a live benchmark that could continuously evolve with scientific advancement and model progress. MAC leverages over 25,000 image-text pairs sourced from issues of top-tier scientific journals such as Nature, Science, and Cell, challenging MLLMs to reason across abstract visual and textual scientific content. Experiments on our most recent yearly snapshot, MAC-2025, reveal that while MLLMs demonstrate strong perceptual abilities, their cross-modal scientific reasoning remains limited. To bridge this gap, we propose DAD, a lightweight inference-time approach that enhances MLLMs by extending MLLM visual features with language space reasoning, achieving performance improvements of up to 11%. Finally, we highlight the live nature of MAC through experiments on updating journal covers and models for curation, illustrating its potential to remain aligned with the frontier of human knowledge. We release our benchmark at https://github.com/mhjiang0408/MAC_Bench.<br>
<span id='abs_ch'>中文：MAC基准被提出作为一个动态评估多模态大语言模型的工具，利用科学期刊内容揭示跨模态推理的局限性，并提出DAD方法将性能提升高达11%。</span><br>
<span id='abs_en'>English: The MAC benchmark is introduced as a dynamic evaluation tool for multimodal large language models, using scientific journal content to reveal limitations in cross-modal reasoning and proposing the DAD method to enhance performance by up to 11%.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>816, <a href='https://arxiv.org/pdf/2508.15746.pdf' target='_blank'>https://arxiv.org/pdf/2508.15746.pdf</a></span>   <span><a href='https://github.com/MAGIC-AI4Med/Deep-DxSearch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiaoyu Zheng, Yuze Sun, Chaoyi Wu, Weike Zhao, Pengcheng Qiu, Yongguo Yu, Kun Sun, Yanfeng Wang, Ya Zhang, Weidi Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15746">End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate diagnosis with medical large language models is hindered by knowledge gaps and hallucinations. Retrieval and tool-augmented methods help, but their impact is limited by weak use of external knowledge and poor feedback-reasoning traceability. To address these challenges, We introduce Deep-DxSearch, an agentic RAG system trained end-to-end with reinforcement learning (RL) that enables steer tracebale retrieval-augmented reasoning for medical diagnosis. In Deep-DxSearch, we first construct a large-scale medical retrieval corpus comprising patient records and reliable medical knowledge sources to support retrieval-aware reasoning across diagnostic scenarios. More crutially, we frame the LLM as the core agent and the retrieval corpus as its environment, using tailored rewards on format, retrieval, reasoning structure, and diagnostic accuracy, thereby evolving the agentic RAG policy from large-scale data through RL.
  Experiments demonstrate that our end-to-end agentic RL training framework consistently outperforms prompt-engineering and training-free RAG approaches across multiple data centers. After training, Deep-DxSearch achieves substantial gains in diagnostic accuracy, surpassing strong diagnostic baselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks for both common and rare disease diagnosis under in-distribution and out-of-distribution settings. Moreover, ablation studies on reward design and retrieval corpus components confirm their critical roles, underscoring the uniqueness and effectiveness of our approach compared with traditional implementations. Finally, case studies and interpretability analyses highlight improvements in Deep-DxSearch's diagnostic policy, providing deeper insight into its performance gains and supporting clinicians in delivering more reliable and precise preliminary diagnoses. See https://github.com/MAGIC-AI4Med/Deep-DxSearch.<br>
<span id='abs_ch'>中文摘要：Deep-DxSearch是一种基于强化学 的智能检索增强生成系统，通过提升外部知识利用和推理可追溯性来改进医疗诊断，在多种临床场景中显著超越现有模型的准确率表现。</span><br>
<span id='abs_en'>English Summary: Deep-DxSearch is an agentic retrieval-augmented generation system trained with reinforcement learning that enhances medical diagnosis by improving knowledge utilization and reasoning traceability, outperforming existing models in accuracy across diverse clinical settings.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>817, <a href='https://arxiv.org/pdf/2508.15693.pdf' target='_blank'>https://arxiv.org/pdf/2508.15693.pdf</a></span>   <span><a href='https://github.com/KempnerInstitute/nicewebrl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wilka Carvalho, Vikram Goddla, Ishaan Sinha, Hoon Shin, Kunal Jha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15693">NiceWebRL: a Python library for human subject experiments with reinforcement learning environments</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present NiceWebRL, a research tool that enables researchers to use machine reinforcement learning (RL) environments for online human subject experiments. NiceWebRL is a Python library that allows any Jax-based environment to be transformed into an online interface, supporting both single-agent and multi-agent environments. As such, NiceWebRL enables AI researchers to compare their algorithms to human performance, cognitive scientists to test ML algorithms as theories for human cognition, and multi-agent researchers to develop algorithms for human-AI collaboration. We showcase NiceWebRL with 3 case studies that demonstrate its potential to help develop Human-like AI, Human-compatible AI, and Human-assistive AI. In the first case study (Human-like AI), NiceWebRL enables the development of a novel RL model of cognition. Here, NiceWebRL facilitates testing this model against human participants in both a grid world and Craftax, a 2D Minecraft domain. In our second case study (Human-compatible AI), NiceWebRL enables the development of a novel multi-agent RL algorithm that can generalize to human partners in the Overcooked domain. Finally, in our third case study (Human-assistive AI), we show how NiceWebRL can allow researchers to study how an LLM can assist humans on complex tasks in XLand-Minigrid, an environment with millions of hierarchical tasks. The library is available at https://github.com/KempnerInstitute/nicewebrl.<br>
<span id='abs_ch'>中文: NiceWebRL是一个Python库，可将基于Jax的强化学 环境转化为在线实验平台，使 究人员能够比较AI算法与人类表现、测试认知模型，并在多领域开发人机协作应用。</span><br>
<span id='abs_en'>English: NiceWebRL is a Python library that transforms Jax-based reinforcement learning environments into online interfaces, enabling researchers to compare AI algorithms with human performance, test cognitive models, and develop human-AI collaboration across various domains.Paperid:354,https://arxiv.org/pdf/2508.15672.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>818, <a href='https://arxiv.org/pdf/2508.15658.pdf' target='_blank'>https://arxiv.org/pdf/2508.15658.pdf</a></span>   <span><a href='https://github.com/oneal2000/SurGE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihang Su, Anzhe Xie, Qingyao Ai, Jianming Long, Jiaxin Mao, Ziyi Ye, Yiqun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15658">Benchmarking Computer Science Survey Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Scientific survey articles play a vital role in summarizing research progress, yet their manual creation is becoming increasingly infeasible due to the rapid growth of academic literature. While large language models (LLMs) offer promising capabilities for automating this process, progress in this area is hindered by the absence of standardized benchmarks and evaluation protocols. To address this gap, we introduce SurGE (Survey Generation Evaluation), a new benchmark for evaluating scientific survey generation in the computer science domain. SurGE consists of (1) a collection of test instances, each including a topic description, an expert-written survey, and its full set of cited references, and (2) a large-scale academic corpus of over one million papers that serves as the retrieval pool. In addition, we propose an automated evaluation framework that measures generated surveys across four dimensions: information coverage, referencing accuracy, structural organization, and content quality. Our evaluation of diverse LLM-based approaches shows that survey generation remains highly challenging, even for advanced self-reflection frameworks. These findings highlight the complexity of the task and the necessity for continued research. We have open-sourced all the code, data, and models at: https://github.com/oneal2000/SurGE<br>
<span id='abs_ch'>中文摘要：SurGE基准通过提供测试实例、大规模学术语料库和多维评估框架，解决了科学文献自动综述领域缺乏 准化评估的问题，揭示了当前大语言模型在此复杂任务中的明显不足。</span><br>
<span id='abs_en'>English Summary: The SurGE benchmark addresses the lack of standardized evaluation for automated scientific survey generation by providing test instances, a large academic corpus, and a multidimensional assessment framework, revealing current LLMs' limitations in this complex task.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>819, <a href='https://arxiv.org/pdf/2508.15658.pdf' target='_blank'>https://arxiv.org/pdf/2508.15658.pdf</a></span>   <span><a href='https://github.com/oneal2000/SurGE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihang Su, Anzhe Xie, Qingyao Ai, Jianming Long, Jiaxin Mao, Ziyi Ye, Yiqun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15658">SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid growth of academic literature makes the manual creation of scientific surveys increasingly infeasible. While large language models show promise for automating this process, progress in this area is hindered by the absence of standardized benchmarks and evaluation protocols. To bridge this critical gap, we introduce SurGE (Survey Generation Evaluation), a new benchmark for scientific survey generation in computer science. SurGE consists of (1) a collection of test instances, each including a topic description, an expert-written survey, and its full set of cited references, and (2) a large-scale academic corpus of over one million papers. In addition, we propose an automated evaluation framework that measures the quality of generated surveys across four dimensions: comprehensiveness, citation accuracy, structural organization, and content quality. Our evaluation of diverse LLM-based methods demonstrates a significant performance gap, revealing that even advanced agentic frameworks struggle with the complexities of survey generation and highlighting the need for future research in this area. We have open-sourced all the code, data, and models at: https://github.com/oneal2000/SurGE<br>
<span id='abs_ch'>中文摘要：SurGE基准通过提供测试实例、大规模学术语料库和多维评估框架，解决了科学文献自动综述领域缺乏 准化评估的问题，揭示了当前大语言模型在此复杂任务中的明显不足。</span><br>
<span id='abs_en'>English Summary: The SurGE benchmark addresses the lack of standardized evaluation for automated scientific survey generation by providing test instances, a large academic corpus, and a multidimensional assessment framework, revealing current LLMs' limitations in this complex task.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>820, <a href='https://arxiv.org/pdf/2508.15610.pdf' target='_blank'>https://arxiv.org/pdf/2508.15610.pdf</a></span>   <span><a href='https://github.com/IBM/Agentics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alfio Gliozzo, Naweed Khan, Christodoulos Constantinides, Nandana Mihindukulasooriya, Nahuel Defosse, Gaetano Rossiello, Junkyu Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15610">Transduction is All You Need for Structured Data Workflows</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper introduces Agentics, a functional agentic AI framework for building LLM-based structured data workflow pipelines. Designed for both research and practical applications, Agentics offers a new data-centric paradigm in which agents are embedded within data types, enabling logical transduction between structured states. This design shifts the focus toward principled data modeling, providing a declarative language where data types are directly exposed to large language models and composed through transductions triggered by type connections. We present a range of structured data workflow tasks and empirical evidence demonstrating the effectiveness of this approach, including data wrangling, text-to-SQL semantic parsing, and domain-specific multiple-choice question answering. The open source Agentics is available at https://github.com/IBM/Agentics.<br>
<span id='abs_ch'>中文摘要：本文介绍了Agentics框架，它通过模块化设计支持基于智能体的系统进行结构化推理和组合泛化，使开发者能够以声明式方法利用大语言模型处理数据，并在多项AI任务中实现最优性能。</span><br>
<span id='abs_en'>English Summary: This paper presents Agentics, a modular framework that enables structured reasoning and compositional generalization for agent-based systems, allowing developers to model data declaratively using LLMs and achieve state-of-the-art results across various AI tasks.</span>Paperid:362,https://arxiv.org/pdf/2508.15601.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>821, <a href='https://arxiv.org/pdf/2508.15610.pdf' target='_blank'>https://arxiv.org/pdf/2508.15610.pdf</a></span>   <span><a href='https://github.com/IBM/agentics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alfio Gliozzo, Naweed Khan, Christodoulos Constantinides, Nandana Mihindukulasooriya, Nahuel Defosse, Junkyu Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15610">Transduction is All You Need for Structured Data Workflows</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper introduces Agentics, a modular framework for building agent-based systems capable of structured reasoning and compositional generalization over complex data. Designed with research and practical applications in mind, Agentics offers a novel perspective on working with data and AI workflows. In this framework, agents are abstracted from the logical flow and they are used internally to the data type to enable logical transduction among data. Agentics encourages AI developers to focus on modeling data rather than crafting prompts, enabling a declarative language in which data types are provided by LLMs and composed through logical transduction, which is executed by LLMs when types are connected. We provide empirical evidence demonstrating the applicability of this framework across domain-specific multiple-choice question answering, semantic parsing for text-to-SQL, and automated prompt optimization tasks, achieving state-of-the-art accuracy or improved scalability without sacrificing performance. The open-source implementation is available at \texttt{https://github.com/IBM/agentics}.<br>
<span id='abs_ch'>中文摘要：本文介绍了Agentics框架，它通过模块化设计支持基于智能体的系统进行结构化推理和组合泛化，使开发者能够以声明式方法利用大语言模型处理数据，并在多项AI任务中实现最优性能。</span><br>
<span id='abs_en'>English Summary: This paper presents Agentics, a modular framework that enables structured reasoning and compositional generalization for agent-based systems, allowing developers to model data declaratively using LLMs and achieve state-of-the-art results across various AI tasks.</span>Paperid:362,https://arxiv.org/pdf/2508.15601.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>822, <a href='https://arxiv.org/pdf/2508.15510.pdf' target='_blank'>https://arxiv.org/pdf/2508.15510.pdf</a></span>   <span><a href='https://github.com/pippot/Superadditive-cooperation-LLMs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Filippo Tonini, Lukas Galke
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15510">Super-additive Cooperation in Language Model Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the prospect of autonomous artificial intelligence (AI) agents, studying their tendency for cooperative behavior becomes an increasingly relevant topic. This study is inspired by the super-additive cooperation theory, where the combined effects of repeated interactions and inter-group rivalry have been argued to be the cause for cooperative tendencies found in humans. We devised a virtual tournament where language model agents, grouped into teams, face each other in a Prisoner's Dilemma game. By simulating both internal team dynamics and external competition, we discovered that this blend substantially boosts both overall and initial, one-shot cooperation levels (the tendency to cooperate in one-off interactions). This research provides a novel framework for large language models to strategize and act in complex social scenarios and offers evidence for how intergroup competition can, counter-intuitively, result in more cooperative behavior. These insights are crucial for designing future multi-agent AI systems that can effectively work together and better align with human values. Source code is available at https://github.com/pippot/Superadditive-cooperation-LLMs.<br>
<span id='abs_ch'>中文摘要：本 究通过设计虚拟锦 赛发现，团队内部重复互动与团队间竞争相结合能显著提升AI代理的合作水平，为开发符合人类价值观的协作式多智能体系统提供了新框架。</span><br>
<span id='abs_en'>English Summary: This study demonstrates that combining repeated interactions within teams and inter-group competition in a Prisoner's Dilemma tournament significantly enhances cooperation among AI agents, offering a framework for developing collaborative multi-agent systems aligned with human values.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>823, <a href='https://arxiv.org/pdf/2508.15501.pdf' target='_blank'>https://arxiv.org/pdf/2508.15501.pdf</a></span>   <span><a href='https://github.com/ZXiiiC/SRDrone' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Deyu Zhang, Xicheng Zhang, Jiahao Li, Tingting Long, Xunhua Dai, Yongjian Fu, Jinrui Zhang, Ju Ren, Yaoxue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15501">LLM-Driven Self-Refinement for Embodied Drone Task Planning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce SRDrone, a novel system designed for self-refinement task planning in industrial-grade embodied drones. SRDrone incorporates two key technical contributions: First, it employs a continuous state evaluation methodology to robustly and accurately determine task outcomes and provide explanatory feedback. This approach supersedes conventional reliance on single-frame final-state assessment for continuous, dynamic drone operations. Second, SRDrone implements a hierarchical Behavior Tree (BT) modification model. This model integrates multi-level BT plan analysis with a constrained strategy space to enable structured reflective learning from experience. Experimental results demonstrate that SRDrone achieves a 44.87% improvement in Success Rate (SR) over baseline methods. Furthermore, real-world deployment utilizing an experience base optimized through iterative self-refinement attains a 96.25% SR. By embedding adaptive task refinement capabilities within an industrial-grade BT planning framework, SRDrone effectively integrates the general reasoning intelligence of Large Language Models (LLMs) with the stringent physical execution constraints inherent to embodied drones. Code is available at https://github.com/ZXiiiC/SRDrone.<br>
<span id='abs_ch'>中文：SRDrone是一种用于工业级 人机的创新系统，通过持续状态评估和分层行为 修改来优化任务规划，相比基准方法显著提升了任务成功率。</span><br>
<span id='abs_en'>English: SRDrone is a novel system for industrial drones that enhances task planning through continuous state evaluation and hierarchical Behavior Tree modifications, achieving significant success rate improvements over baseline methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>824, <a href='https://arxiv.org/pdf/2508.15476.pdf' target='_blank'>https://arxiv.org/pdf/2508.15476.pdf</a></span>   <span><a href='https://github.com/cq-dong/LGMSNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengqi Dong, Fenghe Tang, Rongge Mao, Xinpei Gao, S. Kevin Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15476">LGMSNet: Thinning a medical image segmentation model via dual-level multiscale fusion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Medical image segmentation plays a pivotal role in disease diagnosis and treatment planning, particularly in resource-constrained clinical settings where lightweight and generalizable models are urgently needed. However, existing lightweight models often compromise performance for efficiency and rarely adopt computationally expensive attention mechanisms, severely restricting their global contextual perception capabilities. Additionally, current architectures neglect the channel redundancy issue under the same convolutional kernels in medical imaging, which hinders effective feature extraction. To address these challenges, we propose LGMSNet, a novel lightweight framework based on local and global dual multiscale that achieves state-of-the-art performance with minimal computational overhead. LGMSNet employs heterogeneous intra-layer kernels to extract local high-frequency information while mitigating channel redundancy. In addition, the model integrates sparse transformer-convolutional hybrid branches to capture low-frequency global information. Extensive experiments across six public datasets demonstrate LGMSNet's superiority over existing state-of-the-art methods. In particular, LGMSNet maintains exceptional performance in zero-shot generalization tests on four unseen datasets, underscoring its potential for real-world deployment in resource-limited medical scenarios. The whole project code is in https://github.com/cq-dong/LGMSNet.<br>
<span id='abs_ch'>中文: LGMSNet是一种新颖的轻量级医学图像分割框架，通过异构内 和变换器-卷积混合设计，在最小计算成本下实现卓越性能，并在多个数据集上展现出强大的泛化能力。English: LGMSNet is a novel lightweight medical image segmentation framework that uses heterogeneous kernels and transformer-convolutional hybrids to achieve superior performance with minimal computational cost, demonstrating strong generalization across multiple datasets.=Paperid:369,https://arxiv.org/pdf/2508.15449.pdfGitHub</span><br>
<span id='abs_en'>English: LGMSNet is a novel lightweight medical image segmentation framework that uses heterogeneous kernels and transformer-convolutional hybrids to achieve superior performance with minimal computational cost, demonstrating strong generalization across multiple datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>825, <a href='https://arxiv.org/pdf/2508.15449.pdf' target='_blank'>https://arxiv.org/pdf/2508.15449.pdf</a></span>   <span><a href='https://github.com/ChengcanWu/MRP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengcan Wu, Zeming Wei, Huanran Chen, Yinpeng Dong, Meng Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15449">Reliable Unlearning Harmful Information in LLMs with Metamorphosis Representation Projection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While Large Language Models (LLMs) have demonstrated impressive performance in various domains and tasks, concerns about their safety are becoming increasingly severe. In particular, since models may store unsafe knowledge internally, machine unlearning has emerged as a representative paradigm to ensure model safety. Existing approaches employ various training techniques, such as gradient ascent and negative preference optimization, in attempts to eliminate the influence of undesired data on target models. However, these methods merely suppress the activation of undesired data through parametric training without completely eradicating its informational traces within the model. This fundamental limitation makes it difficult to achieve effective continuous unlearning, rendering these methods vulnerable to relearning attacks. To overcome these challenges, we propose a Metamorphosis Representation Projection (MRP) approach that pioneers the application of irreversible projection properties to machine unlearning. By implementing projective transformations in the hidden state space of specific network layers, our method effectively eliminates harmful information while preserving useful knowledge. Experimental results demonstrate that our approach enables effective continuous unlearning and successfully defends against relearning attacks, achieving state-of-the-art performance in unlearning effectiveness while preserving natural performance. Our code is available in https://github.com/ChengcanWu/MRP.<br>
<span id='abs_ch'>中文: 本文提出的蜕变表示投影（MRP）方法通过在隐藏层实施不可逆变换，有效消除有害知识同时保留有用信息，实现了最先进的遗忘性能并能防御再学 攻击。</span><br>
<span id='abs_en'>English: The proposed Metamorphosis Representation Projection (MRP) method applies irreversible transformations to hidden layers, effectively removing harmful knowledge while maintaining useful information and achieving state-of-the-art unlearning performance with defense against relearning attacks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>826, <a href='https://arxiv.org/pdf/2508.15418.pdf' target='_blank'>https://arxiv.org/pdf/2508.15418.pdf</a></span>   <span><a href='https://github.com/EIT-NLP/LLaSO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yirong Sun, Yizhong Geng, Peidong Wei, Yanjun Chen, Jinghan Yang, Rongfei Chen, Wei Zhang, Xiaoyu Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15418">LLaSO: A Foundational Framework for Reproducible Research in Large Language and Speech Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The development of Large Speech-Language Models (LSLMs) has been slowed by fragmented architectures and a lack of transparency, hindering the systematic comparison and reproducibility of research. Unlike in the vision-language domain, the LSLM field suffers from the common practice of releasing model weights without their corresponding training data and configurations. To address these critical gaps, we introduce LLaSO, the first fully open, end-to-end framework for large-scale speech-language modeling. LLaSO provides the community with three essential resources: (1) LLaSO-Align, a 12M-instance speech-text alignment corpus; (2) LLaSO-Instruct, a 13.5M-instance multi-task instruction-tuning dataset; and (3) LLaSO-Eval, a reproducible benchmark for standardized evaluation. To validate our framework, we build and release LLaSO-Base, a 3.8B-parameter reference model trained exclusively on our public data. It achieves a normalized score of 0.72, establishing a strong, reproducible baseline that surpasses comparable models. Our analysis reveals that while broader training coverage enhances performance, significant generalization gaps persist on unseen tasks, particularly in pure audio scenarios. By releasing the complete stack of data, benchmarks, and models, LLaSO establishes a foundational open standard to unify research efforts and accelerate community-driven progress in LSLMs. We release the code, dataset, pretrained models, and results in https://github.com/EIT-NLP/LLaSO.<br>
<span id='abs_ch'>Chinese: LLaSO框架通过提供开放数据集、基准测试和38亿参数模型，解决了大型语音语言模型领域的碎片化问题，建立了超越同类模型的可复现基线。</span><br>
<span id='abs_en'>English: The LLaSO framework addresses fragmentation in Large Speech-Language Models by providing open datasets, benchmarks, and a 3.8B-parameter model that establishes a reproducible baseline surpassing comparable models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>827, <a href='https://arxiv.org/pdf/2508.15413.pdf' target='_blank'>https://arxiv.org/pdf/2508.15413.pdf</a></span>   <span><a href='https://github.com/kangpx/onlineTiny2023' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pixi Kang, Julian Moosmann, Mengxi Liu, Bo Zhou, Michele Magno, Paul Lukowicz, Sizhen Bian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15413">Bridging Generalization and Personalization in Human Activity Recognition via On-Device Few-Shot Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Human Activity Recognition (HAR) with different sensing modalities requires both strong generalization across diverse users and efficient personalization for individuals. However, conventional HAR models often fail to generalize when faced with user-specific variations, leading to degraded performance. To address this challenge, we propose a novel on-device few-shot learning framework that bridges generalization and personalization in HAR. Our method first trains a generalizable representation across users and then rapidly adapts to new users with only a few labeled samples, updating lightweight classifier layers directly on resource-constrained devices. This approach achieves robust on-device learning with minimal computation and memory cost, making it practical for real-world deployment. We implement our framework on the energy-efficient RISC-V GAP9 microcontroller and evaluate it on three benchmark datasets (RecGym, QVAR-Gesture, Ultrasound-Gesture). Across these scenarios, post-deployment adaptation improves accuracy by 3.73\%, 17.38\%, and 3.70\%, respectively. These results demonstrate that few-shot on-device learning enables scalable, user-aware, and energy-efficient wearable human activity recognition by seamlessly uniting generalization and personalization. The related framework is open sourced for further research\footnote{https://github.com/kangpx/onlineTiny2023}.<br>
<span id='abs_ch'>中文: 本文提出了一种新颖的设备端少 本学 框架，通过先训练跨用户的通用模型，再以少量数据高效适配个体用户，在资源受限设备上以低计算成本显著提升了人类活动识别的准确性。</span><br>
<span id='abs_en'>English: This paper introduces a novel on-device few-shot learning framework that enhances human activity recognition by first training a generalizable model across users and then efficiently adapting it to individual users with minimal data, achieving significant accuracy improvements while maintaining low computational costs on resource-constrained devices.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>828, <a href='https://arxiv.org/pdf/2508.15407.pdf' target='_blank'>https://arxiv.org/pdf/2508.15407.pdf</a></span>   <span><a href='https://github.com/WangCheng0116/MCR-BENCH' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Wang, Gelei Deng, Xianglin Yang, Han Qiu, Tianwei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15407">When Audio and Text Disagree: Revealing Text Bias in Large Audio-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Audio-Language Models (LALMs) are enhanced with audio perception capabilities, enabling them to effectively process and understand multimodal inputs that combine audio and text. However, their performance in handling conflicting information between audio and text modalities remains largely unexamined. This paper introduces MCR-BENCH, the first comprehensive benchmark specifically designed to evaluate how LALMs prioritize information when presented with inconsistent audio-text pairs. Through extensive evaluation across diverse audio understanding tasks, we reveal a concerning phenomenon: when inconsistencies exist between modalities, LALMs display a significant bias toward textual input, frequently disregarding audio evidence. This tendency leads to substantial performance degradation in audio-centric tasks and raises important reliability concerns for real-world applications. We further investigate the influencing factors of text bias, and explore mitigation strategies through supervised finetuning, and analyze model confidence patterns that reveal persistent overconfidence even with contradictory inputs. These findings underscore the need for improved modality balance during training and more sophisticated fusion mechanisms to enhance the robustness when handling conflicting multi-modal inputs. The project is available at https://github.com/WangCheng0116/MCR-BENCH.<br>
<span id='abs_ch'>中文: 本文提出MCR-BENCH基准测试，发现大音频语言模型在处理冲突的音频-文本输入时存在显著文本偏向，导致音频任务性能下降，亟需改进模态平衡机制。</span><br>
<span id='abs_en'>English: This paper introduces MCR-BENCH, a benchmark revealing that Large Audio-Language Models exhibit significant text bias when processing conflicting audio-text inputs, leading to performance degradation in audio tasks and highlighting the need for better modality balance.Paperid:374,https://arxiv.org/pdf/2508.15389.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>829, <a href='https://arxiv.org/pdf/2508.15313.pdf' target='_blank'>https://arxiv.org/pdf/2508.15313.pdf</a></span>   <span><a href='https://github.com/Lwt-diamond/RAG-SEG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wutao Liu, YiDan Wang, Pan Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15313">First RAG, Second SEG: A Training-Free Paradigm for Camouflaged Object Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Camouflaged object detection (COD) poses a significant challenge in computer vision due to the high similarity between objects and their backgrounds. Existing approaches often rely on heavy training and large computational resources. While foundation models such as the Segment Anything Model (SAM) offer strong generalization, they still struggle to handle COD tasks without fine-tuning and require high-quality prompts to yield good performance. However, generating such prompts manually is costly and inefficient. To address these challenges, we propose \textbf{First RAG, Second SEG (RAG-SEG)}, a training-free paradigm that decouples COD into two stages: Retrieval-Augmented Generation (RAG) for generating coarse masks as prompts, followed by SAM-based segmentation (SEG) for refinement. RAG-SEG constructs a compact retrieval database via unsupervised clustering, enabling fast and effective feature retrieval. During inference, the retrieved features produce pseudo-labels that guide precise mask generation using SAM2. Our method eliminates the need for conventional training while maintaining competitive performance. Extensive experiments on benchmark COD datasets demonstrate that RAG-SEG performs on par with or surpasses state-of-the-art methods. Notably, all experiments are conducted on a \textbf{personal laptop}, highlighting the computational efficiency and practicality of our approach. We present further analysis in the Appendix, covering limitations, salient object detection extension, and possible improvements. \textcolor{blue} {Code: https://github.com/Lwt-diamond/RAG-SEG.}<br>
<span id='abs_ch'>中文: 提出的RAG-SEG方法通过检索增强生成创建提示词和SAM分割优化的两阶段设计， 需训练即可实现竞争性伪装物体检测性能，且能在个人笔记本电脑上高效运行。</span><br>
<span id='abs_en'>English: The proposed RAG-SEG method addresses camouflaged object detection by combining retrieval-augmented generation for prompt creation and SAM-based segmentation for refinement, achieving competitive performance without training while operating efficiently on personal laptops.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>830, <a href='https://arxiv.org/pdf/2508.15253.pdf' target='_blank'>https://arxiv.org/pdf/2508.15253.pdf</a></span>   <span><a href='https://github.com/eunseongc/CARE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Eunseong Choi, June Park, Hyeri Lee, Jongwuk Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15253">Conflict-Aware Soft Prompting for Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-augmented generation (RAG) enhances the capabilities of large language models (LLMs) by incorporating external knowledge into their input prompts. However, when the retrieved context contradicts the LLM's parametric knowledge, it often fails to resolve the conflict between incorrect external context and correct parametric knowledge, known as context-memory conflict. To tackle this problem, we introduce Conflict-Aware REtrieval-Augmented Generation (CARE), consisting of a context assessor and a base LLM. The context assessor encodes compact memory token embeddings from raw context tokens. Through grounded/adversarial soft prompting, the context assessor is trained to discern unreliable context and capture a guidance signal that directs reasoning toward the more reliable knowledge source. Extensive experiments show that CARE effectively mitigates context-memory conflicts, leading to an average performance gain of 5.0\% on QA and fact-checking benchmarks, establishing a promising direction for trustworthy and adaptive RAG systems.<br>
<span id='abs_ch'>中文摘要：CARE框架通过上下文评估器与软提示技术解决RAG系统中的上下文记忆冲突问题，能在问答和事实 查基准上实现5.0%的平均性能提升。</span><br>
<span id='abs_en'>English Summary: The CARE framework addresses context-memory conflicts in RAG systems by using a context assessor with soft prompting to identify unreliable external context, achieving 5.0% average performance gains on benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>831, <a href='https://arxiv.org/pdf/2508.15222.pdf' target='_blank'>https://arxiv.org/pdf/2508.15222.pdf</a></span>   <span><a href='https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hantao Zhang, Jingyang Liu, Ed Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15222">See it. Say it. Sorted: Agentic System for Compositional Diagram Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We study sketch-to-diagram generation: converting rough hand sketches into precise, compositional diagrams. Diffusion models excel at photorealism but struggle with the spatial precision, alignment, and symbolic structure required for flowcharts. We introduce See it. Say it. Sorted., a training-free agentic system that couples a Vision-Language Model (VLM) with Large Language Models (LLMs) to produce editable Scalable Vector Graphics (SVG) programs. The system runs an iterative loop in which a Critic VLM proposes a small set of qualitative, relational edits; multiple candidate LLMs synthesize SVG updates with diverse strategies (conservative->aggressive, alternative, focused); and a Judge VLM selects the best candidate, ensuring stable improvement. This design prioritizes qualitative reasoning over brittle numerical estimates, preserves global constraints (e.g., alignment, connectivity), and naturally supports human-in-the-loop corrections. On 10 sketches derived from flowcharts in published papers, our method more faithfully reconstructs layout and structure than two frontier closed-source image generation LLMs (GPT-5 and Gemini-2.5-Pro), accurately composing primitives (e.g., multi-headed arrows) without inserting unwanted text. Because outputs are programmatic SVGs, the approach is readily extensible to presentation tools (e.g., PowerPoint) via APIs and can be specialized with improved prompts and task-specific tools. The codebase is open-sourced at https://github.com/hantaoZhangrichard/see_it_say_it_sorted.git.<br>
<span id='abs_ch'>中文: 本 究提出了一种 需训练的智能系统，通过结合视觉语言模型与大语言模型的迭代优化，将手绘草图转化为精确可编辑的矢量图表，在布局还原度上超越现有模型，并具备程序化扩展能力。</span><br>
<span id='abs_en'>English: This research introduces a training-free agentic system that combines Vision-Language and Large Language Models to convert hand sketches into precise, editable SVG diagrams through iterative refinement, outperforming existing models in layout accuracy while enabling programmatic extensibility.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>832, <a href='https://arxiv.org/pdf/2508.15212.pdf' target='_blank'>https://arxiv.org/pdf/2508.15212.pdf</a></span>   <span><a href='https://github.com/Xnhyacinth/SparK' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huanxuan Liao, Yixing Xu, Shizhu He, Guanchen Li, Xuanwu Yin, Dong Li, Emad Barsoum, Jun Zhao, Kang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15212">SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.<br>
<span id='abs_ch'>中文: SPARK通过通道级剪枝和动态恢复机制，有效缓解大语言模型中的KV缓存瓶颈，在同等内存下可处理更长序列，存储减少超30%且精度 损甚至提升。</span><br>
<span id='abs_en'>English: The KV cache bottleneck in large language models is addressed by SPARK, a training-free method that prunes redundant channels and dynamically restores them during computation, reducing memory usage by over 30% while maintaining or improving accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>833, <a href='https://arxiv.org/pdf/2508.15180.pdf' target='_blank'>https://arxiv.org/pdf/2508.15180.pdf</a></span>   <span><a href='https://github.com/HiThink-Research/PuzzleClone' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Xiong, Yanwei Huang, Rongjunchen Zhang, Kun Chen, Haipang Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15180">PuzzleClone: An SMT-Powered Framework for Synthesizing Verifiable Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>High-quality mathematical and logical datasets with verifiable answers are essential for strengthening the reasoning capabilities of large language models (LLMs). While recent data augmentation techniques have facilitated the creation of large-scale benchmarks, existing LLM-generated datasets often suffer from limited reliability, diversity, and scalability. To address these challenges, we introduce PuzzleClone, a formal framework for synthesizing verifiable data at scale using Satisfiability Modulo Theories (SMT). Our approach features three key innovations: (1) encoding seed puzzles into structured logical specifications, (2) generating scalable variants through systematic variable and constraint randomization, and (3) ensuring validity via a reproduction mechanism. Applying PuzzleClone, we construct a curated benchmark comprising over 83K diverse and programmatically validated puzzles. The generated puzzles span a wide spectrum of difficulty and formats, posing significant challenges to current state-of-the-art models. We conduct post training (SFT and RL) on PuzzleClone datasets. Experimental results show that training on PuzzleClone yields substantial improvements not only on PuzzleClone testset but also on logic and mathematical benchmarks. Post training raises PuzzleClone average from 14.4 to 56.2 and delivers consistent improvements across 7 logic and mathematical benchmarks up to 12.5 absolute percentage points (AMC2023 from 52.5 to 65.0). Our code and data are available at https://github.com/HiThink-Research/PuzzleClone.<br>
<span id='abs_ch'>中文: PuzzleClone提出了一种基于可满足性模理论的框架，用于生成可扩展且可验证的数学逻辑谜题，通过系统化的数据增强显著提升了大语言模型的推理能力，并在多个基准测试中实现了显著性能提升。</span><br>
<span id='abs_en'>English: PuzzleClone introduces a formal SMT-based framework for generating scalable and verifiable mathematical puzzles, significantly enhancing LLMs' reasoning capabilities through systematic data augmentation and achieving notable performance gains across multiple benchmarks.</span>Paperid:392,https://arxiv.org/pdf/2508.15144.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>834, <a href='https://arxiv.org/pdf/2508.15144.pdf' target='_blank'>https://arxiv.org/pdf/2508.15144.pdf</a></span>   <span><a href='https://github.com/X-PLUG/MobileAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, Jitong Liao, Qi Zheng, Fei Huang, Jingren Zhou, Ming Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15144">Mobile-Agent-v3: Fundamental Agents for GUI Automation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at https://github.com/X-PLUG/MobileAgent.<br>
<br>
<div id='section'>Paperid: <span id='pid'>835, <a href='https://arxiv.org/pdf/2508.15126.pdf' target='_blank'>https://arxiv.org/pdf/2508.15126.pdf</a></span>   <span><a href='https://github.com/aixiv-org' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengsong Zhang, Xiang Hu, Guowei Huang, Yang Qi, Heng Zhang, Xiuxu Li, Jiaxing Song, Jiabin Luo, Yijiang Li, Shuo Yin, Chengxiao Dai, Eric Hanchen Jiang, Xiaoyan Zhou, Zhenfei Yin, Boqin Yuan, Jing Dong, Guinan Su, Guanren Qiao, Haiming Tang, Anghong Du, Lili Pan, Zhenzhong Lan, Xinyu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15126">aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in large language models (LLMs) have enabled AI agents to autonomously generate scientific proposals, conduct experiments, author papers, and perform peer reviews. Yet this flood of AI-generated research content collides with a fragmented and largely closed publication ecosystem. Traditional journals and conferences rely on human peer review, making them difficult to scale and often reluctant to accept AI-generated research content; existing preprint servers (e.g. arXiv) lack rigorous quality-control mechanisms. Consequently, a significant amount of high-quality AI-generated research lacks appropriate venues for dissemination, hindering its potential to advance scientific progress. To address these challenges, we introduce aiXiv, a next-generation open-access platform for human and AI scientists. Its multi-agent architecture allows research proposals and papers to be submitted, reviewed, and iteratively refined by both human and AI scientists. It also provides API and MCP interfaces that enable seamless integration of heterogeneous human and AI scientists, creating a scalable and extensible ecosystem for autonomous scientific discovery. Through extensive experiments, we demonstrate that aiXiv is a reliable and robust platform that significantly enhances the quality of AI-generated research proposals and papers after iterative revising and reviewing on aiXiv. Our work lays the groundwork for a next-generation open-access ecosystem for AI scientists, accelerating the publication and dissemination of high-quality AI-generated research content. Code is available at https://github.com/aixiv-org. Website is available at https://forms.gle/DxQgCtXFsJ4paMtn8.<br>
<span id='abs_ch'>中文: 大语言模型的进步催生了AI生成的 究内容，但现有出版平台难以接纳， 此推出了aiXiv这一可扩展的开放平台，整合人类与AI科学家，实现协作的 究提交、评审与改进。</span><br>
<span id='abs_en'>English: Recent advances in LLMs have enabled AI-generated research, but existing publication platforms struggle to accommodate it, leading to the development of aiXiv, a scalable open-access platform that integrates human and AI scientists for collaborative research submission, review, and refinement.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>836, <a href='https://arxiv.org/pdf/2508.15031.pdf' target='_blank'>https://arxiv.org/pdf/2508.15031.pdf</a></span>   <span><a href='https://github.com/kzhao5/ModelExtractionPapers' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaixiang Zhao, Lincan Li, Kaize Ding, Neil Zhenqiang Gong, Yue Zhao, Yushun Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15031">A Systematic Survey of Model Extraction Attacks and Defenses: State-of-the-Art and Perspectives</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Machine learning (ML) models have significantly grown in complexity and utility, driving advances across multiple domains. However, substantial computational resources and specialized expertise have historically restricted their wide adoption. Machine-Learning-as-a-Service (MLaaS) platforms have addressed these barriers by providing scalable, convenient, and affordable access to sophisticated ML models through user-friendly APIs. While this accessibility promotes widespread use of advanced ML capabilities, it also introduces vulnerabilities exploited through Model Extraction Attacks (MEAs). Recent studies have demonstrated that adversaries can systematically replicate a target model's functionality by interacting with publicly exposed interfaces, posing threats to intellectual property, privacy, and system security. In this paper, we offer a comprehensive survey of MEAs and corresponding defense strategies. We propose a novel taxonomy that classifies MEAs according to attack mechanisms, defense approaches, and computing environments. Our analysis covers various attack techniques, evaluates their effectiveness, and highlights challenges faced by existing defenses, particularly the critical trade-off between preserving model utility and ensuring security. We further assess MEAs within different computing paradigms and discuss their technical, ethical, legal, and societal implications, along with promising directions for future research. This systematic survey aims to serve as a valuable reference for researchers, practitioners, and policymakers engaged in AI security and privacy. Additionally, we maintain an online repository continuously updated with related literature at https://github.com/kzhao5/ModelExtractionPapers.<br>
<span id='abs_ch'>中文摘要：本文系统综述了通过机器学 即服务平台窃取模型功能的提取攻击，提出了新型分类法，分析了攻击技术、防御策略及其多维影响，重点探讨了模型效用与安全保障之间的关键平衡问题。</span><br>
<span id='abs_en'>English Summary: This paper surveys Model Extraction Attacks (MEAs) that exploit MLaaS platforms to replicate proprietary models, proposing a novel taxonomy and analyzing attack techniques, defense strategies, and their broader implications while highlighting the security-utility trade-off.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>837, <a href='https://arxiv.org/pdf/2508.14929.pdf' target='_blank'>https://arxiv.org/pdf/2508.14929.pdf</a></span>   <span><a href='https://github.com/ca-joe-yang/regression-without-softarg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chiao-An Yang, Raymond A. Yeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14929">Heatmap Regression without Soft-Argmax for Facial Landmark Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Facial landmark detection is an important task in computer vision with numerous applications, such as head pose estimation, expression analysis, face swapping, etc. Heatmap regression-based methods have been widely used to achieve state-of-the-art results in this task. These methods involve computing the argmax over the heatmaps to predict a landmark. Since argmax is not differentiable, these methods use a differentiable approximation, Soft-argmax, to enable end-to-end training on deep-nets. In this work, we revisit this long-standing choice of using Soft-argmax and demonstrate that it is not the only way to achieve strong performance. Instead, we propose an alternative training objective based on the classic structured prediction framework. Empirically, our method achieves state-of-the-art performance on three facial landmark benchmarks (WFLW, COFW, and 300W), converging 2.2x faster during training while maintaining better/competitive accuracy. Our code is available here: https://github.com/ca-joe-yang/regression-without-softarg.<br>
<span id='abs_ch'>中文摘要：本 究通过引入基于结构化预测的训练目 ，挑战了面部关键点检测中 统使用的Soft-argmax方法，在三个基准测试上以2.2倍更快的收敛速度实现了最优性能。</span><br>
<span id='abs_en'>English Summary: This study challenges the conventional use of Soft-argmax in facial landmark detection by introducing a structured prediction-based training objective, which achieves state-of-the-art performance with 2.2x faster convergence on three benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>838, <a href='https://arxiv.org/pdf/2508.14908.pdf' target='_blank'>https://arxiv.org/pdf/2508.14908.pdf</a></span>   <span><a href='https://github.com/panyue1998/Voice_HF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Pan, Liwei Liu, Changxin Li, Xinyao Wang, Yili Xia, Hanyue Zhang, Ming Chu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14908">A Chinese Heart Failure Status Speech Database with Universal and Personalised Classification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Speech is a cost-effective and non-intrusive data source for identifying acute and chronic heart failure (HF). However, there is a lack of research on whether Chinese syllables contain HF-related information, as observed in other well-studied languages. This study presents the first Chinese speech database of HF patients, featuring paired recordings taken before and after hospitalisation. The findings confirm the effectiveness of the Chinese language in HF detection using both standard 'patient-wise' and personalised 'pair-wise' classification approaches, with the latter serving as an ideal speaker-decoupled baseline for future research. Statistical tests and classification results highlight individual differences as key contributors to inaccuracy. Additionally, an adaptive frequency filter (AFF) is proposed for frequency importance analysis. The data and demonstrations are published at https://github.com/panyue1998/Voice_HF.<br>
<span id='abs_ch'>中文摘要：本 究首次建立中文心力衰竭语音数据库，证实中文音节包含心衰相关信息，验证了患者级和配对级分类方法的有效性，同时发现个体差异是影响准确性的主要  。</span><br>
<span id='abs_en'>English Summary: This study establishes the first Chinese speech database for heart failure detection, demonstrating that Chinese syllables contain HF-related information and validating both patient-wise and pair-wise classification methods, while identifying individual differences as a primary source of inaccuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>839, <a href='https://arxiv.org/pdf/2508.14782.pdf' target='_blank'>https://arxiv.org/pdf/2508.14782.pdf</a></span>   <span><a href='https://github.com/BiYunying/TransLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaming Leng, Yunying Bi, Chuan Qin, Bing Yin, Yanyong Zhang, Chao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14782">TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Urban transportation systems encounter diverse challenges across multiple tasks, such as traffic forecasting, electric vehicle (EV) charging demand prediction, and taxi dispatch. Existing approaches suffer from two key limitations: small-scale deep learning models are task-specific and data-hungry, limiting their generalizability across diverse scenarios, while large language models (LLMs), despite offering flexibility through natural language interfaces, struggle with structured spatiotemporal data and numerical reasoning in transportation domains. To address these limitations, we propose TransLLM, a unified foundation framework that integrates spatiotemporal modeling with large language models through learnable prompt composition. Our approach features a lightweight spatiotemporal encoder that captures complex dependencies via dilated temporal convolutions and dual-adjacency graph attention networks, seamlessly interfacing with LLMs through structured embeddings. A novel instance-level prompt routing mechanism, trained via reinforcement learning, dynamically personalizes prompts based on input characteristics, moving beyond fixed task-specific templates. The framework operates by encoding spatiotemporal patterns into contextual representations, dynamically composing personalized prompts to guide LLM reasoning, and projecting the resulting representations through specialized output layers to generate task-specific predictions. Experiments across seven datasets and three tasks demonstrate the exceptional effectiveness of TransLLM in both supervised and zero-shot settings. Compared to ten baseline models, it delivers competitive performance on both regression and planning problems, showing strong generalization and cross-task adaptability. Our code is available at https://github.com/BiYunying/TransLLM.<br>
<span id='abs_ch'>中文摘要：TransLLM是一个通过动态提示路由将时空建模与大语言模型融合的统一框架，在多种城市交通任务中展现出卓越性能和泛化能力。</span><br>
<span id='abs_en'>English Summary: TransLLM is a unified framework that integrates spatiotemporal modeling with large language models using dynamic prompt routing, demonstrating superior performance and generalization across multiple urban transportation tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>840, <a href='https://arxiv.org/pdf/2508.14735.pdf' target='_blank'>https://arxiv.org/pdf/2508.14735.pdf</a></span>   <span><a href='https://github.com/KurbanIntelligenceLab/nli-stress-testing' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Samir Abdaljalil, Erchin Serpedin, Khalid Qaraqe, Hasan Kurban
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14735">Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are increasingly applied in multilingual contexts, yet their capacity for consistent, logically grounded alignment across languages remains underexplored. We present a controlled evaluation framework for multilingual natural language inference (NLI) that generates synthetic, logic-based premise-hypothesis pairs and translates them into a typologically diverse set of languages. This design enables precise control over semantic relations and allows testing in both monolingual and mixed-language (code-switched) conditions. Surprisingly, code-switching does not degrade, and can even improve, performance, suggesting that translation-induced lexical variation may serve as a regularization signal. We validate semantic preservation through embedding-based similarity analyses and cross-lingual alignment visualizations, confirming the fidelity of translated pairs. Our findings expose both the potential and the brittleness of current LLM cross-lingual reasoning, and identify code-switching as a promising lever for improving multilingual robustness. Code available at: https://github.com/KurbanIntelligenceLab/nli-stress-testing<br>
<span id='abs_ch'>Chinese Summary: 本 究提出了一种基于逻辑的多语言自然语言推理评估框架，发现语 转换可通过充当正则化信号提升模型性能，同时揭示了当前大语言模型跨语言推理的潜力与脆弱性。</span><br>
<span id='abs_en'>English Summary: This study introduces a logic-based framework to evaluate multilingual natural language inference in LLMs, revealing that code-switching can enhance performance by acting as a regularization signal and highlighting both the potential and limitations of cross-lingual reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>841, <a href='https://arxiv.org/pdf/2508.14734.pdf' target='_blank'>https://arxiv.org/pdf/2508.14734.pdf</a></span>   <span><a href='https://github.com/Linusaronsson/AFA-Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Valter SchÃ¼tz, Han Wu, Reza Rezvan, Linus Aronsson, Morteza Haghir Chehreghani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14734">AFABench: A Generic Framework for Benchmarking Active Feature Acquisition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In many real-world scenarios, acquiring all features of a data instance can be expensive or impractical due to monetary cost, latency, or privacy concerns. Active Feature Acquisition (AFA) addresses this challenge by dynamically selecting a subset of informative features for each data instance, trading predictive performance against acquisition cost. While numerous methods have been proposed for AFA, ranging from greedy information-theoretic strategies to non-myopic reinforcement learning approaches, fair and systematic evaluation of these methods has been hindered by the lack of standardized benchmarks. In this paper, we introduce AFABench, the first benchmark framework for AFA. Our benchmark includes a diverse set of synthetic and real-world datasets, supports a wide range of acquisition policies, and provides a modular design that enables easy integration of new methods and tasks. We implement and evaluate representative algorithms from all major categories, including static, greedy, and reinforcement learning-based approaches. To test the lookahead capabilities of AFA policies, we introduce a novel synthetic dataset, AFAContext, designed to expose the limitations of greedy selection. Our results highlight key trade-offs between different AFA strategies and provide actionable insights for future research. The benchmark code is available at: https://github.com/Linusaronsson/AFA-Benchmark.<br>
<span id='abs_ch'>Chinese Summary: 本文提出了首个主动特征获取（AFA） 准化基准AFABench，通过综合评估不同特征选择方法在多 化数据集上的表现，为解决实际应用中特征获取成本高的问题提供了系统评估框架。</span><br>
<span id='abs_en'>English Summary: The paper introduces AFABench, the first standardized benchmark for Active Feature Acquisition (AFA), which evaluates various feature selection methods across diverse datasets to address the challenge of costly feature acquisition in real-world applications.Paperid:404,https://arxiv.org/pdf/2508.14730.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>842, <a href='https://arxiv.org/pdf/2508.14689.pdf' target='_blank'>https://arxiv.org/pdf/2508.14689.pdf</a></span>   <span><a href='https://github.com/yucongzh/ECHO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yucong Zhang, Juan Liu, Ming Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14689">ECHO: Frequency-aware Hierarchical Encoding for Variable-length Signals</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Pre-trained foundation models have demonstrated remarkable success in audio, vision and language, yet their potential for general machine signal modeling with arbitrary sampling rates-covering acoustic, vibration, and other industrial sensor data-remains under-explored. In this work, we propose a novel foundation model ECHO that integrates an advanced band-split architecture with frequency positional embeddings, enabling spectral localization across arbitrary sampling configurations. Moreover, the model incorporates sliding patches to support inputs of variable length without padding or cropping, producing a concise embedding that retains both temporal and spectral fidelity and naturally extends to streaming scenarios. We evaluate our method on various kinds of machine signal datasets, including previous DCASE task 2 challenges (2020-2025), and widely-used industrial signal corpora. Experimental results demonstrate consistent state-of-the-art performance in machine signal anomaly detection and fault classification, confirming the effectiveness and generalization capability of the proposed model. We open-sourced ECHO on https://github.com/yucongzh/ECHO.<br>
<span id='abs_ch'>中文摘要：ECHO基础模型采用频带分割架构与频率位置编 技术，能够处理任意采 率的机器信号，在工业数据集上的异常检测与故障分类任务中均实现了领先性能。</span><br>
<span id='abs_en'>English Summary: The ECHO foundation model introduces a band-split architecture with frequency positional embeddings to handle arbitrary sampling rates in machine signals, achieving state-of-the-art performance in anomaly detection and fault classification across industrial datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>843, <a href='https://arxiv.org/pdf/2508.14644.pdf' target='_blank'>https://arxiv.org/pdf/2508.14644.pdf</a></span>   <span><a href='https://github.com/project-numina/LeanGeo/tree/master' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chendong Song, Zihan Wang, Frederick Pu, Haiming Wang, Xiaohan Lin, Junqi Liu, Jia Li, Zhengying Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14644">LeanGeo: Formalizing Competitional Geometry problems in Lean</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Geometry problems are a crucial testbed for AI reasoning capabilities. Most existing geometry solving systems cannot express problems within a unified framework, thus are difficult to integrate with other mathematical fields. Besides, since most geometric proofs rely on intuitive diagrams, verifying geometry problems is particularly challenging. To address these gaps, we introduce LeanGeo, a unified formal system for formalizing and solving competition-level geometry problems within the Lean 4 theorem prover. LeanGeo features a comprehensive library of high-level geometric theorems with Lean's foundational logic, enabling rigorous proof verification and seamless integration with Mathlib. We also present LeanGeo-Bench, a formal geometry benchmark in LeanGeo, comprising problems from the International Mathematical Olympiad (IMO) and other advanced sources. Our evaluation demonstrates the capabilities and limitations of state-of-the-art Large Language Models on this benchmark, highlighting the need for further advancements in automated geometric reasoning. We open source the theorem library and the benchmark of LeanGeo at https://github.com/project-numina/LeanGeo/tree/master.<br>
<span id='abs_ch'>中文: LeanGeo是在Lean 4中构建的统一形式化系统，通过集成高级 何定理库实现严谨的 何证明验证，并建立了形式化基准来评估人工智能在 何推理方面的能力。</span><br>
<span id='abs_en'>English: LeanGeo is a unified formal system built in Lean 4 that enables rigorous proof verification and integration with mathematical libraries for solving competition-level geometry problems, accompanied by a benchmark to evaluate AI reasoning capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>844, <a href='https://arxiv.org/pdf/2508.14604.pdf' target='_blank'>https://arxiv.org/pdf/2508.14604.pdf</a></span>   <span><a href='https://github.com/wangzy01/UST-SSM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peiming Li, Ziyi Wang, Yulin Yuan, Hong Liu, Xiangming Meng, Junsong Yuan, Mengyuan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14604">UST-SSM: Unified Spatio-Temporal State Space Models for Point Cloud Video Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Point cloud videos capture dynamic 3D motion while reducing the effects of lighting and viewpoint variations, making them highly effective for recognizing subtle and continuous human actions. Although Selective State Space Models (SSMs) have shown good performance in sequence modeling with linear complexity, the spatio-temporal disorder of point cloud videos hinders their unidirectional modeling when directly unfolding the point cloud video into a 1D sequence through temporally sequential scanning. To address this challenge, we propose the Unified Spatio-Temporal State Space Model (UST-SSM), which extends the latest advancements in SSMs to point cloud videos. Specifically, we introduce Spatial-Temporal Selection Scanning (STSS), which reorganizes unordered points into semantic-aware sequences through prompt-guided clustering, thereby enabling the effective utilization of points that are spatially and temporally distant yet similar within the sequence. For missing 4D geometric and motion details, Spatio-Temporal Structure Aggregation (STSA) aggregates spatio-temporal features and compensates. To improve temporal interaction within the sampled sequence, Temporal Interaction Sampling (TIS) enhances fine-grained temporal dependencies through non-anchor frame utilization and expanded receptive fields. Experimental results on the MSR-Action3D, NTU RGB+D, and Synthia 4D datasets validate the effectiveness of our method. Our code is available at https://github.com/wangzy01/UST-SSM.<br>
<span id='abs_ch'>中文摘要：本 究提出的统一时空状态空间模型（UST-SSM）通过语义感知的序列重组和时空特征增强，有效解决了点云视频在序列建模中的时空 序问题，在多个数据集上验证了其优越性能。</span><br>
<span id='abs_en'>English Summary: The proposed Unified Spatio-Temporal State Space Model (UST-SSM) effectively processes point cloud videos by reorganizing unordered points into semantic sequences and enhancing spatio-temporal feature aggregation to overcome limitations in existing sequence modeling approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>845, <a href='https://arxiv.org/pdf/2508.14358.pdf' target='_blank'>https://arxiv.org/pdf/2508.14358.pdf</a></span>   <span><a href='https://github.com/zhujunli1993/HRC-Pose' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhujun Li, Shuo Zhang, Ioannis Stamos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14358">Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Category-level object pose estimation aims to predict the 6D pose and 3D size of objects within given categories. Existing approaches for this task rely solely on 6D poses as supervisory signals without explicitly capturing the intrinsic continuity of poses, leading to inconsistencies in predictions and reduced generalization to unseen poses. To address this limitation, we propose HRC-Pose, a novel depth-only framework for category-level object pose estimation, which leverages contrastive learning to learn point cloud representations that preserve the continuity of 6D poses. HRC-Pose decouples object pose into rotation and translation components, which are separately encoded and leveraged throughout the network. Specifically, we introduce a contrastive learning strategy for multi-task, multi-category scenarios based on our 6D pose-aware hierarchical ranking scheme, which contrasts point clouds from multiple categories by considering rotational and translational differences as well as categorical information. We further design pose estimation modules that separately process the learned rotation-aware and translation-aware embeddings. Our experiments demonstrate that HRC-Pose successfully learns continuous feature spaces. Results on REAL275 and CAMERA25 benchmarks show that our method consistently outperforms existing depth-only state-of-the-art methods and runs in real-time, demonstrating its effectiveness and potential for real-world applications. Our code is at https://github.com/zhujunli1993/HRC-Pose.<br>
<span id='abs_ch'>中文: HRC-Pose是一种新颖的仅使用深度信息的类别级物体姿态估计框架，通过对比学 保持6D姿态连续性，在基准测试中优于现有方法且能实时运行。</span><br>
<span id='abs_en'>English: HRC-Pose is a novel depth-only framework for category-level object pose estimation that uses contrastive learning to preserve 6D pose continuity, outperforming existing methods on benchmarks while running in real-time.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>846, <a href='https://arxiv.org/pdf/2508.14286.pdf' target='_blank'>https://arxiv.org/pdf/2508.14286.pdf</a></span>   <span><a href='https://github.com/anushka-kore/OccluNet.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anushka A. Kore, Frank G. te Nijenhuis, Matthijs van der Sluijs, Wim van Zwam, Charles Majoie, Geert Lycklama Ã  Nijeholt, Danny Ruijters, Frans Vos, Sandra Cornelissen, Ruisheng Su, Theo van Walsum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14286">OccluNet: Spatio-Temporal Deep Learning for Occlusion Detection on DSA</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate detection of vascular occlusions during endovascular thrombectomy (EVT) is critical in acute ischemic stroke (AIS). Interpretation of digital subtraction angiography (DSA) sequences poses challenges due to anatomical complexity and time constraints. This work proposes OccluNet, a spatio-temporal deep learning model that integrates YOLOX, a single-stage object detector, with transformer-based temporal attention mechanisms to automate occlusion detection in DSA sequences. We compared OccluNet with a YOLOv11 baseline trained on either individual DSA frames or minimum intensity projections. Two spatio-temporal variants were explored for OccluNet: pure temporal attention and divided space-time attention. Evaluation on DSA images from the MR CLEAN Registry revealed the model's capability to capture temporally consistent features, achieving precision and recall of 89.02% and 74.87%, respectively. OccluNet significantly outperformed the baseline models, and both attention variants attained similar performance. Source code is available at https://github.com/anushka-kore/OccluNet.git<br>
<span id='abs_ch'>中文摘要：本 究提出OccluNet模型，通过结合YOLOX目 检测器与基于Transformer的时序注意力机制，实现了数字减影血管 影序列中血管闭塞的自动检测，在MR CLEAN Registry数据集上以89.02%的精确率和74.87%的召回率显著优于基线模型。</span><br>
<span id='abs_en'>English Summary: This study introduces OccluNet, a spatio-temporal deep learning model combining YOLOX with transformer-based attention mechanisms to automate vascular occlusion detection in DSA sequences, demonstrating superior performance over baseline models with 89.02% precision and 74.87% recall.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>847, <a href='https://arxiv.org/pdf/2508.14276.pdf' target='_blank'>https://arxiv.org/pdf/2508.14276.pdf</a></span>   <span><a href='https://github.com/djafar1/tooth-diffusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Said Djafar Said, Torkan Gholamalizadeh, Mostafa Mehdipour Ghazi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14276">Tooth-Diffusion: Guided 3D CBCT Synthesis with Fine-Grained Tooth Conditioning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite the growing importance of dental CBCT scans for diagnosis and treatment planning, generating anatomically realistic scans with fine-grained control remains a challenge in medical image synthesis. In this work, we propose a novel conditional diffusion framework for 3D dental volume generation, guided by tooth-level binary attributes that allow precise control over tooth presence and configuration. Our approach integrates wavelet-based denoising diffusion, FiLM conditioning, and masked loss functions to focus learning on relevant anatomical structures. We evaluate the model across diverse tasks, such as tooth addition, removal, and full dentition synthesis, using both paired and distributional similarity metrics. Results show strong fidelity and generalization with low FID scores, robust inpainting performance, and SSIM values above 0.91 even on unseen scans. By enabling realistic, localized modification of dentition without rescanning, this work opens opportunities for surgical planning, patient communication, and targeted data augmentation in dental AI workflows. The codes are available at: https://github.com/djafar1/tooth-diffusion.<br>
<span id='abs_ch'>Chinese: 本 究提出了一种条件扩散框架，用于生成具有精确牙齿属性控制的逼真3D牙科CBCT扫描，在牙齿修改和合成等任务中实现了高保真度和泛化能力。English: This study introduces a conditional diffusion framework for generating realistic 3D dental CBCT scans with precise control over tooth attributes, achieving high fidelity and generalization in tasks like tooth modification and synthesis.Paperid:427,https://arxiv.org/pdf/2508.14197.pdfGitHub</span><br>
<span id='abs_en'>English: This study introduces a conditional diffusion framework for generating realistic 3D dental CBCT scans with precise control over tooth attributes, achieving high fidelity and generalization in tasks like tooth modification and synthesis.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>848, <a href='https://arxiv.org/pdf/2508.14160.pdf' target='_blank'>https://arxiv.org/pdf/2508.14160.pdf</a></span>   <span><a href='https://github.com/alibaba-damo-academy/RynnEC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ronghao Dang, Yuqian Yuan, Yunxuan Mao, Kehan Li, Jiangpin Liu, Zhikai Wang, Xin Li, Fan Wang, Deli Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14160">RynnEC: Bringing MLLMs into Embodied World</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce RynnEC, a video multimodal large language model designed for embodied cognition. Built upon a general-purpose vision-language foundation model, RynnEC incorporates a region encoder and a mask decoder, enabling flexible region-level video interaction. Despite its compact architecture, RynnEC achieves state-of-the-art performance in object property understanding, object segmentation, and spatial reasoning. Conceptually, it offers a region-centric video paradigm for the brain of embodied agents, providing fine-grained perception of the physical world and enabling more precise interactions. To mitigate the scarcity of annotated 3D datasets, we propose an egocentric video based pipeline for generating embodied cognition data. Furthermore, we introduce RynnEC-Bench, a region-centered benchmark for evaluating embodied cognitive capabilities. We anticipate that RynnEC will advance the development of general-purpose cognitive cores for embodied agents and facilitate generalization across diverse embodied tasks. The code, model checkpoints, and benchmark are available at: https://github.com/alibaba-damo-academy/RynnEC<br>
<br>
<div id='section'>Paperid: <span id='pid'>849, <a href='https://arxiv.org/pdf/2508.14153.pdf' target='_blank'>https://arxiv.org/pdf/2508.14153.pdf</a></span>   <span><a href='https://github.com/hustvl/LENS' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/hustvl/LENS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lianghui Zhu, Bin Ouyang, Yuxuan Zhang, Tianheng Cheng, Rui Hu, Haocheng Shen, Longjin Ran, Xiaoxin Chen, Li Yu, Wenyu Liu, Xinggang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14153">LENS: Learning to Segment Anything with Unified Reinforced Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text-prompted image segmentation enables fine-grained visual understanding and is critical for applications such as human-computer interaction and robotics. However, existing supervised fine-tuning methods typically ignore explicit chain-of-thought (CoT) reasoning at test time, which limits their ability to generalize to unseen prompts and domains. To address this issue, we introduce LENS, a scalable reinforcement-learning framework that jointly optimizes the reasoning process and segmentation in an end-to-end manner. We propose unified reinforcement-learning rewards that span sentence-, box-, and segment-level cues, encouraging the model to generate informative CoT rationales while refining mask quality. Using a publicly available 3-billion-parameter vision-language model, i.e., Qwen2.5-VL-3B-Instruct, LENS achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks, outperforming the strong fine-tuned method, i.e., GLaMM, by up to 5.6%. These results demonstrate that RL-driven CoT reasoning serves as a robust prior for text-prompted segmentation and offers a practical path toward more generalizable Segment Anything models. Code is available at https://github.com/hustvl/LENS.<br>
<span id='abs_ch'>Chinese: LENS 是一种强化学 框架，通过联合优化思维链推理与图像分割，显著提升了文本提示图像分割的精度和泛化能力，在多个基准测试中表现优异。English: LENS is a reinforcement learning framework that enhances text-prompted image segmentation by jointly optimizing chain-of-thought reasoning and segmentation, achieving state-of-the-art performance on benchmarks and improving generalization.Paperid:432,https://arxiv.org/pdf/2508.14148.pdfGitHub</span><br>
<span id='abs_en'>English: LENS is a reinforcement learning framework that enhances text-prompted image segmentation by jointly optimizing chain-of-thought reasoning and segmentation, achieving state-of-the-art performance on benchmarks and improving generalization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>850, <a href='https://arxiv.org/pdf/2508.14088.pdf' target='_blank'>https://arxiv.org/pdf/2508.14088.pdf</a></span>   <span><a href='https://github.com/wenhaomin/CoBAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haomin Wen, Shurui Cao, Leman Akoglu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14088">CoBAD: Modeling Collective Behaviors for Human Mobility Anomaly Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Detecting anomalies in human mobility is essential for applications such as public safety and urban planning. While traditional anomaly detection methods primarily focus on individual movement patterns (e.g., a child should stay at home at night), collective anomaly detection aims to identify irregularities in collective mobility behaviors across individuals (e.g., a child is at home alone while the parents are elsewhere) and remains an underexplored challenge. Unlike individual anomalies, collective anomalies require modeling spatiotemporal dependencies between individuals, introducing additional complexity. To address this gap, we propose CoBAD, a novel model designed to capture Collective Behaviors for human mobility Anomaly Detection. We first formulate the problem as unsupervised learning over Collective Event Sequences (CES) with a co-occurrence event graph, where CES represents the event sequences of related individuals. CoBAD then employs a two-stage attention mechanism to model both the individual mobility patterns and the interactions across multiple individuals. Pre-trained on large-scale collective behavior data through masked event and link reconstruction tasks, CoBAD is able to detect two types of collective anomalies: unexpected co-occurrence anomalies and absence anomalies, the latter of which has been largely overlooked in prior work. Extensive experiments on large-scale mobility datasets demonstrate that CoBAD significantly outperforms existing anomaly detection baselines, achieving an improvement of 13%-18% in AUCROC and 19%-70% in AUCPR. All source code is available at https://github.com/wenhaomin/CoBAD.<br>
<span id='abs_ch'>中文摘要：CoBAD是一种通过两阶段注意力机制建模个体间时空依赖关系的新型集体人类移动异常检测模型，在识别共现异常和缺席异常方面显著优于现有方法。</span><br>
<span id='abs_en'>English Summary: CoBAD is a novel model that detects collective human mobility anomalies by modeling spatiotemporal dependencies between individuals through a two-stage attention mechanism, significantly outperforming existing methods in identifying both co-occurrence and absence anomalies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>851, <a href='https://arxiv.org/pdf/2508.14062.pdf' target='_blank'>https://arxiv.org/pdf/2508.14062.pdf</a></span>   <span><a href='https://github.com/akshayaaa10/llm-privacy-research' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Badrinath Ramakrishnan, Akshaya Balaji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14062">Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks, but their tendency to memorize training data poses significant privacy risks, particularly during fine-tuning processes. This paper presents a comprehensive empirical analysis of data memorization in fine-tuned LLMs and introduces a novel multi-layered privacy protection framework. Through controlled experiments on modern LLM architectures including GPT-2, Phi-3, and Gemma-2, we demonstrate that fine-tuning with repeated sensitive data increases privacy leakage rates from baseline levels of 0-5% to 60-75%, representing a 64.2% average increase across tested models. We propose and rigorously evaluate four complementary privacy protection methods: semantic data deduplication, differential privacy during generation, entropy-based filtering, and pattern-based content filtering. Our experimental results show that these techniques can reduce data leakage to 0% while maintaining 94.7% of original model utility.<br>
<span id='abs_ch'>中文: 本文发现大语言模型微调会显著 剧数据记忆风险，使隐私泄露率从0-5%升至60-75%，并提出多层保护框架，在保持94.7%模型性能的同时将泄露率降至0%。</span><br>
<span id='abs_en'>English: This paper reveals that fine-tuning large language models significantly increases data memorization risks, with privacy leakage rates rising from 0-5% to 60-75%, and proposes a multi-layered protection framework that reduces leakage to 0% while preserving 94.7% model utility.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>852, <a href='https://arxiv.org/pdf/2508.14058.pdf' target='_blank'>https://arxiv.org/pdf/2508.14058.pdf</a></span>   <span><a href='https://github.com/zqxwcevrtyui/DP2Rec' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingmao Zhang, Zhiting Zhao, Yunqi Lin, Jianghong Ma, Tianjun Wei, Haijun Zhang, Xiaofeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14058">Dual-Phase Playtime-guided Recommendation: Interest Intensity Exploration and Multimodal Random Walks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The explosive growth of the video game industry has created an urgent need for recommendation systems that can scale with expanding catalogs and maintain user engagement. While prior work has explored accuracy and diversity in recommendations, existing models underutilize playtime, a rich behavioral signal unique to gaming platforms, and overlook the potential of multimodal information to enhance diversity. In this paper, we propose DP2Rec, a novel Dual-Phase Playtime-guided Recommendation model designed to jointly optimize accuracy and diversity. First, we introduce a playtime-guided interest intensity exploration module that separates strong and weak preferences via dual-beta modeling, enabling fine-grained user profiling and more accurate recommendations. Second, we present a playtime-guided multimodal random walks module that simulates player exploration using transitions guided by both playtime-derived interest similarity and multimodal semantic similarity. This mechanism preserves core preferences while promoting cross-category discovery through latent semantic associations and adaptive category balancing. Extensive experiments on a real-world game dataset show that DP2Rec outperforms existing methods in both recommendation accuracy and diversity.<br>
<span id='abs_ch'>视频游戏行业的快速增长需要可扩展的推荐系统，而提出的DP2Rec模型创新性地利用游戏时长数据和多模态信息，在提升游戏推荐准确性的同时增强多 性。</span><br>
<span id='abs_en'>The video game industry's rapid expansion necessitates scalable recommendation systems, and the proposed DP2Rec model uniquely leverages playtime data and multimodal information to enhance both accuracy and diversity in game suggestions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>853, <a href='https://arxiv.org/pdf/2508.14031.pdf' target='_blank'>https://arxiv.org/pdf/2508.14031.pdf</a></span>   <span><a href='https://github.com/HahmDY/prefix_injection_guard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongyoon Hahm, Taywon Min, Woogyeol Jin, Kimin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14031">Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Beyond simple text generation, Large Language Models (LLMs) have evolved into agentic systems capable of planning and interacting with external tools to solve complex tasks. This evolution involves fine-tuning LLMs on agent-specific tasks to enhance their proficiency. However, safety concerns are frequently overlooked during this fine-tuning process. In this work, we show that aligned LLMs can become unintentionally misaligned, leading to a higher likelihood of executing harmful tasks and a reduced tendency to refuse them when fine-tuned to execute agentic tasks. To address these safety challenges, we propose Prefix INjection Guard (PING), a simple yet effective method that prepends automatically generated natural language prefixes to agent responses, guiding them to refuse harmful requests while preserving performance on benign tasks. Specifically, we introduce an iterative approach that alternates between (1) generating candidate prefixes and (2) selecting those that optimize both task performance and refusal behavior. Experimental results demonstrate that PING significantly enhances the safety of fine-tuned LLM agents without sacrificing their effectiveness. PING consistently outperforms existing prompting approaches across diverse benchmarks in both web navigation and code generation tasks. Our analysis of internal hidden states via linear probes reveals that prefix tokens are crucial for behavior modification, explaining the performance gains. WARNING: This paper contains contents that are unethical or offensive in nature.<br>
<span id='abs_ch'>中文摘要：针对智能体任务微调大语言模型可能意外增强其执行有害指令的倾向，而提出的PING方法通过注入自然语言前缀有效提升安全性，能在保持任务性能的同时引导模型拒绝危险请求。</span><br>
<span id='abs_en'>English Summary: Fine-tuning large language models for agentic tasks can inadvertently increase their tendency to execute harmful requests, but the proposed PING method effectively enhances safety by injecting natural language prefixes that guide refusal of dangerous tasks without compromising performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>854, <a href='https://arxiv.org/pdf/2508.14013.pdf' target='_blank'>https://arxiv.org/pdf/2508.14013.pdf</a></span>   <span><a href='https://github.com/NKUShaw/ZOWFKGIF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Xiao, Ruimeng Ye, Bohan Liu, Xiaolong Ma, Bo Hui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.14013">Efficient Knowledge Graph Unlearning with Zeroth-order Information</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Due to regulations like the Right to be Forgotten, there is growing demand for removing training data and its influence from models. Since full retraining is costly, various machine unlearning methods have been proposed. In this paper, we firstly present an efficient knowledge graph (KG) unlearning algorithm. We remark that KG unlearning is nontrivial due to the distinctive structure of KG and the semantic relations between entities. Also, unlearning by estimating the influence of removed components incurs significant computational overhead when applied to large-scale knowledge graphs. To this end, we define an influence function for KG unlearning and propose to approximate the model's sensitivity without expensive computation of first-order and second-order derivatives for parameter updates. Specifically, we use Taylor expansion to estimate the parameter changes caused by data removal. Given that the first-order gradients and second-order derivatives dominate the computational load, we use the Fisher matrices and zeroth-order optimization to approximate the inverse-Hessian vector product without constructing the computational graphs. Our experimental results demonstrate that the proposed method outperforms other state-of-the-art graph unlearning baselines significantly in terms of unlearning efficiency and unlearning quality. Our code is released at https://github.com/NKUShaw/ZOWFKGIF.<br>
<span id='abs_ch'>中文: 本文提出了一种高效的知识图谱遗忘算法，通过泰勒展开和零阶优化近似参数变化，在遗忘效率和遗忘质量上显著优于现有方法。</span><br>
<span id='abs_en'>English: This paper introduces an efficient knowledge graph unlearning algorithm that uses Taylor expansion and zeroth-order optimization to approximate parameter changes, significantly outperforming existing methods in both efficiency and quality.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>855, <a href='https://arxiv.org/pdf/2508.13993.pdf' target='_blank'>https://arxiv.org/pdf/2508.13993.pdf</a></span>   <span><a href='https://github.com/NEUIR/LongMab-PO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaohua Duan, Xinze Li, Zhenghao Liu, Xiaoyuan Yi, Yukun Yan, Shuo Wang, Yu Gu, Ge Yu, Maosong Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13993">Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Long-context modeling is critical for a wide range of real-world tasks, including long-context question answering, summarization, and complex reasoning tasks. Recent studies have explored fine-tuning Large Language Models (LLMs) with synthetic data to enhance their long-context capabilities. However, the effectiveness of such approaches is often limited by the low diversity and factual inconsistencies in the generated data. To address these challenges, we propose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB) rollout strategy to identify the most informative chunks from the given long context for sampling high-quality and diverse responses and constructing preference data pairs for Direct Preference Optimization (DPO) training. Specifically, we treat context chunks as arms of MAB, select chunks based on their expected reward scores to input into LLMs to generate responses, and iteratively update these scores based on reward feedback. This exploration and exploitation process enables the model to focus on the most relevant context segments, thereby generating and collecting high-quality and diverse responses. Finally, we collect these generated responses from the rollout process and apply the DPO method to further optimize the LLM. Experimental results show that LongMab-PO significantly improves the diversity and quality of preference data pairs, achieving state-of-the-art performance on long-context reasoning benchmarks. All code and data will be released on https://github.com/NEUIR/LongMab-PO.<br>
<span id='abs_ch'>Chinese: LongMab-PO是一种创新框架，利用多臂老虎机策略筛选信息丰富的上下文片段，生成多 且高质量的回答，并通过直接偏好优化进一步优化大语言模型，在长上下文推理任务中实现了最先进的性能。</span><br>
<span id='abs_en'>English: LongMab-PO is a novel framework that uses a Multi-Armed Bandit strategy to select informative context chunks for generating diverse, high-quality responses and optimizing LLMs through Direct Preference Optimization, achieving state-of-the-art performance in long-context reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>856, <a href='https://arxiv.org/pdf/2508.13968.pdf' target='_blank'>https://arxiv.org/pdf/2508.13968.pdf</a></span>   <span><a href='https://github.com/tianyiniu/RotBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyi Niu, Jaemin Cho, Elias Stengel-Eskin, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13968">RotBench: Evaluating Multimodal Large Language Models on Identifying Image Rotation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We investigate to what extent Multimodal Large Language Models (MLLMs) can accurately identify the orientation of input images rotated 0Â°, 90Â°, 180Â°, and 270Â°. This task demands robust visual reasoning capabilities to detect rotational cues and contextualize spatial relationships within images, regardless of their orientation. To evaluate MLLMs on these abilities, we introduce RotBench -- a 350-image manually-filtered benchmark comprising lifestyle, portrait, and landscape images. Despite the relatively simple nature of this task, we show that several state-of-the-art open and proprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably identify rotation in input images. Providing models with auxiliary information -- including captions, depth maps, and more -- or using chain-of-thought prompting offers only small and inconsistent improvements. Our results indicate that most models are able to reliably identify right-side-up (0Â°) images, while certain models are able to identify upside-down (180Â°) images. None can reliably distinguish between 90Â° and 270Â°. Simultaneously showing the image rotated in different orientations leads to moderate performance gains for reasoning models, while a modified setup using voting improves the performance of weaker models. We further show that fine-tuning does not improve models' ability to distinguish 90Â° and 270Â° rotations, despite substantially improving the identification of 180Â° images. Together, these results reveal a significant gap between MLLMs' spatial reasoning capabilities and human perception in identifying rotation.<br>
<span id='abs_ch'>中文:  究表明，当前多模态大语言模型在识别图像旋转方面存在明显缺陷，尤其 法可 区分90°和270°旋转，显示出与人类空间感知能力的重要差距。</span><br>
<span id='abs_en'>English: This study reveals that current Multimodal Large Language Models struggle to reliably identify image rotations, particularly distinguishing between 90° and 270° orientations, exposing a significant gap in spatial reasoning compared to human perception.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>857, <a href='https://arxiv.org/pdf/2508.13930.pdf' target='_blank'>https://arxiv.org/pdf/2508.13930.pdf</a></span>   <span><a href='https://github.com/danilotpnta/IR2-project' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Matey Krastev, Miklos Hamar, Danilo Toapanta, Jesse Brouwers, Yibin Lei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13930">InPars+: Supercharging Synthetic Data Generation for Information Retrieval Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This work revisits and extends synthetic query generation pipelines for Neural Information Retrieval (NIR) by leveraging the InPars Toolkit, a reproducible, end-to-end framework for generating training data using large language models (LLMs). We first assess the reproducibility of the original InPars, InPars-V2, and Promptagator pipelines on the SciFact benchmark and validate their effectiveness using open-source reranker and generator models. Building on this foundation, we introduce two key extensions to the pipeline: (1) fine-tuning a query generator LLM via Contrastive Preference Optimization (CPO) to improve the signal quality in generated queries, and (2) replacing static prompt templates with dynamic, Chain-of-Thought (CoT) optimized prompts using the DSPy framework. Our results show that both extensions reduce the need for aggressive filtering while improving retrieval performance. All code, models, and synthetic datasets are publicly released to support further research at: \href{https://github.com/danilotpnta/IR2-project}{this https URL}.<br>
<span id='abs_ch'>中文摘要：本 究通过引入对比偏好优化微调大语言模型和动态思维链提示两项关键扩展，改进了神经信息检索中的合成查询生成流程，在提升检索性能的同时降低了对严 过滤的依赖。</span><br>
<span id='abs_en'>English Summary: This study enhances synthetic query generation for Neural Information Retrieval by introducing two pipeline extensions—fine-tuning LLMs with Contrastive Preference Optimization and implementing dynamic Chain-of-Thought prompts—which improve retrieval performance while reducing aggressive filtering requirements.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>858, <a href='https://arxiv.org/pdf/2508.13843.pdf' target='_blank'>https://arxiv.org/pdf/2508.13843.pdf</a></span>   <span><a href='https://github.com/qzp2018/UniECS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Liang, Yufei Ma, ZhiPeng Qian, Huangyu Dai, Zihan Wang, Ben Chen, Chenyi Lei, Yuqing Ding, Han Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13843">UniECS: Unified Multimodal E-Commerce Search Framework with Gated Cross-modal Fusion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current e-commerce multimodal retrieval systems face two key limitations: they optimize for specific tasks with fixed modality pairings, and lack comprehensive benchmarks for evaluating unified retrieval approaches. To address these challenges, we introduce UniECS, a unified multimodal e-commerce search framework that handles all retrieval scenarios across image, text, and their combinations. Our work makes three key contributions. First, we propose a flexible architecture with a novel gated multimodal encoder that uses adaptive fusion mechanisms. This encoder integrates different modality representations while handling missing modalities. Second, we develop a comprehensive training strategy to optimize learning. It combines cross-modal alignment loss (CMAL), cohesive local alignment loss (CLAL), intra-modal contrastive loss (IMCL), and adaptive loss weighting. Third, we create M-BEER, a carefully curated multimodal benchmark containing 50K product pairs for e-commerce search evaluation. Extensive experiments demonstrate that UniECS consistently outperforms existing methods across four e-commerce benchmarks with fine-tuning or zero-shot evaluation. On our M-BEER bench, UniECS achieves substantial improvements in cross-modal tasks (up to 28\% gain in R@10 for text-to-image retrieval) while maintaining parameter efficiency (0.2B parameters) compared to larger models like GME-Qwen2VL (2B) and MM-Embed (8B). Furthermore, we deploy UniECS in the e-commerce search platform of Kuaishou Inc. across two search scenarios, achieving notable improvements in Click-Through Rate (+2.74\%) and Revenue (+8.33\%). The comprehensive evaluation demonstrates the effectiveness of our approach in both experimental and real-world settings. Corresponding codes, models and datasets will be made publicly available at https://github.com/qzp2018/UniECS.<br>
<span id='abs_ch'>中文: 本文提出的UniECS统一多模态电商搜索框架通过门控编 器、综合训练策略和M-BEER基准测试，解决了现有系统模态配对固定和评估 准不足的问题，在实验环境和快手平台的实际部署中均展现出卓越性能。English: This paper introduces UniECS, a unified multimodal e-commerce search framework that overcomes limitations of fixed modality pairings and benchmark scarcity through a gated encoder, comprehensive training strategy, and the new M-BEER benchmark, demonstrating superior performance in both experiments and real-world deployment.Paperid:452,https://arxiv.org/pdf/2508.13836.pdfGitHub</span><br>
<span id='abs_en'>English: This paper introduces UniECS, a unified multimodal e-commerce search framework that overcomes limitations of fixed modality pairings and benchmark scarcity through a gated encoder, comprehensive training strategy, and the new M-BEER benchmark, demonstrating superior performance in both experiments and real-world deployment.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>859, <a href='https://arxiv.org/pdf/2508.13836.pdf' target='_blank'>https://arxiv.org/pdf/2508.13836.pdf</a></span>   <span><a href='https://github.com/janumiko/pruning-benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>MikoÅaj Janusz, Tomasz Wojnar, Yawei Li, Luca Benini, Kamil Adamczewski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13836">One Shot vs. Iterative: Rethinking Pruning Strategies for Model Compression</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Pruning is a core technique for compressing neural networks to improve computational efficiency. This process is typically approached in two ways: one-shot pruning, which involves a single pass of training and pruning, and iterative pruning, where pruning is performed over multiple cycles for potentially finer network refinement. Although iterative pruning has historically seen broader adoption, this preference is often assumed rather than rigorously tested. Our study presents one of the first systematic and comprehensive comparisons of these methods, providing rigorous definitions, benchmarking both across structured and unstructured settings, and applying different pruning criteria and modalities. We find that each method has specific advantages: one-shot pruning proves more effective at lower pruning ratios, while iterative pruning performs better at higher ratios. Building on these findings, we advocate for patience-based pruning and introduce a hybrid approach that can outperform traditional methods in certain scenarios, providing valuable insights for practitioners selecting a pruning strategy tailored to their goals and constraints. Source code is available at https://github.com/janumiko/pruning-benchmark.<br>
<span id='abs_ch'>Chinese: 本 究系统比较了一次性剪枝与迭代剪枝方法，发现低剪枝率时一次性剪枝更优，高剪枝率时迭代剪枝更佳，并提出一种混合方法可在特定场景下超越 统剪枝策略。</span><br>
<span id='abs_en'>English: This study systematically compares one-shot and iterative neural network pruning methods, finding that one-shot pruning excels at lower ratios while iterative pruning performs better at higher ratios, and introduces a hybrid approach that can surpass traditional methods in specific scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>860, <a href='https://arxiv.org/pdf/2508.13787.pdf' target='_blank'>https://arxiv.org/pdf/2508.13787.pdf</a></span>   <span><a href='https://github.com/MatZaharia/BetaWeb' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Guo, Yuanjian Zhou, Chenyi Wang, Linlin You, Minjie Bian, Weinan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13787">BetaWeb: Towards a Blockchain-enabled Trustworthy Agentic Web</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid development of large language models (LLMs) has significantly propelled the development of artificial intelligence (AI) agents, which are increasingly evolving into diverse autonomous entities, advancing the LLM-based multi-agent systems (LaMAS). However, current agentic ecosystems remain fragmented and closed. Establishing an interconnected and scalable paradigm for Agentic AI has become a critical prerequisite. Although Agentic Web proposes an open architecture to break the ecosystem barriers, its implementation still faces core challenges such as privacy protection, data management, and value measurement. Existing centralized or semi-centralized paradigms suffer from inherent limitations, making them inadequate for supporting large-scale, heterogeneous, and cross-domain autonomous interactions. To address these challenges, this paper introduces the blockchain-enabled trustworthy Agentic Web (BetaWeb). By leveraging the inherent strengths of blockchain, BetaWeb not only offers a trustworthy and scalable infrastructure for LaMAS but also has the potential to advance the Web paradigm from Web3 (centered on data ownership) towards Web3.5, which emphasizes ownership of agent capabilities and the monetization of intelligence. Beyond a systematic examination of the BetaWeb framework, this paper presents a five-stage evolutionary roadmap, outlining the path of LaMAS from passive execution to advanced collaboration and autonomous governance. We also conduct a comparative analysis of existing products and discuss key challenges of BetaWeb from multiple perspectives. Ultimately, we argue that deep integration between blockchain and LaMAS can lay the foundation for a resilient, trustworthy, and sustainably incentivized digital ecosystem. A summary of the enabling technologies for each stage is available at https://github.com/MatZaharia/BetaWeb.<br>
<span id='abs_ch'>中文摘要：大语言模型的快速发展推动了AI智能体的进步，但现有系统存在碎片化和隐私等挑战，BetaWeb框架通过区块链技术构建可信赖的基础设施，促进智能协作与自主治理的数字生态。English Summary: The rapid advancement of large language models has driven the development of AI agents, yet current systems face fragmentation and challenges in privacy and scalability, which the proposed BetaWeb framework aims to resolve using blockchain for a trustworthy and collaborative digital ecosystem.Paperid:454,https://arxiv.org/pdf/2508.13776.pdfGitHub</span><br>
<span id='abs_en'>English Summary: The rapid advancement of large language models has driven the development of AI agents, yet current systems face fragmentation and challenges in privacy and scalability, which the proposed BetaWeb framework aims to resolve using blockchain for a trustworthy and collaborative digital ecosystem.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>861, <a href='https://arxiv.org/pdf/2508.13776.pdf' target='_blank'>https://arxiv.org/pdf/2508.13776.pdf</a></span>   <span><a href='https://github.com/sebastibar/conditional-diffusion-breast-MRI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sebastian Ibarra, Javier del Riego, Alessandro Catanese, Julian Cuba, Julian Cardona, Nataly Leon, Jonathan Infante, Karim Lekadir, Oliver Diaz, Richard Osuala
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13776">Comparing Conditional Diffusion Models for Synthesizing Contrast-Enhanced Breast MRI from Pre-Contrast Images</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Dynamic contrast-enhanced (DCE) MRI is essential for breast cancer diagnosis and treatment. However, its reliance on contrast agents introduces safety concerns, contraindications, increased cost, and workflow complexity. To this end, we present pre-contrast conditioned denoising diffusion probabilistic models to synthesize DCE-MRI, introducing, evaluating, and comparing a total of 22 generative model variants in both single-breast and full breast settings. Towards enhancing lesion fidelity, we introduce both tumor-aware loss functions and explicit tumor segmentation mask conditioning. Using a public multicenter dataset and comparing to respective pre-contrast baselines, we observe that subtraction image-based models consistently outperform post-contrast-based models across five complementary evaluation metrics. Apart from assessing the entire image, we also separately evaluate the region of interest, where both tumor-aware losses and segmentation mask inputs improve evaluation metrics. The latter notably enhance qualitative results capturing contrast uptake, albeit assuming access to tumor localization inputs that are not guaranteed to be available in screening settings. A reader study involving 2 radiologists and 4 MRI technologists confirms the high realism of the synthetic images, indicating an emerging clinical potential of generative contrast-enhancement. We share our codebase at https://github.com/sebastibar/conditional-diffusion-breast-MRI.<br>
<span id='abs_ch'>This study introduces a generative AI method using pre-=
contrast MRI data to synthesize contrast-enhanced breast MRI, demonstrating=
 superior performance over baseline methods through multiple evaluation met=
rics and clinical reader validation, while noting dependency on tumor local=
ization inputs.</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>862, <a href='https://arxiv.org/pdf/2508.13744.pdf' target='_blank'>https://arxiv.org/pdf/2508.13744.pdf</a></span>   <span><a href='https://github.com/yejipark-m/FOCUS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yeji Park, Minyoung Lee, Sanghyuk Chun, Junsuk Choe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13744">Mitigating Cross-Image Information Leakage in LVLMs for Multi-Image Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Vision-Language Models (LVLMs) demonstrate strong performance on single-image tasks. However, we observe that their performance degrades significantly when handling multi-image inputs. This occurs because visual cues from different images become entangled in the model's output. We refer to this phenomenon as cross-image information leakage. To address this issue, we propose FOCUS, a training-free and architecture-agnostic decoding strategy that mitigates cross-image information leakage during inference. FOCUS sequentially masks all but one image with random noise, guiding the model to focus on the single clean image. We repeat this process across all target images to obtain logits under partially masked contexts. These logits are aggregated and then contrastively refined using a noise-only reference input, which suppresses the leakage and yields more accurate outputs. FOCUS consistently improves performance across four multi-image benchmarks and diverse LVLM families. This demonstrates that FOCUS offers a general and practical solution for enhancing multi-image reasoning without additional training or architectural modifications.<br>
<span id='abs_ch'>中文: FOCUS是一种 需训练的通用解 策略，通过顺序用噪声遮蔽图像、聚合对数并对比优化输出，有效缓解大型视觉语言模型中的跨图像信息泄露问题，显著提升多图像推理能力。</span><br>
<span id='abs_en'>English: FOCUS is a training-free decoding strategy that mitigates cross-image information leakage in Large Vision-Language Models by sequentially masking images with noise, aggregating logits, and contrastively refining outputs to enhance multi-image reasoning performance.Paperid:458,https://arxiv.org/pdf/2508.13735.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>863, <a href='https://arxiv.org/pdf/2508.13678.pdf' target='_blank'>https://arxiv.org/pdf/2508.13678.pdf</a></span>   <span><a href='https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao-Wen Yang, Jie-Jing Shao, Lan-Zhe Guo, Bo-Wen Zhang, Zhi Zhou, Lin-Han Jia, Wang-Zhou Dai, Yu-Feng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13678">Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have shown promising results across various tasks, yet their reasoning capabilities remain a fundamental challenge. Developing AI systems with strong reasoning capabilities is regarded as a crucial milestone in the pursuit of Artificial General Intelligence (AGI) and has garnered considerable attention from both academia and industry. Various techniques have been explored to enhance the reasoning capabilities of LLMs, with neuro-symbolic approaches being a particularly promising way. This paper comprehensively reviews recent developments in neuro-symbolic approaches for enhancing LLM reasoning. We first present a formalization of reasoning tasks and give a brief introduction to the neurosymbolic learning paradigm. Then, we discuss neuro-symbolic methods for improving the reasoning capabilities of LLMs from three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic. Finally, we discuss several key challenges and promising future directions. We have also released a GitHub repository including papers and resources related to this survey: https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy.<br>
<span id='abs_ch'>中文: 本文全面综述了提升大语言模型推理能力的神经符号方法，探讨了其当前挑战并展望了未来发展方向。English: This paper provides a comprehensive review of neuro-symbolic approaches aimed at enhancing the reasoning capabilities of Large Language Models, addressing their current limitations and outlining future directions.Paperid:461,https://arxiv.org/pdf/2508.13670.pdfGitHub</span><br>
<span id='abs_en'>English: This paper provides a comprehensive review of neuro-symbolic approaches aimed at enhancing the reasoning capabilities of Large Language Models, addressing their current limitations and outlining future directions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>864, <a href='https://arxiv.org/pdf/2508.13657.pdf' target='_blank'>https://arxiv.org/pdf/2508.13657.pdf</a></span>   <span><a href='https://github.com/amirbalef/CASHPlus' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amir Rezaei Balef, Katharina Eggensperger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13657">In-Context Decision Making for Optimizing Complex AutoML Pipelines</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Combined Algorithm Selection and Hyperparameter Optimization (CASH) has been fundamental to traditional AutoML systems. However, with the advancements of pre-trained models, modern ML workflows go beyond hyperparameter optimization and often require fine-tuning, ensembling, and other adaptation techniques. While the core challenge of identifying the best-performing model for a downstream task remains, the increasing heterogeneity of ML pipelines demands novel AutoML approaches. This work extends the CASH framework to select and adapt modern ML pipelines. We propose PS-PFN to efficiently explore and exploit adapting ML pipelines by extending Posterior Sampling (PS) to the max k-armed bandit problem setup. PS-PFN leverages prior-data fitted networks (PFNs) to efficiently estimate the posterior distribution of the maximal value via in-context learning. We show how to extend this method to consider varying costs of pulling arms and to use different PFNs to model reward distributions individually per arm. Experimental results on one novel and two existing standard benchmark tasks demonstrate the superior performance of PS-PFN compared to other bandit and AutoML strategies. We make our code and data available at https://github.com/amirbalef/CASHPlus.<br>
<span id='abs_ch'>Chinese: 本 究扩展了CASH框架以适应现代机器学 流程，提出PS-PFN方法，通过后验采 结合先验数据拟合网络实现高效模型选择，并在基准测试中展现出优于其他方法的性能。English: This work extends the Combined Algorithm Selection and Hyperparameter Optimization (CASH) framework to adapt modern ML pipelines by introducing PS-PFN, which uses posterior sampling with prior-data fitted networks for efficient model selection and demonstrates superior performance in benchmarks.Paperid:464,https://arxiv.org/pdf/2508.13618.pdfGitHub</span><br>
<span id='abs_en'>English: This work extends the Combined Algorithm Selection and Hyperparameter Optimization (CASH) framework to adapt modern ML pipelines by introducing PS-PFN, which uses posterior sampling with prior-data fitted networks for efficient model selection and demonstrates superior performance in benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>865, <a href='https://arxiv.org/pdf/2508.13579.pdf' target='_blank'>https://arxiv.org/pdf/2508.13579.pdf</a></span>   <span><a href='https://github.com/devilran6/EAG-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Fang, Yuxin Guo, Jiaran Gao, Hongxin Ding, Xinke Jiang, Weibin Liao, Yongxin Xu, Yinghao Zhu, Zhibang Yang, Liantao Ma, Junfeng Zhao, Yasha Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13579">Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Improving large language models (LLMs) for electronic health record (EHR) reasoning is essential for enabling accurate and generalizable clinical predictions. While LLMs excel at medical text understanding, they underperform on EHR-based prediction tasks due to challenges in modeling temporally structured, high-dimensional data. Existing approaches often rely on hybrid paradigms, where LLMs serve merely as frozen prior retrievers while downstream deep learning (DL) models handle prediction, failing to improve the LLM's intrinsic reasoning capacity and inheriting the generalization limitations of DL models. To this end, we propose EAG-RL, a novel two-stage training framework designed to intrinsically enhance LLMs' EHR reasoning ability through expert attention guidance, where expert EHR models refer to task-specific DL models trained on EHR data. Concretely, EAG-RL first constructs high-quality, stepwise reasoning trajectories using expert-guided Monte Carlo Tree Search to effectively initialize the LLM's policy. Then, EAG-RL further optimizes the policy via reinforcement learning by aligning the LLM's attention with clinically salient features identified by expert EHR models. Extensive experiments on two real-world EHR datasets show that EAG-RL improves the intrinsic EHR reasoning ability of LLMs by an average of 14.62%, while also enhancing robustness to feature perturbations and generalization to unseen clinical domains. These results demonstrate the practical potential of EAG-RL for real-world deployment in clinical prediction tasks. Our code have been available at https://github.com/devilran6/EAG-RL.<br>
<span id='abs_ch'>Chinese: EAG-RL框架通过将大语言模型的注意力与专家引导的临床特征对齐，内在提升了其电子健康记录推理能力，实现了平均14.62%的准确率和鲁棒性提升。</span><br>
<span id='abs_en'>English: The EAG-RL framework enhances large language models' intrinsic reasoning for electronic health records by aligning their attention with expert-guided clinical features, achieving a 14.62% average improvement in accuracy and robustness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>866, <a href='https://arxiv.org/pdf/2508.13517.pdf' target='_blank'>https://arxiv.org/pdf/2508.13517.pdf</a></span>   <span><a href='https://github.com/socialalgo/HIM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongru Hou, Jiachen Sun, Wenqing Lin, Wendong Bi, Xiangrong Wang, Deqing Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13517">Heterogeneous Influence Maximization in User Recommendation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>User recommendation systems enhance user engagement by encouraging users to act as inviters to interact with other users (invitees), potentially fostering information propagation. Conventional recommendation methods typically focus on modeling interaction willingness. Influence-Maximization (IM) methods focus on identifying a set of users to maximize the information propagation. However, existing methods face two significant challenges. First, recommendation methods fail to unleash the candidates' spread capability. Second, IM methods fail to account for the willingness to interact. To solve these issues, we propose two models named HeteroIR and HeteroIM. HeteroIR provides an intuitive solution to unleash the dissemination potential of user recommendation systems. HeteroIM fills the gap between the IM method and the recommendation task, improving interaction willingness and maximizing spread coverage. The HeteroIR introduces a two-stage framework to estimate the spread profits. The HeteroIM incrementally selects the most influential invitee to recommend and rerank based on the number of reverse reachable (RR) sets containing inviters and invitees. RR set denotes a set of nodes that can reach a target via propagation. Extensive experiments show that HeteroIR and HeteroIM significantly outperform the state-of-the-art baselines with the p-value < 0.05. Furthermore, we have deployed HeteroIR and HeteroIM in Tencent's online gaming platforms and gained an 8.5\% and 10\% improvement in the online A/B test, respectively. Implementation codes are available at https://github.com/socialalgo/HIM.<br>
<span id='abs_ch'>中文: 提出的 HeteroIR 和 HeteroIM 模型通过增强交互意愿和最大化信息 播范围，解决了现有推荐方法与影响力最大化技术的不足，在离线和腾讯平台的在线测试中均取得了显著效果提升。</span><br>
<span id='abs_en'>English: The proposed HeteroIR and HeteroIM models address limitations in user recommendation and influence maximization by enhancing interaction willingness and maximizing information spread, demonstrating significant improvements in both offline experiments and real-world deployment on Tencent's platforms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>867, <a href='https://arxiv.org/pdf/2508.13500.pdf' target='_blank'>https://arxiv.org/pdf/2508.13500.pdf</a></span>   <span><a href='https://github.com/jaewan7599/L3AE_CIKM2025' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaewan Moon, Seongmin Park, Jongwuk Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13500">LLM-Enhanced Linear Autoencoders for Recommendation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have been widely adopted to enrich the semantic representation of textual item information in recommender systems. However, existing linear autoencoders (LAEs) that incorporate textual information rely on sparse word co-occurrence patterns, limiting their ability to capture rich textual semantics. To address this, we propose L3AE, the first integration of LLMs into the LAE framework. L3AE effectively integrates the heterogeneous knowledge of textual semantics and user-item interactions through a two-phase optimization strategy. (i) L3AE first constructs a semantic item-to-item correlation matrix from LLM-derived item representations. (ii) It then learns an item-to-item weight matrix from collaborative signals while distilling semantic item correlations as regularization. Notably, each phase of L3AE is optimized through closed-form solutions, ensuring global optimality and computational efficiency. Extensive experiments demonstrate that L3AE consistently outperforms state-of-the-art LLM-enhanced models on three benchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20. The source code is available at https://github.com/jaewan7599/L3AE_CIKM2025.<br>
<span id='abs_ch'>中文: L3AE模型通过两阶段优化策略将大语言模型融入线性自编 器，有效整合文本语义与用户-物品交互信息，在三个基准数据集上显著超越了现有最优模型。</span><br>
<span id='abs_en'>English: The proposed L3AE model integrates large language models into linear autoencoders through a two-phase optimization strategy, effectively combining textual semantics with user-item interactions to achieve significant performance improvements over existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>868, <a href='https://arxiv.org/pdf/2508.13250.pdf' target='_blank'>https://arxiv.org/pdf/2508.13250.pdf</a></span>   <span><a href='https://github.com/nuster1128/MPR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Zhang, Yang Zhang, Haoran Tan, Rui Li, Xu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13250">Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In large language model-based agents, memory serves as a critical capability for achieving personalization by storing and utilizing users' information. Although some previous studies have adopted memory to implement user personalization, they typically focus on preference alignment and simple question-answering. However, in the real world, complex tasks often require multi-hop reasoning on a large amount of user information, which poses significant challenges for current memory approaches. To address this limitation, we propose the multi-hop personalized reasoning task to explore how different memory mechanisms perform in multi-hop reasoning over personalized information. We explicitly define this task and construct a dataset along with a unified evaluation framework. Then, we implement various explicit and implicit memory methods and conduct comprehensive experiments. We evaluate their performance on this task from multiple perspectives and analyze their strengths and weaknesses. Besides, we explore hybrid approaches that combine both paradigms and propose the HybridMem method to address their limitations. We demonstrate the effectiveness of our proposed model through extensive experiments. To benefit the research community, we release this project at https://github.com/nuster1128/MPR.<br>
<span id='abs_ch'>Chinese: 本 究提出了多跳个性化推理任务，用于评估不同记忆机制在处理用户特定信息复杂推理时的表现，并通过提出HybridMem混合方法克服现有局限，经全面实验验证了其有效性。</span><br>
<span id='abs_en'>English: This study introduces a multi-hop personalized reasoning task to evaluate how various memory mechanisms handle complex reasoning over user-specific data, proposing the HybridMem method to overcome existing limitations and demonstrating its effectiveness through comprehensive experiments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>869, <a href='https://arxiv.org/pdf/2508.13220.pdf' target='_blank'>https://arxiv.org/pdf/2508.13220.pdf</a></span>   <span><a href='https://github.com/AIS2Lab/MCPSecBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Yang, Daoyuan Wu, Yufan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13220">MCPSecBench: A Systematic Security Benchmark and Playground for Testing Model Context Protocols</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) are increasingly integrated into real-world applications via the Model Context Protocol (MCP), a universal, open standard for connecting AI agents with data sources and external tools. While MCP enhances the capabilities of LLM-based agents, it also introduces new security risks and expands their attack surfaces. In this paper, we present the first systematic taxonomy of MCP security, identifying 17 attack types across 4 primary attack surfaces. We introduce MCPSecBench, a comprehensive security benchmark and playground that integrates prompt datasets, MCP servers, MCP clients, attack scripts, and protection mechanisms to evaluate these attacks across three major MCP providers. Our benchmark is modular and extensible, allowing researchers to incorporate custom implementations of clients, servers, and transport protocols for systematic security assessment. Experimental results show that over 85% of the identified attacks successfully compromise at least one platform, with core vulnerabilities universally affecting Claude, OpenAI, and Cursor, while prompt-based and tool-centric attacks exhibit considerable variability across different hosts and models. In addition, current protection mechanisms have little effect against these attacks. Overall, MCPSecBench standardizes the evaluation of MCP security and enables rigorous testing across all MCP layers.<br>
<span id='abs_ch'>中文: 本文首次系统化分类了模型上下文协议（MCP）的安全风险，在四个攻击面上识别出17种攻击类型，并推出模块化基准测试平台MCPSecBench，实验表明超过85%的攻击能成功突 平台防护，而现有防护机制基本 效。</span><br>
<span id='abs_en'>English: This paper introduces the first systematic taxonomy of security risks in the Model Context Protocol (MCP), identifying 17 attack types across four surfaces, and presents MCPSecBench, a modular benchmark that reveals over 85% of attacks successfully compromise platforms while current protections remain ineffective.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>870, <a href='https://arxiv.org/pdf/2508.13186.pdf' target='_blank'>https://arxiv.org/pdf/2508.13186.pdf</a></span>   <span><a href='https://github.com/MMBrowseComp/MM-BrowseComp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shilong Li, Xingyuan Bu, Wenjie Wang, Jiaheng Liu, Jun Dong, Haoyang He, Hao Lu, Haozhe Zhang, Chenchen Jing, Zhen Li, Chuanhao Li, Jiayi Tian, Chenchen Zhang, Tianhao Peng, Yancheng He, Jihao Gu, Yuanxing Zhang, Jian Yang, Ge Zhang, Wenhao Huang, Wangchunshu Zhou, Zhaoxiang Zhang, Ruizhe Ding, Shilei Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13186">MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>AI agents with advanced reasoning and tool use capabilities have demonstrated impressive performance in web browsing for deep search. While existing benchmarks such as BrowseComp evaluate these browsing abilities, they primarily focus on textual information, overlooking the prevalence of multimodal content. To bridge this gap, we introduce MM-BrowseComp, a novel benchmark comprising 224 challenging, hand-crafted questions specifically designed to assess agents' multimodal retrieval and reasoning capabilities. These questions often incorporate images in prompts, and crucial information encountered during the search and reasoning process may also be embedded within images or videos on webpages. Consequently, methods relying solely on text prove insufficient for our benchmark. Additionally, we provide a verified checklist for each question, enabling fine-grained analysis of multimodal dependencies and reasoning paths. Our comprehensive evaluation of state-of-the-art models on MM-BrowseComp reveals that even top models like OpenAI o3 with tools achieve only 29.02\% accuracy, highlighting the suboptimal multimodal capabilities and lack of native multimodal reasoning in current models.<br>
<span id='abs_ch'>Chinese: 本文提出了MM-BrowseComp这一包含224个手工设计问题的新型基准，用于评估AI代理的多模态网络浏览能力，结果显示即使顶尖模型也 缺乏多模态推理能力而表现不佳，准确率仅为29.02%。</span><br>
<span id='abs_en'>English: This paper introduces MM-BrowseComp, a new benchmark with 224 hand-crafted questions to evaluate AI agents' multimodal web browsing capabilities, revealing that even top models perform poorly with only 29.02% accuracy due to insufficient multimodal reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>871, <a href='https://arxiv.org/pdf/2508.13171.pdf' target='_blank'>https://arxiv.org/pdf/2508.13171.pdf</a></span>   <span><a href='https://github.com/tao-hpu/cognitive-workspace' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao An
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13171">Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) face fundamental limitations in context management despite recent advances extending context windows to millions of tokens. We propose Cognitive Workspace, a novel paradigm that transcends traditional Retrieval-Augmented Generation (RAG) by emulating human cognitive mechanisms of external memory use. Drawing from cognitive science foundations including Baddeley's working memory model, Clark's extended mind thesis, and Hutchins' distributed cognition framework, we demonstrate that current passive retrieval systems fail to capture the dynamic, task-driven nature of human memory management. Our analysis of 2024-2025 developments reveals that while techniques like Infini-attention and StreamingLLM achieve impressive context lengths, they lack the metacognitive awareness and active planning capabilities essential for true cognitive extension. Cognitive Workspace addresses these limitations through three core innovations: (1) active memory management with deliberate information curation, (2) hierarchical cognitive buffers enabling persistent working states, and (3) task-driven context optimization that dynamically adapts to cognitive demands. Empirical validation demonstrates Cognitive Workspace achieves an average 58.6% memory reuse rate (ranging from 54-60% across different tasks) compared to 0% for traditional RAG, with 17-18% net efficiency gain despite 3.3x higher operation counts. Statistical analysis confirms these advantages with p < 0.001 and Cohen's d > 23 across multiple task types, establishing the first quantitative evidence for active memory superiority in LLM systems. We present a comprehensive theoretical framework synthesizing insights from 50+ recent papers, positioning Cognitive Workspace as a fundamental shift from information retrieval to genuine cognitive augmentation.<br>
<span id='abs_ch'>中文: 认知工作区通过模拟人类记忆机制，采用主动管理、分层缓冲和任务驱动的优化，克服了大语言模型的上下文限制，相比 统方法实现了显著的效率提升。</span><br>
<span id='abs_en'>English: Cognitive Workspace overcomes LLMs' context limitations by emulating human memory mechanisms through active management, hierarchical buffers, and task-driven optimization, achieving significant efficiency gains over traditional methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>872, <a href='https://arxiv.org/pdf/2508.13152.pdf' target='_blank'>https://arxiv.org/pdf/2508.13152.pdf</a></span>   <span><a href='https://github.com/NLP2CT/RepreGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Chen, Junchao Wu, Shu Yang, Runzhe Zhan, Zeyu Wu, Ziyang Luo, Di Wang, Min Yang, Lidia S. Chao, Derek F. Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13152">RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Detecting content generated by large language models (LLMs) is crucial for preventing misuse and building trustworthy AI systems. Although existing detection methods perform well, their robustness in out-of-distribution (OOD) scenarios is still lacking. In this paper, we hypothesize that, compared to features used by existing detection methods, the internal representations of LLMs contain more comprehensive and raw features that can more effectively capture and distinguish the statistical pattern differences between LLM-generated texts (LGT) and human-written texts (HWT). We validated this hypothesis across different LLMs and observed significant differences in neural activation patterns when processing these two types of texts. Based on this, we propose RepreGuard, an efficient statistics-based detection method. Specifically, we first employ a surrogate model to collect representation of LGT and HWT, and extract the distinct activation feature that can better identify LGT. We can classify the text by calculating the projection score of the text representations along this feature direction and comparing with a precomputed threshold. Experimental results show that RepreGuard outperforms all baselines with average 94.92% AUROC on both in-distribution (ID) and OOD scenarios, while also demonstrating robust resilience to various text sizes and mainstream attacks. Data and code are publicly available at: https://github.com/NLP2CT/RepreGuard<br>
<span id='abs_ch'>中文: 本文提出RepreGuard检测方法，通过利用大语言模型的内部表征来更好地区分机器生成与人类撰写文本，在多种场景下均展现出卓越的鲁棒性和检测性能。</span><br>
<span id='abs_en'>English: This paper introduces RepreGuard, a detection method that leverages LLMs' internal representations to better distinguish between machine-generated and human-written texts, achieving superior robustness and performance across various scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>873, <a href='https://arxiv.org/pdf/2508.13049.pdf' target='_blank'>https://arxiv.org/pdf/2508.13049.pdf</a></span>   <span><a href='https://github.com/mukullokhande99/XR-NPE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tejas Chaudhari, Akarsh J., Tanushree Dewangan, Mukul Lokhande, Santosh Kumar Vishvakarma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13049">XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This work proposes XR-NPE, a high-throughput Mixed-precision SIMD Neural Processing Engine, designed for extended reality (XR) perception workloads like visual inertial odometry (VIO), object classification, and eye gaze extraction. XR-NPE is first to support FP4, Posit (4,1), Posit (8,0), and Posit (16,1) formats, with layer adaptive hybrid-algorithmic implementation supporting ultra-low bit precision to significantly reduce memory bandwidth requirements, and accompanied by quantization-aware training for minimal accuracy loss. The proposed Reconfigurable Mantissa Multiplication and Exponent processing Circuitry (RMMEC) reduces dark silicon in the SIMD MAC compute engine, assisted by selective power gating to reduce energy consumption, providing 2.85x improved arithmetic intensity. XR-NPE achieves a maximum operating frequency of 1.72 GHz, area 0.016 mm2 , and arithmetic intensity 14 pJ at CMOS 28nm, reducing 42% area, 38% power compared to the best of state-of-the-art MAC approaches. The proposed XR-NPE based AXI-enabled Matrix-multiplication co-processor consumes 1.4x fewer LUTs, 1.77x fewer FFs, and provides 1.2x better energy efficiency compared to SoTA accelerators on VCU129. The proposed co-processor provides 23% better energy efficiency and 4% better compute density for VIO workloads. XR-NPE establishes itself as a scalable, precision-adaptive compute engine for future resource-constrained XR devices. The complete set for codes for results reproducibility are released publicly, enabling designers and researchers to readily adopt and build upon them. https://github.com/mukullokhande99/XR-NPE.<br>
<span id='abs_ch'>中文：XR-NPE是一种面向扩展现实应用的高吞吐量混合精度神经网络处理引擎，采用创新的低精度 式和可重构电路设计，在保持精度的同时显著降低能耗和硬件需求。</span><br>
<span id='abs_en'>English: XR-NPE is a high-throughput mixed-precision neural processing engine designed for extended reality applications, featuring innovative low-precision formats and reconfigurable circuitry to significantly reduce energy consumption and hardware requirements while maintaining accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>874, <a href='https://arxiv.org/pdf/2508.13023.pdf' target='_blank'>https://arxiv.org/pdf/2508.13023.pdf</a></span>   <span><a href='https://github.com/T-Lab-CUHKSZ/G2RPO-A' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongxin Guo, Wenbo Deng, Zhenglin Cheng, Xiaoying Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13023">G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhanced the reasoning abilities of large language models (LLMs). Its success, however, largely depends on strong base models with rich world knowledge, yielding only modest improvements for small-size language models (SLMs). To address this limitation, we investigate Guided GRPO, which injects ground-truth reasoning steps into roll-out trajectories to compensate for SLMs' inherent weaknesses. Through a comprehensive study of various guidance configurations, we find that naively adding guidance delivers limited gains. These insights motivate G$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strength in response to the model's evolving training dynamics. Experiments on mathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-A substantially outperforms vanilla GRPO. Our code and models are available at https://github.com/T-Lab-CUHKSZ/G2RPO-A.<br>
<span id='abs_ch'>中文摘要：G²RPO-A自适应算法通过动态调整指导强度，将真实推理步骤注入训练轨迹以弥补小型语言模型的固有缺陷，在数学推理和代 生成任务中显著优于基础GRPO方法。</span><br>
<span id='abs_en'>English Summary: G²RPO-A, an adaptive algorithm that dynamically adjusts guidance strength, significantly outperforms vanilla GRPO by compensating for small language models' weaknesses through injected ground-truth reasoning steps.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>875, <a href='https://arxiv.org/pdf/2508.13021.pdf' target='_blank'>https://arxiv.org/pdf/2508.13021.pdf</a></span>   <span><a href='https://github.com/NEUIR/PC-Sampler' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengcheng Huang, Shuhao Liu, Zhenghao Liu, Yukun Yan, Shuo Wang, Zulong Chen, Tong Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13021">PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in masked diffusion models (MDMs) have established them as powerful non-autoregressive alternatives for sequence generation. Nevertheless, our preliminary experiments reveal that the generation quality of MDMs is still highly sensitive to the choice of decoding strategy. In particular, widely adopted uncertainty-based samplers suffer from two key limitations: a lack of global trajectory control and a pronounced bias toward trivial tokens in the early stages of decoding. These shortcomings restrict the full potential of MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling (PC-Sampler), a novel decoding strategy that unifies global trajectory planning with content-aware informativeness maximization. PC-Sampler incorporates a position-aware weighting mechanism to regulate the decoding path and a calibrated confidence score to suppress the premature selection of trivial tokens. Extensive experiments on three advanced MDMs across seven challenging benchmarks-including logical reasoning and planning tasks-demonstrate that PC-Sampler consistently outperforms existing MDM decoding strategies by more than 10% on average, significantly narrowing the performance gap with state-of-the-art autoregressive models. All codes are available at https://github.com/NEUIR/PC-Sampler.<br>
<span id='abs_ch'>掩 扩散模型的生成质量高度依赖解 策略，而提出的PC-Sampler方法将全局轨迹规划与内容感知信息量相结合，平均性能超越现有方法超过10%。</span><br>
<span id='abs_en'>Masked diffusion models' generation quality is highly dependent on decoding strategies, and the proposed PC-Sampler method unifies global trajectory planning with content-aware informativeness to significantly outperform existing approaches by over 10% on average.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>876, <a href='https://arxiv.org/pdf/2508.13020.pdf' target='_blank'>https://arxiv.org/pdf/2508.13020.pdf</a></span>   <span><a href='https://github.com/Yu-Maryland/e-boost' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Yin, Zhan Song, Chen Chen, Yaohui Cai, Zhiru Zhang, Cunxi Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13020">e-boost: Boosted E-Graph Extraction with Adaptive Heuristics and Exact Solving</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>E-graphs have attracted growing interest in many fields, particularly in logic synthesis and formal verification. E-graph extraction is a challenging NP-hard combinatorial optimization problem. It requires identifying optimal terms from exponentially many equivalent expressions, serving as the primary performance bottleneck in e-graph based optimization tasks. However, traditional extraction methods face a critical trade-off: heuristic approaches offer speed but sacrifice optimality, while exact methods provide optimal solutions but face prohibitive computational costs on practical problems. We present e-boost, a novel framework that bridges this gap through three key innovations: (1) parallelized heuristic extraction that leverages weak data dependence to compute DAG costs concurrently, enabling efficient multi-threaded performance without sacrificing extraction quality; (2) adaptive search space pruning that employs a parameterized threshold mechanism to retain only promising candidates, dramatically reducing the solution space while preserving near-optimal solutions; and (3) initialized exact solving that formulates the reduced problem as an Integer Linear Program with warm-start capabilities, guiding solvers toward high-quality solutions faster.
  Across the diverse benchmarks in formal verification and logic synthesis fields, e-boost demonstrates 558x runtime speedup over traditional exact approaches (ILP) and 19.04% performance improvement over the state-of-the-art extraction framework (SmoothE). In realistic logic synthesis tasks, e-boost produces 7.6% and 8.1% area improvements compared to conventional synthesis tools with two different technology mapping libraries. e-boost is available at https://github.com/Yu-Maryland/e-boost.<br>
<span id='abs_ch'>Chinese: E-boost是一种新颖框架，通过结合并行化启发式搜索、自适应剪枝和初始化精确求解，克服了 统e图提取方法的局限，在逻辑综合和形式验证任务中实现了显著的速度提升和性能改进。</span><br>
<span id='abs_en'>English: E-boost is a novel framework that overcomes the limitations of traditional e-graph extraction methods by combining parallelized heuristics, adaptive pruning, and initialized exact solving to achieve significant speed improvements and performance gains in logic synthesis and formal verification tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>877, <a href='https://arxiv.org/pdf/2508.13003.pdf' target='_blank'>https://arxiv.org/pdf/2508.13003.pdf</a></span>   <span><a href='https://github.com/SYSUSELab/EvolMathEval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengbo Wang, Mingwei Liu, Zike Li, Anji Li, Yanlin Wang, Xin Peng, Zibin Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13003">EvolMathEval: Towards Evolvable Benchmarks for Mathematical Reasoning via Evolutionary Testing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of LLMs poses a significant challenge to existing mathematical reasoning benchmarks. These benchmarks commonly suffer from issues such as score saturation, temporal decay, and data contamination. To address this challenge, this paper introduces EvolMathEval, an automated mathematical benchmark generation and evolution framework based on evolutionary testing. By dynamically generating unique evaluation instances ab initio, the framework fundamentally eliminates the risk of data contamination, and ensuring the benchmark remains perpetually challenging for future models.The core mechanisms of EvolMathEval include: seed problem generation based on reverse engineering with algebraic guarantees; multi-dimensional genetic operators designed to inject diverse cognitive challenges; and a composite fitness function that can rapidly and accurately assess problem difficulty. Experimental results demonstrate that the proposed composite fitness function can efficiently and precisely quantify the difficulty of mathematical problems. Furthermore, EvolMathEval can not only generate a large volume of high-difficulty problems through continuous self-iteration, but it can also significantly enhance the complexity of public datasets like GSM8K through evolution, reducing model accuracy by an average of 48%. Deeper investigation reveals that when solving these evolved, complex problems, LLMs tend to employ non-rigorous heuristics to bypass complex multi-step logical reasoning, consequently leading to incorrect solutions. We define this phenomenon as "Pseudo Aha Moment". This finding uncovers a cognitive shortcut-taking behavior in the deep reasoning processes of current LLMs, which we find accounts for 77% to 100% of errors on targeted problems. Code and resources are available at:https://github.com/SYSUSELab/EvolMathEval.<br>
<span id='abs_ch'>中文: 本文提出EvolMathEval自动化框架，通过进化测试生成和演化数学基准，有效应对大语言模型对现有基准的适应问题，不仅大幅提升问题复杂度使模型准确率平均下降48%，还揭示了导致77%-100%错误的“伪顿悟时刻”推理现象。</span><br>
<span id='abs_en'>English: This paper introduces EvolMathEval, an automated framework that generates and evolves mathematical benchmarks to counter the diminishing challenge of existing benchmarks for large language models, significantly increasing problem complexity and reducing model accuracy by 48% while identifying a "Pseudo Aha Moment" phenomenon in reasoning errors.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>878, <a href='https://arxiv.org/pdf/2508.12943.pdf' target='_blank'>https://arxiv.org/pdf/2508.12943.pdf</a></span>   <span><a href='https://github.com/marytonwe/OPTIC-ER.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mary Tonwe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12943">OPTIC-ER: A Reinforcement Learning Framework for Real-Time Emergency Response and Equitable Resource Allocation in Underserved African Communities</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Public service systems in many African regions suffer from delayed emergency response and spatial inequity, causing avoidable suffering. This paper introduces OPTIC-ER, a reinforcement learning (RL) framework for real-time, adaptive, and equitable emergency response. OPTIC-ER uses an attention-guided actor-critic architecture to manage the complexity of dispatch environments. Its key innovations are a Context-Rich State Vector, encoding action sub-optimality, and a Precision Reward Function, which penalizes inefficiency. Training occurs in a high-fidelity simulation using real data from Rivers State, Nigeria, accelerated by a precomputed Travel Time Atlas. The system is built on the TALS framework (Thin computing, Adaptability, Low-cost, Scalability) for deployment in low-resource settings. In evaluations on 500 unseen incidents, OPTIC-ER achieved a 100.00% optimality rate with negligible inefficiency, confirming its robustness and generalization. Beyond dispatch, the system generates Infrastructure Deficiency Maps and Equity Monitoring Dashboards to guide proactive governance and data-informed development. This work presents a validated blueprint for AI-augmented public services, showing how context-aware RL can bridge the gap between algorithmic decision-making and measurable human impact.<br>
<span id='abs_ch'>中文摘要：本文提出OPTIC-ER强化学 框架，通过创新的状态表征与奖励机制设计，在真实场景模拟中实现最优应急响应性能，有效解决非洲地区公共服务延迟与空间不平等问题。</span><br>
<span id='abs_en'>English Summary: This paper introduces OPTIC-ER, a reinforcement learning framework that achieves optimal emergency response performance through innovative state representation and reward design, validated in real-world simulations to address service delays and inequity in African regions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>879, <a href='https://arxiv.org/pdf/2508.12932.pdf' target='_blank'>https://arxiv.org/pdf/2508.12932.pdf</a></span>   <span><a href='https://github.com/ShaolingPu/CIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyang Chen, Shaoling Pu, Lingyu Zheng, Zhongwu Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12932">SEDEG:Sequential Enhancement of Decoder and Encoder's Generality for Class Incremental Learning with Small Memory</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In incremental learning, enhancing the generality of knowledge is crucial for adapting to dynamic data inputs. It can develop generalized representations or more balanced decision boundaries, preventing the degradation of long-term knowledge over time and thus mitigating catastrophic forgetting. Some emerging incremental learning methods adopt an encoder-decoder architecture and have achieved promising results. In the encoder-decoder achitecture, improving the generalization capabilities of both the encoder and decoder is critical, as it helps preserve previously learned knowledge while ensuring adaptability and robustness to new, diverse data inputs. However, many existing continual methods focus solely on enhancing one of the two components, which limits their effectiveness in mitigating catastrophic forgetting. And these methods perform even worse in small-memory scenarios, where only a limited number of historical samples can be stored. To mitigate this limitation, we introduces SEDEG, a two-stage training framework for vision transformers (ViT), focusing on sequentially improving the generality of both Decoder and Encoder. Initially, SEDEG trains an ensembled encoder through feature boosting to learn generalized representations, which subsequently enhance the decoder's generality and balance the classifier. The next stage involves using knowledge distillation (KD) strategies to compress the ensembled encoder and develop a new, more generalized encoder. This involves using a balanced KD approach and feature KD for effective knowledge transfer. Extensive experiments on three benchmark datasets show SEDEG's superior performance, and ablation studies confirm the efficacy of its components. The code is available at https://github.com/ShaolingPu/CIL.<br>
<span id='abs_ch'>中文: SEDEG是一种针对视觉变换器的两阶段训练框架，通过依次提升解 器和编 器的泛化能力来缓解增量学 中的灾难性遗忘问题，尤其在小内存场景下表现优异。</span><br>
<span id='abs_en'>English: SEDEG is a two-stage training framework for vision transformers that sequentially enhances the generality of both the decoder and encoder to mitigate catastrophic forgetting in incremental learning, particularly in small-memory scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>880, <a href='https://arxiv.org/pdf/2508.12854.pdf' target='_blank'>https://arxiv.org/pdf/2508.12854.pdf</a></span>   <span><a href='https://github.com/RH-Lin/E3RG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ronghao Lin, Shuai Shen, Weipeng Hu, Qiaolin He, Aolin Xiong, Li Huang, Haifeng Hu, Yap-peng Tan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12854">E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal Empathetic Response Generation (MERG) is crucial for building emotionally intelligent human-computer interactions. Although large language models (LLMs) have improved text-based ERG, challenges remain in handling multimodal emotional content and maintaining identity consistency. Thus, we propose E3RG, an Explicit Emotion-driven Empathetic Response Generation System based on multimodal LLMs which decomposes MERG task into three parts: multimodal empathy understanding, empathy memory retrieval, and multimodal response generation. By integrating advanced expressive speech and video generative models, E3RG delivers natural, emotionally rich, and identity-consistent responses without extra training. Experiments validate the superiority of our system on both zero-shot and few-shot settings, securing Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25. Our code is available at https://github.com/RH-Lin/E3RG.<br>
<span id='abs_ch'>中文摘要：E3RG是一个基于多模态大语言模型的显式情感驱动系统，通过将共情响应生成分解为理解、记忆和生成三阶段， 需额外训练即可产生自然且情感一致的多模态回应，并在权威评测中取得最佳成绩。</span><br>
<span id='abs_en'>English Summary: E3RG is an explicit emotion-driven system that enhances multimodal empathetic response generation by decomposing it into empathy understanding, memory retrieval, and response generation, achieving top performance without additional training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>881, <a href='https://arxiv.org/pdf/2508.12782.pdf' target='_blank'>https://arxiv.org/pdf/2508.12782.pdf</a></span>   <span><a href='https://github.com/stefanrer/HeroBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Petr Anokhin, Roman Khalikov, Stefan Rebrikov, Viktor Volkov, Artyom Sorokin, Vincent Bissonnette
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12782">HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have shown remarkable capabilities in isolated step-by-step reasoning tasks such as mathematics and programming, but their proficiency in long-horizon planning, where solutions require extended, structured sequences of interdependent actions, remains underexplored. Existing benchmarks typically assess LLMs through abstract or low-dimensional algorithmic tasks, failing to capture the complexity of realistic planning environments. We introduce HeroBench, a novel benchmark designed specifically to evaluate long-horizon planning and structured reasoning within complex RPG-inspired virtual worlds. HeroBench provides a rigorously constructed dataset of tasks covering a wide range of difficulties, a simulated environment to execute and validate agent plans, and detailed analytical tools for evaluating model performance. Tasks challenge models to formulate strategic plans, efficiently gather resources, master necessary skills, craft equipment, and defeat adversaries, reflecting practical scenarios' layered dependencies and constraints. Our extensive evaluation of 25 state-of-the-art LLMs, spanning both open-source and proprietary models, including the GPT-5 family, reveals substantial performance disparities rarely observed in conventional reasoning benchmarks. Detailed error analysis further uncovers specific weaknesses in current models' abilities to generate robust high-level plans and reliably execute structured actions. HeroBench thus not only significantly advances the evaluation of LLM reasoning but also provides a flexible, scalable foundation for future research into advanced, autonomous planning in virtual environments.<br>
<span id='abs_ch'>中文: HeroBench作为专门评估大语言模型在复杂角色扮演游戏中长程规划能力的新基准，揭示了现有模型在制定高层策略和执行结构化行动序列方面的显著不足。</span><br>
<span id='abs_en'>English: HeroBench is a new benchmark that evaluates large language models' long-horizon planning in complex RPG worlds, revealing significant performance gaps and specific weaknesses in their ability to create and execute structured action sequences.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>882, <a href='https://arxiv.org/pdf/2508.12769.pdf' target='_blank'>https://arxiv.org/pdf/2508.12769.pdf</a></span>   <span><a href='https://github.com/smduan/CRED-SQL.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoming Duan, Zirui Wang, Chuanyi Liu, Zhibin Zhu, Yuhao Zhang, Peiyi Han, Liang Yan, Zewu Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12769">CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in large language models (LLMs) have significantly improved the accuracy of Text-to-SQL systems. However, a critical challenge remains: the semantic mismatch between natural language questions (NLQs) and their corresponding SQL queries. This issue is exacerbated in large-scale databases, where semantically similar attributes hinder schema linking and semantic drift during SQL generation, ultimately reducing model accuracy. To address these challenges, we introduce CRED-SQL, a framework designed for large-scale databases that integrates Cluster Retrieval and Execution Description. CRED-SQL first performs cluster-based large-scale schema retrieval to pinpoint the tables and columns most relevant to a given NLQ, alleviating schema mismatch. It then introduces an intermediate natural language representation-Execution Description Language (EDL)-to bridge the gap between NLQs and SQL. This reformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL, leveraging LLMs' strong general reasoning capabilities while reducing semantic deviation. Extensive experiments on two large-scale, cross-domain benchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new state-of-the-art (SOTA) performance, validating its effectiveness and scalability. Our code is available at https://github.com/smduan/CRED-SQL.git<br>
<br>
<div id='section'>Paperid: <span id='pid'>883, <a href='https://arxiv.org/pdf/2508.12766.pdf' target='_blank'>https://arxiv.org/pdf/2508.12766.pdf</a></span>   <span><a href='https://github.com/pipixiapipi/ICAF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peihao Li, Yan Fang, Man Liu, Huihui Bai, Anhong Wang, Yunchao Wei, Yao Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12766">Harnessing Group-Oriented Consistency Constraints for Semi-Supervised Semantic Segmentation in CdZnTe Semiconductors</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Labeling Cadmium Zinc Telluride (CdZnTe) semiconductor images is challenging due to the low-contrast defect boundaries, necessitating annotators to cross-reference multiple views. These views share a single ground truth (GT), forming a unique ``many-to-one'' relationship. This characteristic renders advanced semi-supervised semantic segmentation (SSS) methods suboptimal, as they are generally limited by a ``one-to-one'' relationship, where each image is independently associated with its GT. Such limitation may lead to error accumulation in low-contrast regions, further exacerbating confirmation bias. To address this issue, we revisit the SSS pipeline from a group-oriented perspective and propose a human-inspired solution: the Intra-group Consistency Augmentation Framework (ICAF). First, we experimentally validate the inherent consistency constraints within CdZnTe groups, establishing a group-oriented baseline using the Intra-group View Sampling (IVS). Building on this insight, we introduce the Pseudo-label Correction Network (PCN) to enhance consistency representation, which consists of two key modules. The View Augmentation Module (VAM) improves boundary details by dynamically synthesizing a boundary-aware view through the aggregation of multiple views. In the View Correction Module (VCM), this synthesized view is paired with other views for information interaction, effectively emphasizing salient regions while minimizing noise. Extensive experiments demonstrate the effectiveness of our solution for CdZnTe materials. Leveraging DeepLabV3+ with a ResNet-101 backbone as our segmentation model, we achieve a 70.6\% mIoU on the CdZnTe dataset using only 2 group-annotated data (5\textperthousand). The code is available at \href{https://github.com/pipixiapipi/ICAF}{https://github.com/pipixiapipi/ICAF}.<br>
<span id='abs_ch'>中文摘要：针对碲锌镉半导体图像低对比度缺陷边界 注难题，本文提出基于组内一致性增强框架（ICAF），通过视图增强与 正模块强化多视图间一致性表征，仅用千分之五 注数据即在CdZnTe数据集上实现70.6%的mIoU。</span><br>
<span id='abs_en'>English Summary: The proposed Intra-group Consistency Augmentation Framework (ICAF) addresses the limitations of semi-supervised semantic segmentation in low-contrast CdZnTe semiconductor images by leveraging group-oriented consistency constraints and pseudo-label correction, achieving 70.6% mIoU with minimal annotated data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>884, <a href='https://arxiv.org/pdf/2508.12754.pdf' target='_blank'>https://arxiv.org/pdf/2508.12754.pdf</a></span>   <span><a href='https://github.com/alessioGalatolo/AMAeval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessio Galatolo, Luca Alberto Rappuoli, Katie Winkle, Meriem Beloucif
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12754">Beyond Ethical Alignment: Evaluating LLMs as Artificial Moral Assistants</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The recent rise in popularity of large language models (LLMs) has prompted considerable concerns about their moral capabilities. Although considerable effort has been dedicated to aligning LLMs with human moral values, existing benchmarks and evaluations remain largely superficial, typically measuring alignment based on final ethical verdicts rather than explicit moral reasoning. In response, this paper aims to advance the investigation of LLMs' moral capabilities by examining their capacity to function as Artificial Moral Assistants (AMAs), systems envisioned in the philosophical literature to support human moral deliberation. We assert that qualifying as an AMA requires more than what state-of-the-art alignment techniques aim to achieve: not only must AMAs be able to discern ethically problematic situations, they should also be able to actively reason about them, navigating between conflicting values outside of those embedded in the alignment phase. Building on existing philosophical literature, we begin by designing a new formal framework of the specific kind of behaviour an AMA should exhibit, individuating key qualities such as deductive and abductive moral reasoning. Drawing on this theoretical framework, we develop a benchmark to test these qualities and evaluate popular open LLMs against it. Our results reveal considerable variability across models and highlight persistent shortcomings, particularly regarding abductive moral reasoning. Our work connects theoretical philosophy with practical AI evaluation while also emphasising the need for dedicated strategies to explicitly enhance moral reasoning capabilities in LLMs. Code available at https://github.com/alessioGalatolo/AMAeval<br>
<span id='abs_ch'>中文: 本文提出评估大语言模型作为人工道德助手的框架，强调其需要超越表面伦理判断的显性道德推理能力，并通过新基准测试揭示了模型在溯 推理方面存在持续缺陷。</span><br>
<span id='abs_en'>English: This paper introduces a framework to evaluate large language models as Artificial Moral Assistants, highlighting their need for explicit moral reasoning beyond superficial alignment and revealing persistent deficiencies in abductive reasoning through new benchmarks.Paperid:513,https://arxiv.org/pdf/2508.12720.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>885, <a href='https://arxiv.org/pdf/2508.12610.pdf' target='_blank'>https://arxiv.org/pdf/2508.12610.pdf</a></span>   <span><a href='https://github.com/qianchen214/OpenMoCap' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Qian, Danyang Li, Xinran Yu, Zheng Yang, Qiang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12610">OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Optical motion capture is a foundational technology driving advancements in cutting-edge fields such as virtual reality and film production. However, system performance suffers severely under large-scale marker occlusions common in real-world applications. An in-depth analysis identifies two primary limitations of current models: (i) the lack of training datasets accurately reflecting realistic marker occlusion patterns, and (ii) the absence of training strategies designed to capture long-range dependencies among markers. To tackle these challenges, we introduce the CMU-Occlu dataset, which incorporates ray tracing techniques to realistically simulate practical marker occlusion patterns. Furthermore, we propose OpenMoCap, a novel motion-solving model designed specifically for robust motion capture in environments with significant occlusions. Leveraging a marker-joint chain inference mechanism, OpenMoCap enables simultaneous optimization and construction of deep constraints between markers and joints. Extensive comparative experiments demonstrate that OpenMoCap consistently outperforms competing methods across diverse scenarios, while the CMU-Occlu dataset opens the door for future studies in robust motion solving. The proposed OpenMoCap is integrated into the MoSen MoCap system for practical deployment. The code is released at: https://github.com/qianchen214/OpenMoCap.<br>
<span id='abs_ch'>Chinese: 光学动作捕捉系统  记点遮挡导致性能下降，为此提出了模拟真实遮挡的CMU-Occlu数据集和通过 记点-关节链优化实现鲁棒捕捉的OpenMoCap模型，有效解决了遮挡问题。</span><br>
<span id='abs_en'>English: Optical motion capture systems face performance degradation due to marker occlusions, which is addressed by the new CMU-Occlu dataset simulating realistic occlusions and the OpenMoCap model that robustly handles these challenges through marker-joint chain optimization.Paperid:521,https://arxiv.org/pdf/2508.12594.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>886, <a href='https://arxiv.org/pdf/2508.12551.pdf' target='_blank'>https://arxiv.org/pdf/2508.12551.pdf</a></span>   <span><a href='https://github.com/LHY-24/OS-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyu Lin, Yuchen Li, Haoran Luo, Kaichun Yao, Libo Zhang, Mingjie Xing, Yanjun Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12551">OS-R1: Agentic Operating System Kernel Tuning with Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Linux kernel tuning is essential for optimizing operating system (OS) performance. However, existing methods often face challenges in terms of efficiency, scalability, and generalization. This paper introduces OS-R1, an agentic Linux kernel tuning framework powered by rule-based reinforcement learning (RL). By abstracting the kernel configuration space as an RL environment, OS-R1 facilitates efficient exploration by large language models (LLMs) and ensures accurate configuration modifications. Additionally, custom reward functions are designed to enhance reasoning standardization, configuration modification accuracy, and system performance awareness of the LLMs. Furthermore, we propose a two-phase training process that accelerates convergence and minimizes retraining across diverse tuning scenarios. Experimental results show that OS-R1 significantly outperforms existing baseline methods, achieving up to 5.6% performance improvement over heuristic tuning and maintaining high data efficiency. Notably, OS-R1 is adaptable across various real-world applications, demonstrating its potential for practical deployment in diverse environments. Our dataset and code are publicly available at https://github.com/LHY-24/OS-R1.<br>
<span id='abs_ch'>中文: 本文提出OS-R1框架，采用基于规则的强化学 方法，通过大语言模型高效探索Linux内 配置空间，在多种实际应用中实现高达5.6%的性能提升，并展现出优异的跨场景适应能力。</span><br>
<span id='abs_en'>English: This paper introduces OS-R1, a rule-based reinforcement learning framework that optimizes Linux kernel performance by enabling LLMs to efficiently explore configurations, achieving up to 5.6% performance gains over existing methods while maintaining adaptability across diverse applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>887, <a href='https://arxiv.org/pdf/2508.12533.pdf' target='_blank'>https://arxiv.org/pdf/2508.12533.pdf</a></span>   <span><a href='https://github.com/GeQinwen/DataCentricBrainGraphs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinwen Ge, Roza G. Bayrak, Anwar Said, Catie Chang, Xenofon Koutsoukos, Tyler Derr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12533">Defining and Benchmarking a Data-Centric Design Space for Brain Graph Construction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The construction of brain graphs from functional Magnetic Resonance Imaging (fMRI) data plays a crucial role in enabling graph machine learning for neuroimaging. However, current practices often rely on rigid pipelines that overlook critical data-centric choices in how brain graphs are constructed. In this work, we adopt a Data-Centric AI perspective and systematically define and benchmark a data-centric design space for brain graph construction, constrasting with primarily model-centric prior work. We organize this design space into three stages: temporal signal processing, topology extraction, and graph featurization. Our contributions lie less in novel components and more in evaluating how combinations of existing and modified techniques influence downstream performance. Specifically, we study high-amplitude BOLD signal filtering, sparsification and unification strategies for connectivity, alternative correlation metrics, and multi-view node and edge features, such as incorporating lagged dynamics. Experiments on the HCP1200 and ABIDE datasets show that thoughtful data-centric configurations consistently improve classification accuracy over standard pipelines. These findings highlight the critical role of upstream data decisions and underscore the importance of systematically exploring the data-centric design space for graph-based neuroimaging. Our code is available at https://github.com/GeQinwen/DataCentricBrainGraphs.<br>
<span id='abs_ch'>中文摘要：本 究倡导采用数据为中心的方法构建fMRI脑图，证明通过系统探索信号处理和图形构建中的设计选择，相比 准方法能显著提升分类准确性。</span><br>
<span id='abs_en'>English Summary: This study advocates for a data-centric approach to constructing brain graphs from fMRI data, demonstrating that systematic exploration of design choices in signal processing and graph construction significantly enhances classification accuracy over standard methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>888, <a href='https://arxiv.org/pdf/2508.12495.pdf' target='_blank'>https://arxiv.org/pdf/2508.12495.pdf</a></span>   <span><a href='https://github.com/MrLYG/CDCR-SFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuangang Li, Yiqing Shen, Yi Nian, Jiechao Gao, Ziyi Wang, Chenxiao Yu, Shawn Li, Jie Wang, Xiyang Hu, Yue Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12495">Mitigating Hallucinations in Large Language Models via Causal Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) exhibit logically inconsistent hallucinations that appear coherent yet violate reasoning principles, with recent research suggesting an inverse relationship between causal reasoning capabilities and such hallucinations. However, existing reasoning approaches in LLMs, such as Chain-of-Thought (CoT) and its graph-based variants, operate at the linguistic token level rather than modeling the underlying causal relationships between variables, lacking the ability to represent conditional independencies or satisfy causal identification assumptions. To bridge this gap, we introduce causal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning framework that trains LLMs to explicitly construct variable-level directed acyclic graph (DAG) and then perform reasoning over it. Moreover, we present a dataset comprising 25,368 samples (CausalDR), where each sample includes an input question, explicit causal DAG, graph-based reasoning trace, and validated answer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves the causal reasoning capability with the state-of-the-art 95.33% accuracy on CLADDER (surpassing human performance of 94.8% for the first time) and reduces the hallucination on HaluEval with 10% improvements. It demonstrates that explicit causal structure modeling in LLMs can effectively mitigate logical inconsistencies in LLM outputs. Code is available at https://github.com/MrLYG/CDCR-SFT.<br>
<span id='abs_ch'>Chinese: CDCR-SFT框架通过训练大语言模型显式构建并基于 果有向 环图进行推理，将CLADDER上的 果推理准确率显著提升至95.33%，并在HaluEval上使幻觉现象减少10%。</span><br>
<span id='abs_en'>English: The CDCR-SFT framework enhances large language models by training them to explicitly construct and reason over causal directed acyclic graphs, significantly improving causal reasoning accuracy to 95.33% on CLADDER and reducing hallucinations by 10% on HaluEval.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>889, <a href='https://arxiv.org/pdf/2508.12485.pdf' target='_blank'>https://arxiv.org/pdf/2508.12485.pdf</a></span>   <span><a href='https://github.com/ayushgupta4897/DRL-Cache' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aayush Gupta, Arpit Bhayani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12485">Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Web proxies such as NGINX commonly rely on least-recently-used (LRU) eviction, which is size agnostic and can thrash under periodic bursts and mixed object sizes. We introduce Cold-RL, a learned eviction policy for NGINX that replaces LRU's forced-expire path with a dueling Deep Q-Network served by an ONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL samples the K least-recently-used objects, extracts six lightweight features (age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT), and requests a bitmask of victims; a hard timeout of 500 microseconds triggers immediate fallback to native LRU. Policies are trained offline by replaying NGINX access logs through a cache simulator with a simple reward: a retained object earns one point if it is hit again before TTL expiry. We compare against LRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial workloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538, a 146 percent improvement over the best classical baseline; at 100 MB, from 0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods (about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th percentile eviction latency within budget. To our knowledge, this is the first reinforcement learning eviction policy integrated into NGINX with strict SLOs.<br>
<span id='abs_ch'>中文：Cold-RL是一种基于强化学 的NGINX淘汰策略，通过轻量级特征智能选择淘汰对象替代 统LRU缓存，在严 延迟限制下显著提升命中率且仅增 极少开销。</span><br>
<span id='abs_en'>English: Cold-RL is a reinforcement learning-based eviction policy for NGINX that replaces traditional LRU caching by intelligently selecting victims using lightweight features, significantly improving hit ratios with minimal overhead while adhering to strict latency budgets.Paperid:530,https://arxiv.org/pdf/2508.12445.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>890, <a href='https://arxiv.org/pdf/2508.12410.pdf' target='_blank'>https://arxiv.org/pdf/2508.12410.pdf</a></span>   <span><a href='https://github.com/JunZengz/SRMA-Mamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Zeng, Yannan Huang, Elif Keles, Halil Ertugrul Aktas, Gorkem Durak, Nikhil Kumar Tomar, Quoc-Huy Trinh, Deepak Ranjan Nayak, Ulas Bagci, Debesh Jha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12410">SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological Liver Segmentation in MRI Volumes</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Liver Cirrhosis plays a critical role in the prognosis of chronic liver disease. Early detection and timely intervention are critical in significantly reducing mortality rates. However, the intricate anatomical architecture and diverse pathological changes of liver tissue complicate the accurate detection and characterization of lesions in clinical settings. Existing methods underutilize the spatial anatomical details in volumetric MRI data, thereby hindering their clinical effectiveness and explainability. To address this challenge, we introduce a novel Mamba-based network, SRMA-Mamba, designed to model the spatial relationships within the complex anatomical structures of MRI volumes. By integrating the Spatial Anatomy-Based Mamba module (SABMamba), SRMA-Mamba performs selective Mamba scans within liver cirrhotic tissues and combines anatomical information from the sagittal, coronal, and axial planes to construct a global spatial context representation, enabling efficient volumetric segmentation of pathological liver structures. Furthermore, we introduce the Spatial Reverse Attention module (SRMA), designed to progressively refine cirrhotic details in the segmentation map, utilizing both the coarse segmentation map and hierarchical encoding features. Extensive experiments demonstrate that SRMA-Mamba surpasses state-of-the-art methods, delivering exceptional performance in 3D pathological liver segmentation. Our code is available for public: https://github.com/JunZengz/SRMA-Mamba.<br>
<span id='abs_ch'>中文摘要：肝硬化预后关键在于早期发现，而SRMA-Mamba网络通过整合MRI三维空间解剖信息，有效解决了临床诊断难题，实现了卓越的病理肝脏三维分割性能。</span><br>
<span id='abs_en'>English Summary: Liver cirrhosis prognosis depends on early detection, and the proposed SRMA-Mamba network effectively addresses clinical challenges by integrating spatial anatomical details from MRI volumes for superior 3D pathological liver segmentation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>891, <a href='https://arxiv.org/pdf/2508.12341.pdf' target='_blank'>https://arxiv.org/pdf/2508.12341.pdf</a></span>   <span><a href='https://github.com/wzy1111111/SSD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziye Wang, Minghang Yu, Chunyan Xu, Zhen Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12341">Semantic Discrepancy-aware Detector for Image Forgery Identification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the rapid advancement of image generation techniques, robust forgery detection has become increasingly imperative to ensure the trustworthiness of digital media. Recent research indicates that the learned semantic concepts of pre-trained models are critical for identifying fake images. However, the misalignment between the forgery and semantic concept spaces hinders the model's forgery detection performance. To address this problem, we propose a novel Semantic Discrepancy-aware Detector (SDD) that leverages reconstruction learning to align the two spaces at a fine-grained visual level. By exploiting the conceptual knowledge embedded in the pre-trained vision language model, we specifically design a semantic token sampling module to mitigate the space shifts caused by features irrelevant to both forgery traces and semantic concepts. A concept-level forgery discrepancy learning module, built upon a visual reconstruction paradigm, is proposed to strengthen the interaction between visual semantic concepts and forgery traces, effectively capturing discrepancies under the concepts' guidance. Finally, the low-level forgery feature enhancemer integrates the learned concept level forgery discrepancies to minimize redundant forgery information. Experiments conducted on two standard image forgery datasets demonstrate the efficacy of the proposed SDD, which achieves superior results compared to existing methods. The code is available at https://github.com/wzy1111111/SSD.<br>
<span id='abs_ch'>中文摘要：提出的语义差异感知检测器（SDD）通过重建学 和专门设计的模块，在细粒度视觉层面实现伪 与语义概念空间的对齐，从而显著提升伪 图像检测性能。English Summary: The proposed Semantic Discrepancy-aware Detector (SDD) aligns forgery and semantic concept spaces through reconstruction learning and specialized modules to significantly improve fake image detection performance.Paperid:538,https://arxiv.org/pdf/2508.12281.pdfGitHub</span><br>
<span id='abs_en'>English Summary: The proposed Semantic Discrepancy-aware Detector (SDD) aligns forgery and semantic concept spaces through reconstruction learning and specialized modules to significantly improve fake image detection performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>892, <a href='https://arxiv.org/pdf/2508.12263.pdf' target='_blank'>https://arxiv.org/pdf/2508.12263.pdf</a></span>   <span><a href='https://github.com/hongliang-wei/RC-MLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongliang Wei, Xianqi Zhang, Xingtao Wang, Xiaopeng Fan, Debin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12263">Region-Level Context-Aware Multimodal Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite significant progress, existing research on Multimodal Large Language Models (MLLMs) mainly focuses on general visual understanding, overlooking the ability to integrate textual context associated with objects for a more context-aware multimodal understanding -- an ability we refer to as Region-level Context-aware Multimodal Understanding (RCMU). To address this limitation, we first formulate the RCMU task, which requires models to respond to user instructions by integrating both image content and textual information of regions or objects. To equip MLLMs with RCMU capabilities, we propose Region-level Context-aware Visual Instruction Tuning (RCVIT), which incorporates object information into the model input and enables the model to utilize bounding box coordinates to effectively associate objects' visual content with their textual information. To address the lack of datasets, we introduce the RCMU dataset, a large-scale visual instruction tuning dataset that covers multiple RCMU tasks. We also propose RC\&P-Bench, a comprehensive benchmark that can evaluate the performance of MLLMs in RCMU and multimodal personalized understanding tasks. Additionally, we propose a reference-free evaluation metric to perform a comprehensive and fine-grained evaluation of the region-level context-aware image descriptions. By performing RCVIT on Qwen2-VL models with the RCMU dataset, we developed RC-Qwen2-VL models. Experimental results indicate that RC-Qwen2-VL models not only achieve outstanding performance on multiple RCMU tasks but also demonstrate successful applications in multimodal RAG and personalized conversation. Our data, model and benchmark are available at https://github.com/hongliang-wei/RC-MLLM<br>
<span id='abs_ch'>中文: 本 究提出了区域级上下文感知多模态理解（RCMU），通过结合对象文本信息与视觉内容，开发了RCVIT方法和相关数据集及基准，实验表明RC-Qwen2-VL模型在RCMU任务和实际应用中表现卓越。</span><br>
<span id='abs_en'>English: This research introduces Region-level Context-aware Multimodal Understanding (RCMU) to enhance MLLMs by integrating object-specific textual context with visual data, proposing the RCVIT method and a new dataset and benchmark, with the resulting RC-Qwen2-VL models showing superior performance in RCMU tasks and practical applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>893, <a href='https://arxiv.org/pdf/2508.12263.pdf' target='_blank'>https://arxiv.org/pdf/2508.12263.pdf</a></span>   <span><a href='https://github.com/hongliang-wei/RC-MLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongliang Wei, Xianqi Zhang, Xingtao Wang, Xiaopeng Fan, Debin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12263">Region-Level Context-Aware Multimodal Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite significant progress, existing research on Multimodal Large Language Models (MLLMs) mainly focuses on general visual understanding, overlooking the ability to integrate textual context associated with objects for a more context-aware multimodal understanding -- an ability we refer to as Region-level Context-aware Multimodal Understanding (RCMU). To address this limitation, we first formulate the RCMU task, which requires models to respond to user instructions by integrating both image content and textual information of regions or objects. To equip MLLMs with RCMU capabilities, we propose Region-level Context-aware Visual Instruction Tuning (RCVIT), which incorporates object information into the model input and enables the model to utilize bounding box coordinates to effectively associate objects' visual content with their textual information. To address the lack of datasets, we introduce the RCMU dataset, a large-scale visual instruction tuning dataset that covers multiple RCMU tasks. We also propose RC\&P-Bench, a comprehensive benchmark that can evaluate the performance of MLLMs in RCMU and multimodal personalized understanding tasks. Additionally, we propose a reference-free evaluation metric to perform a comprehensive and fine-grained evaluation of the region-level context-aware image descriptions. By performing RCVIT on Qwen2-VL models with the RCMU dataset, we developed RC-Qwen2-VL models. Experimental results indicate that RC-Qwen2-VL models not only achieve outstanding performance on multiple RCMU tasks but also demonstrate successful applications in multimodal RAG and personalized conversation. Our data, model and benchmark are available at https://github.com/hongliang-wei/RC-MLLM<br>
<span id='abs_ch'>中文: 本 究提出了区域级上下文感知多模态理解（RCMU），通过结合对象文本信息与视觉内容，开发了RCVIT方法和相关数据集及基准，实验表明RC-Qwen2-VL模型在RCMU任务和实际应用中表现卓越。</span><br>
<span id='abs_en'>English: This research introduces Region-level Context-aware Multimodal Understanding (RCMU) to enhance MLLMs by integrating object-specific textual context with visual data, proposing the RCVIT method and a new dataset and benchmark, with the resulting RC-Qwen2-VL models showing superior performance in RCMU tasks and practical applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>894, <a href='https://arxiv.org/pdf/2508.12213.pdf' target='_blank'>https://arxiv.org/pdf/2508.12213.pdf</a></span>   <span><a href='https://github.com/rh20624/Awesome-IMU-Sensing,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yize Cai, Baoshen Guo, Flora Salim, Zhiqing Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12213">Towards Generalizable Human Activity Recognition: A Survey</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As a critical component of Wearable AI, IMU-based Human Activity Recognition (HAR) has attracted increasing attention from both academia and industry in recent years. Although HAR performance has improved considerably in specific scenarios, its generalization capability remains a key barrier to widespread real-world adoption. For example, domain shifts caused by variations in users, sensor positions, or environments can significantly decrease the performance in practice. As a result, in this survey, we explore the rapidly evolving field of IMU-based generalizable HAR, reviewing 229 research papers alongside 25 publicly available datasets to provide a broad and insightful overview. We first present the background and overall framework of IMU-based HAR tasks, as well as the generalization-oriented training settings. Then, we categorize representative methodologies from two perspectives: (i) model-centric approaches, including pre-training method, end-to-end method, and large language model (LLM)-based learning method; and (ii) data-centric approaches, including multi-modal learning and data augmentation techniques. In addition, we summarize widely used datasets in this field, as well as relevant tools and benchmarks. Building on these methodological advances, the broad applicability of IMU-based HAR is also reviewed and discussed. Finally, we discuss persistent challenges (e.g., data scarcity, efficient training, and reliable evaluation) and also outline future directions for HAR, including the adoption of foundation and large language models, physics-informed and context-aware reasoning, generative modeling, and resource-efficient training and inference. The complete list of this survey is available at https://github.com/rh20624/Awesome-IMU-Sensing, which will be updated continuously.<br>
<span id='abs_ch'>中文: 本综述探讨基于惯性 感器的可泛化人体活动识别，通过梳理方法论和数据集应对领域偏移挑战，并展望了基础模型与高效训练等未来方向。</span><br>
<span id='abs_en'>English: This survey explores IMU-based generalizable human activity recognition, reviewing methodologies and datasets to address domain shift challenges and outlining future directions like foundation models and efficient training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>895, <a href='https://arxiv.org/pdf/2508.12082.pdf' target='_blank'>https://arxiv.org/pdf/2508.12082.pdf</a></span>   <span><a href='https://github.com/YonseiML/autoeval-det' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungju Yoo, Hyuk Kwon, Joong-Won Hwang, Kibok Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12082">Automated Model Evaluation for Object Detection via Prediction Consistency and Reliablity</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in computer vision have made training object detectors more efficient and effective; however, assessing their performance in real-world applications still relies on costly manual annotation. To address this limitation, we develop an automated model evaluation (AutoEval) framework for object detection. We propose Prediction Consistency and Reliability (PCR), which leverages the multiple candidate bounding boxes that conventional detectors generate before non-maximum suppression (NMS). PCR estimates detection performance without ground-truth labels by jointly measuring 1) the spatial consistency between boxes before and after NMS, and 2) the reliability of the retained boxes via the confidence scores of overlapping boxes. For a more realistic and scalable evaluation, we construct a meta-dataset by applying image corruptions of varying severity. Experimental results demonstrate that PCR yields more accurate performance estimates than existing AutoEval methods, and the proposed meta-dataset covers a wider range of detection performance. The code is available at https://github.com/YonseiML/autoeval-det.<br>
<span id='abs_ch'>中文: AutoEval框架提出预测一致性与可 性（PCR）方法，通过分析边界框的空间一致性和置信度可 性， 需真实 注即可自动评估目 检测性能，经多 化元数据集验证，其评估准确性优于现有方法。</span><br>
<span id='abs_en'>English: The AutoEval framework introduces Prediction Consistency and Reliability (PCR) to automatically estimate object detection performance without ground-truth labels by analyzing spatial consistency and confidence reliability of bounding boxes, validated through a diverse meta-dataset showing superior accuracy over existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>896, <a href='https://arxiv.org/pdf/2508.12082.pdf' target='_blank'>https://arxiv.org/pdf/2508.12082.pdf</a></span>   <span><a href='https://github.com/YonseiML/autoeval-det' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seungju Yoo, Hyuk Kwon, Joong-Won Hwang, Kibok Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.12082">Automated Model Evaluation for Object Detection via Prediction Consistency and Reliability</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in computer vision have made training object detectors more efficient and effective; however, assessing their performance in real-world applications still relies on costly manual annotation. To address this limitation, we develop an automated model evaluation (AutoEval) framework for object detection. We propose Prediction Consistency and Reliability (PCR), which leverages the multiple candidate bounding boxes that conventional detectors generate before non-maximum suppression (NMS). PCR estimates detection performance without ground-truth labels by jointly measuring 1) the spatial consistency between boxes before and after NMS, and 2) the reliability of the retained boxes via the confidence scores of overlapping boxes. For a more realistic and scalable evaluation, we construct a meta-dataset by applying image corruptions of varying severity. Experimental results demonstrate that PCR yields more accurate performance estimates than existing AutoEval methods, and the proposed meta-dataset covers a wider range of detection performance. The code is available at https://github.com/YonseiML/autoeval-det.<br>
<span id='abs_ch'>中文: AutoEval框架提出预测一致性与可 性（PCR）方法，通过分析边界框的空间一致性和置信度可 性， 需真实 注即可自动评估目 检测性能，经多 化元数据集验证，其评估准确性优于现有方法。</span><br>
<span id='abs_en'>English: The AutoEval framework introduces Prediction Consistency and Reliability (PCR) to automatically estimate object detection performance without ground-truth labels by analyzing spatial consistency and confidence reliability of bounding boxes, validated through a diverse meta-dataset showing superior accuracy over existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>897, <a href='https://arxiv.org/pdf/2508.11915.pdf' target='_blank'>https://arxiv.org/pdf/2508.11915.pdf</a></span>   <span><a href='https://github.com/psyonp/core' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Punya Syon Pandey, Yongjin Yang, Jiarui Liu, Zhijing Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11915">CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Game-theoretic interactions between agents with Large Language Models (LLMs) have revealed many emergent capabilities, yet the linguistic diversity of these interactions has not been sufficiently quantified. In this paper, we present the Conversational Robustness Evaluation Score: CORE, a metric to quantify the effectiveness of language use within multi-agent systems across different game-theoretic interactions. CORE integrates measures of cluster entropy, lexical repetition, and semantic similarity, providing a direct lens of dialog quality. We apply CORE to pairwise LLM dialogs across competitive, cooperative, and neutral settings, further grounding our analysis in Zipf's and Heaps' Laws to characterize word frequency distributions and vocabulary growth. Our findings show that cooperative settings exhibit both steeper Zipf distributions and higher Heap exponents, indicating more repetition alongside greater vocabulary expansion. In contrast, competitive interactions display lower Zipf and Heaps exponents, reflecting less repetition and more constrained vocabularies. These results provide new insights into how social incentives influence language adaptation, and highlight CORE as a robust diagnostic for measuring linguistic robustness in multi-agent LLM systems. Our code is available at https://github.com/psyonp/core.<br>
<span id='abs_ch'>中文摘要：本文提出CORE指 ，用于量化多智能体系统中语言使用的有效性， 究发现合作场景促进词汇扩展但伴随重复，而竞争场景则导致词汇受限。</span><br>
<span id='abs_en'>English Summary: The paper introduces CORE, a metric evaluating linguistic effectiveness in multi-agent LLM systems across game-theoretic scenarios, revealing that cooperative interactions foster vocabulary expansion with repetition while competitive ones yield constrained vocabularies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>898, <a href='https://arxiv.org/pdf/2508.11850.pdf' target='_blank'>https://arxiv.org/pdf/2508.11850.pdf</a></span>   <span><a href='https://github.com/milad1378yz/EvoCut' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Milad Yazdani, Mahdi Mostajabdaveh, Samin Aref, Zirui Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11850">EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Integer programming lies at the heart of crucial combinatorial optimization tasks but remains challenging due to its NP-hard nature. An effective approach for practically solving integer programs is the manual design of acceleration cuts, i.e. inequalities that improve solver performance. However, this creative process demands deep expertise and is yet to be automated. Our proposed framework, EvoCut, automates the generation of acceleration cuts by combining large language models (LLMs) with an evolutionary search. EvoCut (i) initializes a diverse population of candidate cuts via an LLM-based initializer agent; (ii) for each cut empirically evaluates both preservation of the optimal solution and its ability to cut off fractional solutions across a verification set; and (iii) iteratively refines the population through evolutionary crossover and mutation agents. We quantify each cut's utility by its relative reduction in the solver's optimality gap. Our comparisons against standard integer programming practice show that EvoCut reduces optimality gap by 17-57% within a fixed time. It obtains the same solutions up to 4 times as fast, and obtains higher-quality solutions within the same time limit. Requiring no human expert input, EvoCut reliably generates, improves, and empirically verifies cuts that generalize to unseen instances. The code is available at https://github.com/milad1378yz/EvoCut.<br>
<span id='abs_ch'>中文: EvoCut通过结合大语言模型与进化搜索，自动化生成整数规划的 速割平面， 需人工干预即可显著降低最优性差距并提升求解速度与质量。</span><br>
<span id='abs_en'>English: EvoCut automates the generation of acceleration cuts for integer programming by integrating large language models with evolutionary search, significantly reducing optimality gaps and improving solution speed and quality without human intervention.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>899, <a href='https://arxiv.org/pdf/2508.11695.pdf' target='_blank'>https://arxiv.org/pdf/2508.11695.pdf</a></span>   <span><a href='https://github.com/Anonymous-Name-139/RefAdgen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyun Chen, Weikai Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11695">RefAdGen: High-Fidelity Advertising Image Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of Artificial Intelligence Generated Content (AIGC) techniques has unlocked opportunities in generating diverse and compelling advertising images based on referenced product images and textual scene descriptions. This capability substantially reduces human labor and production costs in traditional marketing workflows. However, existing AIGC techniques either demand extensive fine-tuning for each referenced image to achieve high fidelity, or they struggle to maintain fidelity across diverse products, making them impractical for e-commerce and marketing industries. To tackle this limitation, we first construct AdProd-100K, a large-scale advertising image generation dataset. A key innovation in its construction is our dual data augmentation strategy, which fosters robust, 3D-aware representations crucial for realistic and high-fidelity image synthesis. Leveraging this dataset, we propose RefAdGen, a generation framework that achieves high fidelity through a decoupled design. The framework enforces precise spatial control by injecting a product mask at the U-Net input, and employs an efficient Attention Fusion Module (AFM) to integrate product features. This design effectively resolves the fidelity-efficiency dilemma present in existing methods. Extensive experiments demonstrate that RefAdGen achieves state-of-the-art performance, showcasing robust generalization by maintaining high fidelity and remarkable visual results for both unseen products and challenging real-world, in-the-wild images. This offers a scalable and cost-effective alternative to traditional workflows. Code and datasets are publicly available at https://github.com/Anonymous-Name-139/RefAdgen.<br>
<br>
<div id='section'>Paperid: <span id='pid'>900, <a href='https://arxiv.org/pdf/2508.11676.pdf' target='_blank'>https://arxiv.org/pdf/2508.11676.pdf</a></span>   <span><a href='https://github.com/mshamrai/deep-language-geometry' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maksym Shamrai, Vladyslav Hamolia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11676">Deep Language Geometry: Constructing a Metric Space from LLM Weights</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce a novel framework that utilizes the internal weight activations of modern Large Language Models (LLMs) to construct a metric space of languages. Unlike traditional approaches based on hand-crafted linguistic features, our method automatically derives high-dimensional vector representations by computing weight importance scores via an adapted pruning algorithm. Our approach captures intrinsic language characteristics that reflect linguistic phenomena. We validate our approach across diverse datasets and multilingual LLMs, covering 106 languages. The results align well with established linguistic families while also revealing unexpected inter-language connections that may indicate historical contact or language evolution. The source code, computed language latent vectors, and visualization tool are made publicly available at https://github.com/mshamrai/deep-language-geometry.<br>
<span id='abs_ch'>中文: 本文提出了一种利用大语言模型权重激活构建语言度量空间的新框架，通过自动生成的向量表征捕捉语言内在特征，在106种语言中既验证了已知语系关系，又揭示了可能反 历史接触或语言演化的意外关联。</span><br>
<span id='abs_en'>English: This paper presents a novel framework that constructs a metric space of languages using LLM weight activations, automatically generating vector representations that capture intrinsic linguistic characteristics and reveal both established language families and unexpected inter-language connections across 106 languages.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>901, <a href='https://arxiv.org/pdf/2508.11673.pdf' target='_blank'>https://arxiv.org/pdf/2508.11673.pdf</a></span>   <span><a href='https://github.com/VentusAislant/MSLoRA_CR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haojie Zhang, Yixiong Liang, Hulin Kuang, Lihui Cen, Zhe Qu, Yigang Cen, Min Zeng, Shichao Kan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11673">Contrastive Regularization over LoRA for Multimodal Biomedical Image Incremental Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal Biomedical Image Incremental Learning (MBIIL) is essential for handling diverse tasks and modalities in the biomedical domain, as training separate models for each modality or task significantly increases inference costs. Existing incremental learning methods focus on task expansion within a single modality, whereas MBIIL seeks to train a unified model incrementally across modalities. The MBIIL faces two challenges: I) How to preserve previously learned knowledge during incremental updates? II) How to effectively leverage knowledge acquired from existing modalities to support new modalities? To address these challenges, we propose MSLoRA-CR, a method that fine-tunes Modality-Specific LoRA modules while incorporating Contrastive Regularization to enhance intra-modality knowledge sharing and promote inter-modality knowledge differentiation. Our approach builds upon a large vision-language model (LVLM), keeping the pretrained model frozen while incrementally adapting new LoRA modules for each modality or task. Experiments on the incremental learning of biomedical images demonstrate that MSLoRA-CR outperforms both the state-of-the-art (SOTA) approach of training separate models for each modality and the general incremental learning method (incrementally fine-tuning LoRA). Specifically, MSLoRA-CR achieves a 1.88% improvement in overall performance compared to unconstrained incremental learning methods while maintaining computational efficiency. Our code is publicly available at https://github.com/VentusAislant/MSLoRA_CR.<br>
<span id='abs_ch'>中文摘要：MSLoRA-CR是一种新颖的多模态生物医学图像增量学 方法，通过对比正则化微调模态特定的LoRA模块，在保持计算效率的同时实现跨模态知识共享，性能比现有方法提升1.88%。</span><br>
<span id='abs_en'>English Summary: MSLoRA-CR is a novel multimodal biomedical image incremental learning method that fine-tunes modality-specific LoRA modules with contrastive regularization to enable knowledge sharing across modalities while maintaining computational efficiency, outperforming existing approaches by 1.88%.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>902, <a href='https://arxiv.org/pdf/2508.11667.pdf' target='_blank'>https://arxiv.org/pdf/2508.11667.pdf</a></span>   <span><a href='https://github.com/ReDASers/representation-stability' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bryan E. Tuck, Rakesh M. Verma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11667">Assessing Representation Stability for Transformer Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Adversarial text attacks remain a persistent threat to transformer models, yet existing defenses are typically attack-specific or require costly model retraining. We introduce Representation Stability (RS), a model-agnostic detection framework that identifies adversarial examples by measuring how embedding representations change when important words are masked. RS first ranks words using importance heuristics, then measures embedding sensitivity to masking top-k critical words, and processes the resulting patterns with a BiLSTM detector. Experiments show that adversarially perturbed words exhibit disproportionately high masking sensitivity compared to naturally important words. Across three datasets, three attack types, and two victim models, RS achieves over 88% detection accuracy and demonstrates competitive performance compared to existing state-of-the-art methods, often at lower computational cost. Using Normalized Discounted Cumulative Gain (NDCG) to measure perturbation identification quality, we reveal that gradient-based ranking outperforms attention and random selection approaches, with identification quality correlating with detection performance for word-level attacks. RS also generalizes well to unseen datasets, attacks, and models without retraining, providing a practical solution for adversarial text detection.<br>
<span id='abs_ch'>中文: 本文提出表征稳定性（RS）框架，通过掩蔽重要词汇时测量嵌入表示的敏感性来检测对抗文本，在多种数据集和攻击中 需重新训练即可实现超过88%的检测准确率。</span><br>
<span id='abs_en'>English: This paper introduces Representation Stability (RS), a model-agnostic framework that detects adversarial text by measuring embedding sensitivity when masking important words, achieving over 88% detection accuracy across various datasets and attacks without requiring retraining.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>903, <a href='https://arxiv.org/pdf/2508.11503.pdf' target='_blank'>https://arxiv.org/pdf/2508.11503.pdf</a></span>   <span><a href='https://github.com/AndrejOrsula/space_robotics_bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrej Orsula, Matthieu Geist, Miguel Olivares-Mendez, Carol Martinez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11503">Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reliable autonomous navigation across the unstructured terrains of distant planetary surfaces is a critical enabler for future space exploration. However, the deployment of learning-based controllers is hindered by the inherent sim-to-real gap, particularly for the complex dynamics of wheel interactions with granular media. This work presents a complete sim-to-real framework for developing and validating robust control policies for dynamic waypoint tracking on such challenging surfaces. We leverage massively parallel simulation to train reinforcement learning agents across a vast distribution of procedurally generated environments with randomized physics. These policies are then transferred zero-shot to a physical wheeled rover operating in a lunar-analogue facility. Our experiments systematically compare multiple reinforcement learning algorithms and action smoothing filters to identify the most effective combinations for real-world deployment. Crucially, we provide strong empirical evidence that agents trained with procedural diversity achieve superior zero-shot performance compared to those trained on static scenarios. We also analyze the trade-offs of fine-tuning with high-fidelity particle physics, which offers minor gains in low-speed precision at a significant computational cost. Together, these contributions establish a validated workflow for creating reliable learning-based navigation systems, marking a critical step towards deploying autonomous robots in the final frontier.<br>
<span id='abs_ch'>中文: 本 究提出一个完整的仿真到现实框架，通过在多 化仿真环境中训练强化学 智能体，实现了物理月球探测车的零 本鲁棒控制，证实了程序化生成环境优于静态场景训练，为极端地形下的自主导航建立了可 的工作流程。</span><br>
<span id='abs_en'>English: This study introduces a comprehensive sim-to-real framework that trains reinforcement learning agents in diverse simulated environments to achieve robust zero-shot performance on a physical rover, demonstrating the superiority of procedural diversity over static training and validating a reliable workflow for autonomous navigation on challenging planetary terrains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>904, <a href='https://arxiv.org/pdf/2508.11408.pdf' target='_blank'>https://arxiv.org/pdf/2508.11408.pdf</a></span>   <span><a href='https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhao Zhang, Yuexiang Xie, Yuchang Sun, Yanxi Chen, Guoyin Wang, Yaliang Li, Bolin Ding, Jingren Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11408">On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two prominent post-training paradigms for refining the capabilities and aligning the behavior of Large Language Models (LLMs). Existing approaches that integrate SFT and RL often face the risk of disrupting established model patterns and inducing overfitting to expert data. To address this, we present a novel investigation into the unified view of SFT and RL through an off-policy versus on-policy lens. We propose CHORD, a framework for the Controllable Harmonization of On- and Off-Policy Reinforcement Learning via Dynamic Weighting, which reframes SFT not as a separate stage but as a dynamically weighted auxiliary objective within the on-policy RL process. Based on an analysis of off-policy expert data's influence at both holistic and granular levels, we incorporate a dual-control mechanism in CHORD. Specifically, the framework first employs a global coefficient to holistically guide the transition from off-policy imitation to on-policy exploration, and then applies a token-wise weighting function that enables granular learning from expert tokens, which preserves on-policy exploration and mitigates disruption from off-policy data. We conduct extensive experiments on widely used benchmarks, providing empirical evidence that CHORD achieves a stable and efficient learning process. By effectively harmonizing off-policy expert data with on-policy exploration, CHORD demonstrates significant improvements over baselines. We release the implementation at https://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to inspire further research.<br>
<br>
<div id='section'>Paperid: <span id='pid'>905, <a href='https://arxiv.org/pdf/2508.11383.pdf' target='_blank'>https://arxiv.org/pdf/2508.11383.pdf</a></span>   <span><a href='https://github.com/AIRI-Institute/when-punctuation-matters' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mikhail Seleznyov, Mikhail Chaichuk, Gleb Ershov, Alexander Panchenko, Elena Tutubalina, Oleg Somov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11383">When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) are highly sensitive to subtle, non-semantic variations in prompt phrasing and formatting. In this work, we present the first systematic evaluation of 5 methods for improving prompt robustness within a unified experimental framework. We benchmark these techniques on 8 models from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions dataset. Our evaluation covers robustness methods from both fine-tuned and in-context learning paradigms, and tests their generalization against multiple types of distribution shifts. Finally, we extend our analysis to GPT-4.1 and DeepSeek V3 to assess frontier models' current robustness to format perturbations. Our findings offer actionable insights into the relative effectiveness of these robustness methods, enabling practitioners to make informed decisions when aiming for stable and reliable LLM performance in real-world applications. Code: https://github.com/AIRI-Institute/when-punctuation-matters.<br>
<span id='abs_ch'>中文: 本 究系统评估了提升大语言模型提示鲁棒性的五种方法，通过多模型多任务基准测试，为实际应用中的稳定性能提供了可操作的指导。</span><br>
<span id='abs_en'>English: This study systematically evaluates five methods to enhance prompt robustness in large language models, benchmarking them across multiple models and tasks to provide actionable insights for stable real-world performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>906, <a href='https://arxiv.org/pdf/2508.11347.pdf' target='_blank'>https://arxiv.org/pdf/2508.11347.pdf</a></span>   <span><a href='https://github.com/lyfxjtu/Dynamic-Embedding' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/lyfxjtu/Dynamic-Embedding' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifei Li, Lingling Zhang, Hang Yan, Tianzhe Zhao, Zihan Ma, Muye Huang, Jun Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11347">SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Traditional knowledge graph (KG) embedding methods aim to represent entities and relations in a low-dimensional space, primarily focusing on static graphs. However, real-world KGs are dynamically evolving with the constant addition of entities, relations and facts. To address such dynamic nature of KGs, several continual knowledge graph embedding (CKGE) methods have been developed to efficiently update KG embeddings to accommodate new facts while maintaining learned knowledge. As KGs grow at different rates and scales in real-world scenarios, existing CKGE methods often fail to consider the varying scales of updates and lack systematic evaluation throughout the entire update process. In this paper, we propose SAGE, a scale-aware gradual evolution framework for CKGE. Specifically, SAGE firstly determine the embedding dimensions based on the update scales and expand the embedding space accordingly. The Dynamic Distillation mechanism is further employed to balance the preservation of learned knowledge and the incorporation of new facts. We conduct extensive experiments on seven benchmarks, and the results show that SAGE consistently outperforms existing baselines, with a notable improvement of 1.38% in MRR, 1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with methods using fixed embedding dimensions show that SAGE achieves optimal performance on every snapshot, demonstrating the importance of adaptive embedding dimensions in CKGE. The codes of SAGE are publicly available at: https://github.com/lyfxjtu/Dynamic-Embedding.<br>
<span id='abs_ch'>中文: 本文提出SAGE框架，这是一种面向持续知识图谱嵌入的规模感知渐进演化方法，能 据更新规模动态调整嵌入维度并采用动态蒸馏机制平衡新旧知识，在多个基准测试中均实现了最优性能表现。</span><br>
<span id='abs_en'>English: This paper introduces SAGE, a scale-aware gradual evolution framework for continual knowledge graph embedding that dynamically adjusts embedding dimensions based on update scales and employs a dynamic distillation mechanism to balance knowledge preservation with new fact integration, achieving superior performance across multiple benchmarks.Paperid:583,https://arxiv.org/pdf/2508.11331.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>907, <a href='https://arxiv.org/pdf/2508.11256.pdf' target='_blank'>https://arxiv.org/pdf/2508.11256.pdf</a></span>   <span><a href='https://github.com/xiaomoguhz/DeCLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junjie Wang, Keyu Chen, Yulin Li, Bin Chen, Hengshuang Zhao, Xiaojuan Qi, Zhuotao Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11256">Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Dense visual perception tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct application to dense perception often leads to suboptimal performance due to limitations in local feature representation. In this work, we present our observation that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. To address this issue, we propose DeCLIP, a novel framework that enhances CLIP by decoupling the self-attention module to obtain ``content'' and ``context'' features respectively. \revise{The context features are enhanced by jointly distilling semantic correlations from Vision Foundation Models (VFMs) and object integrity cues from diffusion models, thereby enhancing spatial consistency. In parallel, the content features are aligned with image crop representations and constrained by region correlations from VFMs to improve local discriminability. Extensive experiments demonstrate that DeCLIP establishes a solid foundation for open-vocabulary dense perception, consistently achieving state-of-the-art performance across a broad spectrum of tasks, including 2D detection and segmentation, 3D instance segmentation, video instance segmentation, and 6D object pose estimation.} Code is available at https://github.com/xiaomoguhz/DeCLIP<br>
<span id='abs_ch'>Chinese: DeCLIP通过解耦自注意力机制为内容和上下文特征，提升了局部区分度和空间一致性，在多种开放词汇密集感知任务中实现了最优性能。</span><br>
<span id='abs_en'>English: DeCLIP enhances CLIP by decoupling self-attention into content and context features, improving local discriminability and spatial consistency for superior open-vocabulary dense perception across multiple tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>908, <a href='https://arxiv.org/pdf/2508.11074.pdf' target='_blank'>https://arxiv.org/pdf/2508.11074.pdf</a></span>   <span><a href='https://github.com/deepreasonings/long-form-video2audio' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haomin Zhang, Kristin Qi, Shuxin Yang, Zihao Chen, Chaofan Ding, Xinhan Di
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11074">LD-LAudio-V1: Video-to-Long-Form-Audio Generation Extension with Dual Lightweight Adapters</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Generating high-quality and temporally synchronized audio from video content is essential for video editing and post-production tasks, enabling the creation of semantically aligned audio for silent videos. However, most existing approaches focus on short-form audio generation for video segments under 10 seconds or rely on noisy datasets for long-form video-to-audio zsynthesis. To address these limitations, we introduce LD-LAudio-V1, an extension of state-of-the-art video-to-audio models and it incorporates dual lightweight adapters to enable long-form audio generation. In addition, we release a clean and human-annotated video-to-audio dataset that contains pure sound effects without noise or artifacts. Our method significantly reduces splicing artifacts and temporal inconsistencies while maintaining computational efficiency. Compared to direct fine-tuning with short training videos, LD-LAudio-V1 achieves significant improvements across multiple metrics: $FD_{\text{passt}}$ 450.00 $\rightarrow$ 327.29 (+27.27%), $FD_{\text{panns}}$ 34.88 $\rightarrow$ 22.68 (+34.98%), $FD_{\text{vgg}}$ 3.75 $\rightarrow$ 1.28 (+65.87%), $KL_{\text{panns}}$ 2.49 $\rightarrow$ 2.07 (+16.87%), $KL_{\text{passt}}$ 1.78 $\rightarrow$ 1.53 (+14.04%), $IS_{\text{panns}}$ 4.17 $\rightarrow$ 4.30 (+3.12%), $IB_{\text{score}}$ 0.25 $\rightarrow$ 0.28 (+12.00%), $Energy\Delta10\text{ms}$ 0.3013 $\rightarrow$ 0.1349 (+55.23%), $Energy\Delta10\text{ms(vs.GT)}$ 0.0531 $\rightarrow$ 0.0288 (+45.76%), and $Sem.\,Rel.$ 2.73 $\rightarrow$ 3.28 (+20.15%). Our dataset aims to facilitate further research in long-form video-to-audio generation and is available at https://github.com/deepreasonings/long-form-video2audio.<br>
<span id='abs_ch'>中文: 该 究提出LD-LAudio-V1模型，通过集成双轻量适配器和发布纯净 注数据集，显著提升长视频音频生成的性能，减少拼接伪影和时间不一致性。</span><br>
<span id='abs_en'>English: The study introduces LD-LAudio-V1, a model that enhances long-form video-to-audio generation by incorporating dual lightweight adapters and a clean, annotated dataset, significantly reducing artifacts and improving performance metrics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>909, <a href='https://arxiv.org/pdf/2508.11016.pdf' target='_blank'>https://arxiv.org/pdf/2508.11016.pdf</a></span>   <span><a href='https://github.com/bytedance/CURE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingbin Li, Rongkun Xue, Jie Wang, Ming Zhou, Zhi Li, Xiaofeng Ji, Yongqi Wang, Miao Liu, Zheming Yang, Minghui Qiu, Jing Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11016">CURE: Critical-Token-Guided Re-Concatenation for Entropy-Collapse Prevention</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in Reinforcement Learning with Verified Reward (RLVR) have driven the emergence of more sophisticated cognitive behaviors in large language models (LLMs), thereby enhancing their reasoning capabilities. However, in prior RLVR pipelines, the repeated use of static initial-state sampling drawn exactly from the dataset distribution during each sampling phase produced overly deterministic, low diversity model behavior, which manifested as rapid entropy collapse and hindered sustained performance gains during prolonged training. To address this issue, we introduce CURE (Critical-token-gUided Re concatenation for Entropy-collapse prevention), a two-stage framework that balances exploration and exploitation. Specifically, in the first stage, to deliberately steer the model toward novel yet coherent contexts, we re-generate at high-entropy critical tokens and jointly optimize the original and the branched trajectories. The further comparison with vanilla DAPO shows that the regeneration process achieves a better performance on math reasoning tasks while sustaining a high-level entropy degree for exploration. In the second stage, we continue training with static initial-state sampling by DAPO, intentionally placing the model in a familiar state to gradually strengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that, compared to other RLVR methods, CURE achieves a 5% performance gain across six math benchmarks, establishing state-of-the-art performance in both entropy and accuracy. A series of experiments further validate the effectiveness of our approach. Code is available at https://github.com/bytedance/CURE.<br>
<span id='abs_ch'>中文: CURE框架通过两阶段方法解决RLVR中的熵崩溃问题，首先生成高熵关键令牌以增强探索，随后利用静态采  强利用，在数学基准测试中实现了5%的性能提升。</span><br>
<span id='abs_en'>English: The CURE framework addresses the entropy collapse in RLVR pipelines by introducing a two-stage approach that first regenerates high-entropy critical tokens to enhance exploration and then uses static sampling to strengthen exploitation, achieving a 5% performance gain on math benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>910, <a href='https://arxiv.org/pdf/2508.10971.pdf' target='_blank'>https://arxiv.org/pdf/2508.10971.pdf</a></span>   <span><a href='https://github.com/idirlab/KGRule2NL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nasim Shirvani-Mahdavi, Chengkai Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10971">Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Knowledge graphs (KGs) can be enhanced through rule mining; however, the resulting logical rules are often difficult for humans to interpret due to their inherent complexity and the idiosyncratic labeling conventions of individual KGs. This work presents Rule2Text, a comprehensive framework that leverages large language models (LLMs) to generate natural language explanations for mined logical rules, thereby improving KG accessibility and usability. We conduct extensive experiments using multiple datasets, including Freebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the ogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically evaluate several LLMs across a comprehensive range of prompting strategies, including zero-shot, few-shot, variable type incorporation, and Chain-of-Thought reasoning. To systematically assess models' performance, we conduct a human evaluation of generated explanations on correctness and clarity. To address evaluation scalability, we develop and validate an LLM-as-a-judge framework that demonstrates strong agreement with human evaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge, and human-in-the-loop feedback, we construct high-quality ground truth datasets, which we use to fine-tune the open-source Zephyr model. Our results demonstrate significant improvements in explanation quality after fine-tuning, with particularly strong gains in the domain-specific dataset. Additionally, we integrate a type inference module to support KGs lacking explicit type information. All code and data are publicly available at https://github.com/idirlab/KGRule2NL.<br>
<span id='abs_ch'>本 究提出了Rule2Text框架，利用大语言模型为知识图谱中的复杂逻辑规则自动生成自然语言解释，通过系统化评估和微调方法显著提升了规则的可解释性。</span><br>
<span id='abs_en'>This study introduces Rule2Text, a framework that uses large language models to automatically generate natural language explanations for complex logical rules in knowledge graphs, improving interpretability through systematic evaluation and fine-tuning methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>911, <a href='https://arxiv.org/pdf/2508.10916.pdf' target='_blank'>https://arxiv.org/pdf/2508.10916.pdf</a></span>   <span><a href='https://github.com/tapri-lab/gig-interveners' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ojas Shirekar, Wim Pouw, Chenxu Hao, Vrushank Phadnis, Thabo Beeler, Chirag Raman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10916">Multimodal Quantitative Measures for Multiparty Behaviour Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Digital humans are emerging as autonomous agents in multiparty interactions, yet existing evaluation metrics largely ignore contextual coordination dynamics. We introduce a unified, intervention-driven framework for objective assessment of multiparty social behaviour in skeletal motion data, spanning three complementary dimensions: (1) synchrony via Cross-Recurrence Quantification Analysis, (2) temporal alignment via Multiscale Empirical Mode Decompositionbased Beat Consistency, and (3) structural similarity via Soft Dynamic Time Warping. We validate metric sensitivity through three theory-driven perturbations -- gesture kinematic dampening, uniform speech-gesture delays, and prosodic pitch-variance reduction-applied to $\approx 145$ 30-second thin slices of group interactions from the DnD dataset. Mixed-effects analyses reveal predictable, joint-independent shifts: dampening increases CRQA determinism and reduces beat consistency, delays weaken cross-participant coupling, and pitch flattening elevates F0 Soft-DTW costs. A complementary perception study ($N=27$) compares judgments of full-video and skeleton-only renderings to quantify representation effects. Our three measures deliver orthogonal insights into spatial structure, timing alignment, and behavioural variability. Thereby forming a robust toolkit for evaluating and refining socially intelligent agents. Code available on \href{https://github.com/tapri-lab/gig-interveners}{GitHub}.<br>
<span id='abs_ch'>中文: 本文提出了一种基于干预的统一框架，通过同步性、时间对齐和结构相似性三个互补维度，客观评估骨骼运动数据中的多方社交行为，并利用理论驱动的干扰和感知 究验证了其有效性，为评估社交智能体提供了可 工具集。</span><br>
<span id='abs_en'>English: This paper introduces an intervention-driven framework to objectively assess multiparty social behavior in skeletal motion data through three complementary metrics—synchrony, temporal alignment, and structural similarity—validated via theory-driven perturbations and perceptual studies, forming a robust toolkit for evaluating socially intelligent agents.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>912, <a href='https://arxiv.org/pdf/2508.10875.pdf' target='_blank'>https://arxiv.org/pdf/2508.10875.pdf</a></span>   <span><a href='https://github.com/VILA-Lab/Awesome-DLMs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyi Li, Mingda Chen, Bowei Guo, Zhiqiang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10875">A Survey on Diffusion Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.<br>
<span id='abs_ch'>中文: 扩散语言模型通过迭代去噪实现并行令牌生成，在保持与自回归模型相当性能的同时显著提升推理速度，为自然语言处理任务提供了高效可控的新范式。</span><br>
<span id='abs_en'>English: Diffusion Language Models (DLMs) offer a competitive alternative to autoregressive models by enabling parallel token generation through iterative denoising, achieving comparable performance with faster inference and enhanced control over language generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>913, <a href='https://arxiv.org/pdf/2508.10869.pdf' target='_blank'>https://arxiv.org/pdf/2508.10869.pdf</a></span>   <span><a href='https://github.com/simula/MediaEval-Medico-2025' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sushant Gautam, Vajira Thambawita, Michael Riegler, PÃ¥l Halvorsen, Steven Hicks
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10869">Medico 2025: Visual Question Answering for Gastrointestinal Imaging</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The Medico 2025 challenge addresses Visual Question Answering (VQA) for Gastrointestinal (GI) imaging, organized as part of the MediaEval task series. The challenge focuses on developing Explainable Artificial Intelligence (XAI) models that answer clinically relevant questions based on GI endoscopy images while providing interpretable justifications aligned with medical reasoning. It introduces two subtasks: (1) answering diverse types of visual questions using the Kvasir-VQA-x1 dataset, and (2) generating multimodal explanations to support clinical decision-making. The Kvasir-VQA-x1 dataset, created from 6,500 images and 159,549 complex question-answer (QA) pairs, serves as the benchmark for the challenge. By combining quantitative performance metrics and expert-reviewed explainability assessments, this task aims to advance trustworthy Artificial Intelligence (AI) in medical image analysis. Instructions, data access, and an updated guide for participation are available in the official competition repository: https://github.com/simula/MediaEval-Medico-2025<br>
<span id='abs_ch'>中文摘要：Medico 2025挑战赛通过基于Kvasir-VQA-x1数据集的视觉问答任务推进胃 影像可解释人工智能发展，结合量化指 与专家评估以构建可信赖的医疗AI系统。English Summary: The Medico 2025 challenge advances explainable AI for gastrointestinal imaging through Visual Question Answering tasks using the Kvasir-VQA-x1 dataset, combining performance metrics and expert evaluations to build trustworthy medical AI systems.Paperid:605,https://arxiv.org/pdf/2508.10868.pdfGitHub</span><br>
<span id='abs_en'>English Summary: The Medico 2025 challenge advances explainable AI for gastrointestinal imaging through Visual Question Answering tasks using the Kvasir-VQA-x1 dataset, combining performance metrics and expert evaluations to build trustworthy medical AI systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>914, <a href='https://arxiv.org/pdf/2508.10785.pdf' target='_blank'>https://arxiv.org/pdf/2508.10785.pdf</a></span>   <span><a href='https://github.com/Tlhey/decaf_code' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shouju Wang, Yuchen Song, Sheng'en Li, Dongmian Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10785">Enhancing Fairness in Autoencoders for Node-Level Graph Anomaly Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Graph anomaly detection (GAD) has become an increasingly important task across various domains. With the rapid development of graph neural networks (GNNs), GAD methods have achieved significant performance improvements. However, fairness considerations in GAD remain largely underexplored. Indeed, GNN-based GAD models can inherit and amplify biases present in training data, potentially leading to unfair outcomes. While existing efforts have focused on developing fair GNNs, most approaches target node classification tasks, where models often rely on simple layer architectures rather than autoencoder-based structures, which are the most widely used architecturs for anomaly detection. To address fairness in autoencoder-based GAD models, we propose \textbf{D}is\textbf{E}ntangled \textbf{C}ounterfactual \textbf{A}dversarial \textbf{F}air (DECAF)-GAD, a framework that alleviates bias while preserving GAD performance. Specifically, we introduce a structural causal model (SCM) to disentangle sensitive attributes from learned representations. Based on this causal framework, we formulate a specialized autoencoder architecture along with a fairness-guided loss function. Through extensive experiments on both synthetic and real-world datasets, we demonstrate that DECAF-GAD not only achieves competitive anomaly detection performance but also significantly enhances fairness metrics compared to baseline GAD methods. Our code is available at https://github.com/Tlhey/decaf_code.<br>
<span id='abs_ch'>中文: 本文提出DECAF-GAD框架，通过结构 果模型和专门设计的损失函数在基于自编 器的图异常检测中实现敏感属性解耦，在保持优异检测性能的同时显著提升了公平性指 。</span><br>
<span id='abs_en'>English: The paper introduces DECAF-GAD, a framework that addresses fairness in autoencoder-based graph anomaly detection by disentangling sensitive attributes through a structural causal model and specialized loss function, achieving both competitive detection performance and improved fairness metrics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>915, <a href='https://arxiv.org/pdf/2508.10779.pdf' target='_blank'>https://arxiv.org/pdf/2508.10779.pdf</a></span>   <span><a href='https://github.com/nkicsl/TriFlowSR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenning Shi, Zizheng Yan, Yuhang Yu, Clara Xue, Jingyu Zhuang, Qi Zhang, Jinwei Chen, Tao Li, Qingnan Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10779">Ultra-High-Definition Reference-Based Landmark Image Super-Resolution with Generative Diffusion Prior</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reference-based Image Super-Resolution (RefSR) aims to restore a low-resolution (LR) image by utilizing the semantic and texture information from an additional reference high-resolution (reference HR) image. Existing diffusion-based RefSR methods are typically built upon ControlNet, which struggles to effectively align the information between the LR image and the reference HR image. Moreover, current RefSR datasets suffer from limited resolution and poor image quality, resulting in the reference images lacking sufficient fine-grained details to support high-quality restoration. To overcome the limitations above, we propose TriFlowSR, a novel framework that explicitly achieves pattern matching between the LR image and the reference HR image. Meanwhile, we introduce Landmark-4K, the first RefSR dataset for Ultra-High-Definition (UHD) landmark scenarios. Considering the UHD scenarios with real-world degradation, in TriFlowSR, we design a Reference Matching Strategy to effectively match the LR image with the reference HR image. Experimental results show that our approach can better utilize the semantic and texture information of the reference HR image compared to previous methods. To the best of our knowledge, we propose the first diffusion-based RefSR pipeline for ultra-high definition landmark scenarios under real-world degradation. Our code and model will be available at https://github.com/nkicsl/TriFlowSR.<br>
<span id='abs_ch'>中文：提出的TriFlowSR框架通过显式模式匹配有效对齐低分辨率与参考高分辨率图像，结合专为超高清场景设计的Landmark-4K数据集，在利用参考图像信息方面优于现有方法。</span><br>
<span id='abs_en'>English: The proposed TriFlowSR framework effectively aligns low-resolution and reference high-resolution images through explicit pattern matching, supported by the new Landmark-4K dataset for ultra-high-definition restoration, outperforming previous methods in utilizing reference information.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>916, <a href='https://arxiv.org/pdf/2508.10729.pdf' target='_blank'>https://arxiv.org/pdf/2508.10729.pdf</a></span>   <span><a href='https://github.com/MyUniverse0726/EgoCross' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanjun Li, Yuqian Fu, Tianwen Qian, Qi'ao Xu, Silong Dai, Danda Pani Paudel, Luc Van Gool, Xiaoling Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10729">EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain Egocentric Video Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in Multimodal Large Language Models (MLLMs) have significantly pushed the frontier of egocentric video question answering (EgocentricQA). However, existing benchmarks and studies are mainly limited to common daily activities such as cooking and cleaning. In contrast, real-world deployment inevitably encounters domain shifts, where target domains differ substantially in both visual style and semantic content. To bridge this gap, we introduce \textbf{EgoCross}, a comprehensive benchmark designed to evaluate the cross-domain generalization of MLLMs in EgocentricQA. EgoCross covers four diverse and challenging domains, including surgery, industry, extreme sports, and animal perspective, representing realistic and high-impact application scenarios. It comprises approximately 1,000 QA pairs across 798 video clips, spanning four key QA tasks: prediction, recognition, localization, and counting. Each QA pair provides both OpenQA and CloseQA formats to support fine-grained evaluation. Extensive experiments show that most existing MLLMs, whether general-purpose or egocentric-specialized, struggle to generalize to domains beyond daily life, highlighting the limitations of current models. Furthermore, we conduct several pilot studies, \eg, fine-tuning and reinforcement learning, to explore potential improvements. We hope EgoCross and our accompanying analysis will serve as a foundation for advancing domain-adaptive, robust egocentric video understanding. Data and codes will be released at: \href{https://github.com/MyUniverse0726/EgoCross}{https://github.com/MyUniverse0726/EgoCross.}<br>
<span id='abs_ch'>Chinese: EgoCross基准测试旨在评估多模态大语言模型在第一人称视频问答中的跨领域泛化能力，揭示了其在日常活动之外领域的局限性并探索了改进方法。</span><br>
<span id='abs_en'>English: The EgoCross benchmark is introduced to assess multimodal large language models' cross-domain generalization in egocentric video question answering, revealing their limitations beyond daily activities and exploring improvement strategies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>917, <a href='https://arxiv.org/pdf/2508.10672.pdf' target='_blank'>https://arxiv.org/pdf/2508.10672.pdf</a></span>   <span><a href='https://github.com/Ferry-Li/datacv_fr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Feiran Li, Qianqian Xu, Shilong Bao, Boyu Han, Zhiyong Yang, Qingming Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10672">Hybrid Generative Fusion for Efficient and Privacy-Preserving Face Recognition Dataset Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this paper, we present our approach to the DataCV ICCV Challenge, which centers on building a high-quality face dataset to train a face recognition model. The constructed dataset must not contain identities overlapping with any existing public face datasets. To handle this challenge, we begin with a thorough cleaning of the baseline HSFace dataset, identifying and removing mislabeled or inconsistent identities through a Mixture-of-Experts (MoE) strategy combining face embedding clustering and GPT-4o-assisted verification. We retain the largest consistent identity cluster and apply data augmentation up to a fixed number of images per identity. To further diversify the dataset, we generate synthetic identities using Stable Diffusion with prompt engineering. As diffusion models are computationally intensive, we generate only one reference image per identity and efficiently expand it using Vec2Face, which rapidly produces 49 identity-consistent variants. This hybrid approach fuses GAN-based and diffusion-based samples, enabling efficient construction of a diverse and high-quality dataset. To address the high visual similarity among synthetic identities, we adopt a curriculum learning strategy by placing them early in the training schedule, allowing the model to progress from easier to harder samples. Our final dataset contains 50 images per identity, and all newly generated identities are checked with mainstream face datasets to ensure no identity leakage. Our method achieves \textbf{1st place} in the competition, and experimental results show that our dataset improves model performance across 10K, 20K, and 100K identity scales. Code is available at https://github.com/Ferry-Li/datacv_fr.<br>
<span id='abs_ch'>中文: 本文介绍了DataCV ICCV挑战赛的夺 方案，通过专家混合策略清洗HSFace数据集、增强真实身份图像，并结合Stable Diffusion与Vec2Face生成合成身份，采用课程学 优化训练过程，成功构建了 身份重 的高质量人脸数据集。</span><br>
<span id='abs_en'>English: This paper details a winning approach for the DataCV ICCV Challenge that constructs a non-overlapping, high-quality face dataset by cleaning HSFace with a Mixture-of-Experts strategy, augmenting real identities, and generating synthetic ones via Stable Diffusion and Vec2Face, while employing curriculum learning to enhance model training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>918, <a href='https://arxiv.org/pdf/2508.10669.pdf' target='_blank'>https://arxiv.org/pdf/2508.10669.pdf</a></span>   <span><a href='https://github.com/Alex-bupt/STEP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenye Yang, Jinpeng Chen, Huan Li, Xiongnan Jin, Xuanyang Li, Junwei Zhang, Hongbo Gao, Kaimin Wei, Senzhang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10669">STEP: Stepwise Curriculum Learning for Context-Knowledge Fusion in Conversational Recommendation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Conversational recommender systems (CRSs) aim to proactively capture user preferences through natural language dialogue and recommend high-quality items. To achieve this, CRS gathers user preferences via a dialog module and builds user profiles through a recommendation module to generate appropriate recommendations. However, existing CRS faces challenges in capturing the deep semantics of user preferences and dialogue context. In particular, the efficient integration of external knowledge graph (KG) information into dialogue generation and recommendation remains a pressing issue. Traditional approaches typically combine KG information directly with dialogue content, which often struggles with complex semantic relationships, resulting in recommendations that may not align with user expectations.
  To address these challenges, we introduce STEP, a conversational recommender centered on pre-trained language models that combines curriculum-guided context-knowledge fusion with lightweight task-specific prompt tuning. At its heart, an F-Former progressively aligns the dialogue context with knowledge-graph entities through a three-stage curriculum, thus resolving fine-grained semantic mismatches. The fused representation is then injected into the frozen language model via two minimal yet adaptive prefix prompts: a conversation prefix that steers response generation toward user intent and a recommendation prefix that biases item ranking toward knowledge-consistent candidates. This dual-prompt scheme allows the model to share cross-task semantics while respecting the distinct objectives of dialogue and recommendation. Experimental results show that STEP outperforms mainstream methods in the precision of recommendation and dialogue quality in two public datasets.<br>
<span id='abs_ch'>中文: 本文提出STEP对话推荐系统，通过课程引导的上下文与知识图谱实体融合及自适应提示调优，有效解决语义不匹配问题并整合外部知识，从而提升了推荐准确性和对话质量。</span><br>
<span id='abs_en'>English: The paper introduces STEP, a conversational recommender system that uses curriculum-guided fusion of dialogue context and knowledge graph entities, along with adaptive prompt tuning, to enhance recommendation accuracy and dialogue quality by addressing semantic mismatches and integrating external knowledge effectively.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>919, <a href='https://arxiv.org/pdf/2508.10655.pdf' target='_blank'>https://arxiv.org/pdf/2508.10655.pdf</a></span>   <span><a href='https://github.com/Zhangyong-Tang/UniBench300' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangyong Tang, Tianyang Xu, Xuefeng Zhu, Chunyang Cheng, Tao Zhou, Xiaojun Wu, Josef Kittler
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10655">Serial Over Parallel: Learning Continual Unification for Multi-Modal Visual Object Tracking and Benchmarking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Unifying multiple multi-modal visual object tracking (MMVOT) tasks draws increasing attention due to the complementary nature of different modalities in building robust tracking systems. Existing practices mix all data sensor types in a single training procedure, structuring a parallel paradigm from the data-centric perspective and aiming for a global optimum on the joint distribution of the involved tasks. However, the absence of a unified benchmark where all types of data coexist forces evaluations on separated benchmarks, causing \textit{inconsistency} between training and testing, thus leading to performance \textit{degradation}. To address these issues, this work advances in two aspects: \ding{182} A unified benchmark, coined as UniBench300, is introduced to bridge the inconsistency by incorporating multiple task data, reducing inference passes from three to one and cutting time consumption by 27\%. \ding{183} The unification process is reformulated in a serial format, progressively integrating new tasks. In this way, the performance degradation can be specified as knowledge forgetting of previous tasks, which naturally aligns with the philosophy of continual learning (CL), motivating further exploration of injecting CL into the unification process. Extensive experiments conducted on two baselines and four benchmarks demonstrate the significance of UniBench300 and the superiority of CL in supporting a stable unification process. Moreover, while conducting dedicated analyses, the performance degradation is found to be negatively correlated with network capacity. Additionally, modality discrepancies contribute to varying degradation levels across tasks (RGBT > RGBD > RGBE in MMVOT), offering valuable insights for future multi-modal vision research. Source codes and the proposed benchmark is available at \textit{https://github.com/Zhangyong-Tang/UniBench300}.<br>
<span id='abs_ch'>中文摘要：本文提出UniBench300统一基准，通过序列化重构多模态视觉目 跟踪的统一流程并引入持续学 机制，有效解决了性能下降问题，同时揭示了网络容量与模态差异对性能的影响规律。</span><br>
<span id='abs_en'>English Summary: This paper introduces UniBench300, a unified benchmark for multi-modal visual object tracking that addresses performance degradation by reformulating the unification process in a serial format and incorporating continual learning principles, while also revealing correlations between network capacity and modality discrepancies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>920, <a href='https://arxiv.org/pdf/2508.10491.pdf' target='_blank'>https://arxiv.org/pdf/2508.10491.pdf</a></span>   <span><a href='https://github.com/YuChou20/Automated-Codebook-Learning-with-Error-Correcting-Output-Code-Technique' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Che-Yu Chou, Hung-Hsuan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10491">Contrastive ECOC: Learning Output Codes for Adversarial Defense</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although one-hot encoding is commonly used for multiclass classification, it is not always the most effective encoding mechanism. Error Correcting Output Codes (ECOC) address multiclass classification by mapping each class to a unique codeword used as a label. Traditional ECOC methods rely on manually designed or randomly generated codebooks, which are labor-intensive and may yield suboptimal, dataset-agnostic results. This paper introduces three models for automated codebook learning based on contrastive learning, allowing codebooks to be learned directly and adaptively from data. Across four datasets, our proposed models demonstrate superior robustness to adversarial attacks compared to two baselines. The source is available at https://github.com/YuChou20/Automated-Codebook-Learning-with-Error-Correcting-Output-Code-Technique.<br>
<span id='abs_ch'>中文: 本文提出了三种基于对比学 的自动 本学 模型，能够直接从数据中自适应地生成 错输出 ，在四个数据集上相比 统方法展现出更强的对抗攻击鲁棒性。English: This paper introduces three automated codebook learning models using contrastive learning to adaptively generate error-correcting output codes from data, demonstrating enhanced robustness against adversarial attacks across four datasets compared to traditional methods.Paperid:631,https://arxiv.org/pdf/2508.10461.pdfGitHub</span><br>
<span id='abs_en'>English: This paper introduces three automated codebook learning models using contrastive learning to adaptively generate error-correcting output codes from data, demonstrating enhanced robustness against adversarial attacks across four datasets compared to traditional methods.Paperid:631,https://arxiv.org/pdf/2508.10461.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>921, <a href='https://arxiv.org/pdf/2508.10461.pdf' target='_blank'>https://arxiv.org/pdf/2508.10461.pdf</a></span>   <span><a href='https://github.com/basiralab/X-Node' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Prajit Sengupta, Islem Rekik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10461">X-Node: Self-Explanation is All We Need</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Graph neural networks (GNNs) have achieved state-of-the-art results in computer vision and medical image classification tasks by capturing structural dependencies across data instances. However, their decision-making remains largely opaque, limiting their trustworthiness in high-stakes clinical applications where interpretability is essential. Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct a structured context vector encoding interpretable cues such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. A lightweight Reasoner module maps this context into a compact explanation vector, which serves three purposes: (1) reconstructing the node's latent embedding via a decoder to enforce faithfulness, (2) generating a natural language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3) guiding the GNN itself via a "text-injection" mechanism that feeds explanations back into the message-passing pipeline. We evaluate X-Node on two graph datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT, and GIN backbones. Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations. Repository: https://github.com/basiralab/X-Node.<br>
<span id='abs_ch'>中文: 图神经网络在医学图像分类等任务中表现出色但缺乏透明度， 此X-Node作为一种自解释框架被提出，它利用可解释线索为每个节点生成解释，并在保持准确性的同时增强了模型的可理解性。</span><br>
<span id='abs_en'>English: Graph neural networks (GNNs) excel in tasks like medical image classification but lack transparency, so X-Node is introduced as a self-explaining framework that generates per-node explanations using interpretable cues and maintains accuracy while enhancing interpretability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>922, <a href='https://arxiv.org/pdf/2508.10419.pdf' target='_blank'>https://arxiv.org/pdf/2508.10419.pdf</a></span>   <span><a href='https://github.com/EternityJune25/ComoRAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Juyuan Wang, Rongchen Zhao, Wei Wei, Yufeng Wang, Mo Yu, Jie Zhou, Jin Xu, Liyan Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10419">ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Narrative comprehension on long stories and novels has been a challenging domain attributed to their intricate plotlines and entangled, often evolving relations among characters and entities. Given the LLM's diminished reasoning over extended context and high computational cost, retrieval-based approaches remain a pivotal role in practice. However, traditional RAG methods can fall short due to their stateless, single-step retrieval process, which often overlooks the dynamic nature of capturing interconnected relations within long-range context. In this work, we propose ComoRAG, holding the principle that narrative reasoning is not a one-shot process, but a dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation, analogous to human cognition when reasoning with memory-related signals in the brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes iterative reasoning cycles while interacting with a dynamic memory workspace. In each cycle, it generates probing queries to devise new exploratory paths, then integrates the retrieved evidence of new aspects into a global memory pool, thereby supporting the emergence of a coherent context for the query resolution. Across four challenging long-context narrative benchmarks (200K+ tokens), ComoRAG outperforms strong RAG baselines with consistent relative gains up to 11% compared to the strongest baseline. Further analysis reveals that ComoRAG is particularly advantageous for complex queries requiring global comprehension, offering a principled, cognitively motivated paradigm for retrieval-based long context comprehension towards stateful reasoning. Our code is publicly released at https://github.com/EternityJune25/ComoRAG<br>
<span id='abs_ch'>Chinese: ComoRAG 提出了一种动态迭代检索方法，模拟人类认知过程，通过整合新证据与巩固记忆来提升长篇叙事理解能力，相比 统 RAG 基线实现了最高 11% 的性能提升。</span><br>
<span id='abs_en'>English: ComoRAG introduces a dynamic, iterative retrieval method that mimics human cognitive processes to enhance narrative comprehension in long contexts, achieving up to 11% improvement over traditional RAG baselines by integrating new evidence with consolidated memory.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>923, <a href='https://arxiv.org/pdf/2508.10391.pdf' target='_blank'>https://arxiv.org/pdf/2508.10391.pdf</a></span>   <span><a href='https://github.com/RaZzzyz/LeanRAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaoze Zhang, Rong Wu, Pinlong Cai, Xiaoman Wang, Guohang Yan, Song Mao, Ding Wang, Botian Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10391">LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large Language Models by leveraging external knowledge, whereas the effectiveness is often compromised by the retrieval of contextually flawed or incomplete information. To address this, knowledge graph-based RAG methods have evolved towards hierarchical structures, organizing knowledge into multi-level summaries. However, these approaches still suffer from two critical, unaddressed challenges: high-level conceptual summaries exist as disconnected ``semantic islands'', lacking the explicit relations needed for cross-community reasoning; and the retrieval process itself remains structurally unaware, often degenerating into an inefficient flat search that fails to exploit the graph's rich topology. To overcome these limitations, we introduce LeanRAG, a framework that features a deeply collaborative design combining knowledge aggregation and retrieval strategies. LeanRAG first employs a novel semantic aggregation algorithm that forms entity clusters and constructs new explicit relations among aggregation-level summaries, creating a fully navigable semantic network. Then, a bottom-up, structure-guided retrieval strategy anchors queries to the most relevant fine-grained entities and then systematically traverses the graph's semantic pathways to gather concise yet contextually comprehensive evidence sets. The LeanRAG can mitigate the substantial overhead associated with path retrieval on graphs and minimizes redundant information retrieval. Extensive experiments on four challenging QA benchmarks with different domains demonstrate that LeanRAG significantly outperforming existing methods in response quality while reducing 46\% retrieval redundancy. Code is available at: https://github.com/RaZzzyz/LeanRAG<br>
<span id='abs_ch'>中文: LeanRAG提出了一种协作框架，通过构建可导航的语义网络并采用结构引导的检索策略，显著提升了检索增强生成的响应质量，同时将冗余减少了46%。</span><br>
<span id='abs_en'>English: LeanRAG introduces a collaborative framework that enhances retrieval-augmented generation by creating navigable semantic networks and employing structure-guided retrieval, significantly improving response quality while reducing redundancy by 46%.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>924, <a href='https://arxiv.org/pdf/2508.10230.pdf' target='_blank'>https://arxiv.org/pdf/2508.10230.pdf</a></span>   <span><a href='https://github.com/NeuroscienceAI/Audio\_Embeddings' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenggang Chen, Zhiyu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10230">No Free Lunch from Audio Pretraining in Bioacoustics: A Benchmark Study of Embeddings</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Bioacoustics, the study of animal sounds, offers a non-invasive method to monitor ecosystems. Extracting embeddings from audio-pretrained deep learning (DL) models without fine-tuning has become popular for obtaining bioacoustic features for tasks. However, a recent benchmark study reveals that while fine-tuned audio-pretrained VGG and transformer models achieve state-of-the-art performance in some tasks, they fail in others. This study benchmarks 11 DL models on the same tasks by reducing their learned embeddings' dimensionality and evaluating them through clustering. We found that audio-pretrained DL models 1) without fine-tuning even underperform fine-tuned AlexNet, 2) both with and without fine-tuning fail to separate the background from labeled sounds, but ResNet does, and 3) outperform other models when fewer background sounds are included during fine-tuning. This study underscores the necessity of fine-tuning audio-pretrained models and checking the embeddings after fine-tuning. Our codes are available: https://github.com/NeuroscienceAI/Audio\_Embeddings<br>
<span id='abs_ch'>中文:  究表明，未微调的音频预训练深度学 模型在生物声学分析中表现不佳，而微调后的模型能显著提升性能，其中ResNet在分离背景与 记声音方面表现突出。</span><br>
<span id='abs_en'>English: Fine-tuning audio-pretrained deep learning models is essential for optimal bioacoustic analysis, as non-fine-tuned models underperform and struggle to distinguish background sounds, with ResNet showing unique effectiveness in sound separation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>925, <a href='https://arxiv.org/pdf/2508.10052.pdf' target='_blank'>https://arxiv.org/pdf/2508.10052.pdf</a></span>   <span><a href='https://github.com/pzambare3/NetMoniAI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pallavi Zambare, Venkata Nikhil Thanikella, Nikhil Padmanabh Kottur, Sree Akhil Akula, Ying Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10052">NetMoniAI: An Agentic AI Framework for Network Security & Monitoring</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this paper, we present NetMoniAI, an agentic AI framework for automatic network monitoring and security that integrates decentralized analysis with lightweight centralized coordination. The framework consists of two layers: autonomous micro-agents at each node perform local traffic analysis and anomaly detection. A central controller then aggregates insights across nodes to detect coordinated attacks and maintain system-wide situational awareness. We evaluated NetMoniAI on a local micro-testbed and through NS-3 simulations. Results confirm that the two-tier agentic-AI design scales under resource constraints, reduces redundancy, and improves response time without compromising accuracy. To facilitate broader adoption and reproducibility, the complete framework is available as open source. This enables researchers and practitioners to replicate, validate, and extend it across diverse network environments and threat scenarios. Github link: https://github.com/pzambare3/NetMoniAI<br>
<span id='abs_ch'>中文: NetMoniAI是一个双层智能体AI框架，通过节点分散分析与中央协调相结合实现高效网络监控，在保证可扩展性和准确性的同时提升威胁检测能力，并已开源以促进广泛应用。</span><br>
<span id='abs_en'>English: NetMoniAI is a two-tier agentic AI framework for network monitoring that combines decentralized node-level analysis with centralized coordination to efficiently detect threats while maintaining scalability and accuracy, and it is available as open source for broader use.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>926, <a href='https://arxiv.org/pdf/2508.10034.pdf' target='_blank'>https://arxiv.org/pdf/2508.10034.pdf</a></span>   <span><a href='https://github.com/Basjuven/Jet_Images_Tagging_EM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Juvenal Bassa, Vidya Manian, Sudhir Malik, Arghya Chattopadhyay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10034">Jet Image Tagging Using Deep Learning: An Ensemble Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Jet classification in high-energy particle physics is important for understanding fundamental interactions and probing phenomena beyond the Standard Model. Jets originate from the fragmentation and hadronization of quarks and gluons, and pose a challenge for identification due to their complex, multidimensional structure. Traditional classification methods often fall short in capturing these intricacies, necessitating advanced machine learning approaches. In this paper, we employ two neural networks simultaneously as an ensemble to tag various jet types. We convert the jet data to two-dimensional histograms instead of representing them as points in a higher-dimensional space. Specifically, this ensemble approach, hereafter referred to as Ensemble Model, is used to tag jets into classes from the JetNet dataset, corresponding to: Top Quarks, Light Quarks (up or down), and W and Z bosons. For the jet classes mentioned above, we show that the Ensemble Model can be used for both binary and multi-categorical classification. This ensemble approach learns jet features by leveraging the strengths of each constituent network achieving superior performance compared to either individual network.<br>
<span id='abs_ch'>中文摘要：本文提出一种集成模型，通过将喷注数据转换为二维直方图并协同使用两个神经网络，实现了对顶夸克、W/Z玻色子等喷注类别的精准分类，其互补特征学 能力显著提升了分类性能。</span><br>
<span id='abs_en'>English Summary: This paper introduces an Ensemble Model using two neural networks to classify jets into categories like Top Quarks and W/Z bosons by converting data into 2D histograms, achieving superior performance through complementary feature learning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>927, <a href='https://arxiv.org/pdf/2508.09992.pdf' target='_blank'>https://arxiv.org/pdf/2508.09992.pdf</a></span>   <span><a href='https://github.com/daniegr/OpenFPL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Groos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09992">OpenFPL: An open-source forecasting method rivaling state-of-the-art Fantasy Premier League services</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fantasy Premier League engages the football community in selecting the Premier League players who will perform best from gameweek to gameweek. Access to accurate performance forecasts gives participants an edge over competitors by guiding expectations about player outcomes and reducing uncertainty in squad selection. However, high-accuracy forecasts are currently limited to commercial services whose inner workings are undisclosed and that rely on proprietary data. This paper aims to democratize access to highly accurate forecasts of player performance by presenting OpenFPL, an open-source Fantasy Premier League forecasting method developed exclusively from public data. Comprising position-specific ensemble models optimized on Fantasy Premier League and Understat data from four previous seasons (2020-21 to 2023-24), OpenFPL achieves accuracy comparable to a leading commercial service when tested prospectively on data from the 2024-25 season. OpenFPL also surpasses the commercial benchmark for high-return players ($>$ 2 points), which are most influential for rank gains. These findings hold across one-, two-, and three-gameweek forecast horizons, supporting long-term planning of transfers and strategies while also informing final-day decisions.<br>
<span id='abs_ch'>中文摘要：OpenFPL作为一种开源预测方法，通过使用公开数据实现了英超球员表现的高精度预测，其准确度媲美商业服务且在识别高回报球员方面表现更优，为长期战略和临场决策提供了可 依据。</span><br>
<span id='abs_en'>English Summary: OpenFPL is an open-source forecasting method that democratizes access to highly accurate Premier League player performance predictions using public data, achieving commercial-level accuracy and excelling at identifying high-return players across multiple gameweek horizons.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>928, <a href='https://arxiv.org/pdf/2508.09919.pdf' target='_blank'>https://arxiv.org/pdf/2508.09919.pdf</a></span>   <span><a href='https://github.com/xiaojiao929/T-CACE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaojiao Xiao, Jianfeng Zhao, Qinmin Vivian Hu, Guanghui Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09919">T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Magnetic resonance imaging (MRI) is a leading modality for the diagnosis of liver cancer, significantly improving the classification of the lesion and patient outcomes. However, traditional MRI faces challenges including risks from contrast agent (CA) administration, time-consuming manual assessment, and limited annotated datasets. To address these limitations, we propose a Time-Conditioned Autoregressive Contrast Enhancement (T-CACE) framework for synthesizing multi-phase contrast-enhanced MRI (CEMRI) directly from non-contrast MRI (NCMRI). T-CACE introduces three core innovations: a conditional token encoding (CTE) mechanism that unifies anatomical priors and temporal phase information into latent representations; and a dynamic time-aware attention mask (DTAM) that adaptively modulates inter-phase information flow using a Gaussian-decayed attention mechanism, ensuring smooth and physiologically plausible transitions across phases. Furthermore, a constraint for temporal classification consistency (TCC) aligns the lesion classification output with the evolution of the physiological signal, further enhancing diagnostic reliability. Extensive experiments on two independent liver MRI datasets demonstrate that T-CACE outperforms state-of-the-art methods in image synthesis, segmentation, and lesion classification. This framework offers a clinically relevant and efficient alternative to traditional contrast-enhanced imaging, improving safety, diagnostic efficiency, and reliability for the assessment of liver lesion. The implementation of T-CACE is publicly available at: https://github.com/xiaojiao929/T-CACE.<br>
<span id='abs_ch'>中文: T-CACE框架通过创新的时间建模和分类一致性机制，直接从非增强MRI合成多期相增强MRI，为肝脏病变诊断提供了更安全可 的解决方案。</span><br>
<span id='abs_en'>English: The T-CACE framework synthesizes multi-phase contrast-enhanced MRI from non-contrast MRI, improving diagnostic safety and accuracy for liver lesions through innovative temporal modeling and classification consistency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>929, <a href='https://arxiv.org/pdf/2508.09834.pdf' target='_blank'>https://arxiv.org/pdf/2508.09834.pdf</a></span>   <span><a href='https://github.com/weigao266/Awesome-Efficient-Arch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weigao Sun, Jiaxi Hu, Yucheng Zhou, Jusen Du, Disen Lan, Kexin Wang, Tong Zhu, Xiaoye Qu, Yu Zhang, Xiaoyu Mo, Daizong Liu, Yuxuan Liang, Wenliang Chen, Guoqi Li, Yu Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09834">Speed Always Wins: A Survey on Efficient Architectures for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have delivered impressive results in language understanding, generation, reasoning, and pushes the ability boundary of multimodal models. Transformer models, as the foundation of modern LLMs, offer a strong baseline with excellent scaling properties. However, the traditional transformer architecture requires substantial computations and poses significant obstacles for large-scale training and practical deployment. In this survey, we offer a systematic examination of innovative LLM architectures that address the inherent limitations of transformers and boost the efficiency. Starting from language modeling, this survey covers the background and technical details of linear and sparse sequence modeling methods, efficient full attention variants, sparse mixture-of-experts, hybrid model architectures incorporating the above techniques, and emerging diffusion LLMs. Additionally, we discuss applications of these techniques to other modalities and consider their wider implications for developing scalable, resource-aware foundation models. By grouping recent studies into the above category, this survey presents a blueprint of modern efficient LLM architectures, and we hope this could help motivate future research toward more efficient, versatile AI systems.<br>
<span id='abs_ch'>中文: 本综述系统梳理了克服 统Transformer计算局限的创新大语言模型架构，涵盖线性序列建模和稀疏注意力等技术，旨在提升模型效率与可扩展性。</span><br>
<span id='abs_en'>English: This survey systematically reviews innovative Large Language Model architectures that overcome the computational limitations of traditional transformers, covering techniques like linear sequence modeling and sparse attention to enhance efficiency and scalability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>930, <a href='https://arxiv.org/pdf/2508.09830.pdf' target='_blank'>https://arxiv.org/pdf/2508.09830.pdf</a></span>   <span><a href='https://github.com/vLAR-group/RayletDF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shenxing Wei, Jinxi Li, Yafei Yang, Siyuan Zhou, Bo Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09830">RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this paper, we present a generalizable method for 3D surface reconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from RGB images. Unlike existing coordinate-based methods which are often computationally intensive when rendering explicit surfaces, our proposed method, named RayletDF, introduces a new technique called raylet distance field, which aims to directly predict surface points from query rays. Our pipeline consists of three key modules: a raylet feature extractor, a raylet distance field predictor, and a multi-raylet blender. These components work together to extract fine-grained local geometric features, predict raylet distances, and aggregate multiple predictions to reconstruct precise surface points. We extensively evaluate our method on multiple public real-world datasets, demonstrating superior performance in surface reconstruction from point clouds or 3D Gaussians. Most notably, our method achieves exceptional generalization ability, successfully recovering 3D surfaces in a single-forward pass across unseen datasets in testing.<br>
<span id='abs_ch'>Chinese: 本文提出RayletDF方法，通过光线元距离场直接从查询光线预测表面点，实现了从点云或3D高斯的高效三维表面重建，在多个数据集上展现出卓越性能和强大泛化能力。</span><br>
<span id='abs_en'>English: This paper introduces RayletDF, a novel method for efficient 3D surface reconstruction from point clouds or 3D Gaussians that uses a raylet distance field to directly predict surface points, demonstrating superior performance and exceptional generalization across diverse datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>931, <a href='https://arxiv.org/pdf/2508.09811.pdf' target='_blank'>https://arxiv.org/pdf/2508.09811.pdf</a></span>   <span><a href='https://github.com/vLAR-group/TRACE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinxi Li, Ziyang Song, Bo Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09811">TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this paper, we aim to model 3D scene geometry, appearance, and physical information just from dynamic multi-view videos in the absence of any human labels. By leveraging physics-informed losses as soft constraints or integrating simple physics models into neural nets, existing works often fail to learn complex motion physics, or doing so requires additional labels such as object types or masks. We propose a new framework named TRACE to model the motion physics of complex dynamic 3D scenes. The key novelty of our method is that, by formulating each 3D point as a rigid particle with size and orientation in space, we directly learn a translation rotation dynamics system for each particle, explicitly estimating a complete set of physical parameters to govern the particle's motion over time. Extensive experiments on three existing dynamic datasets and one newly created challenging synthetic datasets demonstrate the extraordinary performance of our method over baselines in the task of future frame extrapolation. A nice property of our framework is that multiple objects or parts can be easily segmented just by clustering the learned physical parameters.<br>
<span id='abs_ch'>中文: 本文提出TRACE框架，通过将三维点视为具有物理属性的刚性粒子来学 运动规律，在动态场景预测中表现卓越，并能通过物理参数聚类实现对象分割。</span><br>
<span id='abs_en'>English: This paper introduces TRACE, a novel framework that models 3D scene dynamics by treating each point as a rigid particle and learning its physical parameters, achieving superior performance in future frame prediction and enabling object segmentation through parameter clustering.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>932, <a href='https://arxiv.org/pdf/2508.09632.pdf' target='_blank'>https://arxiv.org/pdf/2508.09632.pdf</a></span>   <span><a href='https://github.com/Gen-Verse/Paper2Video' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingwei Liu, Ling Yang, Hao Luo, Fan Wang, Hongyan Li, Mengdi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09632">Preacher: Paper-to-Video Agentic System</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The paper-to-video task converts a research paper into a structured video abstract, distilling key concepts, methods, and conclusions into an accessible, well-organized format. While state-of-the-art video generation models demonstrate potential, they are constrained by limited context windows, rigid video duration constraints, limited stylistic diversity, and an inability to represent domain-specific knowledge. To address these limitations, we introduce Preacher, the first paper-to-video agentic system. Preacher employs a topdown approach to decompose, summarize, and reformulate the paper, followed by bottom-up video generation, synthesizing diverse video segments into a coherent abstract. To align cross-modal representations, we define key scenes and introduce a Progressive Chain of Thought (P-CoT) for granular, iterative planning. Preacher successfully generates high-quality video abstracts across five research fields, demonstrating expertise beyond current video generation models. Code will be released at: https://github.com/Gen-Verse/Paper2Video<br>
<span id='abs_ch'>中文: 本文提出首个论文转视频系统Preacher，通过自上而下的分解重构与自下而上的视频合成，结合渐进式思维链实现跨模态对齐，能够生成超越现有模型的高质量学术视频摘要。</span><br>
<span id='abs_en'>English: The paper introduces Preacher, an agentic system that overcomes limitations of current video generation models by employing top-down decomposition and bottom-up synthesis with Progressive Chain of Thought planning to create high-quality video abstracts from research papers.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>933, <a href='https://arxiv.org/pdf/2508.09621.pdf' target='_blank'>https://arxiv.org/pdf/2508.09621.pdf</a></span>   <span><a href='https://github.com/snt-arg/robot_suite' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ingrid MaÃ©va Chekam, Ines Pastor-Martinez, Ali Tourani, Jose Andres Millan-Romera, Laura Ribeiro, Pedro Miguel Bastos Soares, Holger Voos, Jose Luis Sanchez-Lopez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09621">Interpretable Robot Control via Structured Behavior Trees and Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As intelligent robots become more integrated into human environments, there is a growing need for intuitive and reliable Human-Robot Interaction (HRI) interfaces that are adaptable and more natural to interact with. Traditional robot control methods often require users to adapt to interfaces or memorize predefined commands, limiting usability in dynamic, unstructured environments. This paper presents a novel framework that bridges natural language understanding and robotic execution by combining Large Language Models (LLMs) with Behavior Trees. This integration enables robots to interpret natural language instructions given by users and translate them into executable actions by activating domain-specific plugins. The system supports scalable and modular integration, with a primary focus on perception-based functionalities, such as person tracking and hand gesture recognition. To evaluate the system, a series of real-world experiments was conducted across diverse environments. Experimental results demonstrate that the proposed approach is practical in real-world scenarios, with an average cognition-to-execution accuracy of approximately 94%, making a significant contribution to HRI systems and robots. The complete source code of the framework is publicly available at https://github.com/snt-arg/robot_suite.<br>
<span id='abs_ch'>Chinese: 本文提出了一种将大型语言模型与行为 相结合的新框架，使机器人能够通过领域特定插件解析自然语言指令并执行相应动作，在真实环境实验中达到约94%的准确率，显著推动了直观人机交互的发展。</span><br>
<span id='abs_en'>English: This paper introduces a novel framework that integrates Large Language Models with Behavior Trees to enable robots to interpret natural language instructions and execute actions via domain-specific plugins, achieving approximately 94% accuracy in real-world experiments and advancing intuitive Human-Robot Interaction.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>934, <a href='https://arxiv.org/pdf/2508.09621.pdf' target='_blank'>https://arxiv.org/pdf/2508.09621.pdf</a></span>   <span><a href='https://github.com/snt-arg/robot_suite' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ingrid Maéva Chekam, Ines Pastor-Martinez, Ali Tourani, Jose Andres Millan-Romera, Laura Ribeiro, Pedro Miguel Bastos Soares, Holger Voos, Jose Luis Sanchez-Lopez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09621">Interpretable Robot Control via Structured Behavior Trees and Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As intelligent robots become more integrated into human environments, there is a growing need for intuitive and reliable Human-Robot Interaction (HRI) interfaces that are adaptable and more natural to interact with. Traditional robot control methods often require users to adapt to interfaces or memorize predefined commands, limiting usability in dynamic, unstructured environments. This paper presents a novel framework that bridges natural language understanding and robotic execution by combining Large Language Models (LLMs) with Behavior Trees. This integration enables robots to interpret natural language instructions given by users and translate them into executable actions by activating domain-specific plugins. The system supports scalable and modular integration, with a primary focus on perception-based functionalities, such as person tracking and hand gesture recognition. To evaluate the system, a series of real-world experiments was conducted across diverse environments. Experimental results demonstrate that the proposed approach is practical in real-world scenarios, with an average cognition-to-execution accuracy of approximately 94%, making a significant contribution to HRI systems and robots. The complete source code of the framework is publicly available at https://github.com/snt-arg/robot_suite.<br>
<span id='abs_ch'>Chinese: 本文提出了一种将大型语言模型与行为 相结合的新框架，使机器人能够通过领域特定插件解析自然语言指令并执行相应动作，在真实环境实验中达到约94%的准确率，显著推动了直观人机交互的发展。</span><br>
<span id='abs_en'>English: This paper introduces a novel framework that integrates Large Language Models with Behavior Trees to enable robots to interpret natural language instructions and execute actions via domain-specific plugins, achieving approximately 94% accuracy in real-world experiments and advancing intuitive Human-Robot Interaction.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>935, <a href='https://arxiv.org/pdf/2508.09547.pdf' target='_blank'>https://arxiv.org/pdf/2508.09547.pdf</a></span>   <span><a href='https://github.com/F1y1113/GoViG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fengyi Wu, Yifei Dong, Zhi-Qi Cheng, Yilong Dai, Guangyu Chen, Hang Wang, Qi Dai, Alexander G. Hauptmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09547">GoViG: Goal-Conditioned Visual Navigation Instruction Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce Goal-Conditioned Visual Navigation Instruction Generation (GoViG), a new task that aims to autonomously generate precise and contextually coherent navigation instructions solely from egocentric visual observations of initial and goal states. Unlike conventional approaches that rely on structured inputs such as semantic annotations or environmental maps, GoViG exclusively leverages raw egocentric visual data, substantially improving its adaptability to unseen and unstructured environments. Our method addresses this task by decomposing it into two interconnected subtasks: (1) visual forecasting, which predicts intermediate visual states bridging the initial and goal views; and (2) instruction generation, which synthesizes linguistically coherent instructions grounded in both observed and anticipated visuals. These subtasks are integrated within an autoregressive multimodal large language model trained with tailored objectives to ensure spatial accuracy and linguistic clarity. Furthermore, we introduce two complementary multimodal reasoning strategies, one-pass and interleaved reasoning, to mimic incremental human cognitive processes during navigation. To evaluate our method, we propose the R2R-Goal dataset, combining diverse synthetic and real-world trajectories. Empirical results demonstrate significant improvements over state-of-the-art methods, achieving superior BLEU-4 and CIDEr scores along with robust cross-domain generalization.<br>
<span id='abs_ch'>中文摘要：GoViG是一项仅通过初始与目 位置的原始视觉观测自主生成导航指令的新任务，它通过视觉预测与指令生成的双子任务框架，在跨领域环境中实现了卓越的适应性和评估指 提升。</span><br>
<span id='abs_en'>English Summary: GoViG is a novel task that generates navigation instructions using only raw egocentric visual inputs from start to goal positions, employing visual forecasting and instruction generation within a multimodal model to achieve superior adaptability and performance metrics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>936, <a href='https://arxiv.org/pdf/2508.09459.pdf' target='_blank'>https://arxiv.org/pdf/2508.09459.pdf</a></span>   <span><a href='https://github.com/WenOOI/RelayFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Huang, Jiarui Yang, Tao Dai, Jiawei Li, Shaoxiong Zhan, Bin Wang, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09459">RelayFormer: A Unified Local-Global Attention Framework for Scalable Image and Video Manipulation Localization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Visual manipulation localization (VML) -- across both images and videos -- is a crucial task in digital forensics that involves identifying tampered regions in visual content. However, existing methods often lack cross-modal generalization and struggle to handle high-resolution or long-duration inputs efficiently.
  We propose RelayFormer, a unified and modular architecture for visual manipulation localization across images and videos. By leveraging flexible local units and a Global-Local Relay Attention (GLoRA) mechanism, it enables scalable, resolution-agnostic processing with strong generalization. Our framework integrates seamlessly with existing Transformer-based backbones, such as ViT and SegFormer, via lightweight adaptation modules that require only minimal architectural changes, ensuring compatibility without disrupting pretrained representations.
  Furthermore, we design a lightweight, query-based mask decoder that supports one-shot inference across video sequences with linear complexity. Extensive experiments across multiple benchmarks demonstrate that our approach achieves state-of-the-art localization performance, setting a new baseline for scalable and modality-agnostic VML. Code is available at: https://github.com/WenOOI/RelayFormer.<br>
<span id='abs_ch'>Chinese: RelayFormer通过将输入分割为固定大小的子图像并引入全局-局部中继注意力机制，有效解决了视觉篡改定位中的分辨率多 性和模态差异问题，在多个基准测试中以高效方式实现了最先进的性能。</span><br>
<span id='abs_en'>English: RelayFormer is a unified framework that addresses resolution diversity and modality gaps in visual manipulation localization by using fixed-size sub-images and a global-local relay attention mechanism, achieving state-of-the-art performance efficiently across various benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>937, <a href='https://arxiv.org/pdf/2508.09459.pdf' target='_blank'>https://arxiv.org/pdf/2508.09459.pdf</a></span>   <span><a href='https://github.com/WenOOI/RelayFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen Huang, Jiarui Yang, Tao Dai, Jiawei Li, Shaoxiong Zhan, Bin Wang, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09459">RelayFormer: A Unified Local-Global Attention Framework for Scalable Image and Video Manipulation Localization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Visual manipulation localization (VML) aims to identify tampered regions in images and videos, a task that has become increasingly challenging with the rise of advanced editing tools. Existing methods face two main issues: resolution diversity, where resizing or padding distorts forensic traces and reduces efficiency, and the modality gap, as images and videos often require separate models. To address these challenges, we propose RelayFormer, a unified framework that adapts to varying resolutions and modalities. RelayFormer partitions inputs into fixed-size sub-images and introduces Global-Local Relay (GLR) tokens, which propagate structured context through a global-local relay attention (GLRA) mechanism. This enables efficient exchange of global cues, such as semantic or temporal consistency, while preserving fine-grained manipulation artifacts. Unlike prior methods that rely on uniform resizing or sparse attention, RelayFormer naturally scales to arbitrary resolutions and video sequences without excessive overhead. Experiments across diverse benchmarks demonstrate that RelayFormer achieves state-of-the-art performance with notable efficiency, combining resolution adaptivity without interpolation or excessive padding, unified modeling for both images and videos, and a strong balance between accuracy and computational cost. Code is available at: https://github.com/WenOOI/RelayFormer.<br>
<span id='abs_ch'>Chinese: RelayFormer通过将输入分割为固定大小的子图像并引入全局-局部中继注意力机制，有效解决了视觉篡改定位中的分辨率多 性和模态差异问题，在多个基准测试中以高效方式实现了最先进的性能。</span><br>
<span id='abs_en'>English: RelayFormer is a unified framework that addresses resolution diversity and modality gaps in visual manipulation localization by using fixed-size sub-images and a global-local relay attention mechanism, achieving state-of-the-art performance efficiently across various benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>938, <a href='https://arxiv.org/pdf/2508.09381.pdf' target='_blank'>https://arxiv.org/pdf/2508.09381.pdf</a></span>   <span><a href='https://github.com/sfu-mial/skin-IAV' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kumar Abhishek, Jeremy Kawahara, Ghassan Hamarneh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09381">What Can We Learn from Inter-Annotator Variability in Skin Lesion Segmentation?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Medical image segmentation exhibits intra- and inter-annotator variability due to ambiguous object boundaries, annotator preferences, expertise, and tools, among other factors. Lesions with ambiguous boundaries, e.g., spiculated or infiltrative nodules, or irregular borders per the ABCD rule, are particularly prone to disagreement and are often associated with malignancy. In this work, we curate IMA++, the largest multi-annotator skin lesion segmentation dataset, on which we conduct an in-depth study of variability due to annotator, malignancy, tool, and skill factors. We find a statistically significant (p<0.001) association between inter-annotator agreement (IAA), measured using Dice, and the malignancy of skin lesions. We further show that IAA can be accurately predicted directly from dermoscopic images, achieving a mean absolute error of 0.108. Finally, we leverage this association by utilizing IAA as a "soft" clinical feature within a multi-task learning objective, yielding a 4.2% improvement in balanced accuracy averaged across multiple model architectures and across IMA++ and four public dermoscopic datasets. The code is available at https://github.com/sfu-mial/skin-IAV.<br>
<span id='abs_ch'>Chinese: 本 究推出了最大的多 注者皮肤病变分割数据集IMA++，揭示了 注者间一致性与病变恶性程度之间的显著关联，并证明将该一致性作为临床特征可有效提升多个数据集的诊断准确性。English: This study introduces IMA++, the largest multi-annotator skin lesion segmentation dataset, revealing a significant link between inter-annotator agreement and lesion malignancy and demonstrating that leveraging this agreement as a clinical feature improves diagnostic accuracy across multiple datasets.Paperid:696,https://arxiv.org/pdf/2508.09372.pdfGitHub</span><br>
<span id='abs_en'>English: This study introduces IMA++, the largest multi-annotator skin lesion segmentation dataset, revealing a significant link between inter-annotator agreement and lesion malignancy and demonstrating that leveraging this agreement as a clinical feature improves diagnostic accuracy across multiple datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>939, <a href='https://arxiv.org/pdf/2508.09372.pdf' target='_blank'>https://arxiv.org/pdf/2508.09372.pdf</a></span>   <span><a href='https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Rezwanul Haque, Md. Milon Islam, S M Taslim Uddin Raju, Fakhri Karray
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09372">A Signer-Invariant Conformer and Multi-Scale Fusion Transformer for Continuous Sign Language Recognition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Continuous Sign Language Recognition (CSLR) faces multiple challenges, including significant inter-signer variability and poor generalization to novel sentence structures. Traditional solutions frequently fail to handle these issues efficiently. For overcoming these constraints, we propose a dual-architecture framework. For the Signer-Independent (SI) challenge, we propose a Signer-Invariant Conformer that combines convolutions with multi-head self-attention to learn robust, signer-agnostic representations from pose-based skeletal keypoints. For the Unseen-Sentences (US) task, we designed a Multi-Scale Fusion Transformer with a novel dual-path temporal encoder that captures both fine-grained posture dynamics, enabling the model's ability to comprehend novel grammatical compositions. Experiments on the challenging Isharah-1000 dataset establish a new standard for both CSLR benchmarks. The proposed conformer architecture achieves a Word Error Rate (WER) of 13.07% on the SI challenge, a reduction of 13.53% from the state-of-the-art. On the US task, the transformer model scores a WER of 47.78%, surpassing previous work. In the SignEval 2025 CSLR challenge, our team placed 2nd in the US task and 4th in the SI task, demonstrating the performance of these models. The findings validate our key hypothesis: that developing task-specific networks designed for the particular challenges of CSLR leads to considerable performance improvements and establishes a new baseline for further research. The source code is available at: https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah.<br>
<span id='abs_ch'>中文: 本 究提出一种双架构框架用于连续手语识别，通过手语者 关Conformer解决手语者独立性问题，并采用多尺度融合Transformer处理未知句式任务，在Isharah-1000数据集上取得最优性能，验证了任务专用网络设计的有效性。</span><br>
<span id='abs_en'>English: This study introduces a dual-architecture framework for Continuous Sign Language Recognition, employing a Signer-Invariant Conformer for signer-independent challenges and a Multi-Scale Fusion Transformer for unseen-sentence tasks, achieving state-of-the-art performance on the Isharah-1000 dataset and validating task-specific network designs.Paperid:697,https://arxiv.org/pdf/2508.09362.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>940, <a href='https://arxiv.org/pdf/2508.09362.pdf' target='_blank'>https://arxiv.org/pdf/2508.09362.pdf</a></span>   <span><a href='https://github.com/rezwanh001/Multimodal-Isolated-Italian-Sign-Language-Recognition' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Md. Milon Islam, Md Rezwanul Haque, S M Taslim Uddin Raju, Fakhri Karray
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09362">FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate recognition of sign language in healthcare communication poses a significant challenge, requiring frameworks that can accurately interpret complex multimodal gestures. To deal with this, we propose FusionEnsemble-Net, a novel attention-based ensemble of spatiotemporal networks that dynamically fuses visual and motion data to enhance recognition accuracy. The proposed approach processes RGB video and range Doppler map radar modalities synchronously through four different spatiotemporal networks. For each network, features from both modalities are continuously fused using an attention-based fusion module before being fed into an ensemble of classifiers. Finally, the outputs of these four different fused channels are combined in an ensemble classification head, thereby enhancing the model's robustness. Experiments demonstrate that FusionEnsemble-Net outperforms state-of-the-art approaches with a test accuracy of 99.44% on the large-scale MultiMeDaLIS dataset for Italian Sign Language. Our findings indicate that an ensemble of diverse spatiotemporal networks, unified by attention-based fusion, yields a robust and accurate framework for complex, multimodal isolated gesture recognition tasks. The source code is available at: https://github.com/rezwanh001/Multimodal-Isolated-Italian-Sign-Language-Recognition.<br>
<span id='abs_ch'>Chinese: FusionEnsemble-Net提出了一种基于注意力的时空网络集成方法，动态融合视觉与运动数据，在意大利手语识别中以99.44%的准确率超越了现有最优方法。</span><br>
<span id='abs_en'>English: FusionEnsemble-Net introduces an attention-based ensemble of spatiotemporal networks that dynamically fuses visual and motion data, achieving 99.44% accuracy in Italian Sign Language recognition and outperforming existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>941, <a href='https://arxiv.org/pdf/2508.09294.pdf' target='_blank'>https://arxiv.org/pdf/2508.09294.pdf</a></span>   <span><a href='https://github.com/xuanxixi/Fake-Mamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Xuan, Zimo Zhu, Wenxin Zhang, Yi-Cheng Lin, Tomi Kinnunen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09294">Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Advances in speech synthesis intensify security threats, motivating real-time deepfake detection research. We investigate whether bidirectional Mamba can serve as a competitive alternative to Self-Attention in detecting synthetic speech. Our solution, Fake-Mamba, integrates an XLSR front-end with bidirectional Mamba to capture both local and global artifacts. Our core innovation introduces three efficient encoders: TransBiMamba, ConBiMamba, and PN-BiMamba. Leveraging XLSR's rich linguistic representations, PN-BiMamba can effectively capture the subtle cues of synthetic speech. Evaluated on ASVspoof 21 LA, 21 DF, and In-The-Wild benchmarks, Fake-Mamba achieves 0.97%, 1.74%, and 5.85% EER, respectively, representing substantial relative gains over SOTA models XLSR-Conformer and XLSR-Mamba. The framework maintains real-time inference across utterance lengths, demonstrating strong generalization and practical viability. The code is available at https://github.com/xuanxixi/Fake-Mamba.<br>
<span id='abs_ch'>中文摘要：本 究提出Fake-Mamba实时深度伪 检测系统，通过双向Mamba架构与XLSR特征结合，在多项测试基准中显著超越现有最优模型，同时保持高效计算性能。</span><br>
<span id='abs_en'>English Summary: The study introduces Fake-Mamba, a real-time deepfake detection system using bidirectional Mamba and XLSR features to outperform state-of-the-art models across multiple benchmarks while maintaining computational efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>942, <a href='https://arxiv.org/pdf/2508.09288.pdf' target='_blank'>https://arxiv.org/pdf/2508.09288.pdf</a></span>   <span><a href='https://github.com/ayushgupta4897/Contextual-Integrity-Verification' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aayush Gupta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09288">Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) remain acutely vulnerable to prompt injection and related jailbreak attacks; heuristic guardrails (rules, filters, LLM judges) are routinely bypassed. We present Contextual Integrity Verification (CIV), an inference-time security architecture that attaches cryptographically signed provenance labels to every token and enforces a source-trust lattice inside the transformer via a pre-softmax hard attention mask (with optional FFN/residual gating). CIV provides deterministic, per-token non-interference guarantees on frozen models: lower-trust tokens cannot influence higher-trust representations. On benchmarks derived from recent taxonomies of prompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack success rate under the stated threat model while preserving 93.1% token-level similarity and showing no degradation in model perplexity on benign tasks; we note a latency overhead attributable to a non-optimized data path. Because CIV is a lightweight patch -- no fine-tuning required -- we demonstrate drop-in protection for Llama-3-8B and Mistral-7B. We release a reference implementation, an automated certification harness, and the Elite-Attack corpus to support reproducible research.<br>
<span id='abs_ch'>Large language models are highly susceptible to prompt =
injection attacks, but the proposed Contextual Integrity Verification (CIV)=
 architecture provides deterministic security by cryptographically labeling=
 tokens and enforcing trust hierarchies, achieving perfect attack preventio=
n with minimal performance impact.</span><br>
<span id='abs_en'>English Summary:</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>943, <a href='https://arxiv.org/pdf/2508.09212.pdf' target='_blank'>https://arxiv.org/pdf/2508.09212.pdf</a></span>   <span><a href='https://github.com/SihanXXX/DiscreteGenoGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sihan Xie, Thierry Tribout, Didier Boichard, Blaise Hanczar, Julien Chiquet, Eric Barrey
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09212">Deep Generative Models for Discrete Genotype Simulation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Deep generative models open new avenues for simulating realistic genomic data while preserving privacy and addressing data accessibility constraints. While previous studies have primarily focused on generating gene expression or haplotype data, this study explores generating genotype data in both unconditioned and phenotype-conditioned settings, which is inherently more challenging due to the discrete nature of genotype data. In this work, we developed and evaluated commonly used generative models, including Variational Autoencoders (VAEs), Diffusion Models, and Generative Adversarial Networks (GANs), and proposed adaptation tailored to discrete genotype data. We conducted extensive experiments on large-scale datasets, including all chromosomes from cow and multiple chromosomes from human. Model performance was assessed using a well-established set of metrics drawn from both deep learning and quantitative genetics literature. Our results show that these models can effectively capture genetic patterns and preserve genotype-phenotype association. Our findings provide a comprehensive comparison of these models and offer practical guidelines for future research in genotype simulation. We have made our code publicly available at https://github.com/SihanXXX/DiscreteGenoGen.<br>
<br>
<div id='section'>Paperid: <span id='pid'>944, <a href='https://arxiv.org/pdf/2508.09205.pdf' target='_blank'>https://arxiv.org/pdf/2508.09205.pdf</a></span>   <span><a href='https://github.com/nki-ai/x2x' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yoni Schirris, Eric Marcus, Jonas Teuwen, Hugo Horlings, Efstratios Gavves
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09205">From Explainable to Explained AI: Ideas for Falsifying and Quantifying Explanations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Explaining deep learning models is essential for clinical integration of medical image analysis systems. A good explanation highlights if a model depends on spurious features that undermines generalization and harms a subset of patients or, conversely, may present novel biological insights. Although techniques like GradCAM can identify influential features, they are measurement tools that do not themselves form an explanation. We propose a human-machine-VLM interaction system tailored to explaining classifiers in computational pathology, including multi-instance learning for whole-slide images. Our proof of concept comprises (1) an AI-integrated slide viewer to run sliding-window experiments to test claims of an explanation, and (2) quantification of an explanation's predictiveness using general-purpose vision-language models. The results demonstrate that this allows us to qualitatively test claims of explanations and can quantifiably distinguish competing explanations. This offers a practical path from explainable AI to explained AI in digital pathology and beyond. Code and prompts are available at https://github.com/nki-ai/x2x.<br>
<span id='abs_ch'>中文摘要：本 究提出了一种人机-VLM交互系统，用于解释计算病理学中的深度学 分类器，通过定性测试和定量比较解释，推动从可解释AI向已解释AI的演进。</span><br>
<span id='abs_en'>English Summary: This study introduces a human-machine-VLM interaction system for explaining deep learning classifiers in computational pathology, enabling qualitative testing and quantitative comparison of explanations to advance from explainable to explained AI.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>945, <a href='https://arxiv.org/pdf/2508.09196.pdf' target='_blank'>https://arxiv.org/pdf/2508.09196.pdf</a></span>   <span><a href='https://github.com/asimukaye/fiva' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Asim Ukaye, Numan Saeed, Karthik Nandakumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09196">FIVA: Federated Inverse Variance Averaging for Universal CT Segmentation with Uncertainty Estimation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Different CT segmentation datasets are typically obtained from different scanners under different capture settings and often provide segmentation labels for a limited and often disjoint set of organs. Using these heterogeneous data effectively while preserving patient privacy can be challenging. This work presents a novel federated learning approach to achieve universal segmentation across diverse abdominal CT datasets by utilizing model uncertainty for aggregation and predictive uncertainty for inference. Our approach leverages the inherent noise in stochastic mini-batch gradient descent to estimate a distribution over the model weights to provide an on-the-go uncertainty over the model parameters at the client level. The parameters are then aggregated at the server using the additional uncertainty information using a Bayesian-inspired inverse-variance aggregation scheme. Furthermore, the proposed method quantifies prediction uncertainty by propagating the uncertainty from the model weights, providing confidence measures essential for clinical decision-making. In line with recent work shown, predictive uncertainty is utilized in the inference stage to improve predictive performance. Experimental evaluations demonstrate the effectiveness of this approach in improving both the quality of federated aggregation and uncertainty-weighted inference compared to previously established baselines. The code for this work is made available at: https://github.com/asimukaye/fiva<br>
<span id='abs_ch'>中文: 本 究提出一种新颖的联邦学 方法，利用模型不确定性和预测不确定性来提升跨异构腹部CT数据集的通用分割效果，在保护患者隐私的同时显著改善了聚合质量和推理性能。</span><br>
<span id='abs_en'>English: This study introduces a novel federated learning method that employs model and predictive uncertainty to enhance universal segmentation across heterogeneous abdominal CT datasets, improving both aggregation quality and inference performance while ensuring patient privacy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>946, <a href='https://arxiv.org/pdf/2508.09195.pdf' target='_blank'>https://arxiv.org/pdf/2508.09195.pdf</a></span>   <span><a href='https://github.com/maryjis/mtcp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Boyko, Aleksandra Beliaeva, Dmitriy Kornilov, Alexander Bernstein, Maxim Sharaev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09195">impuTMAE: Multi-modal Transformer with Masked Pre-training for Missing Modalities Imputation in Cancer Survival Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The use of diverse modalities, such as omics, medical images, and clinical data can not only improve the performance of prognostic models but also deepen an understanding of disease mechanisms and facilitate the development of novel treatment approaches. However, medical data are complex, often incomplete, and contains missing modalities, making effective handling its crucial for training multimodal models. We introduce impuTMAE, a novel transformer-based end-to-end approach with an efficient multimodal pre-training strategy. It learns inter- and intra-modal interactions while simultaneously imputing missing modalities by reconstructing masked patches. Our model is pre-trained on heterogeneous, incomplete data and fine-tuned for glioma survival prediction using TCGA-GBM/LGG and BraTS datasets, integrating five modalities: genetic (DNAm, RNA-seq), imaging (MRI, WSI), and clinical data. By addressing missing data during pre-training and enabling efficient resource utilization, impuTMAE surpasses prior multimodal approaches, achieving state-of-the-art performance in glioma patient survival prediction. Our code is available at https://github.com/maryjis/mtcp<br>
<br>
<div id='section'>Paperid: <span id='pid'>947, <a href='https://arxiv.org/pdf/2508.09192.pdf' target='_blank'>https://arxiv.org/pdf/2508.09192.pdf</a></span>   <span><a href='https://github.com/zhijie-group/Discrete-Diffusion-Forcing' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xu Wang, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, Zhijie Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09192">Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs for text generation, with the potential to decode multiple tokens in a single iteration. However, none of the existing open-source dLLMs have achieved superior inference speed over AR LLMs of similar size. This paper breaks this barrier based on a simple and effective strategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key capabilities: (1) block-wise autoregressive generation to enable KV cache utilization; (2) prediction of following tokens without requiring completion of prior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs are refurbished into an AR-diffusion hybrid paradigm for efficient inference. D2F can be implemented with an asymmetric distillation process based on pre-trained dLLMs. We further propose a pipelined parallel decoding algorithm, which enables a trade-off between efficiency and efficacy. Empirically, D2F dLLMs achieve more than $\mathbf{2.5\times}$ inference speed than LLaMA3 and Qwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the acceleration can be more than $\mathbf{50\times}$ while maintaining comparable output quality. The code is available at https://github.com/zhijie-group/Discrete-Diffusion-Forcing.<br>
<span id='abs_ch'>中文摘要：本文提出离散扩散强制（D2F）策略，将扩散大语言模型改 为自回归-扩散混合范式，在保持输出质量的同时实现了比 统模型超过2.5倍的推理 速。</span><br>
<span id='abs_en'>English Summary: This paper introduces Discrete Diffusion Forcing (D2F), a novel strategy that transforms diffusion Large Language Models into an autoregressive-diffusion hybrid paradigm, achieving over 2.5× inference speedup compared to conventional models while maintaining output quality.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>948, <a href='https://arxiv.org/pdf/2508.09178.pdf' target='_blank'>https://arxiv.org/pdf/2508.09178.pdf</a></span>   <span><a href='https://github.com/Yanhui-Lee/IAD-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanhui Li, Yunkang Cao, Chengliang Liu, Yuan Xiong, Xinghui Dong, Chao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09178">IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Industrial anomaly detection is a critical component of modern manufacturing, yet the scarcity of defective samples restricts traditional detection methods to scenario-specific applications. Although Vision-Language Models (VLMs) demonstrate significant advantages in generalization capabilities, their performance in industrial anomaly detection remains limited. To address this challenge, we propose IAD-R1, a universal post-training framework applicable to VLMs of different architectures and parameter scales, which substantially enhances their anomaly detection capabilities. IAD-R1 employs a two-stage training strategy: the Perception Activation Supervised Fine-Tuning (PA-SFT) stage utilizes a meticulously constructed high-quality Chain-of-Thought dataset (Expert-AD) for training, enhancing anomaly perception capabilities and establishing reasoning-to-answer correlations; the Structured Control Group Relative Policy Optimization (SC-GRPO) stage employs carefully designed reward functions to achieve a capability leap from "Anomaly Perception" to "Anomaly Interpretation". Experimental results demonstrate that IAD-R1 achieves significant improvements across 7 VLMs, the largest improvement was on the DAGM dataset, with average accuracy 43.3% higher than the 0.5B baseline. Notably, the 0.5B parameter model trained with IAD-R1 surpasses commercial models including GPT-4.1 and Claude-Sonnet-4 in zero-shot settings, demonstrating the effectiveness and superiority of IAD-R1. The dataset, code, and all model weights will be publicly available at https://github.com/Yanhui-Lee/IAD-R1.<br>
<span id='abs_ch'>中文摘要：提出的IAD-R1框架通过两阶段训练策略显著提升了视觉语言模型在工业异常检测中的能力，在零 本设置下性能超越包括GPT-4.1在内的商业模型。</span><br>
<span id='abs_en'>English Summary: The proposed IAD-R1 framework significantly enhances industrial anomaly detection in Vision-Language Models through a two-stage training approach, achieving superior performance over commercial models in zero-shot settings.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>949, <a href='https://arxiv.org/pdf/2508.08992.pdf' target='_blank'>https://arxiv.org/pdf/2508.08992.pdf</a></span>   <span><a href='https://github.com/HKUST-KnowComp/MarPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Wang, Qihan Lin, Jiayu Liu, Qing Zong, Tianshi Zheng, Weiqi Wang, Yangqiu Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08992">Prospect Theory Fails for LLMs: Revealing Instability of Decision-Making under Epistemic Uncertainty</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Prospect Theory (PT) models human decision-making under uncertainty, while epistemic markers (e.g., maybe) serve to express uncertainty in language. However, it remains largely unexplored whether Prospect Theory applies to contemporary Large Language Models and whether epistemic markers, which express human uncertainty, affect their decision-making behaviour. To address these research gaps, we design a three-stage experiment based on economic questionnaires. We propose a more general and precise evaluation framework to model LLMs' decision-making behaviour under PT, introducing uncertainty through the empirical probability values associated with commonly used epistemic markers in comparable contexts. We then incorporate epistemic markers into the evaluation framework based on their corresponding probability values to examine their influence on LLM decision-making behaviours. Our findings suggest that modelling LLMs' decision-making with PT is not consistently reliable, particularly when uncertainty is expressed in diverse linguistic forms. Our code is released in https://github.com/HKUST-KnowComp/MarPT.<br>
<span id='abs_ch'>Chinese Summary: 前景理论在大型语言模型中的适用性并不一致，特别是在通过如认知 记等多 语言形式表达不确定性时，一项新评估框架揭示了这一点。</span><br>
<span id='abs_en'>English Summary: Prospect Theory's applicability to Large Language Models is inconsistent, especially when uncertainty is conveyed through varied linguistic forms like epistemic markers, as revealed by a novel evaluation framework.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>950, <a href='https://arxiv.org/pdf/2508.08940.pdf' target='_blank'>https://arxiv.org/pdf/2508.08940.pdf</a></span>   <span><a href='https://github.com/hammoudhasan/curriculum_grpo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hasan Abed Al Kader Hammoud, Kumail Alhamoud, Abed Hammoud, Elie Bou-Zeid, Marzyeh Ghassemi, Bernard Ghanem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08940">Train Long, Think Short: Curriculum Learning for Efficient Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent work on enhancing the reasoning abilities of large language models (LLMs) has introduced explicit length control as a means of constraining computational cost while preserving accuracy. However, existing approaches rely on fixed-length training budgets, which do not take advantage of the natural progression from exploration to compression during learning. In this work, we propose a curriculum learning strategy for length-controlled reasoning using Group Relative Policy Optimization (GRPO). Our method starts with generous token budgets and gradually tightens them over training, encouraging models to first discover effective solution strategies and then distill them into more concise reasoning traces. We augment GRPO with a reward function that balances three signals: task correctness (via verifier feedback), length efficiency, and formatting adherence (via structural tags). Experiments on GSM8K, MATH500, SVAMP, College Math, and GSM+ demonstrate that curriculum-based training consistently outperforms fixed-budget baselines at the same final budget, achieving higher accuracy and significantly improved token efficiency. We further ablate the impact of reward weighting and decay schedule design, showing that progressive constraint serves as a powerful inductive bias for training efficient reasoning models. Our code and checkpoints are released at: https://github.com/hammoudhasan/curriculum_grpo.<br>
<span id='abs_ch'>中文摘要：本 究提出一种基于群组相对策略优化的课程学 方法，通过在训练中逐步收紧推理长度约束，使大语言模型在保持准确性的同时显著提升计算效率，优于 统固定预算方法。</span><br>
<span id='abs_en'>English Summary: This study introduces a curriculum learning strategy using Group Relative Policy Optimization to progressively reduce reasoning length in large language models, achieving higher accuracy and token efficiency than fixed-budget methods across multiple benchmarks.Paperid:722,https://arxiv.org/pdf/2508.08920.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>951, <a href='https://arxiv.org/pdf/2508.08712.pdf' target='_blank'>https://arxiv.org/pdf/2508.08712.pdf</a></span>   <span><a href='https://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingzhe Zhang, Liancheng Fang, Chiming Duan, Minghua He, Leyi Pan, Pei Xiao, Shiyu Huang, Yunpeng Zhai, Xuming Hu, Philip S. Yu, Aiwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08712">A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As text generation has become a core capability of modern Large Language Models (LLMs), it underpins a wide range of downstream applications. However, most existing LLMs rely on autoregressive (AR) generation, producing one token at a time based on previously generated context-resulting in limited generation speed due to the inherently sequential nature of the process. To address this challenge, an increasing number of researchers have begun exploring parallel text generation-a broad class of techniques aimed at breaking the token-by-token generation bottleneck and improving inference efficiency. Despite growing interest, there remains a lack of comprehensive analysis on what specific techniques constitute parallel text generation and how they improve inference performance. To bridge this gap, we present a systematic survey of parallel text generation methods. We categorize existing approaches into AR-based and Non-AR-based paradigms, and provide a detailed examination of the core techniques within each category. Following this taxonomy, we assess their theoretical trade-offs in terms of speed, quality, and efficiency, and examine their potential for combination and comparison with alternative acceleration strategies. Finally, based on our findings, we highlight recent advancements, identify open challenges, and outline promising directions for future research in parallel text generation. We have also created a GitHub repository for indexing relevant papers and open resources available at https://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation.<br>
<span id='abs_ch'>中文: 本综述系统分类并分析了并行文本生成方法，旨在突 自回归大语言模型的顺序生成瓶颈，评估了它们在速度、质量和效率上的权衡，并指出了未来 究方向。English: This survey systematically categorizes and analyzes parallel text generation methods to overcome the sequential bottleneck of autoregressive LLMs, evaluating their trade-offs in speed, quality, and efficiency while identifying future research directions.Paperid:731,https://arxiv.org/pdf/2508.08705.pdfGitHub</span><br>
<span id='abs_en'>English: This survey systematically categorizes and analyzes parallel text generation methods to overcome the sequential bottleneck of autoregressive LLMs, evaluating their trade-offs in speed, quality, and efficiency while identifying future research directions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>952, <a href='https://arxiv.org/pdf/2508.08701.pdf' target='_blank'>https://arxiv.org/pdf/2508.08701.pdf</a></span>   <span><a href='https://github.com/oxu2/SafeFix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ouyang Xu, Baoming Zhang, Ruiyu Mao, Yunhui Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08701">SafeFix: Targeted Model Repair via Controlled Image Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Deep learning models for visual recognition often exhibit systematic errors due to underrepresented semantic subpopulations. Although existing debugging frameworks can pinpoint these failures by identifying key failure attributes, repairing the model effectively remains difficult. Current solutions often rely on manually designed prompts to generate synthetic training images -- an approach prone to distribution shift and semantic errors. To overcome these challenges, we introduce a model repair module that builds on an interpretable failure attribution pipeline. Our approach uses a conditional text-to-image model to generate semantically faithful and targeted images for failure cases. To preserve the quality and relevance of the generated samples, we further employ a large vision-language model (LVLM) to filter the outputs, enforcing alignment with the original data distribution and maintaining semantic consistency. By retraining vision models with this rare-case-augmented synthetic dataset, we significantly reduce errors associated with rare cases. Our experiments demonstrate that this targeted repair strategy improves model robustness without introducing new bugs. Code is available at https://github.com/oxu2/SafeFix<br>
<span id='abs_ch'>中文摘要：本文提出一种针对性模型修复方法，通过条件文本-图像生成器和大型视觉语言模型为代表性不足的故障案例生成语义一致的训练图像，在保持模型鲁棒性的同时显著减少了识别错误。</span><br>
<span id='abs_en'>English Summary: This paper introduces a targeted model repair method that uses a conditional text-to-image generator and a large vision-language model to create semantically consistent training images for underrepresented failure cases, effectively reducing recognition errors while maintaining model robustness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>953, <a href='https://arxiv.org/pdf/2508.08477.pdf' target='_blank'>https://arxiv.org/pdf/2508.08477.pdf</a></span>   <span><a href='https://github.com/jsalvasoler/trigger_arc_tsp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joan SalvÃ  Soler, GrÃ©goire de Lambertye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08477">A Fast GRASP Metaheuristic for the Trigger Arc TSP with MIP-Based Construction and Multi-Neighborhood Local Search</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The Trigger Arc Traveling Salesman Problem (TA-TSP) extends the classical TSP by introducing dynamic arc costs that change when specific "trigger" arcs are traversed, modeling scenarios such as warehouse operations with compactable storage systems. This paper introduces a GRASP-based metaheuristic that combines multiple construction heuristics with a multi-neighborhood local search. The construction phase uses mixed-integer programming (MIP) techniques to transform the TA-TSP into a sequence of tailored TSP instances, while the improvement phase applies 2-Opt, Swap, and Relocate operators. Computational experiments on MESS 2024 competition instances achieved average optimality gaps of 0.77\% and 0.40\% relative to the best-known solutions within a 60-second limit. On smaller, synthetically generated datasets, the method produced solutions 11.3\% better than the Gurobi solver under the same time constraints. The algorithm finished in the top three at MESS 2024, demonstrating its suitability for real-time routing applications with state-dependent travel costs.<br>
<span id='abs_ch'>中文: 本文针对触发弧旅行商问题提出一种基于GRASP的元启发式算法，通过混合整数规划构建和多重邻域搜索，在竞赛中取得前三名的优异表现。</span><br>
<span id='abs_en'>English: This paper presents a GRASP-based metaheuristic for the Trigger Arc Travelman Problem, achieving top competition results with near-optimal solutions through MIP-based construction and multi-neighborhood search.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>954, <a href='https://arxiv.org/pdf/2508.08477.pdf' target='_blank'>https://arxiv.org/pdf/2508.08477.pdf</a></span>   <span><a href='https://github.com/jsalvasoler/trigger_arc_tsp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Joan Salvà Soler, Grégoire de Lambertye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08477">A Fast GRASP Metaheuristic for the Trigger Arc TSP with MIP-Based Construction and Multi-Neighborhood Local Search</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The Trigger Arc Traveling Salesman Problem (TA-TSP) extends the classical TSP by introducing dynamic arc costs that change when specific "trigger" arcs are traversed, modeling scenarios such as warehouse operations with compactable storage systems. This paper introduces a GRASP-based metaheuristic that combines multiple construction heuristics with a multi-neighborhood local search. The construction phase uses mixed-integer programming (MIP) techniques to transform the TA-TSP into a sequence of tailored TSP instances, while the improvement phase applies 2-Opt, Swap, and Relocate operators. Computational experiments on MESS 2024 competition instances achieved average optimality gaps of 0.77% and 0.40% relative to the best-known solutions within a 60-second limit. On smaller, synthetically generated datasets, the method produced solutions 11.3% better than the Gurobi solver under the same time constraints. The algorithm finished in the top three at MESS 2024, demonstrating its suitability for real-time routing applications with state-dependent travel costs.<br>
<span id='abs_ch'>中文: 本文针对触发弧旅行商问题提出一种基于GRASP的元启发式算法，通过混合整数规划构建和多重邻域搜索，在竞赛中取得前三名的优异表现。</span><br>
<span id='abs_en'>English: This paper presents a GRASP-based metaheuristic for the Trigger Arc Travelman Problem, achieving top competition results with near-optimal solutions through MIP-based construction and multi-neighborhood search.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>955, <a href='https://arxiv.org/pdf/2508.08446.pdf' target='_blank'>https://arxiv.org/pdf/2508.08446.pdf</a></span>   <span><a href='https://github.com/friendshipkim/overfill' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Woojeong Kim, Junxiong Wang, Jing Nathan Yan, Mohamed Abdelfattah, Alexander M. Rush
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08446">OverFill: Two-Stage Models for Efficient Language Model Decoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) excel across diverse tasks but face significant deployment challenges due to high inference costs. LLM inference comprises prefill (compute-bound) and decode (memory-bound) stages, with decode dominating latency particularly for long sequences. Current decoder-only models handle both stages uniformly, despite their distinct computational profiles. We propose OverFill, which decouples these stages to optimize accuracy-efficiency tradeoffs. OverFill begins with a full model for prefill, processing system and user inputs in parallel. It then switches to a dense pruned model, while generating tokens sequentially. Leveraging more compute during prefill, OverFill improves generation quality with minimal latency overhead. Our 3B-to-1B OverFill configuration outperforms 1B pruned models by 83.2%, while the 8B-to-3B configuration improves over 3B pruned models by 79.2% on average across standard benchmarks. OverFill matches the performance of same-sized models trained from scratch, while using significantly less training data. Our code is available at https://github.com/friendshipkim/overfill.<br>
<span id='abs_ch'>中文：OverFill通过解耦LLM推理的预填充和解 阶段进行效率优化，在预填充时使用完整模型，解 时采用剪枝模型，以最小延迟代价实现显著性能提升。</span><br>
<span id='abs_en'>English: OverFill decouples the prefill and decode stages of LLM inference to optimize efficiency, using a full model for prefill and a pruned model for decoding, achieving significant performance gains with minimal latency overhead.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>956, <a href='https://arxiv.org/pdf/2508.08292.pdf' target='_blank'>https://arxiv.org/pdf/2508.08292.pdf</a></span>   <span><a href='https://github.com/brando90/putnam-axiom' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aryan Gulati, Brando Miranda, Eric Chen, Emily Xia, Kai Fronsdal, Bruno Dumont, Elyas Obbad, Sanmi Koyejo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08292">Putnam-AXIOM: A Functional and Static Benchmark for Measuring Higher Level Mathematical Reasoning in LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current mathematical reasoning benchmarks for large language models (LLMs) are approaching saturation, with some achieving > 90% accuracy, and are increasingly compromised by training-set contamination. We introduce Putnam-AXIOM, a benchmark of 522 university-level competition problems drawn from the prestigious William Lowell Putnam Mathematical Competition, and Putnam-AXIOM Variation, an unseen companion set of 100 functional variants generated by programmatically perturbing variables and constants. The variation protocol produces an unlimited stream of equally difficult, unseen instances -- yielding a contamination-resilient test bed. On the Original set, OpenAI's o1-preview -- the strongest evaluated model -- scores 41.9%, but its accuracy drops by 19.6% (46.8% relative decrease) on the paired Variations. The remaining eighteen models show the same downward trend, ten of them with non-overlapping 95% confidence intervals. These gaps suggest memorization and highlight the necessity of dynamic benchmarks. We complement "boxed" accuracy with Teacher-Forced Accuracy (TFA), a lightweight metric that directly scores reasoning traces and automates natural language proof evaluations. Putnam-AXIOM therefore provides a rigorous, contamination-resilient evaluation framework for assessing advanced mathematical reasoning of LLMs. Data and evaluation code are publicly available at https://github.com/brando90/putnam-axiom.<br>
<span id='abs_ch'>中文摘要：作者提出了Putnam-AXIOM这一抗污染基准，通过大学数学竞赛题目及其程序化生成的变体，揭示了大型语言模型准确率显著下降的问题，凸显了记忆效应和动态评估的必要性。</span><br>
<span id='abs_en'>English Summary: The authors introduce Putnam-AXIOM, a contamination-resilient benchmark using university-level math competition problems and their programmatically generated variations, revealing significant accuracy drops in LLMs that highlight memorization issues and the need for dynamic evaluation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>957, <a href='https://arxiv.org/pdf/2508.08280.pdf' target='_blank'>https://arxiv.org/pdf/2508.08280.pdf</a></span>   <span><a href='https://github.com/seonyoungKimm/MoSSDA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seonyoung Kim, Dongil Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08280">MoSSDA: A Semi-Supervised Domain Adaptation Framework for Multivariate Time-Series Classification using Momentum Encoder</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Deep learning has emerged as the most promising approach in various fields; however, when the distributions of training and test data are different (domain shift), the performance of deep learning models can degrade. Semi-supervised domain adaptation (SSDA) is a major approach for addressing this issue, assuming that a fully labeled training set (source domain) is available, but the test set (target domain) provides labels only for a small subset. In this study, we propose a novel two-step momentum encoder-utilized SSDA framework, MoSSDA, for multivariate time-series classification. Time series data are highly sensitive to noise, and sequential dependencies cause domain shifts resulting in critical performance degradation. To obtain a robust, domain-invariant and class-discriminative representation, MoSSDA employs a domain-invariant encoder to learn features from both source and target domains. Subsequently, the learned features are fed to a mixup-enhanced positive contrastive module consisting of an online momentum encoder. The final classifier is trained with learned features that exhibit consistency and discriminability with limited labeled target domain data, without data augmentation. We applied a two-stage process by separating the gradient flow between the encoders and the classifier to obtain rich and complex representations. Through extensive experiments on six diverse datasets, MoSSDA achieved state-of-the-art performance for three different backbones and various unlabeled ratios in the target domain data. The Ablation study confirms that each module, including two-stage learning, is effective in improving the performance. Our code is available at https://github.com/seonyoungKimm/MoSSDA<br>
<br>
<div id='section'>Paperid: <span id='pid'>958, <a href='https://arxiv.org/pdf/2508.08180.pdf' target='_blank'>https://arxiv.org/pdf/2508.08180.pdf</a></span>   <span><a href='https://github.com/Snarci/RedDino,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Zedda, Andrea Loddo, Cecilia Di Ruberto, Carsten Marr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08180">RedDino: A foundation model for red blood cell analysis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Red blood cells (RBCs) are essential to human health, and their precise morphological analysis is important for diagnosing hematological disorders. Despite the promise of foundation models in medical diagnostics, comprehensive AI solutions for RBC analysis remain scarce. We present RedDino, a self-supervised foundation model designed for RBC image analysis. RedDino uses an RBC-specific adaptation of the DINOv2 self-supervised learning framework and is trained on a curated dataset of 1.25 million RBC images from diverse acquisition modalities and sources. Extensive evaluations show that RedDino outperforms existing state-of-the-art models on RBC shape classification. Through assessments including linear probing and nearest neighbor classification, we confirm its strong feature representations and generalization ability. Our main contributions are: (1) a foundation model tailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations for RBC modeling, and (3) a detailed evaluation of generalization performance. RedDino addresses key challenges in computational hematology by capturing nuanced morphological features, advancing the development of reliable diagnostic tools. The source code and pretrained models for RedDino are available at https://github.com/Snarci/RedDino, and the pretrained models can be downloaded from our Hugging Face collection at https://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc<br>
<br>
<div id='section'>Paperid: <span id='pid'>959, <a href='https://arxiv.org/pdf/2508.08172.pdf' target='_blank'>https://arxiv.org/pdf/2508.08172.pdf</a></span>   <span><a href='https://github.com/VincentPerreault0/NeuralLogicNetworks' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vincent Perreault, Katsumi Inoue, Richard Labib, Alain Hertz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08172">Neural Logic Networks for Interpretable Classification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Traditional neural networks have an impressive classification performance, but what they learn cannot be inspected, verified or extracted. Neural Logic Networks on the other hand have an interpretable structure that enables them to learn a logical mechanism relating the inputs and outputs with AND and OR operations. We generalize these networks with NOT operations and biases that take into account unobserved data and develop a rigorous logical and probabilistic modeling in terms of concept combinations to motivate their use. We also propose a novel factorized IF-THEN rule structure for the model as well as a modified learning algorithm. Our method improves the state-of-the-art in Boolean networks discovery and is able to learn relevant, interpretable rules in tabular classification, notably on examples from the medical and industrial fields where interpretability has tangible value.<br>
<br>
<div id='section'>Paperid: <span id='pid'>960, <a href='https://arxiv.org/pdf/2508.08127.pdf' target='_blank'>https://arxiv.org/pdf/2508.08127.pdf</a></span>   <span><a href='https://github.com/MR9812/BlindGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Miao, Yixin Liu, Yili Wang, Xu Shen, Yue Tan, Yiwei Dai, Shirui Pan, Xin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08127">BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The security of LLM-based multi-agent systems (MAS) is critically threatened by propagation vulnerability, where malicious agents can distort collective decision-making through inter-agent message interactions. While existing supervised defense methods demonstrate promising performance, they may be impractical in real-world scenarios due to their heavy reliance on labeled malicious agents to train a supervised malicious detection model. To enable practical and generalizable MAS defenses, in this paper, we propose BlindGuard, an unsupervised defense method that learns without requiring any attack-specific labels or prior knowledge of malicious behaviors. To this end, we establish a hierarchical agent encoder to capture individual, neighborhood, and global interaction patterns of each agent, providing a comprehensive understanding for malicious agent detection. Meanwhile, we design a corruption-guided detector that consists of directional noise injection and contrastive learning, allowing effective detection model training solely on normal agent behaviors. Extensive experiments show that BlindGuard effectively detects diverse attack types (i.e., prompt injection, memory poisoning, and tool attack) across MAS with various communication patterns while maintaining superior generalizability compared to supervised baselines. The code is available at: https://github.com/MR9812/BlindGuard.<br>
<span id='abs_ch'>中文摘要：BlindGuard是一种 需 注攻击数据的 监督防御方法，通过分析智能体交互模式并采用对比学 来检测多智能体系统中的恶意行为，在不同攻击类型中展现出卓越的泛化能力。</span><br>
<span id='abs_en'>English Summary: BlindGuard is an unsupervised defense method that detects malicious agents in multi-agent systems by analyzing interaction patterns and using contrastive learning without requiring labeled attack data, demonstrating superior generalizability across diverse attacks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>961, <a href='https://arxiv.org/pdf/2508.08088.pdf' target='_blank'>https://arxiv.org/pdf/2508.08088.pdf</a></span>   <span><a href='https://github.com/plageon/HierSearch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiejun Tan, Zhicheng Dou, Yan Yu, Jiehan Cheng, Qiang Ju, Jian Xie, Ji-Rong Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08088">HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, large reasoning models have demonstrated strong mathematical and coding abilities, and deep search leverages their reasoning capabilities in challenging information retrieval tasks. Existing deep search works are generally limited to a single knowledge source, either local or the Web. However, enterprises often require private deep search systems that can leverage search tools over both local and the Web corpus. Simply training an agent equipped with multiple search tools using flat reinforcement learning (RL) is a straightforward idea, but it has problems such as low training data efficiency and poor mastery of complex tools. To address the above issue, we propose a hierarchical agentic deep search framework, HierSearch, trained with hierarchical RL. At the low level, a local deep search agent and a Web deep search agent are trained to retrieve evidence from their corresponding domains. At the high level, a planner agent coordinates low-level agents and provides the final answer. Moreover, to prevent direct answer copying and error propagation, we design a knowledge refiner that filters out hallucinations and irrelevant evidence returned by low-level agents. Experiments show that HierSearch achieves better performance compared to flat RL, and outperforms various deep search and multi-source retrieval-augmented generation baselines in six benchmarks across general, finance, and medical domains.<br>
<span id='abs_ch'>中文: HierSearch提出了一种分层强化学 框架，通过规划器和知识精炼器协调本地与网络搜索代理，提升多源检索能力并减少错误，在多个领域的基准测试中优于现有方法。English: HierSearch introduces a hierarchical reinforcement learning framework for enterprise deep search, coordinating local and web agents through a planner and knowledge refiner to enhance multi-source retrieval while reducing errors, outperforming existing methods across diverse benchmarks.Paperid:767,https://arxiv.org/pdf/2508.08082.pdfGitHub</span><br>
<span id='abs_en'>English: HierSearch introduces a hierarchical reinforcement learning framework for enterprise deep search, coordinating local and web agents through a planner and knowledge refiner to enhance multi-source retrieval while reducing errors, outperforming existing methods across diverse benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>962, <a href='https://arxiv.org/pdf/2508.08053.pdf' target='_blank'>https://arxiv.org/pdf/2508.08053.pdf</a></span>   <span><a href='https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Runchuan Zhu, Bowen Jiang, Lingrui Mei, Fangkai Yang, Lu Wang, Haoxiang Gao, Fengshuo Bai, Pu Zhao, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08053">AdaptFlow: Adaptive Workflow Optimization via Meta-Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in large language models (LLMs) have sparked growing interest in agentic workflows, which are structured sequences of LLM invocations intended to solve complex tasks. However, existing approaches often rely on static templates or manually designed workflows, which limit adaptability to diverse tasks and hinder scalability. We propose AdaptFlow, a natural language-based meta-learning framework inspired by model-agnostic meta-learning (MAML). AdaptFlow learns a generalizable workflow initialization that enables rapid subtask-level adaptation. It employs a bi-level optimization scheme: the inner loop refines the workflow for a specific subtask using LLM-generated feedback, while the outer loop updates the shared initialization to perform well across tasks. This setup allows AdaptFlow to generalize effectively to unseen tasks by adapting the initialized workflow through language-guided modifications. Evaluated across question answering, code generation, and mathematical reasoning benchmarks, AdaptFlow consistently outperforms both manually crafted and automatically searched baselines, achieving state-of-the-art results with strong generalization across tasks and models. The source code and data are available at https://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.<br>
<span id='abs_ch'>中文摘要：AdaptFlow是一种基于自然语言的元学 框架，通过双层优化实现智能体工作流的快速自适应，在多项基准测试中均达到最优性能。</span><br>
<span id='abs_en'>English Summary: AdaptFlow is a natural language-based meta-learning framework that enables rapid adaptation of agentic workflows for complex tasks through bi-level optimization, achieving state-of-the-art performance across various benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>963, <a href='https://arxiv.org/pdf/2508.08042.pdf' target='_blank'>https://arxiv.org/pdf/2508.08042.pdf</a></span>   <span><a href='https://github.com/L2R-UET/MAMEX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Van-Khang Nguyen, Duc-Hoang Pham, Huy-Son Nguyen, Cam-Van Thi Nguyen, Hoang-Quynh Le, Duc-Trong Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08042">Multi-modal Adaptive Mixture of Experts for Cold-start Recommendation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recommendation systems have faced significant challenges in cold-start scenarios, where new items with a limited history of interaction need to be effectively recommended to users. Though multimodal data (e.g., images, text, audio, etc.) offer rich information to address this issue, existing approaches often employ simplistic integration methods such as concatenation, average pooling, or fixed weighting schemes, which fail to capture the complex relationships between modalities. Our study proposes a novel Mixture of Experts (MoE) framework for multimodal cold-start recommendation, named MAMEX, which dynamically leverages latent representation from different modalities. MAMEX utilizes modality-specific expert networks and introduces a learnable gating mechanism that adaptively weights the contribution of each modality based on its content characteristics. This approach enables MAMEX to emphasize the most informative modalities for each item while maintaining robustness when certain modalities are less relevant or missing. Extensive experiments on benchmark datasets show that MAMEX outperforms state-of-the-art methods in cold-start scenarios, with superior accuracy and adaptability. For reproducibility, the code has been made available on Github https://github.com/L2R-UET/MAMEX.<br>
<span id='abs_ch'>中文: 本 究提出了MAMEX框架，采用专家混合模型和自适应门控机制，动态整合多模态数据，显著提升了冷启动推荐系统的准确性和适应性。</span><br>
<span id='abs_en'>English: The study introduces MAMEX, a novel Mixture of Experts framework that dynamically integrates multimodal data through adaptive gating to enhance cold-start recommendation accuracy and robustness.Paperid:771,https://arxiv.org/pdf/2508.08038.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>964, <a href='https://arxiv.org/pdf/2508.07976.pdf' target='_blank'>https://arxiv.org/pdf/2508.07976.pdf</a></span>   <span><a href='https://github.com/inclusionAI/ASearcher' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, Yi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07976">Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in LLM-based agents have demonstrated remarkable capabilities in handling complex, knowledge-intensive tasks by integrating external tools. Among diverse choices of tools, search tools play a pivotal role in accessing vast external knowledge. However, open-source agents still fall short of achieving expert-level Search Intelligence, the ability to resolve ambiguous queries, generate precise searches, analyze results, and conduct thorough exploration. Existing approaches fall short in scalability, efficiency, and data quality. For example, small turn limits in existing online RL methods, e.g. <=10, restrict complex strategy learning. This paper introduces ASearcher, an open-source project for large-scale RL training of search agents. Our key contributions include: (1) Scalable fully asynchronous RL training that enables long-horizon search while maintaining high training efficiency. (2) A prompt-based LLM agent that autonomously synthesizes high-quality and challenging QAs, creating a large-scale QA dataset. Through RL training, our prompt-based QwQ-32B agent achieves substantial improvements, with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns and output tokens exceeding 150k during training time. With a simple agent design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We open-source our models, training data, and codes in https://github.com/inclusionAI/ASearcher.<br>
<span id='abs_ch'>中文: 本文介绍了ASearcher开源项目，通过大规模强化学 训练搜索代理，在复杂长程搜索任务中实现显著性能提升，并在基准测试中超越了现有开源模型。</span><br>
<span id='abs_en'>English: This paper introduces ASearcher, an open-source project that enables large-scale reinforcement learning for search agents, achieving significant improvements in handling complex, long-horizon search tasks and outperforming existing open-source models on benchmark tests.</span>Paperid:774,https://arxiv.org/pdf/2508.07969.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>965, <a href='https://arxiv.org/pdf/2508.07616.pdf' target='_blank'>https://arxiv.org/pdf/2508.07616.pdf</a></span>   <span><a href='https://github.com/3rdAT/ThinkTuning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aswin RRV, Jacob Dineen, Divij Handa, Md Nayem Uddin, Mihir Parmar, Chitta Baral, Ben Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07616">ThinkTuning: Instilling Cognitive Reflections without Distillation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in test-time scaling have led to the emergence of thinking LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL drives this self-improvement paradigm, a recent study (Gandhi et al., 2025) shows that RL alone does not truly instill these new reasoning abilities - it merely draws out behaviors already present in the base models. This raises a question: How can we train the models that don't exhibit such thinking behavior to develop it in the first place? To this end, we propose ThinkTuning, a GRPO-based interactive training approach where we augment the rollouts of a student model with the guidance from a teacher model. A simple idea from classroom practice inspires our method: a teacher poses a problem, lets the student try an answer, then gives corrective feedback -- enough to point the mind in the right direction and then show the solution. Each piece of feedback reshapes the student's thoughts, leading them to arrive at the correct solution. Similarly, we find that this type of implicit supervision through feedback from a teacher model of the same size improves the reasoning capabilities of the student model. In particular, on average, our method shows a 3.85% improvement over zero-shot baselines across benchmarks, and on MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements over the vanilla-GRPO baseline. Source code is available at https://github.com/3rdAT/ThinkTuning.<br>
<span id='abs_ch'>中文: 最新 究表明仅 强化学  法开发大型语言模型的新推理能力， 此提出ThinkTuning方法——基于GRPO的互动训练框架，通过教师模型提供 错反馈来提升学生模型的推理水平，在多项基准测试中实现了显著性能提升。</span><br>
<span id='abs_en'>English: Recent research reveals that reinforcement learning alone fails to develop new reasoning abilities in LLMs, prompting the introduction of ThinkTuning, a GRPO-based interactive training method where teacher models provide corrective feedback to enhance student models' reasoning, achieving notable performance improvements across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>966, <a href='https://arxiv.org/pdf/2508.07407.pdf' target='_blank'>https://arxiv.org/pdf/2508.07407.pdf</a></span>   <span><a href='https://github.com/EvoAgentX/Awesome-Self-Evolving-Agents' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei Liu, Zihao Li, Zhaochun Ren, Nikos Aletras, Xi Wang, Han Zhou, Zaiqiao Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07407">A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in large language models have sparked growing interest in AI agents capable of solving complex, real-world tasks. However, most existing agent systems rely on manually crafted configurations that remain static after deployment, limiting their ability to adapt to dynamic and evolving environments. To this end, recent research has explored agent evolution techniques that aim to automatically enhance agent systems based on interaction data and environmental feedback. This emerging direction lays the foundation for self-evolving AI agents, which bridge the static capabilities of foundation models with the continuous adaptability required by lifelong agentic systems. In this survey, we provide a comprehensive review of existing techniques for self-evolving agentic systems. Specifically, we first introduce a unified conceptual framework that abstracts the feedback loop underlying the design of self-evolving agentic systems. The framework highlights four key components: System Inputs, Agent System, Environment, and Optimisers, serving as a foundation for understanding and comparing different strategies. Based on this framework, we systematically review a wide range of self-evolving techniques that target different components of the agent system. We also investigate domain-specific evolution strategies developed for specialised fields such as biomedicine, programming, and finance, where optimisation objectives are tightly coupled with domain constraints. In addition, we provide a dedicated discussion on the evaluation, safety, and ethical considerations for self-evolving agentic systems, which are critical to ensuring their effectiveness and reliability. This survey aims to provide researchers and practitioners with a systematic understanding of self-evolving AI agents, laying the foundation for the development of more adaptive, autonomous, and lifelong agentic systems.<br>
<span id='abs_ch'>中文: 本综述系统探讨了通过环境反馈实现自我进化的AI智能体，提出了统一框架并分析多领域应用技术，同时涵盖评估、安全与伦理考量，为开发自适应终身智能系统 定基础。</span><br>
<span id='abs_en'>English: This survey comprehensively reviews self-evolving AI agents that enhance their capabilities through environmental feedback, presenting a unified framework and examining techniques across various domains while addressing evaluation, safety, and ethical considerations.Paperid:802,https://arxiv.org/pdf/2508.07402.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>967, <a href='https://arxiv.org/pdf/2508.07371.pdf' target='_blank'>https://arxiv.org/pdf/2508.07371.pdf</a></span>   <span><a href='https://github.com/liusu-orange/AutoAssert-1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Zhong, Hongchao Liu, Di ZHao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07371">AutoAssert 1: A LoRA Fine-Tuned LLM Model for Efficient Automated Assertion Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As the complexity of software systems continues to increase, the demand for automated testing and maintenance tools is growing exponentially. To meet this urgent need, we propose a new assertion generation method based on Hardware Description Language (HDL). This method combines a lightweight, parameter-adjustable large language model (LLM) with the Unsloth platform to automatically generate test cases, thereby significantly reducing training costs without sacrificing accuracy or generalization performance. Empirical evaluation shows that our method can efficiently generate assertions that strictly conform to the hardware logic. This framework provides a robust and flexible solution to modern software testing and maintenance challenges. https://github.com/liusu-orange/AutoAssert-1 and https://gitee.com/OpenBPU/auto-assert1 are the locations of the source code.<br>
<span id='abs_ch'>中文: 本文提出了一种基于硬件描述语言的新型断言生成方法，结合轻量级可调参数大语言模型与Unsloth平台自动生成测试用例，在保证准确性和泛化能力的同时显著降低训练成本，实证评估验证了其高效生成严 符合硬件逻辑断言的能力。</span><br>
<span id='abs_en'>English: This paper introduces a novel HDL-based assertion generation method that integrates a lightweight, parameter-tunable LLM with the Unsloth platform to automatically produce test cases, effectively lowering training expenses while preserving accuracy and generalization, as validated by empirical results.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>968, <a href='https://arxiv.org/pdf/2508.07353.pdf' target='_blank'>https://arxiv.org/pdf/2508.07353.pdf</a></span>   <span><a href='https://github.com/Anya-RB-Chen/COMP-COMP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rubing Chen, Jiaxin Wu, Jian Wang, Xulu Zhang, Wenqi Fan, Chenghua Lin, Xiao-Yong Wei, Qing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07353">Benchmarking for Domain-Specific LLMs: A Case Study on Academia and Beyond</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The increasing demand for domain-specific evaluation of large language models (LLMs) has led to the development of numerous benchmarks. These efforts often adhere to the principle of data scaling, relying on large corpora or extensive question-answer (QA) sets to ensure broad coverage. However, the impact of corpus and QA set design on the precision and recall of domain-specific LLM performance remains poorly understood. In this paper, we argue that data scaling is not always the optimal principle for domain-specific benchmark construction. Instead, we introduce Comp-Comp, an iterative benchmarking framework grounded in the principle of comprehensiveness and compactness. Comprehensiveness ensures semantic recall by covering the full breadth of the domain, while compactness improves precision by reducing redundancy and noise. To demonstrate the effectiveness of our approach, we present a case study conducted at a well-renowned university, resulting in the creation of PolyBench, a large-scale, high-quality academic benchmark. Although this study focuses on academia, the Comp-Comp framework is domain-agnostic and readily adaptable to a wide range of specialized fields. The source code and datasets can be accessed at https://github.com/Anya-RB-Chen/COMP-COMP.<br>
<span id='abs_ch'>中文摘要：Comp-Comp框架提出以全面性和紧凑性为 心原则的领域 关基准构建方法，通过开发PolyBench学术基准验证其有效性，可广泛应用于各专业领域。</span><br>
<span id='abs_en'>English Summary: The Comp-Comp framework introduces a domain-agnostic benchmarking approach prioritizing comprehensiveness and compactness over data scaling, validated through the creation of PolyBench as a high-quality academic benchmark.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>969, <a href='https://arxiv.org/pdf/2508.07307.pdf' target='_blank'>https://arxiv.org/pdf/2508.07307.pdf</a></span>   <span><a href='https://github.com/Ghy0501/MCITlib' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiyang Guo, Fei Zhu, Hongbo Zhao, Fanhu Zeng, Wenzhuo Liu, Shijie Ma, Da-Han Wang, Xu-Yao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07307">MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Continual learning aims to equip AI systems with the ability to continuously acquire and adapt to new knowledge without forgetting previously learned information, similar to human learning. While traditional continual learning methods focusing on unimodal tasks have achieved notable success, the emergence of Multimodal Large Language Models has brought increasing attention to Multimodal Continual Learning tasks involving multiple modalities, such as vision and language. In this setting, models are expected to not only mitigate catastrophic forgetting but also handle the challenges posed by cross-modal interactions and coordination. To facilitate research in this direction, we introduce MCITlib, a comprehensive and constantly evolving code library for continual instruction tuning of Multimodal Large Language Models. In MCITlib, we have currently implemented 8 representative algorithms for Multimodal Continual Instruction Tuning and systematically evaluated them on 2 carefully selected benchmarks. MCITlib will be continuously updated to reflect advances in the Multimodal Continual Learning field. The codebase is released at https://github.com/Ghy0501/MCITlib.<br>
<span id='abs_ch'>中文: 持续学 旨在让AI系统能够不断获取新知识而不遗忘已学内容，随着多模态大语言模型的出现，涉及视觉和语言等多模态的持续学 任务受到关注，为此我们开发了MCITlib代 库，用于多模态持续指令调优，目前已实现8种算法并在2个基准上进行了系统评估。</span><br>
<span id='abs_en'>English: Continual learning enables AI to continuously learn new knowledge without forgetting past information, and with the rise of Multimodal Large Language Models, Multimodal Continual Learning has gained attention for handling tasks across multiple modalities like vision and language, leading to the development of MCITlib, a code library for continual instruction tuning that includes 8 algorithms and evaluations on 2 benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>970, <a href='https://arxiv.org/pdf/2508.07195.pdf' target='_blank'>https://arxiv.org/pdf/2508.07195.pdf</a></span>   <span><a href='https://github.com/syrGitHub/TALON' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanru Sun, Emadeldeen Eldele, Zongxia Xie, Yucheng Wang, Wenzhe Niu, Qinghua Hu, Chee Keong Kwoh, Min Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07195">Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have recently demonstrated impressive capabilities in natural language processing due to their strong generalization and sequence modeling capabilities. However, their direct application to time series forecasting remains challenging due to two fundamental issues: the inherent heterogeneity of temporal patterns and the modality gap between continuous numerical signals and discrete language representations. In this work, we propose TALON, a unified framework that enhances LLM-based forecasting by modeling temporal heterogeneity and enforcing semantic alignment. Specifically, we design a Heterogeneous Temporal Encoder that partitions multivariate time series into structurally coherent segments, enabling localized expert modeling across diverse temporal patterns. To bridge the modality gap, we introduce a Semantic Alignment Module that aligns temporal features with LLM-compatible representations, enabling effective integration of time series into language-based models while eliminating the need for handcrafted prompts during inference. Extensive experiments on seven real-world benchmarks demonstrate that TALON achieves superior performance across all datasets, with average MSE improvements of up to 11\% over recent state-of-the-art methods. These results underscore the effectiveness of incorporating both pattern-aware and semantic-aware designs when adapting LLMs for time series forecasting. The code is available at: https://github.com/syrGitHub/TALON.<br>
<span id='abs_ch'>中文: TALON框架通过异构时序编 器处理时间模式差异，并利用语义对齐模块弥合模态鸿沟，从而在LLM时序预测中实现高达11%的均方误差提升，显著优于现有方法。</span><br>
<span id='abs_en'>English: TALON enhances LLM-based time series forecasting by addressing temporal heterogeneity through a specialized encoder and bridging the modality gap with semantic alignment, achieving superior performance with up to 11% MSE improvement across benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>971, <a href='https://arxiv.org/pdf/2508.07178.pdf' target='_blank'>https://arxiv.org/pdf/2508.07178.pdf</a></span>   <span><a href='https://github.com/liukejin-up/PHG-DIF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kejin Liu, Junhong Lian, Xiang Ao, Ningtao Wang, Xing Fu, Yu Cheng, Weiqiang Wang, Xinyu Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07178">Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate personalized headline generation hinges on precisely capturing user interests from historical behaviors. However, existing methods neglect personalized-irrelevant click noise in entire historical clickstreams, which may lead to hallucinated headlines that deviate from genuine user preferences. In this paper, we reveal the detrimental impact of click noise on personalized generation quality through rigorous analysis in both user and news dimensions. Based on these insights, we propose a novel Personalized Headline Generation framework via Denoising Fake Interests from Implicit Feedback (PHG-DIF). PHG-DIF first employs dual-stage filtering to effectively remove clickstream noise, identified by short dwell times and abnormal click bursts, and then leverages multi-level temporal fusion to dynamically model users' evolving and multi-faceted interests for precise profiling. Moreover, we release DT-PENS, a new benchmark dataset comprising the click behavior of 1,000 carefully curated users and nearly 10,000 annotated personalized headlines with historical dwell time annotations. Extensive experiments demonstrate that PHG-DIF substantially mitigates the adverse effects of click noise and significantly improves headline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our framework implementation and dataset are available at https://github.com/liukejin-up/PHG-DIF.<br>
<span id='abs_ch'>中文: 本文提出PHG-DIF框架，通过双阶段过滤和多层次时序融合有效消除用户点击历史中的噪声，并在新发布的DT-PENS基准数据集上实现了最先进的个性化 题生成效果。</span><br>
<span id='abs_en'>English: This paper introduces PHG-DIF, a personalized headline generation framework that addresses click noise in user histories through dual-stage filtering and multi-level temporal fusion, achieving state-of-the-art results on the newly released DT-PENS benchmark dataset.Paperid:821,https://arxiv.org/pdf/2508.07171.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>972, <a href='https://arxiv.org/pdf/2508.07170.pdf' target='_blank'>https://arxiv.org/pdf/2508.07170.pdf</a></span>   <span><a href='https://github.com/Shi-Yun-peng/LMFNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunpeng Shi, Lei Chen, Xiaolu Shen, Yanju Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07170">Lightweight Multi-Scale Feature Extraction with Fully Connected LMF Layer for Salient Object Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In the domain of computer vision, multi-scale feature extraction is vital for tasks such as salient object detection. However, achieving this capability in lightweight networks remains challenging due to the trade-off between efficiency and performance. This paper proposes a novel lightweight multi-scale feature extraction layer, termed the LMF layer, which employs depthwise separable dilated convolutions in a fully connected structure. By integrating multiple LMF layers, we develop LMFNet, a lightweight network tailored for salient object detection. Our approach significantly reduces the number of parameters while maintaining competitive performance. Here, we show that LMFNet achieves state-of-the-art or comparable results on five benchmark datasets with only 0.81M parameters, outperforming several traditional and lightweight models in terms of both efficiency and accuracy. Our work not only addresses the challenge of multi-scale learning in lightweight networks but also demonstrates the potential for broader applications in image processing tasks. The related code files are available at https://github.com/Shi-Yun-peng/LMFNet<br>
<span id='abs_ch'>中文: 本文提出LMFNet轻量级网络，通过新型多尺度特征提取层在显著目 检测中仅用0.81M参数就实现最优性能，成功解决了轻量网络中效率与精度的平衡难题。</span><br>
<span id='abs_en'>English: This paper introduces LMFNet, a lightweight network using novel multi-scale layers that achieve state-of-the-art salient object detection with minimal parameters while maintaining high efficiency and accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>973, <a href='https://arxiv.org/pdf/2508.07050.pdf' target='_blank'>https://arxiv.org/pdf/2508.07050.pdf</a></span>   <span><a href='https://github.com/8421BCD/ReasonRank' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhan Liu, Xinyu Ma, Weiwei Sun, Yutao Zhu, Yuchen Li, Dawei Yin, Zhicheng Dou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07050">ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Model (LLM) based listwise ranking has shown superior performance in many passage ranking tasks. With the development of Large Reasoning Models, many studies have demonstrated that step-by-step reasoning during test-time helps improve listwise ranking performance. However, due to the scarcity of reasoning-intensive training data, existing rerankers perform poorly in many complex ranking scenarios and the ranking ability of reasoning-intensive rerankers remains largely underdeveloped. In this paper, we first propose an automated reasoning-intensive training data synthesis framework, which sources training queries and passages from diverse domains and applies DeepSeek-R1 to generate high-quality training labels. A self-consistency data filtering mechanism is designed to ensure the data quality. To empower the listwise reranker with strong reasoning ability, we further propose a two-stage post-training approach, which includes a cold-start supervised fine-tuning (SFT) stage for reasoning pattern learning and a reinforcement learning (RL) stage for further ranking ability enhancement. During the RL stage, based on the nature of listwise ranking, we design a multi-view ranking reward, which is more effective than a ranking metric-based reward. Extensive experiments demonstrate that our trained reasoning-intensive reranker \textbf{ReasonRank} outperforms existing baselines significantly and also achieves much lower latency than pointwise reranker Rank1. \textbf{Through further experiments, our ReasonRank has achieved state-of-the-art (SOTA) performance 40.6 on the BRIGHT leaderboard\footnote{https://brightbenchmark.github.io/}.} Our codes are available at https://github.com/8421BCD/ReasonRank.<br>
<span id='abs_ch'>Chinese: 本文提出了ReasonRank，一种基于自动化数据合成框架和两阶段训练方法的推理密集型列表重排器，在排序任务中实现了最优性能并显著降低了延迟。</span><br>
<span id='abs_en'>English: This paper introduces ReasonRank, a reasoning-intensive listwise reranker trained using an automated data synthesis framework and a two-stage post-training approach, which achieves state-of-the-art performance on ranking tasks with significantly lower latency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>974, <a href='https://arxiv.org/pdf/2508.07048.pdf' target='_blank'>https://arxiv.org/pdf/2508.07048.pdf</a></span>   <span><a href='https://github.com/taeyoun811/Whisfusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taeyoun Kwon, Junhyuk Ahn, Taegeun Yun, Heeju Jwa, Yoonchae Choi, Siwon Park, Nam-Joon Kim, Jangchan Kim, Hyun Gon Ryu, Hyuk-Jae Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07048">Whisfusion: Parallel ASR Decoding via a Diffusion Transformer</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fast Automatic Speech Recognition (ASR) is critical for latency-sensitive applications such as real-time captioning and meeting transcription. However, truly parallel ASR decoding remains challenging due to the sequential nature of autoregressive (AR) decoders and the context limitations of non-autoregressive (NAR) methods. While modern ASR encoders can process up to 30 seconds of audio at once, AR decoders still generate tokens sequentially, creating a latency bottleneck. We propose Whisfusion, the first framework to fuse a pre-trained Whisper encoder with a text diffusion decoder. This NAR architecture resolves the AR latency bottleneck by processing the entire acoustic context in parallel at every decoding step. A lightweight cross-attention adapter trained via parameter-efficient fine-tuning (PEFT) bridges the two modalities. We also introduce a batch-parallel, multi-step decoding strategy that improves accuracy by increasing the number of candidates with minimal impact on speed. Fine-tuned solely on LibriSpeech (960h), Whisfusion achieves a lower WER than Whisper-tiny (8.3% vs. 9.7%), and offers comparable latency on short audio. For longer utterances (>20s), it is up to 2.6x faster than the AR baseline, establishing a new, efficient operating point for long-form ASR. The implementation and training scripts are available at https://github.com/taeyoun811/Whisfusion.<br>
<span id='abs_ch'>中文：Whisfusion是一种创新的非自回归自动语音识别框架，通过融合Whisper编 器和文本扩散解 器实现并行处理，在保持准确性的同时显著降低了长语音识别的延迟。</span><br>
<span id='abs_en'>English: Whisfusion is a novel non-autoregressive ASR framework that combines a Whisper encoder with a text diffusion decoder, enabling parallel processing to significantly reduce latency for long-form speech recognition while maintaining accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>975, <a href='https://arxiv.org/pdf/2508.06997.pdf' target='_blank'>https://arxiv.org/pdf/2508.06997.pdf</a></span>   <span><a href='https://github.com/paathelb/conformal_hai_multiple' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Helbert Paat, Guohao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06997">Conformal Set-based Human-AI Complementarity with Multiple Experts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Decision support systems are designed to assist human experts in classification tasks by providing conformal prediction sets derived from a pre-trained model. This human-AI collaboration has demonstrated enhanced classification performance compared to using either the model or the expert independently. In this study, we focus on the selection of instance-specific experts from a pool of multiple human experts, contrasting it with existing research that typically focuses on single-expert scenarios. We characterize the conditions under which multiple experts can benefit from the conformal sets. With the insight that only certain experts may be relevant for each instance, we explore the problem of subset selection and introduce a greedy algorithm that utilizes conformal sets to identify the subset of expert predictions that will be used in classifying an instance. This approach is shown to yield better performance compared to naive methods for human subset selection. Based on real expert predictions from the CIFAR-10H and ImageNet-16H datasets, our simulation study indicates that our proposed greedy algorithm achieves near-optimal subsets, resulting in improved classification performance among multiple experts.<br>
<span id='abs_ch'>Chinese: 本 究提出了一种贪心算法，利用保形预测集从多位专家中优化选择子集进行分类，在CIFAR-10H和ImageNet-16H数据集上的模拟实验表明，该方法优于简单选择策略并提升了分类性能。</span><br>
<span id='abs_en'>English: This study introduces a greedy algorithm that leverages conformal prediction sets to optimally select subsets of human experts for classification tasks, demonstrating improved performance over naive methods in simulations using CIFAR-10H and ImageNet-16H datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>976, <a href='https://arxiv.org/pdf/2508.06960.pdf' target='_blank'>https://arxiv.org/pdf/2508.06960.pdf</a></span>   <span><a href='https://github.com/GAIR-NLP/DatasetResearch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Keyu Li, Mohan Jiang, Dayuan Fu, Yunze Wu, Xiangkun Hu, Dequan Wang, Pengfei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06960">DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of large language models has fundamentally shifted the bottleneck in AI development from computational power to data availability-with countless valuable datasets remaining hidden across specialized repositories, research appendices, and domain platforms. As reasoning capabilities and deep research methodologies continue to evolve, a critical question emerges: can AI agents transcend conventional search to systematically discover any dataset that meets specific user requirements, enabling truly autonomous demand-driven data curation? We introduce DatasetResearch, the first comprehensive benchmark evaluating AI agents' ability to discover and synthesize datasets from 208 real-world demands across knowledge-intensive and reasoning-intensive tasks. Our tri-dimensional evaluation framework reveals a stark reality: even advanced deep research systems achieve only 22% score on our challenging DatasetResearch-pro subset, exposing the vast gap between current capabilities and perfect dataset discovery. Our analysis uncovers a fundamental dichotomy-search agents excel at knowledge tasks through retrieval breadth, while synthesis agents dominate reasoning challenges via structured generation-yet both catastrophically fail on "corner cases" outside existing distributions. These findings establish the first rigorous baseline for dataset discovery agents and illuminate the path toward AI systems capable of finding any dataset in the digital universe. Our benchmark and comprehensive analysis provide the foundation for the next generation of self-improving AI systems and are publicly available at https://github.com/GAIR-NLP/DatasetResearch.<br>
<span id='abs_ch'>Chinese: DatasetResearch基准测试显示，当前AI智能体在应对现实需求时仅实现22%的数据集发现成功率，尽管搜索型智能体擅长知识任务而合成型精于推理挑战，但二者均 法处理分布外极端案例，暴露出自主数据获取能力的重大缺陷。</span><br>
<span id='abs_en'>English: The DatasetResearch benchmark reveals that current AI agents achieve only 22% success in discovering datasets from real-world demands, exposing a critical gap in autonomous data curation despite a dichotomy where search agents excel in knowledge tasks and synthesis agents in reasoning challenges.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>977, <a href='https://arxiv.org/pdf/2508.06944.pdf' target='_blank'>https://arxiv.org/pdf/2508.06944.pdf</a></span>   <span><a href='https://github.com/hlxtsyj/AMFT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/hlxtsyj/AMFT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lixuan He, Jie Feng, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06944">AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) are typically fine-tuned for reasoning tasks through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL), a process fraught with catastrophic forgetting and suboptimal trade-offs between imitation and exploration. Recent single-stage methods attempt to unify SFT and RL using heuristics, but lack a principled mechanism for dynamically balancing the two paradigms. In this paper, we reframe this challenge through the theoretical lens of \textbf{implicit rewards}, viewing SFT and RL not as distinct methods but as complementary reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel single-stage algorithm that learns the optimal balance between SFT's implicit, path-level reward and RL's explicit, outcome-based reward. The core of AMFT is a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL balance as a learnable parameter, dynamically optimizing it to maximize long-term task performance. This forward-looking approach, regularized by policy entropy for stability, autonomously discovers an effective training curriculum. We conduct a comprehensive evaluation on challenging benchmarks spanning mathematical reasoning, abstract visual reasoning (General Points), and vision-language navigation (V-IRL). AMFT consistently establishes a new state-of-the-art and demonstrats superior generalization on out-of-distribution (OOD) tasks. Ablation studies and training dynamic analysis confirm that the meta-learning controller is crucial for AMFT's stability, sample efficiency, and performance, offering a more principled and effective paradigm for LLM alignment. Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.<br>
<span id='abs_ch'>中文: 本文提出自适应元微调（AMFT）算法，通过元梯度控制器动态平衡监督微调与强化学 ，在多项推理任务中实现了最优性能并展现出卓越的泛化能力。</span><br>
<span id='abs_en'>English: This paper introduces Adaptive Meta Fine-Tuning (AMFT), a single-stage algorithm that dynamically balances supervised fine-tuning and reinforcement learning through meta-gradient control to achieve state-of-the-art performance across multiple reasoning tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>978, <a href='https://arxiv.org/pdf/2508.06800.pdf' target='_blank'>https://arxiv.org/pdf/2508.06800.pdf</a></span>   <span><a href='https://github.com/HARDY-MER/HARDY-MER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Liu, Haolin Zuo, Zheng Lian, Hongyu Yuan, Qi Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06800">Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Missing modalities have recently emerged as a critical research direction in multimodal emotion recognition (MER). Conventional approaches typically address this issue through missing modality reconstruction. However, these methods fail to account for variations in reconstruction difficulty across different samples, consequently limiting the model's ability to handle hard samples effectively. To overcome this limitation, we propose a novel Hardness-Aware Dynamic Curriculum Learning framework, termed HARDY-MER. Our framework operates in two key stages: first, it estimates the hardness level of each sample, and second, it strategically emphasizes hard samples during training to enhance model performance on these challenging instances. Specifically, we first introduce a Multi-view Hardness Evaluation mechanism that quantifies reconstruction difficulty by considering both Direct Hardness (modality reconstruction errors) and Indirect Hardness (cross-modal mutual information). Meanwhile, we introduce a Retrieval-based Dynamic Curriculum Learning strategy that dynamically adjusts the training curriculum by retrieving samples with similar semantic information and balancing the learning focus between easy and hard instances. Extensive experiments on benchmark datasets demonstrate that HARDY-MER consistently outperforms existing methods in missing-modality scenarios. Our code will be made publicly available at https://github.com/HARDY-MER/HARDY-MER.<br>
<span id='abs_ch'>中文摘要：提出的HARDY-MER框架通过多视角难度评估机制量化 本重建难度，并采用基于检索的动态课程学 策略重点训练困难 本，在缺失模态的多模态情感识别任务中展现出优越性能。</span><br>
<span id='abs_en'>English Summary: The proposed HARDY-MER framework introduces a hardness-aware dynamic curriculum learning approach that evaluates sample difficulty through multi-view metrics and strategically prioritizes challenging instances during training, demonstrating superior performance in multimodal emotion recognition with missing modalities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>979, <a href='https://arxiv.org/pdf/2508.06729.pdf' target='_blank'>https://arxiv.org/pdf/2508.06729.pdf</a></span>   <span><a href='https://github.com/kc6699c/LLM4OralHistoryAnalysis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Komala Subramanyam Cherukuri, Pranav Abishai Moses, Aisa Sakata, Jiangping Chen, Haihua Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06729">Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Oral histories are vital records of lived experience, particularly within communities affected by systemic injustice and historical erasure. Effective and efficient analysis of their oral history archives can promote access and understanding of the oral histories. However, Large-scale analysis of these archives remains limited due to their unstructured format, emotional complexity, and high annotation costs. This paper presents a scalable framework to automate semantic and sentiment annotation for Japanese American Incarceration Oral History. Using LLMs, we construct a high-quality dataset, evaluate multiple models, and test prompt engineering strategies in historically sensitive contexts. Our multiphase approach combines expert annotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We labeled 558 sentences from 15 narrators for sentiment and semantic classification, then evaluated zero-shot, few-shot, and RAG strategies. For semantic classification, ChatGPT achieved the highest F1 score (88.71%), followed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama slightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models showing comparable results. The best prompt configurations were used to annotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our findings show that LLMs can effectively perform semantic and sentiment annotation across large oral history collections when guided by well-designed prompts. This study provides a reusable annotation pipeline and practical guidance for applying LLMs in culturally sensitive archival analysis. By bridging archival ethics with scalable NLP techniques, this work lays the groundwork for responsible use of artificial intelligence in digital humanities and preservation of collective memory. GitHub: https://github.com/kc6699c/LLM4OralHistoryAnalysis.<br>
<span id='abs_ch'>中文摘要：本 究提出一个可扩展框架，利用大语言模型对日裔美国人拘禁口述历史进行自动化语义与情感 注，证明精心设计的提示词能有效分析大规模档案，同时兼顾文化敏感材料的伦理考量。</span><br>
<span id='abs_en'>English Summary: This study introduces a scalable framework using large language models to automate semantic and sentiment annotation for Japanese American incarceration oral histories, demonstrating that well-designed prompts enable effective analysis of large collections while addressing ethical considerations in culturally sensitive archives.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>980, <a href='https://arxiv.org/pdf/2508.06701.pdf' target='_blank'>https://arxiv.org/pdf/2508.06701.pdf</a></span>   <span><a href='https://github.com/rezwanh001/Large-Scale-Multimodal-Depression-Detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Rezwanul Haque, Md. Milon Islam, S M Taslim Uddin Raju, Hamdi Altaheri, Lobna Nassar, Fakhri Karray
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06701">MMFformer: Multimodal Fusion Transformer Network for Depression Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Depression is a serious mental health illness that significantly affects an individual's well-being and quality of life, making early detection crucial for adequate care and treatment. Detecting depression is often difficult, as it is based primarily on subjective evaluations during clinical interviews. Hence, the early diagnosis of depression, thanks to the content of social networks, has become a prominent research area. The extensive and diverse nature of user-generated information poses a significant challenge, limiting the accurate extraction of relevant temporal information and the effective fusion of data across multiple modalities. This paper introduces MMFformer, a multimodal depression detection network designed to retrieve depressive spatio-temporal high-level patterns from multimodal social media information. The transformer network with residual connections captures spatial features from videos, and a transformer encoder is exploited to design important temporal dynamics in audio. Moreover, the fusion architecture fused the extracted features through late and intermediate fusion strategies to find out the most relevant intermodal correlations among them. Finally, the proposed network is assessed on two large-scale depression detection datasets, and the results clearly reveal that it surpasses existing state-of-the-art approaches, improving the F1-Score by 13.92% for D-Vlog dataset and 7.74% for LMVD dataset. The code is made available publicly at https://github.com/rezwanh001/Large-Scale-Multimodal-Depression-Detection.<br>
<span id='abs_ch'>中文: 本文提出的MMFformer多模态网络通过从社交媒体数据中提取时空特征来检测抑郁，在基准数据集上的表现显著优于现有方法。</span><br>
<span id='abs_en'>English: This paper introduces MMFformer, a multimodal network that effectively detects depression by extracting spatio-temporal patterns from social media data, significantly outperforming existing methods on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>981, <a href='https://arxiv.org/pdf/2508.06627.pdf' target='_blank'>https://arxiv.org/pdf/2508.06627.pdf</a></span>   <span><a href='https://github.com/MosbahAouad/EarlyPDAC-MML' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mosbah Aouad, Anirudh Choudhary, Awais Farooq, Steven Nevers, Lusine Demirkhanyan, Bhrandon Harris, Suguna Pappu, Christopher Gondi, Ravishankar Iyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06627">Early Detection of Pancreatic Cancer Using Multimodal Learning on Electronic Health Records</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest cancers, and early detection remains a major clinical challenge due to the absence of specific symptoms and reliable biomarkers. In this work, we propose a new multimodal approach that integrates longitudinal diagnosis code histories and routinely collected laboratory measurements from electronic health records to detect PDAC up to one year prior to clinical diagnosis. Our method combines neural controlled differential equations to model irregular lab time series, pretrained language models and recurrent networks to learn diagnosis code trajectory representations, and cross-attention mechanisms to capture interactions between the two modalities. We develop and evaluate our approach on a real-world dataset of nearly 4,700 patients and achieve significant improvements in AUC ranging from 6.5% to 15.5% over state-of-the-art methods. Furthermore, our model identifies diagnosis codes and laboratory panels associated with elevated PDAC risk, including both established and new biomarkers. Our code is available at https://github.com/MosbahAouad/EarlyPDAC-MML.<br>
<span id='abs_ch'>中文: 本 究提出一种多模态方法，利用电子健康记录提前一年预测胰腺癌，显著提升检测准确性并识别出关键风险指 。</span><br>
<span id='abs_en'>English: This study introduces a multimodal method using electronic health records to detect pancreatic cancer up to a year early, significantly improving prediction accuracy and identifying key risk factors.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>982, <a href='https://arxiv.org/pdf/2508.06595.pdf' target='_blank'>https://arxiv.org/pdf/2508.06595.pdf</a></span>   <span><a href='https://github.com/xyzhu123/Synthetic_Textbook' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyuan Zhu, Muru Zhang, Ollie Liu, Robin Jia, Willie Neiswanger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06595">LLM Unlearning Without an Expert Curated Dataset</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Modern large language models often encode sensitive, harmful, or copyrighted knowledge, raising the need for post-hoc unlearning-the ability to remove specific domains of knowledge from a model without full retraining. A major bottleneck in current unlearning pipelines is constructing effective forget sets-datasets that approximate the target domain and guide the model to forget it. In this work, we introduce a scalable, automated approach to generate high-quality forget sets using language models themselves. Our method synthesizes textbook-style data through a structured prompting pipeline, requiring only a domain name as input. Through experiments on unlearning biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic datasets consistently outperform the baseline synthetic alternatives and are comparable to the expert-curated ones. Additionally, ablation studies reveal that the multi-step generation pipeline significantly boosts data diversity, which in turn improves unlearning utility. Overall, our findings suggest that synthetic datasets offer a promising path toward practical, scalable unlearning for a wide range of emerging domains without the need for manual intervention. We release our code and dataset at https://github.com/xyzhu123/Synthetic_Textbook.<br>
<br>
<div id='section'>Paperid: <span id='pid'>983, <a href='https://arxiv.org/pdf/2508.06595.pdf' target='_blank'>https://arxiv.org/pdf/2508.06595.pdf</a></span>   <span><a href='https://github.com/xyzhu123/Synthetic_Textbook' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyuan Zhu, Muru Zhang, Ollie Liu, Robin Jia, Willie Neiswanger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06595">LLM Unlearning Without an Expert Curated Dataset</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Modern large language models often encode sensitive, harmful, or copyrighted knowledge, raising the need for post-hoc unlearning-the ability to remove specific domains of knowledge from a model without full retraining. A major bottleneck in current unlearning pipelines is constructing effective forget sets-datasets that approximate the target domain and guide the model to forget it. In this work, we introduce a scalable, automated approach to generate high-quality forget sets using language models themselves. Our method synthesizes textbook-style data through a structured prompting pipeline, requiring only a domain name as input. Through experiments on unlearning biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic datasets consistently outperform the baseline synthetic alternatives and are comparable to the expert-curated ones. Additionally, ablation studies reveal that the multi-step generation pipeline significantly boosts data diversity, which in turn improves unlearning utility. Overall, our findings suggest that synthetic datasets offer a promising path toward practical, scalable unlearning for a wide range of emerging domains without the need for manual intervention. We release our code and dataset at https://github.com/xyzhu123/Synthetic_Textbook.<br>
<br>
<div id='section'>Paperid: <span id='pid'>984, <a href='https://arxiv.org/pdf/2508.06526.pdf' target='_blank'>https://arxiv.org/pdf/2508.06526.pdf</a></span>   <span><a href='https://github.com/NoakLiu/PiKVpress' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/NoakLiu/PiKV' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/NoakLiu/PiKV' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong Liu, Yanxuan Yu, Ben Lengerich, Ying Nian Wu, Xuhong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06526">PiKV: KV Cache Management System for Mixture of Experts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models continue to scale up in both size and context length, the memory and communication cost of key-value (KV) cache storage has become a major bottleneck in multi-GPU and multi-node inference. While MoE-based architectures sparsify computation across experts, the corresponding KV caches remain dense and globally synchronized, resulting in significant overhead.
  We introduce \textbf{PiKV}, a parallel and distributed KV cache serving framework tailored for MoE architecture. PiKV leverages \textit{expert-sharded KV storage} to partition caches across GPUs, \textit{PiKV routing} to reduce token-to-KV access, and a \textit{PiKV Scheduling} to adaptively retain query-relevant entries. To further reduce memory usage, PiKV integrates \textit{PiKV Compression} modules the caching pipeline for acceleration.
  PiKV is recently publicly available as an open-source software library: \href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}. Experiments details is recorded at: \href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\_Results}. We also have PiKV integrated with Nvidia kvpress for acceleration, details see \href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}. PiKV is still a living project, aiming to become a comprehesive KV Cache management system for MoE Architectures.<br>
<br>
<div id='section'>Paperid: <span id='pid'>985, <a href='https://arxiv.org/pdf/2508.06499.pdf' target='_blank'>https://arxiv.org/pdf/2508.06499.pdf</a></span>   <span><a href='https://github.com/Corsi01/algo2025' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Corsico, Giorgia Rigamonti, Simone Zini, Luigi Celona, Paolo Napoletano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06499">Network-Specific Models for Multimodal Brain Response Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this work, we present a network-specific approach for predicting brain responses to complex multimodal movies, leveraging the Yeo 7-network parcellation of the Schaefer atlas. Rather than treating the brain as a homogeneous system, we grouped the seven functional networks into four clusters and trained separate multi-subject, multi-layer perceptron (MLP) models for each. This architecture supports cluster-specific optimization and adaptive memory modeling, allowing each model to adjust temporal dynamics and modality weighting based on the functional role of its target network. Our results demonstrate that this clustered strategy significantly enhances prediction accuracy across the 1,000 cortical regions of the Schaefer atlas. The final model achieved an eighth-place ranking in the Algonauts Project 2025 Challenge, with out-of-distribution (OOD) correlation scores nearly double those of the baseline model used in the selection phase. Code is available at https://github.com/Corsi01/algo2025.<br>
<span id='abs_ch'>中文: 本 究提出了一种基于功能网络分组的特异性方法，通过训练集群化模型预测大脑对多模态电影的反应，显著提升了预测精度，并在Algonauts 2025挑战赛中取得优异排名。</span><br>
<span id='abs_en'>English: This study introduces a network-specific method using clustered functional networks to predict brain responses to multimodal movies, significantly improving accuracy and achieving top performance in the Algonauts Project 2025 Challenge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>986, <a href='https://arxiv.org/pdf/2508.06485.pdf' target='_blank'>https://arxiv.org/pdf/2508.06485.pdf</a></span>   <span><a href='https://github.com/Sofianebouaziz1/WGAST.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sofiane Bouaziz, Adel Hafiane, Raphael Canals, Rachid Nedjai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06485">WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Urbanization, climate change, and agricultural stress are increasing the demand for precise and timely environmental monitoring. Land Surface Temperature (LST) is a key variable in this context and is retrieved from remote sensing satellites. However, these systems face a trade-off between spatial and temporal resolution. While spatio-temporal fusion methods offer promising solutions, few have addressed the estimation of daily LST at 10 m resolution. In this study, we present WGAST, a Weakly-Supervised Generative Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning framework designed for this task. It adopts a conditional generative adversarial architecture, with a generator composed of four stages: feature extraction, fusion, LST reconstruction, and noise suppression. The first stage employs a set of encoders to extract multi-level latent representations from the inputs, which are then fused in the second stage using cosine similarity, normalization, and temporal attention mechanisms. The third stage decodes the fused features into high-resolution LST, followed by a Gaussian filter to suppress high-frequency noise. Training follows a weakly supervised strategy based on physical averaging principles and reinforced by a PatchGAN discriminator. Experiments demonstrate that WGAST outperforms existing methods in both quantitative and qualitative evaluations. Compared to the best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and effectively captures fine-scale thermal patterns, as validated against 33 ground-based sensors. The code is available at https://github.com/Sofianebouaziz1/WGAST.git.<br>
<span id='abs_ch'>中文: 本 究提出了WGAST，首个端到端深度学 框架，通过弱监督生成网络融合多卫星数据，实现了10米分辨率的日地表温度估算，在环境监测中展现出卓越的精度和鲁棒性。</span><br>
<span id='abs_en'>English: This study introduces WGAST, the first end-to-end deep learning framework that uses a weakly-supervised generative network to estimate daily 10-meter resolution land surface temperature by fusing data from multiple satellites, achieving superior accuracy and robustness in environmental monitoring.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>987, <a href='https://arxiv.org/pdf/2508.06453.pdf' target='_blank'>https://arxiv.org/pdf/2508.06453.pdf</a></span>   <span><a href='https://github.com/ruida/LLM-Swin-UMamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruida Cheng, Tejas Sudharshan Mathai, Pritam Mukherjee, Benjamin Hou, Qingqing Zhu, Zhiyong Lu, Matthew McAuliffe, Ronald M. Summers
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06453">Text Embedded Swin-UMamba for DeepLesion Segmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Segmentation of lesions on CT enables automatic measurement for clinical assessment of chronic diseases (e.g., lymphoma). Integrating large language models (LLMs) into the lesion segmentation workflow offers the potential to combine imaging features with descriptions of lesion characteristics from the radiology reports. In this study, we investigate the feasibility of integrating text into the Swin-UMamba architecture for the task of lesion segmentation. The publicly available ULS23 DeepLesion dataset was used along with short-form descriptions of the findings from the reports. On the test dataset, a high Dice Score of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for lesion segmentation. The proposed Text-Swin-UMamba model outperformed prior approaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p < 0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by 1.74% and 0.22%, respectively. The dataset and code can be accessed at https://github.com/ruida/LLM-Swin-UMamba<br>
<br>
<div id='section'>Paperid: <span id='pid'>988, <a href='https://arxiv.org/pdf/2508.06434.pdf' target='_blank'>https://arxiv.org/pdf/2508.06434.pdf</a></span>   <span><a href='https://github.com/T6Yang/CLIPin' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengzhu Yang, Jiawei Du, Shuai Lu, Weihang Zhang, Ningli Wang, Huiqi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06434">CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large-scale natural image-text datasets, especially those automatically collected from the web, often suffer from loose semantic alignment due to weak supervision, while medical datasets tend to have high cross-modal correlation but low content diversity. These properties pose a common challenge for contrastive language-image pretraining (CLIP): they hinder the model's ability to learn robust and generalizable representations. In this work, we propose CLIPin, a unified non-contrastive plug-in that can be seamlessly integrated into CLIP-style architectures to improve multimodal semantic alignment, providing stronger supervision and enhancing alignment robustness. Furthermore, two shared pre-projectors are designed for image and text modalities respectively to facilitate the integration of contrastive and non-contrastive learning in a parameter-compromise manner. Extensive experiments on diverse downstream tasks demonstrate the effectiveness and generality of CLIPin as a plug-and-play component compatible with various contrastive frameworks. Code is available at https://github.com/T6Yang/CLIPin.<br>
<span id='abs_ch'>中文：提出的CLIPin框架通过非对比插件和共享预投影器，增强了CLIP类模型的多模态语义对齐能力，在不同任务中提升了鲁棒性和泛化性能。</span><br>
<span id='abs_en'>English: The proposed CLIPin framework enhances multimodal semantic alignment in CLIP-style models through a non-contrastive plug-in and shared pre-projectors, improving robustness and generalization across diverse tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>989, <a href='https://arxiv.org/pdf/2508.06429.pdf' target='_blank'>https://arxiv.org/pdf/2508.06429.pdf</a></span>   <span><a href='https://github.com/GuidoManni/SPARSE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guido Manni, Clemente Lauretti, Loredana Zollo, Paolo Soda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06429">SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Deep learning has revolutionized medical imaging, but its effectiveness is severely limited by insufficient labeled training data. This paper introduces a novel GAN-based semi-supervised learning framework specifically designed for low labeled-data regimes, evaluated across settings with 5 to 50 labeled samples per class. Our approach integrates three specialized neural networks -- a generator for class-conditioned image translation, a discriminator for authenticity assessment and classification, and a dedicated classifier -- within a three-phase training framework. The method alternates between supervised training on limited labeled data and unsupervised learning that leverages abundant unlabeled images through image-to-image translation rather than generation from noise. We employ ensemble-based pseudo-labeling that combines confidence-weighted predictions from the discriminator and classifier with temporal consistency through exponential moving averaging, enabling reliable label estimation for unlabeled data. Comprehensive evaluation across eleven MedMNIST datasets demonstrates that our approach achieves statistically significant improvements over six state-of-the-art GAN-based semi-supervised methods, with particularly strong performance in the extreme 5-shot setting where the scarcity of labeled data is most challenging. The framework maintains its superiority across all evaluated settings (5, 10, 20, and 50 shots per class). Our approach offers a practical solution for medical imaging applications where annotation costs are prohibitive, enabling robust classification performance even with minimal labeled data. Code is available at https://github.com/GuidoManni/SPARSE.<br>
<span id='abs_ch'>中文: 本文提出了一种基于GAN的半监督学 框架，通过整合图像翻译和集成伪 记技术，有效解决了医学影像中 注数据稀缺的难题，在每类仅五个 注 本的极端条件下仍能实现卓越的分类性能。</span><br>
<span id='abs_en'>English: This paper presents a GAN-based semi-supervised learning framework that effectively addresses the challenge of limited labeled data in medical imaging by integrating image translation and ensemble pseudo-labeling, achieving superior performance across multiple datasets with as few as five labeled samples per class.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>990, <a href='https://arxiv.org/pdf/2508.06259.pdf' target='_blank'>https://arxiv.org/pdf/2508.06259.pdf</a></span>   <span><a href='https://github.com/zhangquanchen/SIFThinker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhangquan Chen, Ruihui Zhao, Chuwei Luo, Mingze Sun, Xinlei Yu, Yangyang Kang, Ruqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06259">SIFThinker: Spatially-Aware Image Focus for Visual Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current multimodal large language models (MLLMs) still face significant challenges in complex visual tasks (e.g., spatial understanding, fine-grained perception). Prior methods have tried to incorporate visual reasoning, however, they fail to leverage attention correction with spatial cues to iteratively refine their focus on prompt-relevant regions. In this paper, we introduce SIFThinker, a spatially-aware "think-with-images" framework that mimics human visual perception. Specifically, SIFThinker enables attention correcting and image region focusing by interleaving depth-enhanced bounding boxes and natural language. Our contributions are twofold: First, we introduce a reverse-expansion-forward-inference strategy that facilitates the generation of interleaved image-text chains of thought for process-level supervision, which in turn leads to the construction of the SIF-50K dataset. Besides, we propose GRPO-SIF, a reinforced training paradigm that integrates depth-informed visual grounding into a unified reasoning pipeline, teaching the model to dynamically correct and focus on prompt-relevant regions. Extensive experiments demonstrate that SIFThinker outperforms state-of-the-art methods in spatial understanding and fine-grained visual perception, while maintaining strong general capabilities, highlighting the effectiveness of our method. Code: https://github.com/zhangquanchen/SIFThinker.<br>
<span id='abs_ch'>中文: 当前多模态大语言模型在复杂视觉任务中仍面临挑战，而SIFThinker提出了一种空间感知框架，通过深度增强边界框和自然语言动态 正注意力并聚焦相关区域，在空间理解和细粒度感知方面超越了现有最优方法。</span><br>
<span id='abs_en'>English: Current multimodal large language models struggle with complex visual tasks, but SIFThinker introduces a spatially-aware framework that uses depth-enhanced bounding boxes and natural language to dynamically correct attention and focus on relevant regions, outperforming state-of-the-art methods in spatial understanding and fine-grained perception.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>991, <a href='https://arxiv.org/pdf/2508.06165.pdf' target='_blank'>https://arxiv.org/pdf/2508.06165.pdf</a></span>   <span><a href='https://github.com/Tsinghua-dhy/UR2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weitao Li, Boran Xiang, Xiaolong Wang, Zhinan Gou, Weizhi Ma, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06165">UR$^2$: Unify RAG and Reasoning through Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have shown remarkable capabilities through two complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR), which optimizes complex reasoning abilities. However, these two capabilities are often developed in isolation, and existing efforts to unify them remain narrow in scope -- typically limited to open-domain QA with fixed retrieval settings and task-specific constraints. This lack of integration constrains generalization and limits the applicability of RAG-RL methods to broader domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a general framework that unifies retrieval and reasoning through reinforcement learning. UR2 introduces two key contributions: a difficulty-aware curriculum training that selectively invokes retrieval only for challenging problems, and a hybrid knowledge access strategy combining domain-specific offline corpora with LLM-generated summaries. These components are designed to enable dynamic coordination between retrieval and reasoning, improving adaptability across a diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical, and mathematical reasoning tasks demonstrate that UR$^2$ (built on Qwen-2.5-3/7B and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several benchmarks. We have released all code, models, and data at https://github.com/Tsinghua-dhy/UR2.<br>
<span id='abs_ch'>中文: UR2框架通过难度感知课程训练和混合知识访问策略，将检索增强生成与可验证奖励的强化学 相统一，在多项基准测试中显著优于现有方法。English: The UR2 framework unifies retrieval-augmented generation and reinforcement learning with verifiable rewards through difficulty-aware curriculum training and hybrid knowledge access, significantly outperforming existing methods across multiple benchmarks.Paperid:870,https://arxiv.org/pdf/2508.06160.pdfGitHub</span><br>
<span id='abs_en'>English: The UR2 framework unifies retrieval-augmented generation and reinforcement learning with verifiable rewards through difficulty-aware curriculum training and hybrid knowledge access, significantly outperforming existing methods across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>992, <a href='https://arxiv.org/pdf/2508.06021.pdf' target='_blank'>https://arxiv.org/pdf/2508.06021.pdf</a></span>   <span><a href='https://github.com/utkuozbulak/svp-generative-ai' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Utku Ozbulak, Michaela Cohrs, Hristo L. Svilenov, Joris Vankerschaver, Wesley De Neve
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06021">Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Sub-visible particle analysis using flow imaging microscopy combined with deep learning has proven effective in identifying particle types, enabling the distinction of harmless components such as silicone oil from protein particles. However, the scarcity of available data and severe imbalance between particle types within datasets remain substantial hurdles when applying multi-class classifiers to such problems, often forcing researchers to rely on less effective methods. The aforementioned issue is particularly challenging for particle types that appear unintentionally and in lower numbers, such as silicone oil and air bubbles, as opposed to protein particles, where obtaining large numbers of images through controlled settings is comparatively straightforward. In this work, we develop a state-of-the-art diffusion model to address data imbalance by generating high-fidelity images that can augment training datasets, enabling the effective training of multi-class deep neural networks. We validate this approach by demonstrating that the generated samples closely resemble real particle images in terms of visual quality and structure. To assess the effectiveness of using diffusion-generated images in training datasets, we conduct large-scale experiments on a validation dataset comprising 500,000 protein particle images and demonstrate that this approach improves classification performance with no negligible downside. Finally, to promote open research and reproducibility, we publicly release both our diffusion models and the trained multi-class deep neural network classifiers, along with a straightforward interface for easy integration into future studies, at https://github.com/utkuozbulak/svp-generative-ai.<br>
<span id='abs_ch'>Chinese: 本 究开发了一种先进的扩散模型，通过生成高质量粒子图像有效解决了亚可见颗粒分析中数据稀缺和类别不平衡的问题，从而提升了多类深度神经网络的分类性能且 明显弊端。</span><br>
<span id='abs_en'>English: This study introduces a state-of-the-art diffusion model to generate high-fidelity particle images, effectively addressing data scarcity and imbalance in training multi-class deep neural networks for sub-visible particle analysis, thereby improving classification performance without significant drawbacks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>993, <a href='https://arxiv.org/pdf/2508.05731.pdf' target='_blank'>https://arxiv.org/pdf/2508.05731.pdf</a></span>   <span><a href='https://github.com/InfiXAI/InfiGUI-G1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Liu, Zeyu Liu, Shuanghe Zhu, Pengxiang Li, Congkai Xie, Jiasheng Wang, Xueyu Hu, Xiaotian Han, Jianbo Yuan, Xinyao Wang, Shengyu Zhang, Hongxia Yang, Fei Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05731">InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The emergence of Multimodal Large Language Models (MLLMs) has propelled the development of autonomous agents that operate on Graphical User Interfaces (GUIs) using pure visual input. A fundamental challenge is robustly grounding natural language instructions. This requires a precise spatial alignment, which accurately locates the coordinates of each element, and, more critically, a correct semantic alignment, which matches the instructions to the functionally appropriate UI element. Although Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be effective at improving spatial alignment for these MLLMs, we find that inefficient exploration bottlenecks semantic alignment, which prevent models from learning difficult semantic associations. To address this exploration problem, we present Adaptive Exploration Policy Optimization (AEPO), a new policy optimization framework. AEPO employs a multi-answer generation strategy to enforce broader exploration, which is then guided by a theoretically grounded Adaptive Exploration Reward (AER) function derived from first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B and InfiGUI-G1-7B, establish new state-of-the-art results across multiple challenging GUI grounding benchmarks, achieving significant relative improvements of up to 9.0% against the naive RLVR baseline on benchmarks designed to test generalization and semantic understanding. Resources are available at https://github.com/InfiXAI/InfiGUI-G1.<br>
<span id='abs_ch'>中文摘要：本 究提出自适应探索策略优化（AEPO）方法，通过多答案生成策略和理论推导的自适应探索奖励函数，有效提升多模态大语言模型在图形用户界面中的语义对齐能力，在多项基准测试中创下性能新纪录。</span><br>
<span id='abs_en'>English Summary: The study introduces Adaptive Exploration Policy Optimization (AEPO) to enhance semantic alignment in Multimodal Large Language Models for GUI interactions, achieving state-of-the-art performance on grounding benchmarks with significant improvements over baseline methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>994, <a href='https://arxiv.org/pdf/2508.05728.pdf' target='_blank'>https://arxiv.org/pdf/2508.05728.pdf</a></span>   <span><a href='https://github.com/santiagocasas/clapp,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Santiago Casas, Christian Fidler, Boris Bolliet, Francisco Villaescusa-Navarro, Julien Lesgourgues
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05728">CLAPP: The CLASS LLM Agent for Pair Programming</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce CLAPP (CLASS LLM Agent for Pair Programming), an interactive AI assistant designed to support researchers working with the Einstein-Boltzmann solver CLASS. CLAPP leverages large language models (LLMs) and domain-specific retrieval to provide conversational coding support for CLASS-answering questions, generating code, debugging errors, and producing plots. Its architecture combines multi-agent LLM orchestration, semantic search across CLASS documentation, and a live Python execution environment. Deployed as a user-friendly web application, CLAPP lowers the entry barrier for scientists unfamiliar with AI tools and enables more productive human-AI collaboration in computational and numerical cosmology. The app is available at https://classclapp.streamlit.app<br>
<br>
<div id='section'>Paperid: <span id='pid'>995, <a href='https://arxiv.org/pdf/2508.05710.pdf' target='_blank'>https://arxiv.org/pdf/2508.05710.pdf</a></span>   <span><a href='https://github.com/Kwai-Klear/CodeTest' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia Fu, Xinyu Yang, Hongzhi Zhang, Yahui Liu, Jingyuan Zhang, Qi Wang, Fuzheng Zhang, Guorui Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05710">Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Precise, correct feedback is crucial for effectively training large language models (LLMs) in code reinforcement learning. However, synthesizing high-quality test cases remains a profoundly challenging and unsolved problem. In this work, we present Klear-CodeTest, a comprehensive test case synthesis framework featuring rigorous verification to ensure quality and reliability of test cases. Our approach achieves broad coverage of programming problems via a novel Generator-Validation (G-V) framework, ensuring correctness through a consistency validation mechanism that verifies outputs against gold solutions. The proposed G-V framework generates comprehensive test cases including both regular and corner cases, enhancing test coverage and discriminative power for solution correctness assessment in code reinforcement learning. In addition, we design a multi-layered security sandbox system optimized for online verification platforms, guaranteeing safe and reliable code execution. Through comprehensive experiments, we demonstrate the effectiveness of our curated dataset, showing significant improvements in model performance and training stability. The source codes, curated dataset and sandbox system are available at: https://github.com/Kwai-Klear/CodeTest.<br>
<span id='abs_ch'>中文: Klear-CodeTest通过生成器-验证框架和多层安全沙箱，为代 强化学 合成高质量测试用例，凭借全面覆盖和可 验证显著提升了大语言模型的训练效果。</span><br>
<span id='abs_en'>English: Klear-CodeTest introduces a Generator-Validation framework with multi-layered security to synthesize high-quality test cases for code reinforcement learning, significantly improving LLM training through comprehensive coverage and reliable verification.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>996, <a href='https://arxiv.org/pdf/2508.05705.pdf' target='_blank'>https://arxiv.org/pdf/2508.05705.pdf</a></span>   <span><a href='https://github.com/mosqueralopez/T1DSim_AI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Valentina Roquemen-Echeverri, Taisa Kushner, Peter G. Jacobs, Clara Mosquera-Lopez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05705">A Physiologically-Constrained Neural Network Digital Twin Framework for Replicating Glucose Dynamics in Type 1 Diabetes</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Simulating glucose dynamics in individuals with type 1 diabetes (T1D) is critical for developing personalized treatments and supporting data-driven clinical decisions. Existing models often miss key physiological aspects and are difficult to individualize. Here, we introduce physiologically-constrained neural network (NN) digital twins to simulate glucose dynamics in T1D. To ensure interpretability and physiological consistency, we first build a population-level NN state-space model aligned with a set of ordinary differential equations (ODEs) describing glucose regulation. This model is formally verified to conform to known T1D dynamics. Digital twins are then created by augmenting the population model with individual-specific models, which include personal data, such as glucose management and contextual information, capturing both inter- and intra-individual variability. We validate our approach using real-world data from the T1D Exercise Initiative study. Two weeks of data per participant were split into 5-hour sequences and simulated glucose profiles were compared to observed ones. Clinically relevant outcomes were used to assess similarity via paired equivalence t-tests with predefined clinical equivalence margins. Across 394 digital twins, glucose outcomes were equivalent between simulated and observed data: time in range (70-180 mg/dL) was 75.1$\pm$21.2% (simulated) vs. 74.4$\pm$15.4% (real; P<0.001); time below range (<70 mg/dL) 2.5$\pm$5.2% vs. 3.0$\pm$3.3% (P=0.022); and time above range (>180 mg/dL) 22.4$\pm$22.0% vs. 22.6$\pm$15.9% (P<0.001). Our framework can incorporate unmodeled factors like sleep and activity while preserving key dynamics. This approach enables personalized in silico testing of treatments, supports insulin optimization, and integrates physics-based and data-driven modeling. Code: https://github.com/mosqueralopez/T1DSim_AI<br>
<br>
<div id='section'>Paperid: <span id='pid'>997, <a href='https://arxiv.org/pdf/2508.05691.pdf' target='_blank'>https://arxiv.org/pdf/2508.05691.pdf</a></span>   <span><a href='https://github.com/PSMLab/authprint' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Yao, Marc Juarez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05691">AuthPrint: Fingerprinting Generative Models Against Malicious Model Providers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Generative models are increasingly adopted in high-stakes domains, yet current deployments offer no mechanisms to verify whether a given output truly originates from the certified model. We address this gap by extending model fingerprinting techniques beyond the traditional collaborative setting to one where the model provider itself may act adversarially, replacing the certified model with a cheaper or lower-quality substitute. To our knowledge, this is the first work to study fingerprinting for provenance attribution under such a threat model. Our approach introduces a trusted verifier that, during a certification phase, extracts hidden fingerprints from the authentic model's output space and trains a detector to recognize them. During verification, this detector can determine whether new outputs are consistent with the certified model, without requiring specialized hardware or model modifications. In extensive experiments, our methods achieve near-zero FPR@95%TPR on both GANs and diffusion models, and remain effective even against subtle architectural or training changes. Furthermore, the approach is robust to adaptive adversaries that actively manipulate outputs in an attempt to evade detection.<br>
<span id='abs_ch'>中文摘要：本 究提出了一种指纹识别方法，用于验证生成模型输出是否来自认证模型，即使提供商可能替换模型，也能在不改变硬件的情况下实现高精度检测。</span><br>
<span id='abs_en'>English Summary: This study introduces a fingerprinting method to verify if generative model outputs originate from certified models, even when providers may substitute them, achieving high detection accuracy without hardware changes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>998, <a href='https://arxiv.org/pdf/2508.05675.pdf' target='_blank'>https://arxiv.org/pdf/2508.05675.pdf</a></span>   <span><a href='https://github.com/friyawang/VeriOptim' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Wang, Zheng Li, Lei Li, Fan He, Liyu Lin, Yao Lai, Yan Li, Xiaoyang Zeng, Yufeng Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05675">Principle-Guided Verilog Optimization: IP-Safe Knowledge Transfer via Local-Cloud Collaboration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent years have witnessed growing interest in adopting large language models (LLMs) for Register Transfer Level (RTL) code optimization. While powerful cloud-based LLMs offer superior optimization capabilities, they pose unacceptable intellectual property (IP) leakage risks when processing proprietary hardware designs. In this paper, we propose a new scenario where Verilog code must be optimized for specific attributes without leaking sensitive IP information. We introduce the first IP-preserving edge-cloud collaborative framework that leverages the benefits of both paradigms. Our approach employs local small LLMs (e.g., Qwen-2.5-Coder-7B) to perform secure comparative analysis between paired high-quality target designs and novice draft codes, yielding general design principles that summarize key insights for improvements. These principles are then used to query stronger cloud LLMs (e.g., Deepseek-V3) for targeted code improvement, ensuring that only abstracted and IP-safe guidance reaches external services. Our experimental results demonstrate that the framework achieves significantly higher optimization success rates compared to baseline methods. For example, combining Qwen-2.5-Coder-7B and Deepseek-V3 achieves a 66.67\% optimization success rate for power utilization, outperforming Deepseek-V3 alone (49.81\%) and even commercial models like GPT-4o (55.81\%). Further investigation of local and cloud LLM combinations reveals that different model pairings exhibit varying strengths for specific optimization objectives, with interesting trends emerging when varying the number of comparative code pairs. Our work establishes a new paradigm for secure hardware design optimization that balances performance gains with IP protection.<br>
<span id='abs_ch'>中文: 本文提出了一种保护知识产权的边云协同框架，通过本地小型大语言模型进行安全对比分析提取设计原则，再指导云端强大模型优化RTL代 ，在实现性能提升的同时有效防止敏感信息泄露。</span><br>
<span id='abs_en'>English: This paper introduces an IP-preserving edge-cloud collaborative framework that uses local small LLMs for secure comparative analysis to extract design principles, which then guide powerful cloud LLMs to optimize RTL code while preventing IP leakage.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>999, <a href='https://arxiv.org/pdf/2508.05674.pdf' target='_blank'>https://arxiv.org/pdf/2508.05674.pdf</a></span>   <span><a href='https://github.com/NYU-LLM-CTF/CTFJudge' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/NYU-LLM-CTF/CTFTiny' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minghao Shao, Nanda Rani, Kimberly Milner, Haoran Xi, Meet Udeshi, Saksham Aggarwal, Venkata Sai Charan Putrevu, Sandeep Kumar Shukla, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Muhammad Shafique
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05674">Towards Effective Offensive Security LLM Agents: Hyperparameter Tuning, LLM as a Judge, and a Lightweight CTF Benchmark</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in LLM agentic systems have improved the automation of offensive security tasks, particularly for Capture the Flag (CTF) challenges. We systematically investigate the key factors that drive agent success and provide a detailed recipe for building effective LLM-based offensive security agents. First, we present CTFJudge, a framework leveraging LLM as a judge to analyze agent trajectories and provide granular evaluation across CTF solving steps. Second, we propose a novel metric, CTF Competency Index (CCI) for partial correctness, revealing how closely agent solutions align with human-crafted gold standards. Third, we examine how LLM hyperparameters, namely temperature, top-p, and maximum token length, influence agent performance and automated cybersecurity task planning. For rapid evaluation, we present CTFTiny, a curated benchmark of 50 representative CTF challenges across binary exploitation, web, reverse engineering, forensics, and cryptography. Our findings identify optimal multi-agent coordination settings and lay the groundwork for future LLM agent research in cybersecurity. We make CTFTiny open source to public https://github.com/NYU-LLM-CTF/CTFTiny along with CTFJudge on https://github.com/NYU-LLM-CTF/CTFJudge.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1000, <a href='https://arxiv.org/pdf/2508.05673.pdf' target='_blank'>https://arxiv.org/pdf/2508.05673.pdf</a></span>   <span><a href='https://github.com/Tiny-Snow/IR-Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiqin Yang, Jiawei Chen, Shengjia Zhang, Peng Wu, Yuegang Sun, Yan Feng, Chun Chen, Can Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05673">Breaking the Top-$K$ Barrier: Advancing Top-$K$ Ranking Metrics Optimization in Recommender Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In the realm of recommender systems (RS), Top-$K$ ranking metrics such as NDCG@$K$ are the gold standard for evaluating recommendation performance. However, during the training of recommendation models, optimizing NDCG@$K$ poses significant challenges due to its inherent discontinuous nature and the intricate Top-$K$ truncation. Recent efforts to optimize NDCG@$K$ have either overlooked the Top-$K$ truncation or suffered from high computational costs and training instability. To overcome these limitations, we propose SoftmaxLoss@$K$ (SL@$K$), a novel recommendation loss tailored for NDCG@$K$ optimization. Specifically, we integrate the quantile technique to handle Top-$K$ truncation and derive a smooth upper bound for optimizing NDCG@$K$ to address discontinuity. The resulting SL@$K$ loss has several desirable properties, including theoretical guarantees, ease of implementation, computational efficiency, gradient stability, and noise robustness. Extensive experiments on four real-world datasets and three recommendation backbones demonstrate that SL@$K$ outperforms existing losses with a notable average improvement of 6.03%. The code is available at https://github.com/Tiny-Snow/IR-Benchmark.<br>
<span id='abs_ch'>中文: 本文提出SoftmaxLoss@K（SL@K）这一新型推荐损失函数，通过分位数技术处理Top-K截断并构建平滑上界来优化NDCG@K，在多个数据集上实现6.03%的平均性能提升，具有理论保证和高效稳定的优势。</span><br>
<span id='abs_en'>English: This paper introduces SoftmaxLoss@K (SL@K), a novel recommendation loss that effectively optimizes NDCG@K by addressing its discontinuity and Top-K truncation challenges through quantile integration and smooth upper bounds, demonstrating superior performance with a 6.03% average improvement across multiple datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1001, <a href='https://arxiv.org/pdf/2508.05669.pdf' target='_blank'>https://arxiv.org/pdf/2508.05669.pdf</a></span>   <span><a href='https://github.com/jinkhye/MyFinMarkdown' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Khye Tan, En Jun Choong, Ethan Jeremiah Chitty, Yan Pheng Choo, John Hsin Yang Wong, Chern Eu Cheah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05669">Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurately extracting and representing the structure of tabular data from financial documents remains a critical challenge in document understanding, particularly for regulatory and analytical use cases. This study addresses the complexity of converting financial tables from Malaysian audited financial reports into Markdown format, a task complicated by rotated layouts, multi-level headers, and implicit structural cues. We propose a fine-tuned vision-language model (VLM), based on Qwen2.5-VL-7B, optimized for high-fidelity Markdown generation from document images. Our approach includes a curated dataset of 2,152 image-text pairs with augmentations and a supervised fine-tuning strategy using LoRA. To assess performance, we evaluated our model on 100 out-of-sample tables using a dual framework: a criteria-based LLM-as-a-judge for fine-grained accuracy and our novel Markdown Tree-Edit-Distance-based Similarity (TEDS) metric for holistic structural fidelity. Our model achieves a 92.20% overall accuracy on the criteria-based assessment and a 96.53% Markdown TEDS score. This performance significantly surpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized reasoning-enabled models. Compared to these self-hosted alternatives, it also significantly reduces inference time. Furthermore, its accuracy exceeds that of widely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash. These results demonstrate that domain-specific fine-tuning provides an effective and efficient method to bridge the gap between unstructured financial documents and downstream automation, rivalling much larger and more general models without their computational overhead.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1002, <a href='https://arxiv.org/pdf/2508.05668.pdf' target='_blank'>https://arxiv.org/pdf/2508.05668.pdf</a></span>   <span><a href='https://github.com/YunjiaXi/Awesome-Search-Agent-Papers' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunjia Xi, Jianghao Lin, Yongzhao Xiao, Zheli Zhou, Rong Shan, Te Gao, Jiachen Zhu, Weiwen Liu, Yong Yu, Weinan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05668">A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The advent of Large Language Models (LLMs) has significantly revolutionized web search. The emergence of LLM-based Search Agents marks a pivotal shift towards deeper, dynamic, autonomous information seeking. These agents can comprehend user intentions and environmental context and execute multi-turn retrieval with dynamic planning, extending search capabilities far beyond the web. Leading examples like OpenAI's Deep Research highlight their potential for deep information mining and real-world applications. This survey provides the first systematic analysis of search agents. We comprehensively analyze and categorize existing works from the perspectives of architecture, optimization, application, and evaluation, ultimately identifying critical open challenges and outlining promising future research directions in this rapidly evolving field. Our repository is available on https://github.com/YunjiaXi/Awesome-Search-Agent-Papers.<br>
<span id='abs_ch'>中文：大语言模型通过支持理解用户意图并执行动态多轮信息检索的自主代理，彻底改变了网络搜索，本综述首次系统分析了其架构、优化和应用，同时指出了未来挑战。</span><br>
<span id='abs_en'>English: Large Language Models have transformed web search by enabling autonomous agents that understand user intent and perform dynamic, multi-turn information retrieval, with this survey offering the first systematic analysis of their architecture, optimization, and applications while identifying future challenges.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1003, <a href='https://arxiv.org/pdf/2508.05667.pdf' target='_blank'>https://arxiv.org/pdf/2508.05667.pdf</a></span>   <span><a href='https://github.com/hellolzk/ITDR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zekun Liu, Xiaowen Huang, Jitao Sang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05667">ITDR: An Instruction Tuning Dataset for Enhancing Large Language Models in Recommendations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have demonstrated outstanding performance in natural language processing tasks. However, in the field of recommendation systems, due to the structural differences between user behavior data and natural language, LLMs struggle to effectively model the associations between user preferences and items. Although prompt-based methods can generate recommendation results, their inadequate understanding of recommendation tasks leads to constrained performance. To address this gap, in this work, we construct a sufficient instruction tuning dataset, ITDR, which encompasses 7 subtasks across two core root tasks--user-item interaction and user-item understanding. The dataset integrates data from 13 public recommendation datasets and is built using manually crafted standardized templates, comprising approximately 200,000 instances. Experimental results demonstrate that ITDR significantly enhances the performance of mainstream open-source LLMs such as GLM-4, Qwen2.5, Qwen2.5-Instruct and LLaMA-3.2 on recommendation tasks. Furthermore, we analyze the correlations between tasks and explore the impact of task descriptions and data scale on instruction tuning effectiveness. Finally, we perform comparative experiments against closed-source LLMs with substantial parameters. Our tuning dataset ITDR and the fine-tuned large recommendation models can be accessed at https://github.com/hellolzk/ITDR.<br>
<span id='abs_ch'>Chinese: 本 究提出了ITDR指令调优数据集，通过增强大型语言模型对用户-物品交互的理解，有效弥补了其在推荐系统中的性能局限，显著提升了GLM-4和LLaMA-3.2等模型在推荐任务上的表现。</span><br>
<span id='abs_en'>English: This study introduces ITDR, a comprehensive instruction tuning dataset designed to bridge the gap between large language models and recommendation systems by enhancing their understanding of user-item interactions, which significantly improves the performance of models like GLM-4 and LLaMA-3.2 on recommendation tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1004, <a href='https://arxiv.org/pdf/2508.05666.pdf' target='_blank'>https://arxiv.org/pdf/2508.05666.pdf</a></span>   <span><a href='https://github.com/agodinezmm2007/docling_mod' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alejandro Godinez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05666">HySemRAG: A Hybrid Semantic Retrieval-Augmented Generation Framework for Automated Literature Synthesis and Methodological Gap Analysis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present HySemRAG, a framework that combines Extract, Transform, Load (ETL) pipelines with Retrieval-Augmented Generation (RAG) to automate large-scale literature synthesis and identify methodological research gaps. The system addresses limitations in existing RAG architectures through a multi-layered approach: hybrid retrieval combining semantic search, keyword filtering, and knowledge graph traversal; an agentic self-correction framework with iterative quality assurance; and post-hoc citation verification ensuring complete traceability. Our implementation processes scholarly literature through eight integrated stages: multi-source metadata acquisition, asynchronous PDF retrieval, custom document layout analysis using modified Docling architecture, bibliographic management, LLM-based field extraction, topic modeling, semantic unification, and knowledge graph construction. The system creates dual data products - a Neo4j knowledge graph enabling complex relationship queries and Qdrant vector collections supporting semantic search - serving as foundational infrastructure for verifiable information synthesis. Evaluation across 643 observations from 60 testing sessions demonstrates structured field extraction achieving 35.1% higher semantic similarity scores (0.655 $\pm$ 0.178) compared to PDF chunking approaches (0.485 $\pm$ 0.204, p < 0.000001). The agentic quality assurance mechanism achieves 68.3% single-pass success rates with 99.0% citation accuracy in validated responses. Applied to geospatial epidemiology literature on ozone exposure and cardiovascular disease, the system identifies methodological trends and research gaps, demonstrating broad applicability across scientific domains for accelerating evidence synthesis and discovery.<br>
<span id='abs_ch'>中文：HySemRAG框架通过将ETL流程与检索增强生成相结合，采用混合检索、自主修正和引文验证机制，实现了大规模文献自动整合与方法学缺口识别，在多个科学领域展现出卓越的提取精度与质量保障能力。</span><br>
<span id='abs_en'>English: HySemRAG is a framework integrating ETL pipelines with RAG to automate literature synthesis and identify research gaps through hybrid retrieval, agentic self-correction, and citation verification, demonstrating superior performance in field extraction and quality assurance across scientific domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1005, <a href='https://arxiv.org/pdf/2508.05650.pdf' target='_blank'>https://arxiv.org/pdf/2508.05650.pdf</a></span>   <span><a href='https://github.com/Garnett-Liang/Omnibench-RAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxuan Liang, Shide Zhou, Kailong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05650">OmniBench-RAG: A Multi-Domain Evaluation Platform for Retrieval-Augmented Generation Tools</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While Retrieval Augmented Generation (RAG) is now widely adopted to enhance LLMs, evaluating its true performance benefits in a reproducible and interpretable way remains a major hurdle. Existing methods often fall short: they lack domain coverage, employ coarse metrics that miss sub document precision, and fail to capture computational trade offs. Most critically, they provide no standardized framework for comparing RAG effectiveness across different models and domains.
  We introduce OmniBench RAG, a novel automated platform for multi domain evaluation of RAG systems. The platform quantifies performance gains across accuracy and efficiency dimensions, spanning nine knowledge fields including culture, geography, and health. We introduce two standardized metrics: Improvements (accuracy gains) and Transformation (efficiency differences between pre RAG and post RAG models), enabling reproducible comparisons across models and tasks. The platform features dynamic test generation, modular evaluation pipelines, and automated knowledge base construction. Our evaluation reveals striking variability in RAG effectiveness, from significant gains in culture to declines in mathematics, highlighting the critical importance of systematic, domain aware assessment. A demonstration video is available at: https://www.youtube.com/watch?v=BZx83QFcTCI. Code and datasets: https://github.com/Garnett-Liang/Omnibench-RAG.<br>
<span id='abs_ch'>中文: OmniBench RAG 是一个自动化平台，用于跨多个领域评估检索增强生成系统，通过 准化指 衡量准确性的提升和效率的差异，以实现可复现的比较。</span><br>
<span id='abs_en'>English: OmniBench RAG is an automated platform that evaluates Retrieval Augmented Generation systems across multiple domains, using standardized metrics to measure accuracy gains and efficiency differences for reproducible comparisons.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1006, <a href='https://arxiv.org/pdf/2508.05616.pdf' target='_blank'>https://arxiv.org/pdf/2508.05616.pdf</a></span>   <span><a href='https://github.com/ai4co/trajevo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhikai Zhao, Chuanbo Hua, Federico Berto, Kanghoon Lee, Zihan Ma, Jiachen Li, Jinkyoo Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05616">TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven Evolution</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Trajectory prediction is a critical task in modeling human behavior, especially in safety-critical domains such as social robotics and autonomous vehicle navigation. Traditional heuristics based on handcrafted rules often lack accuracy and generalizability. Although deep learning approaches offer improved performance, they typically suffer from high computational cost, limited explainability, and, importantly, poor generalization to out-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a framework that leverages Large Language Models (LLMs) to automatically design trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to generate and refine prediction heuristics from past trajectory data. We propose two key innovations: Cross-Generation Elite Sampling to encourage population diversity, and a Statistics Feedback Loop that enables the LLM to analyze and improve alternative predictions. Our evaluations demonstrate that TrajEvo outperforms existing heuristic methods across multiple real-world datasets, and notably surpasses both heuristic and deep learning methods in generalizing to an unseen OOD real-world dataset. TrajEvo marks a promising step toward the automated design of fast, explainable, and generalizable trajectory prediction heuristics. We release our source code to facilitate future research at https://github.com/ai4co/trajevo.<br>
<span id='abs_ch'>中文摘要：TrajEvo是一个创新框架，利用大型语言模型和进化算法自动设计轨迹预测启发式规则，在准确性和对未见场景的泛化能力上均超越了 统方法和深度学 方法。</span><br>
<span id='abs_en'>English Summary: TrajEvo is an innovative framework that uses Large Language Models and evolutionary algorithms to automatically design trajectory prediction heuristics, outperforming both traditional and deep learning methods in accuracy and generalization to unseen scenarios.Paperid:908,https://arxiv.org/pdf/2508.05615.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1007, <a href='https://arxiv.org/pdf/2508.05615.pdf' target='_blank'>https://arxiv.org/pdf/2508.05615.pdf</a></span>   <span><a href='https://github.com/zju-real/gui-rcpo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong Du, Yuchen Yan, Fei Tang, Zhengxi Lu, Chang Zong, Weiming Lu, Shengpei Jiang, Yongliang Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05615">Test-Time Reinforcement Learning for GUI Grounding via Region Consistency</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Graphical User Interface (GUI) grounding, the task of mapping natural language instructions to precise screen coordinates, is fundamental to autonomous GUI agents. While existing methods achieve strong performance through extensive supervised training or reinforcement learning with labeled rewards, they remain constrained by the cost and availability of pixel-level annotations. We observe that when models generate multiple predictions for the same GUI element, the spatial overlap patterns reveal implicit confidence signals that can guide more accurate localization. Leveraging this insight, we propose GUI-RC (Region Consistency), a test-time scaling method that constructs spatial voting grids from multiple sampled predictions to identify consensus regions where models show highest agreement. Without any training, GUI-RC improves accuracy by 2-3% across various architectures on ScreenSpot benchmarks. We further introduce GUI-RCPO (Region Consistency Policy Optimization), which transforms these consistency patterns into rewards for test-time reinforcement learning. By computing how well each prediction aligns with the collective consensus, GUI-RCPO enables models to iteratively refine their outputs on unlabeled data during inference. Extensive experiments demonstrate the generality of our approach: GUI-RC boosts Qwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO further improves it to 85.14% through self-supervised optimization. Our approach reveals the untapped potential of test-time scaling and test-time reinforcement learning for GUI grounding, offering a promising path toward more robust and data-efficient GUI agents.<br>
<span id='abs_ch'>Chinese Summary: 本 究提出了GUI-RC和GUI-RCPO方法，通过利用多预测的空间一致性来提升图形用户界面定位精度， 需额外训练即可实现高达5%的性能提升，或通过自监督优化进一步增强效果。</span><br>
<span id='abs_en'>English Summary: The study introduces GUI-RC and GUI-RCPO, two methods that enhance GUI grounding accuracy by leveraging spatial consensus from multiple predictions, achieving up to 5% improvement without additional training or through self-supervised optimization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1008, <a href='https://arxiv.org/pdf/2508.05614.pdf' target='_blank'>https://arxiv.org/pdf/2508.05614.pdf</a></span>   <span><a href='https://github.com/ZJU-REAL/OmniEmbodied' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixuan Wang, Dingming Li, Hongxing Li, Shuo Chen, Yuchen Yan, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05614">OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models excel at abstract reasoning but their capacity for embodied agent reasoning remains largely unexplored. We present OmniEAR, a comprehensive framework for evaluating how language models reason about physical interactions, tool usage, and multi-agent coordination in embodied tasks. Unlike existing benchmarks that provide predefined tool sets or explicit collaboration directives, OmniEAR requires agents to dynamically acquire capabilities and autonomously determine coordination strategies based on task demands. Through text-based environment representation, we model continuous physical properties and complex spatial relationships across 1,500 scenarios spanning household and industrial domains. Our systematic evaluation reveals severe performance degradation when models must reason from constraints: while achieving 85-96% success with explicit instructions, performance drops to 56-85% for tool reasoning and 63-85% for implicit collaboration, with compound tasks showing over 50% failure rates. Surprisingly, complete environmental information degrades coordination performance, indicating models cannot filter task-relevant constraints. Fine-tuning improves single-agent tasks dramatically (0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing fundamental architectural limitations. These findings demonstrate that embodied reasoning poses fundamentally different challenges than current models can address, establishing OmniEAR as a rigorous benchmark for evaluating and advancing embodied AI systems. Our code and data are included in the supplementary materials and will be open-sourced upon acceptance.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1009, <a href='https://arxiv.org/pdf/2508.05613.pdf' target='_blank'>https://arxiv.org/pdf/2508.05613.pdf</a></span>   <span><a href='https://github.com/zju-real/cooper' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haitao Hong, Yuchen Yan, Xingyu Wu, Guiyang Hou, Wenqi Zhang, Weiming Lu, Yongliang Shen, Jun Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05613">Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have demonstrated remarkable performance in reasoning tasks, where reinforcement learning (RL) serves as a key algorithm for enhancing their reasoning capabilities. Currently, there are two mainstream reward paradigms: model-based rewards and rule-based rewards. However, both approaches suffer from limitations: rule-based rewards lack robustness, while model-based rewards are vulnerable to reward hacking. To address these issues, we propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework that jointly optimizes both the policy model and the reward model. Cooper leverages the high precision of rule-based rewards when identifying correct responses, and dynamically constructs and selects positive-negative sample pairs for continued training the reward model. This design enhances robustness and mitigates the risk of reward hacking. To further support Cooper, we introduce a hybrid annotation strategy that efficiently and accurately generates training data for the reward model. We also propose a reference-based reward modeling paradigm, where the reward model takes a reference answer as input. Based on this design, we train a reward model named VerifyRM, which achieves higher accuracy on VerifyBench compared to other models of the same size. We conduct reinforcement learning using both VerifyRM and Cooper. Our experiments show that Cooper not only alleviates reward hacking but also improves end-to-end RL performance, for instance, achieving a 0.54% gain in average accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that dynamically updating reward model is an effective way to combat reward hacking, providing a reference for better integrating reward models into RL.<br>
<span id='abs_ch'>中文: Cooper框架通过联合优化策略模型和奖励模型，利用规则奖励的高精度动态构建训练 本，有效增强鲁棒性并缓解奖励 解问题，从而提升强化学 的整体性能。</span><br>
<span id='abs_en'>English: The proposed Cooper framework jointly optimizes policy and reward models to enhance robustness and mitigate reward hacking by dynamically selecting training samples and leveraging rule-based precision, achieving improved performance in reinforcement learning tasks.Paperid:911,https://arxiv.org/pdf/2508.05599.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1010, <a href='https://arxiv.org/pdf/2508.05547.pdf' target='_blank'>https://arxiv.org/pdf/2508.05547.pdf</a></span>   <span><a href='https://github.com/tim-learn/Awesome-LabelFree-VLMs' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/tim-learn/Awesome-LabelFree-VLMs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Dong, Lijun Sheng, Jian Liang, Ran He, Eleni Chatzi, Olga Fink
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05547">Adapting Vision-Language Models Without Labels: A Comprehensive Survey</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision-Language Models (VLMs) have demonstrated remarkable generalization capabilities across a wide range of tasks. However, their performance often remains suboptimal when directly applied to specific downstream scenarios without task-specific adaptation. To enhance their utility while preserving data efficiency, recent research has increasingly focused on unsupervised adaptation methods that do not rely on labeled data. Despite the growing interest in this area, there remains a lack of a unified, task-oriented survey dedicated to unsupervised VLM adaptation. To bridge this gap, we present a comprehensive and structured overview of the field. We propose a taxonomy based on the availability and nature of unlabeled visual data, categorizing existing approaches into four key paradigms: Data-Free Transfer (no data), Unsupervised Domain Transfer (abundant data), Episodic Test-Time Adaptation (batch data), and Online Test-Time Adaptation (streaming data). Within this framework, we analyze core methodologies and adaptation strategies associated with each paradigm, aiming to establish a systematic understanding of the field. Additionally, we review representative benchmarks across diverse applications and highlight open challenges and promising directions for future research. An actively maintained repository of relevant literature is available at https://github.com/tim-learn/Awesome-LabelFree-VLMs.<br>
<span id='abs_ch'>Chinese: 本综述系统梳理了视觉语言模型的 监督自适应方法，依据未 注数据的可用性将其划分为四种范式，并分析各类方法以提升模型在特定下游任务中的性能表现。English: This survey provides a structured overview of unsupervised adaptation methods for Vision-Language Models, categorizing them into four paradigms based on unlabeled data availability and analyzing their methodologies to address performance gaps in downstream tasks.<br>Paperid:913,https://arxiv.org/pdf/2508.05521.pdfGitHub</span><br>
<span id='abs_en'>English: This survey provides a structured overview of unsupervised adaptation methods for Vision-Language Models, categorizing them into four paradigms based on unlabeled data availability and analyzing their methodologies to address performance gaps in downstream tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1011, <a href='https://arxiv.org/pdf/2508.05498.pdf' target='_blank'>https://arxiv.org/pdf/2508.05498.pdf</a></span>   <span><a href='https://github.com/Changgeww/GRAIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ge Chang, Jinbo Su, Jiacheng Liu, Pengfei Yang, Yuhao Shang, Huiwen Zheng, Hongli Ma, Yan Liang, Yuanchun Li, Yunxin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05498">GRAIL:Learning to Interact with Large Knowledge Graphs for Retrieval Augmented Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) integrated with Retrieval-Augmented Generation (RAG) techniques have exhibited remarkable performance across a wide range of domains. However, existing RAG approaches primarily operate on unstructured data and demonstrate limited capability in handling structured knowledge such as knowledge graphs. Meanwhile, current graph retrieval methods fundamentally struggle to capture holistic graph structures while simultaneously facing precision control challenges that manifest as either critical information gaps or excessive redundant connections, collectively undermining reasoning performance. To address this challenge, we propose GRAIL: Graph-Retrieval Augmented Interactive Learning, a framework designed to interact with large-scale graphs for retrieval-augmented reasoning. Specifically, GRAIL integrates LLM-guided random exploration with path filtering to establish a data synthesis pipeline, where a fine-grained reasoning trajectory is automatically generated for each task. Based on the synthesized data, we then employ a two-stage training process to learn a policy that dynamically decides the optimal actions at each reasoning step. The overall objective of precision-conciseness balance in graph retrieval is decoupled into fine-grained process-supervised rewards to enhance data efficiency and training stability. In practical deployment, GRAIL adopts an interactive retrieval paradigm, enabling the model to autonomously explore graph paths while dynamically balancing retrieval breadth and precision. Extensive experiments have shown that GRAIL achieves an average accuracy improvement of 21.01% and F1 improvement of 22.43% on three knowledge graph question-answering datasets. Our source code and datasets is available at https://github.com/Changgeww/GRAIL.<br>
<span id='abs_ch'>中文：GRAIL提出了一种交互式学 框架，通过结合大语言模型引导的探索与精度可控的检索技术，显著提升了知识图谱上的推理性能，在多个数据集上实现了准确率和F1值的大幅提高。</span><br>
<span id='abs_en'>English: GRAIL introduces an interactive learning framework that enhances reasoning on knowledge graphs by combining LLM-guided exploration with precision-controlled retrieval, achieving significant improvements in accuracy and F1 scores across multiple datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1012, <a href='https://arxiv.org/pdf/2508.05399.pdf' target='_blank'>https://arxiv.org/pdf/2508.05399.pdf</a></span>   <span><a href='https://github.com/furiosa-ai/uncage' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/furiosa-ai/uncage' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wonjun Kang, Byeongkeun Ahn, Minjae Lee, Kevin Galim, Seunghyuk Oh, Hyung Il Koo, Nam Ik Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05399">UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text-to-image (T2I) generation has been actively studied using Diffusion Models and Autoregressive Models. Recently, Masked Generative Transformers have gained attention as an alternative to Autoregressive Models to overcome the inherent limitations of causal attention and autoregressive decoding through bidirectional attention and parallel decoding, enabling efficient and high-quality image generation. However, compositional T2I generation remains challenging, as even state-of-the-art Diffusion Models often fail to accurately bind attributes and achieve proper text-image alignment. While Diffusion Models have been extensively studied for this issue, Masked Generative Transformers exhibit similar limitations but have not been explored in this context. To address this, we propose Unmasking with Contrastive Attention Guidance (UNCAGE), a novel training-free method that improves compositional fidelity by leveraging attention maps to prioritize the unmasking of tokens that clearly represent individual objects. UNCAGE consistently improves performance in both quantitative and qualitative evaluations across multiple benchmarks and metrics, with negligible inference overhead. Our code is available at https://github.com/furiosa-ai/uncage.<br>
<span id='abs_ch'>中文: 提出的UNCAGE方法通过对比注意力引导在去掩 过程中优先处理对象 记， 需额外训练即可提升组合式文本到图像生成的准确性。</span><br>
<span id='abs_en'>English: The proposed UNCAGE method enhances compositional text-to-image generation by using contrastive attention guidance to prioritize object tokens during unmasking, improving fidelity without additional training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1013, <a href='https://arxiv.org/pdf/2508.05299.pdf' target='_blank'>https://arxiv.org/pdf/2508.05299.pdf</a></span>   <span><a href='https://github.com/wmeiqi/VS-LLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Meiqi Wu, Yaxuan Kang, Xuchen Li, Shiyu Hu, Xiaotang Chen, Yunfeng Kang, Weiqiang Wang, Kaiqi Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05299">VS-LLM: Visual-Semantic Depression Assessment based on LLM for Drawing Projection Test</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The Drawing Projection Test (DPT) is an essential tool in art therapy, allowing psychologists to assess participants' mental states through their sketches. Specifically, through sketches with the theme of "a person picking an apple from a tree (PPAT)", it can be revealed whether the participants are in mental states such as depression. Compared with scales, the DPT can enrich psychologists' understanding of an individual's mental state. However, the interpretation of the PPAT is laborious and depends on the experience of the psychologists. To address this issue, we propose an effective identification method to support psychologists in conducting a large-scale automatic DPT. Unlike traditional sketch recognition, DPT more focus on the overall evaluation of the sketches, such as color usage and space utilization. Moreover, PPAT imposes a time limit and prohibits verbal reminders, resulting in low drawing accuracy and a lack of detailed depiction. To address these challenges, we propose the following efforts: (1) Providing an experimental environment for automated analysis of PPAT sketches for depression assessment; (2) Offering a Visual-Semantic depression assessment based on LLM (VS-LLM) method; (3) Experimental results demonstrate that our method improves by 17.6% compared to the psychologist assessment method. We anticipate that this work will contribute to the research in mental state assessment based on PPAT sketches' elements recognition. Our datasets and codes are available at https://github.com/wmeiqi/VS-LLM.<br>
<span id='abs_ch'>Chinese: 该 究提出了一种基于视觉语义分析和大型语言模型的自动化方法，用于通过PPAT绘画评估抑郁状态，相比心理学家评估方法准确率提升了17.6%。</span><br>
<span id='abs_en'>English: The study introduces an automated method using Visual-Semantic analysis with LLMs to efficiently assess depression through PPAT sketches, improving accuracy by 17.6% over traditional psychologist evaluations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1014, <a href='https://arxiv.org/pdf/2508.05264.pdf' target='_blank'>https://arxiv.org/pdf/2508.05264.pdf</a></span>   <span><a href='https://github.com/boshizhang123/SGDFuse' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyang Zhang, jinjiang Li, Guodong Fan, Yakun Ju, Linwei Fan, Jun Liu, Alex C. Kot
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05264">SGDFuse: SAM-Guided Diffusion for High-Fidelity Infrared and Visible Image Fusion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Infrared and visible image fusion (IVIF) aims to combine the thermal radiation information from infrared images with the rich texture details from visible images to enhance perceptual capabilities for downstream visual tasks. However, existing methods often fail to preserve key targets due to a lack of deep semantic understanding of the scene, while the fusion process itself can also introduce artifacts and detail loss, severely compromising both image quality and task performance. To address these issues, this paper proposes SGDFuse, a conditional diffusion model guided by the Segment Anything Model (SAM), to achieve high-fidelity and semantically-aware image fusion. The core of our method is to utilize high-quality semantic masks generated by SAM as explicit priors to guide the optimization of the fusion process via a conditional diffusion model. Specifically, the framework operates in a two-stage process: it first performs a preliminary fusion of multi-modal features, and then utilizes the semantic masks from SAM jointly with the preliminary fused image as a condition to drive the diffusion model's coarse-to-fine denoising generation. This ensures the fusion process not only has explicit semantic directionality but also guarantees the high fidelity of the final result. Extensive experiments demonstrate that SGDFuse achieves state-of-the-art performance in both subjective and objective evaluations, as well as in its adaptability to downstream tasks, providing a powerful solution to the core challenges in image fusion. The code of SGDFuse is available at https://github.com/boshizhang123/SGDFuse.<br>
<span id='abs_ch'>中文：本文提出SGDFuse，一种由Segment Anything Model（SAM）引导的条件扩散模型，利用高质量语义掩 实现高保真和语义感知的红外与可见光图像融合，在主观和客观评估中均优于现有方法。</span><br>
<span id='abs_en'>English: This paper introduces SGDFuse, a conditional diffusion model guided by the Segment Anything Model (SAM), which leverages high-quality semantic masks to achieve high-fidelity and semantically-aware infrared and visible image fusion, outperforming existing methods in both subjective and objective evaluations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1015, <a href='https://arxiv.org/pdf/2508.05239.pdf' target='_blank'>https://arxiv.org/pdf/2508.05239.pdf</a></span>   <span><a href='https://github.com/WhatAboutMyStar/LLM_ACTIVATION' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiheng Liu, Junhao Ning, Sichen Xia, Xiaohui Gao, Ning Qiang, Bao Ge, Junwei Han, Xintao Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05239">Pruning Large Language Models by Identifying and Preserving Functional Networks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Structured pruning is one of the representative techniques for compressing large language models (LLMs) to reduce GPU memory consumption and accelerate inference speed. It offers significant practical value in improving the efficiency of LLMs in real-world applications. Current structured pruning methods typically rely on assessment of the importance of the structure units and pruning the units with less importance. Most of them overlooks the interaction and collaboration among artificial neurons that are crucial for the functionalities of LLMs, leading to a disruption in the macro functional architecture of LLMs and consequently a pruning performance degradation. Inspired by the inherent similarities between artificial neural networks and functional neural networks in the human brain, we alleviate this challenge and propose to prune LLMs by identifying and preserving functional networks within LLMs in this study. To achieve this, we treat an LLM as a digital brain and decompose the LLM into functional networks, analogous to identifying functional brain networks in neuroimaging data. Afterwards, an LLM is pruned by preserving the key neurons within these functional networks. Experimental results demonstrate that the proposed method can successfully identify and locate functional networks and key neurons in LLMs, enabling efficient model pruning. Our code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.<br>
<span id='abs_ch'>中文: 结构化剪枝通过识别并保留大语言模型中的功能性网络和关键神经元，基于与人脑神经网络的相似性，有效压缩模型并保持其 心功能，提升实际应用效率。</span><br>
<span id='abs_en'>English: Structured pruning compresses large language models by preserving key functional networks and neurons, inspired by neural similarities to the human brain, enhancing efficiency without disrupting core functionalities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1016, <a href='https://arxiv.org/pdf/2508.05221.pdf' target='_blank'>https://arxiv.org/pdf/2508.05221.pdf</a></span>   <span><a href='https://github.com/Event-AHU/Open_VLTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Liye Jin, Xufeng Lou, Shiao Wang, Lan Chen, Bo Jiang, Zhipeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05221">ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision-language tracking has received increasing attention in recent years, as textual information can effectively address the inflexibility and inaccuracy associated with specifying the target object to be tracked. Existing works either directly fuse the fixed language with vision features or simply modify using attention, however, their performance is still limited. Recently, some researchers have explored using text generation to adapt to the variations in the target during tracking, however, these works fail to provide insights into the model's reasoning process and do not fully leverage the advantages of large models, which further limits their overall performance. To address the aforementioned issues, this paper proposes a novel reasoning-based vision-language tracking framework, named ReasoningTrack, based on a pre-trained vision-language model Qwen2.5-VL. Both SFT (Supervised Fine-Tuning) and reinforcement learning GRPO are used for the optimization of reasoning and language generation. We embed the updated language descriptions and feed them into a unified tracking backbone network together with vision features. Then, we adopt a tracking head to predict the specific location of the target object. In addition, we propose a large-scale long-term vision-language tracking benchmark dataset, termed TNLLT, which contains 200 video sequences. 20 baseline visual trackers are re-trained and evaluated on this dataset, which builds a solid foundation for the vision-language visual tracking task. Extensive experiments on multiple vision-language tracking benchmark datasets fully validated the effectiveness of our proposed reasoning-based natural language generation strategy. The source code of this paper will be released on https://github.com/Event-AHU/Open_VLTrack<br>
<span id='abs_ch'>中文: 本文提出了一种基于推理的视觉语言跟踪框架ReasoningTrack，通过预训练视觉语言模型融合动态语言描述与视觉特征，有效提升目 跟踪性能，并在多个基准数据集上验证了其优越性。</span><br>
<span id='abs_en'>English: This paper introduces ReasoningTrack, a novel reasoning-based vision-language tracking framework that leverages a pre-trained vision-language model and integrates updated language descriptions with visual features to enhance target tracking accuracy, validated through extensive experiments on multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1017, <a href='https://arxiv.org/pdf/2508.05198.pdf' target='_blank'>https://arxiv.org/pdf/2508.05198.pdf</a></span>   <span><a href='https://github.com/sisinflab/Sub-id-Popularity' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chiara Mallamaci, Aleksandr Vladimirovich Petrov, Alberto Carlo Maria Mancino, Vito Walter Anelli, Tommaso Di Noia, Craig Macdonald
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05198">Balancing Accuracy and Novelty with Sub-Item Popularity</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In the realm of music recommendation, sequential recommenders have shown promise in capturing the dynamic nature of music consumption. A key characteristic of this domain is repetitive listening, where users frequently replay familiar tracks. To capture these repetition patterns, recent research has introduced Personalised Popularity Scores (PPS), which quantify user-specific preferences based on historical frequency. While PPS enhances relevance in recommendation, it often reinforces already-known content, limiting the system's ability to surface novel or serendipitous items - key elements for fostering long-term user engagement and satisfaction. To address this limitation, we build upon RecJPQ, a Transformer-based framework initially developed to improve scalability in large-item catalogues through sub-item decomposition. We repurpose RecJPQ's sub-item architecture to model personalised popularity at a finer granularity. This allows us to capture shared repetition patterns across sub-embeddings - latent structures not accessible through item-level popularity alone. We propose a novel integration of sub-ID-level personalised popularity within the RecJPQ framework, enabling explicit control over the trade-off between accuracy and personalised novelty. Our sub-ID-level PPS method (sPPS) consistently outperforms item-level PPS by achieving significantly higher personalised novelty without compromising recommendation accuracy. Code and experiments are publicly available at https://github.com/sisinflab/Sub-id-Popularity.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1018, <a href='https://arxiv.org/pdf/2508.05197.pdf' target='_blank'>https://arxiv.org/pdf/2508.05197.pdf</a></span>   <span><a href='https://github.com/jzzzzh/QA-Dragon' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuohang Jiang, Pangjing Wu, Xu Yuan, Wenqi Fan, Qing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05197">QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-Augmented Generation (RAG) has been introduced to mitigate hallucinations in Multimodal Large Language Models (MLLMs) by incorporating external knowledge into the generation process, and it has become a widely adopted approach for knowledge-intensive Visual Question Answering (VQA). However, existing RAG methods typically retrieve from either text or images in isolation, limiting their ability to address complex queries that require multi-hop reasoning or up-to-date factual knowledge. To address this limitation, we propose QA-Dragon, a Query-Aware Dynamic RAG System for Knowledge-Intensive VQA. Specifically, QA-Dragon introduces a domain router to identify the query's subject domain for domain-specific reasoning, along with a search router that dynamically selects optimal retrieval strategies. By orchestrating both text and image search agents in a hybrid setup, our system supports multimodal, multi-turn, and multi-hop reasoning, enabling it to tackle complex VQA tasks effectively. We evaluate our QA-Dragon on the Meta CRAG-MM Challenge at KDD Cup 2025, where it significantly enhances the reasoning performance of base models under challenging scenarios. Our framework achieves substantial improvements in both answer accuracy and knowledge overlap scores, outperforming baselines by 5.06% on the single-source task, 6.35% on the multi-source task, and 5.03% on the multi-turn task.<br>
<span id='abs_ch'>中文: 提出的QA-Dragon系统通过动态选择最优检索策略并结合文本与图像搜索，增强了检索增强生成方法，显著提升了复杂视觉问答任务中的推理能力和准确性。</span><br>
<span id='abs_en'>English: Retrieval-Augmented Generation (RAG) is enhanced by the proposed QA-Dragon system, which dynamically selects optimal retrieval strategies and combines text and image search to improve reasoning and accuracy in complex Visual Question Answering tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1019, <a href='https://arxiv.org/pdf/2508.05087.pdf' target='_blank'>https://arxiv.org/pdf/2508.05087.pdf</a></span>   <span><a href='https://github.com/thu-coai/JPS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Renmiao Chen, Shiyao Cui, Xuancheng Huang, Chengwei Pan, Victor Shea-Jay Huang, QingLin Zhang, Xuan Ouyang, Zhexin Zhang, Hongning Wang, Minlie Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05087">JPS: Jailbreak Multimodal Large Language Models with Collaborative Visual Perturbation and Textual Steering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Jailbreak attacks against multimodal large language Models (MLLMs) are a significant research focus. Current research predominantly focuses on maximizing attack success rate (ASR), often overlooking whether the generated responses actually fulfill the attacker's malicious intent. This oversight frequently leads to low-quality outputs that bypass safety filters but lack substantial harmful content. To address this gap, we propose JPS, \underline{J}ailbreak MLLMs with collaborative visual \underline{P}erturbation and textual \underline{S}teering, which achieves jailbreaks via corporation of visual image and textually steering prompt. Specifically, JPS utilizes target-guided adversarial image perturbations for effective safety bypass, complemented by "steering prompt" optimized via a multi-agent system to specifically guide LLM responses fulfilling the attackers' intent. These visual and textual components undergo iterative co-optimization for enhanced performance. To evaluate the quality of attack outcomes, we propose the Malicious Intent Fulfillment Rate (MIFR) metric, assessed using a Reasoning-LLM-based evaluator. Our experiments show JPS sets a new state-of-the-art in both ASR and MIFR across various MLLMs and benchmarks, with analyses confirming its efficacy. Codes are available at \href{https://github.com/thu-coai/JPS}{https://github.com/thu-coai/JPS}. \color{warningcolor}{Warning: This paper contains potentially sensitive contents.}<br>
<span id='abs_ch'>中文: 本文提出JPS方法，通过协同优化视觉扰动和文本引导，在实现多模态大语言模型越狱攻击时不仅有效绕过安全防护，更能确保生成内容符合攻击者恶意意图，实验证明该方法在攻击成功率和恶意意图实现率上均达到最优水平。</span><br>
<span id='abs_en'>English: This paper introduces JPS, a collaborative visual and textual method that enhances jailbreak attacks on multimodal large language models by combining adversarial image perturbations with steering prompts to effectively bypass safety measures while ensuring the malicious intent is fulfilled, as measured by the new MIFR metric.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1020, <a href='https://arxiv.org/pdf/2508.05078.pdf' target='_blank'>https://arxiv.org/pdf/2508.05078.pdf</a></span>   <span><a href='https://github.com/jinda-liu/Align-LoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinda Liu, Bo Cheng, Yi Chang, Yuan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.05078">Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Parameter-Efficient Fine-Tuning (PEFT) is essential for adapting Large Language Models (LLMs). In practice, LLMs are often required to handle a diverse set of tasks from multiple domains, a scenario naturally addressed by multi-task learning (MTL). Within this MTL context, a prevailing trend involves LoRA variants with multiple adapters or heads, which advocate for structural diversity to capture task-specific knowledge. Our findings present a direct challenge to this paradigm. We first show that a simplified multi-head architecture with high inter-head similarity substantially outperforms complex multi-adapter and multi-head systems. This leads us to question the multi-component paradigm itself, and we further demonstrate that a standard single-adapter LoRA, with a sufficiently increased rank, also achieves highly competitive performance. These results lead us to a new hypothesis: effective MTL generalization hinges on learning robust shared representations, not isolating task-specific features. To validate this, we propose Align-LoRA, which incorporates an explicit loss to align task representations within the shared adapter space. Experiments confirm that Align-LoRA significantly surpasses all baselines, establishing a simpler yet more effective paradigm for adapting LLMs to multiple tasks. The code is available at https://github.com/jinda-liu/Align-LoRA.<br>
<span id='abs_ch'>中文: Align-LoRA通过证明采用单一高秩适配器并显式对齐任务表征，能依 稳健共享表征实现更优性能，从而挑战了多任务学 中复杂多适配器系统的必要性。</span><br>
<span id='abs_en'>English: Align-LoRA challenges the need for complex multi-adapter systems in multi-task learning by demonstrating that a single high-rank adapter with explicit representation alignment achieves superior performance through robust shared representations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1021, <a href='https://arxiv.org/pdf/2508.04943.pdf' target='_blank'>https://arxiv.org/pdf/2508.04943.pdf</a></span>   <span><a href='https://github.com/XZPKU/TRKT.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhu Xu, Ting Lei, Zhimin Li, Guan Wang, Qingchao Chen, Yuxin Peng, Yang liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04943">TRKT: Weakly Supervised Dynamic Scene Graph Generation with Temporal-enhanced Relation-aware Knowledge Transferring</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Dynamic Scene Graph Generation (DSGG) aims to create a scene graph for each video frame by detecting objects and predicting their relationships. Weakly Supervised DSGG (WS-DSGG) reduces annotation workload by using an unlocalized scene graph from a single frame per video for training. Existing WS-DSGG methods depend on an off-the-shelf external object detector to generate pseudo labels for subsequent DSGG training. However, detectors trained on static, object-centric images struggle in dynamic, relation-aware scenarios required for DSGG, leading to inaccurate localization and low-confidence proposals. To address the challenges posed by external object detectors in WS-DSGG, we propose a Temporal-enhanced Relation-aware Knowledge Transferring (TRKT) method, which leverages knowledge to enhance detection in relation-aware dynamic scenarios. TRKT is built on two key components:(1)Relation-aware knowledge mining: we first employ object and relation class decoders that generate category-specific attention maps to highlight both object regions and interactive areas. Then we propose an Inter-frame Attention Augmentation strategy that exploits optical flow for neighboring frames to enhance the attention maps, making them motion-aware and robust to motion blur. This step yields relation- and motion-aware knowledge mining for WS-DSGG. (2) we introduce a Dual-stream Fusion Module that integrates category-specific attention maps into external detections to refine object localization and boost confidence scores for object proposals. Extensive experiments demonstrate that TRKT achieves state-of-the-art performance on Action Genome dataset. Our code is avaliable at https://github.com/XZPKU/TRKT.git.<br>
<span id='abs_ch'>中文: 提出的时序增强关系感知知识迁移（TRKT）方法通过结合关系感知知识挖掘与光流增强及双流融合模块，解决了弱监督动态场景图生成中外部物体检测器的局限性，从而提升了物体定位精度和置信度，在Action Genome数据集上取得了领先性能。</span><br>
<span id='abs_en'>English: The proposed Temporal-enhanced Relation-aware Knowledge Transferring (TRKT) method addresses the limitations of external object detectors in weakly supervised dynamic scene graph generation by integrating relation-aware knowledge mining with optical flow enhancement and a dual-stream fusion module to improve object localization and confidence, achieving state-of-the-art results on the Action Genome dataset.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1022, <a href='https://arxiv.org/pdf/2508.04928.pdf' target='_blank'>https://arxiv.org/pdf/2508.04928.pdf</a></span>   <span><a href='https://github.com/JungHeeKim29/calibration-token' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Suchisrit Gangopadhyay, Jung-Hee Kim, Xien Chen, Patrick Rim, Hyoungseob Park, Alex Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04928">Extending Foundational Monocular Depth Estimators to Fisheye Cameras with Calibration Tokens</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We propose a method to extend foundational monocular depth estimators (FMDEs), trained on perspective images, to fisheye images. Despite being trained on tens of millions of images, FMDEs are susceptible to the covariate shift introduced by changes in camera calibration (intrinsic, distortion) parameters, leading to erroneous depth estimates. Our method aligns the distribution of latent embeddings encoding fisheye images to those of perspective images, enabling the reuse of FMDEs for fisheye cameras without retraining or finetuning. To this end, we introduce a set of Calibration Tokens as a light-weight adaptation mechanism that modulates the latent embeddings for alignment. By exploiting the already expressive latent space of FMDEs, we posit that modulating their embeddings avoids the negative impact of artifacts and loss introduced in conventional recalibration or map projection to a canonical reference frame in the image space. Our method is self-supervised and does not require fisheye images but leverages publicly available large-scale perspective image datasets. This is done by recalibrating perspective images to fisheye images, and enforcing consistency between their estimates during training. We evaluate our approach with several FMDEs, on both indoors and outdoors, where we consistently improve over state-of-the-art methods using a single set of tokens for both. Code available at: https://github.com/JungHeeKim29/calibration-token.<br>
<span id='abs_ch'>Chinese: 该方法通过 准令牌将鱼眼图像的潜在嵌入与透视图像对齐， 需重新训练或鱼眼数据即可扩展基础单目深度估计器至鱼眼图像。</span><br>
<span id='abs_en'>English: This method extends foundational monocular depth estimators to fisheye images by aligning their latent embeddings with perspective images using calibration tokens, enabling adaptation without retraining or fisheye data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1023, <a href='https://arxiv.org/pdf/2508.04915.pdf' target='_blank'>https://arxiv.org/pdf/2508.04915.pdf</a></span>   <span><a href='https://github.com/PKU-AICare/ConfAgents' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huiya Zhao, Yinghao Zhu, Zixiang Wang, Yasha Wang, Junyi Gao, Liantao Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04915">ConfAgents: A Conformal-Guided Multi-Agent Framework for Cost-Efficient Medical Diagnosis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The efficacy of AI agents in healthcare research is hindered by their reliance on static, predefined strategies. This creates a critical limitation: agents can become better tool-users but cannot learn to become better strategic planners, a crucial skill for complex domains like healthcare. We introduce HealthFlow, a self-evolving AI agent that overcomes this limitation through a novel meta-level evolution mechanism. HealthFlow autonomously refines its own high-level problem-solving policies by distilling procedural successes and failures into a durable, strategic knowledge base. To anchor our research and facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark featuring complex, realistic health data analysis tasks derived from peer-reviewed clinical research. Our comprehensive experiments demonstrate that HealthFlow's self-evolving approach significantly outperforms state-of-the-art agent frameworks. This work marks a necessary shift from building better tool-users to designing smarter, self-evolving task-managers, paving the way for more autonomous and effective AI for scientific discovery.<br>
<span id='abs_ch'>中文: HealthFlow通过元级进化机制引入自我进化的AI代理，能自主优化策略规划，在医疗任务中显著超越现有框架，推动从工具使用者向智能任务管理者的转变。</span><br>
<span id='abs_en'>English: HealthFlow introduces a self-evolving AI agent with a meta-level evolution mechanism that autonomously refines strategic planning, significantly outperforming existing frameworks in healthcare tasks and shifting focus from tool-users to smarter task-managers.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1024, <a href='https://arxiv.org/pdf/2508.04900.pdf' target='_blank'>https://arxiv.org/pdf/2508.04900.pdf</a></span>   <span><a href='https://github.com/Multimodal-Intelligence-Lab-MIL/HatefulVideoLabelNoise' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuonan Yang, Tailin Chen, Rahul Singh, Jiangbei Yue, Jianbo Jiao, Zeyu Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04900">Revealing Temporal Label Noise in Multimodal Hateful Video Classification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid proliferation of online multimedia content has intensified the spread of hate speech, presenting critical societal and regulatory challenges. While recent work has advanced multimodal hateful video detection, most approaches rely on coarse, video-level annotations that overlook the temporal granularity of hateful content. This introduces substantial label noise, as videos annotated as hateful often contain long non-hateful segments. In this paper, we investigate the impact of such label ambiguity through a fine-grained approach. Specifically, we trim hateful videos from the HateMM and MultiHateClip English datasets using annotated timestamps to isolate explicitly hateful segments. We then conduct an exploratory analysis of these trimmed segments to examine the distribution and characteristics of both hateful and non-hateful content. This analysis highlights the degree of semantic overlap and the confusion introduced by coarse, video-level annotations. Finally, controlled experiments demonstrated that time-stamp noise fundamentally alters model decision boundaries and weakens classification confidence, highlighting the inherent context dependency and temporal continuity of hate speech expression. Our findings provide new insights into the temporal dynamics of multimodal hateful videos and highlight the need for temporally aware models and benchmarks for improved robustness and interpretability. Code and data are available at https://github.com/Multimodal-Intelligence-Lab-MIL/HatefulVideoLabelNoise.<br>
<span id='abs_ch'>中文: 本 究通过分析多模态数据集中经裁剪的仇恨内容片段，揭示了粗粒度视频 注会引入 签噪声，证明时间粒度对模型性能有显著影响，并强调需要开发具备时间感知能力的检测方法。</span><br>
<span id='abs_en'>English: This study examines how coarse video-level annotations introduce label noise in hate speech detection by analyzing trimmed hateful segments from multimodal datasets, revealing that temporal granularity significantly impacts model performance and highlighting the need for temporally-aware approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1025, <a href='https://arxiv.org/pdf/2508.04748.pdf' target='_blank'>https://arxiv.org/pdf/2508.04748.pdf</a></span>   <span><a href='https://github.com/szu-tera/AttriLens-Mol' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Lin, Long Chen, Yile Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04748">AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have shown promise in assisting molecular property prediction tasks but often rely on human-crafted prompts and chain-of-thought templates. While recent advanced large reasoning models like DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process, their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol, an attribute-guided reinforcement learning framework for molecular property prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1) a format reward encouraging attribute-based structured output, (2) a count reward to avoid enumerating irrelevant attributes, and (3) a rationality reward using advanced LLMs and RDKit to verify the relatedness of the generated attributes. This approach implicitly elicits the model's inherent knowledge of relevant molecular attributes during reasoning, enables making predictions for the molecular property more effectively. Experiments on both in-distribution and out-of-distribution datasets show that, training both 7B-size R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our proposed AttriLens-Mol method significantly boosts the performance, getting comparable or better results than supervised fine-tuning models (Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o, DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the target property, when used as features for an interpretable decision tree model, yield superior performance compared to attributes generated by prompting LLMs. This shows that AttriLens-Mol effectively elicits more relevant and predictive molecular attributes, leading to enhanced interpretability and performance for property prediction. We release the code in https://github.com/szu-tera/AttriLens-Mol.<br>
<span id='abs_ch'>中文: AttriLens-Mol提出了一种属性引导的强化学 框架，通过 式、计数和合理性奖励机制引导大语言模型生成结构化的相关分子属性，在分子性质预测任务中实现了优于现有方法的性能和可解释性。</span><br>
<span id='abs_en'>English: AttriLens-Mol introduces an attribute-guided reinforcement learning framework that enhances molecular property prediction by steering LLMs to generate structured, relevant attributes through format, count, and rationality rewards, achieving superior performance and interpretability compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1026, <a href='https://arxiv.org/pdf/2508.04748.pdf' target='_blank'>https://arxiv.org/pdf/2508.04748.pdf</a></span>   <span><a href='https://github.com/szu-tera/AttriLens-Mol' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Lin, Long Chen, Yile Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04748">AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have shown promise in assisting molecular property prediction tasks but often rely on human-crafted prompts and chain-of-thought templates. While recent advanced large reasoning models like DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process, their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol, an attribute-guided reinforcement learning framework for molecular property prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1) a format reward encouraging attribute-based structured output, (2) a count reward to avoid enumerating irrelevant attributes, and (3) a rationality reward using advanced LLMs and RDKit to verify the relatedness of the generated attributes. This approach implicitly elicits the model's inherent knowledge of relevant molecular attributes during reasoning, enables making predictions for the molecular property more effectively. Experiments on both in-distribution and out-of-distribution datasets show that, training both 7B-size R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our proposed AttriLens-Mol method significantly boosts the performance, getting comparable or better results than supervised fine-tuning models (Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o, DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the target property, when used as features for an interpretable decision tree model, yield superior performance compared to attributes generated by prompting LLMs. This shows that AttriLens-Mol effectively elicits more relevant and predictive molecular attributes, leading to enhanced interpretability and performance for property prediction. We release the code in https://github.com/szu-tera/AttriLens-Mol.<br>
<span id='abs_ch'>中文: AttriLens-Mol提出了一种属性引导的强化学 框架，通过 式、计数和合理性奖励机制引导大语言模型生成结构化的相关分子属性，在分子性质预测任务中实现了优于现有方法的性能和可解释性。</span><br>
<span id='abs_en'>English: AttriLens-Mol introduces an attribute-guided reinforcement learning framework that enhances molecular property prediction by steering LLMs to generate structured, relevant attributes through format, count, and rationality rewards, achieving superior performance and interpretability compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1027, <a href='https://arxiv.org/pdf/2508.04735.pdf' target='_blank'>https://arxiv.org/pdf/2508.04735.pdf</a></span>   <span><a href='https://github.com/OSUPCVLab/ERDES' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pouyan Navard, Yasemin Ozkut, Srikar Adhikari, Elaine Situ-LaCasse, Josie AcuÃ±a, Adrienne Yarnish, Alper Yilmaz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04735">ERDES: A Benchmark Video Dataset for Retinal Detachment and Macular Status Classification in Ocular Ultrasound</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retinal detachment (RD) is a vision-threatening condition that requires timely intervention to preserve vision. Macular involvement -- whether the macula is still intact (macula-intact) or detached (macula-detached) -- is the key determinant of visual outcomes and treatment urgency. Point-of-care ultrasound (POCUS) offers a fast, non-invasive, cost-effective, and accessible imaging modality widely used in diverse clinical settings to detect RD. However, ultrasound image interpretation is limited by a lack of expertise among healthcare providers, especially in resource-limited settings. Deep learning offers the potential to automate ultrasound-based assessment of RD. However, there are no ML ultrasound algorithms currently available for clinical use to detect RD and no prior research has been done on assessing macular status using ultrasound in RD cases -- an essential distinction for surgical prioritization. Moreover, no public dataset currently supports macular-based RD classification using ultrasound video clips. We introduce Eye Retinal DEtachment ultraSound, ERDES, the first open-access dataset of ocular ultrasound clips labeled for (i) presence of retinal detachment and (ii) macula-intact versus macula-detached status. The dataset is intended to facilitate the development and evaluation of machine learning models for detecting retinal detachment. We also provide baseline benchmarks using multiple spatiotemporal convolutional neural network (CNN) architectures. All clips, labels, and training code are publicly available at https://osupcvlab.github.io/ERDES/.<br>
<span id='abs_ch'>中文摘要：ERDES数据集作为首个开放获取的眼部超声视频集，旨在支持机器学 模型开发，用于检测视网膜脱离及判断黄斑状态，填补了自动化诊断和手术优先级评估领域的关键空白。</span><br>
<span id='abs_en'>English Summary: The ERDES dataset is introduced as the first open-access collection of ocular ultrasound clips to support machine learning development for detecting retinal detachment and classifying macular status, addressing a critical gap in automated diagnosis and surgical prioritization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1028, <a href='https://arxiv.org/pdf/2508.04700.pdf' target='_blank'>https://arxiv.org/pdf/2508.04700.pdf</a></span>   <span><a href='https://github.com/SunzeY/SEAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyi Sun, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Tong Wu, Dahua Lin, Jiaqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04700">SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Repurposing large vision-language models (LVLMs) as computer use agents (CUAs) has led to substantial breakthroughs, primarily driven by human-labeled data. However, these models often struggle with novel and specialized software, particularly in scenarios lacking human annotations. To address this challenge, we propose SEAgent, an agentic self-evolving framework enabling CUAs to autonomously evolve through interactions with unfamiliar software. Specifically, SEAgent empowers computer-use agents to autonomously master novel software environments via experiential learning, where agents explore new software, learn through iterative trial-and-error, and progressively tackle auto-generated tasks organized from simple to complex. To achieve this goal, we design a World State Model for step-wise trajectory assessment, along with a Curriculum Generator that generates increasingly diverse and challenging tasks. The agent's policy is updated through experiential learning, comprised of adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist training strategy that integrates individual experiential insights from specialist agents, facilitating the development of a stronger generalist CUA capable of continuous autonomous evolution. This unified agent ultimately achieves performance surpassing ensembles of individual specialist agents on their specialized software. We validate the effectiveness of SEAgent across five novel software environments within OS-World. Our approach achieves a significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a competitive open-source CUA, i.e., UI-TARS.<br>
<span id='abs_ch'>中文：SEAgent框架通过经验学 使计算机使用代理能自主掌握新型软件，结合自我进化机制和专家知识，在成功率上比现有模型提升了23.2%。</span><br>
<span id='abs_en'>English: The proposed SEAgent framework enables computer-use agents to autonomously master novel software through experiential learning, achieving a 23.2% improvement in success rate over existing models by integrating self-evolving mechanisms and specialist knowledge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1029, <a href='https://arxiv.org/pdf/2508.04676.pdf' target='_blank'>https://arxiv.org/pdf/2508.04676.pdf</a></span>   <span><a href='https://github.com/Qznan/GeRe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunan Zhang, Shuoran Jiang, Mengchen Zhao, Yuefeng Li, Yang Fan, Xiangping Wu, Qingcai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04676">GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The continual learning capability of large language models (LLMs) is crucial for advancing artificial general intelligence. However, continual fine-tuning LLMs across various domains often suffers from catastrophic forgetting, characterized by: 1) significant forgetting of their general capabilities, and 2) sharp performance declines in previously learned tasks. To simultaneously address both issues in a simple yet stable manner, we propose General Sample Replay (GeRe), a framework that use usual pretraining texts for efficient anti-forgetting. Beyond revisiting the most prevalent replay-based practices under GeRe, we further leverage neural states to introduce a enhanced activation states constrained optimization method using threshold-based margin (TM) loss, which maintains activation state consistency during replay learning. We are the first to validate that a small, fixed set of pre-collected general replay samples is sufficient to resolve both concerns--retaining general capabilities while promoting overall performance across sequential tasks. Indeed, the former can inherently facilitate the latter. Through controlled experiments, we systematically compare TM with different replay strategies under the GeRe framework, including vanilla label fitting, logit imitation via KL divergence and feature imitation via L1/L2 losses. Results demonstrate that TM consistently improves performance and exhibits better robustness. Our work paves the way for efficient replay of LLMs for the future. Our code and data are available at https://github.com/Qznan/GeRe.<br>
<span id='abs_ch'>中文: GeRe框架通过固定通用回放 本集和增强的激活状态优化方法，有效缓解大语言模型持续微调中的灾难性遗忘，确保通用能力保留的同时提升任务性能。</span><br>
<span id='abs_en'>English: The GeRe framework effectively mitigates catastrophic forgetting in large language models during continual fine-tuning by using a fixed set of general replay samples and an enhanced activation state optimization method, ensuring both general capability retention and improved task performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1030, <a href='https://arxiv.org/pdf/2508.04655.pdf' target='_blank'>https://arxiv.org/pdf/2508.04655.pdf</a></span>   <span><a href='https://github.com/wanghao9610/X-SAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Wang, Limeng Qiao, Zequn Jie, Zhijian Huang, Chengjian Feng, Qingfang Zheng, Lin Ma, Xiangyuan Lan, Xiaodan Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04655">X-SAM: From Segment Anything to Any Segmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) demonstrate strong capabilities in broad knowledge representation, yet they are inherently deficient in pixel-level perceptual understanding. Although the Segment Anything Model (SAM) represents a significant advancement in visual-prompt-driven image segmentation, it exhibits notable limitations in multi-mask prediction and category-specific segmentation tasks, and it cannot integrate all segmentation tasks within a unified model architecture. To address these limitations, we present X-SAM, a streamlined Multimodal Large Language Model (MLLM) framework that extends the segmentation paradigm from \textit{segment anything} to \textit{any segmentation}. Specifically, we introduce a novel unified framework that enables more advanced pixel-level perceptual comprehension for MLLMs. Furthermore, we propose a new segmentation task, termed Visual GrounDed (VGD) segmentation, which segments all instance objects with interactive visual prompts and empowers MLLMs with visual grounded, pixel-wise interpretative capabilities. To enable effective training on diverse data sources, we present a unified training strategy that supports co-training across multiple datasets. Experimental results demonstrate that X-SAM achieves state-of-the-art performance on a wide range of image segmentation benchmarks, highlighting its efficiency for multimodal, pixel-level visual understanding. Code is available at https://github.com/wanghao9610/X-SAM.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1031, <a href='https://arxiv.org/pdf/2508.04476.pdf' target='_blank'>https://arxiv.org/pdf/2508.04476.pdf</a></span>   <span><a href='https://github.com/RamyaLab/metric-learning-RKHS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gokcan Tatli, Yi Chen, Blake Mason, Robert Nowak, Ramya Korlakai Vinayak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04476">Metric Learning in an RKHS</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Metric learning from a set of triplet comparisons in the form of "Do you think item h is more similar to item i or item j?", indicating similarity and differences between items, plays a key role in various applications including image retrieval, recommendation systems, and cognitive psychology. The goal is to learn a metric in the RKHS that reflects the comparisons. Nonlinear metric learning using kernel methods and neural networks have shown great empirical promise. While previous works have addressed certain aspects of this problem, there is little or no theoretical understanding of such methods. The exception is the special (linear) case in which the RKHS is the standard Euclidean space $\mathbb{R}^d$; there is a comprehensive theory for metric learning in $\mathbb{R}^d$. This paper develops a general RKHS framework for metric learning and provides novel generalization guarantees and sample complexity bounds. We validate our findings through a set of simulations and experiments on real datasets. Our code is publicly available at https://github.com/RamyaLab/metric-learning-RKHS.<br>
<span id='abs_ch'>中文: 本文提出了一个基于再生 希尔伯特空间的通用度量学 框架，从三元组比较中学 度量，并提供了理论保证和在真实数据集上的实证验证。</span><br>
<span id='abs_en'>English: This paper introduces a general RKHS framework for metric learning from triplet comparisons, providing theoretical guarantees and empirical validation on real datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1032, <a href='https://arxiv.org/pdf/2508.04361.pdf' target='_blank'>https://arxiv.org/pdf/2508.04361.pdf</a></span>   <span><a href='https://github.com/fuqingbie/omni-game-benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fuqing Bie, Shiyu Huang, Xijia Tao, Zhiqin Fang, Leyi Pan, Junzhe Chen, Min Ren, Liuyu Xiang, Zhaofeng He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04361">OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While generalist foundation models like Gemini and GPT-4o demonstrate impressive multi-modal competence, existing evaluations fail to test their intelligence in dynamic, interactive worlds. Static benchmarks lack agency, while interactive benchmarks suffer from a severe modal bottleneck, typically ignoring crucial auditory and temporal cues. To bridge this evaluation chasm, we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate, but to probe the fusion and reasoning capabilities of agentic models across the full sensory spectrum. Built on a core philosophy of modality interdependence, OmniPlay comprises a suite of five game environments that systematically create scenarios of both synergy and conflict, forcing agents to perform genuine cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal models reveals a critical dichotomy: they exhibit superhuman performance on high-fidelity memory tasks but suffer from systemic failures in challenges requiring robust reasoning and strategic planning. We demonstrate that this fragility stems from brittle fusion mechanisms, which lead to catastrophic performance degradation under modality conflict and uncover a counter-intuitive "less is more" paradox, where removing sensory information can paradoxically improve performance. Our findings suggest that the path toward robust AGI requires a research focus beyond scaling to explicitly address synergistic fusion. Our platform is available for anonymous review at https://github.com/fuqingbie/omni-game-benchmark.<br>
<span id='abs_ch'>中文: OmniPlay基准测试表明，现有全模态模型在记忆任务中表现卓越，但 脆弱的跨模态融合机制而在推理挑战中失败，指出实现强人工智能需聚焦于协同融合 究而非单纯规模扩展。</span><br>
<span id='abs_en'>English: The OmniPlay benchmark reveals that current omni-modal models excel in memory tasks but fail in reasoning challenges due to brittle cross-modal fusion, suggesting robust AGI requires focused research on synergistic integration rather than mere scaling.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1033, <a href='https://arxiv.org/pdf/2508.04361.pdf' target='_blank'>https://arxiv.org/pdf/2508.04361.pdf</a></span>   <span><a href='https://github.com/fuqingbie/omni-game-benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fuqing Bie, Shiyu Huang, Xijia Tao, Zhiqin Fang, Leyi Pan, Junzhe Chen, Min Ren, Liuyu Xiang, Zhaofeng He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04361">OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While generalist foundation models like Gemini and GPT-4o demonstrate impressive multi-modal competence, existing evaluations fail to test their intelligence in dynamic, interactive worlds. Static benchmarks lack agency, while interactive benchmarks suffer from a severe modal bottleneck, typically ignoring crucial auditory and temporal cues. To bridge this evaluation chasm, we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate, but to probe the fusion and reasoning capabilities of agentic models across the full sensory spectrum. Built on a core philosophy of modality interdependence, OmniPlay comprises a suite of five game environments that systematically create scenarios of both synergy and conflict, forcing agents to perform genuine cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal models reveals a critical dichotomy: they exhibit superhuman performance on high-fidelity memory tasks but suffer from systemic failures in challenges requiring robust reasoning and strategic planning. We demonstrate that this fragility stems from brittle fusion mechanisms, which lead to catastrophic performance degradation under modality conflict and uncover a counter-intuitive "less is more" paradox, where removing sensory information can paradoxically improve performance. Our findings suggest that the path toward robust AGI requires a research focus beyond scaling to explicitly address synergistic fusion. Our platform is available for anonymous review at https://github.com/fuqingbie/omni-game-benchmark.<br>
<span id='abs_ch'>中文: OmniPlay基准测试表明，现有全模态模型在记忆任务中表现卓越，但 脆弱的跨模态融合机制而在推理挑战中失败，指出实现强人工智能需聚焦于协同融合 究而非单纯规模扩展。</span><br>
<span id='abs_en'>English: The OmniPlay benchmark reveals that current omni-modal models excel in memory tasks but fail in reasoning challenges due to brittle cross-modal fusion, suggesting robust AGI requires focused research on synergistic integration rather than mere scaling.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1034, <a href='https://arxiv.org/pdf/2508.04260.pdf' target='_blank'>https://arxiv.org/pdf/2508.04260.pdf</a></span>   <span><a href='https://github.com/Event-AHU/SAV' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Ziwen Wang, Wentao Wu, Anjie Wang, Jiashu Wu, Yantao Pan, Chenglong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04260">Segment Any Vehicle: Semantic and Visual Context Driven SAM and A Benchmark</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the rapid advancement of autonomous driving, vehicle perception, particularly detection and segmentation, has placed increasingly higher demands on algorithmic performance. Pre-trained large segmentation models, especially Segment Anything Model (SAM), have sparked significant interest and inspired new research directions in artificial intelligence. However, SAM cannot be directly applied to the fine-grained task of vehicle part segmentation, as its text-prompted segmentation functionality is not publicly accessible, and the mask regions generated by its default mode lack semantic labels, limiting its utility in structured, category-specific segmentation tasks. To address these limitations, we propose SAV, a novel framework comprising three core components: a SAM-based encoder-decoder, a vehicle part knowledge graph, and a context sample retrieval encoding module. The knowledge graph explicitly models the spatial and geometric relationships among vehicle parts through a structured ontology, effectively encoding prior structural knowledge. Meanwhile, the context retrieval module enhances segmentation by identifying and leveraging visually similar vehicle instances from training data, providing rich contextual priors for improved generalization. Furthermore, we introduce a new large-scale benchmark dataset for vehicle part segmentation, named VehicleSeg10K, which contains 11,665 high-quality pixel-level annotations across diverse scenes and viewpoints. We conduct comprehensive experiments on this dataset and two other datasets, benchmarking multiple representative baselines to establish a solid foundation for future research and comparison. % Both the dataset and source code of this paper will be released upon acceptance. Both the dataset and source code of this paper will be released on https://github.com/Event-AHU/SAV<br>
<span id='abs_ch'>中文: 本文提出SAV框架，通过结合基于SAM的编 器-解 器、知识图谱和上下文检索模块来改进车辆部件分割，并发布了VehicleSeg10K数据集以推动该领域 究。English: This paper introduces SAV, a novel framework that enhances vehicle part segmentation by integrating a SAM-based encoder-decoder with a knowledge graph and context retrieval module, and releases the VehicleSeg10K dataset to advance research in this field.Paperid:985,https://arxiv.org/pdf/2508.04251.pdfGitHub</span><br>
<span id='abs_en'>English: This paper introduces SAV, a novel framework that enhances vehicle part segmentation by integrating a SAM-based encoder-decoder with a knowledge graph and context retrieval module, and releases the VehicleSeg10K dataset to advance research in this field.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1035, <a href='https://arxiv.org/pdf/2508.04197.pdf' target='_blank'>https://arxiv.org/pdf/2508.04197.pdf</a></span>   <span><a href='https://github.com/zhangyan-ucas/GAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yan Zhang, Gangyan Zeng, Daiqing Wu, Huawen Shen, Binbin Li, Yu Zhou, Can Ma, Xiaojun Bi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04197">Gather and Trace: Rethinking Video TextVQA from an Instance-oriented Perspective</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Video text-based visual question answering (Video TextVQA) aims to answer questions by explicitly reading and reasoning about the text involved in a video. Most works in this field follow a frame-level framework which suffers from redundant text entities and implicit relation modeling, resulting in limitations in both accuracy and efficiency. In this paper, we rethink the Video TextVQA task from an instance-oriented perspective and propose a novel model termed GAT (Gather and Trace). First, to obtain accurate reading result for each video text instance, a context-aggregated instance gathering module is designed to integrate the visual appearance, layout characteristics, and textual contents of the related entities into a unified textual representation. Then, to capture dynamic evolution of text in the video flow, an instance-focused trajectory tracing module is utilized to establish spatio-temporal relationships between instances and infer the final answer. Extensive experiments on several public Video TextVQA datasets validate the effectiveness and generalization of our framework. GAT outperforms existing Video TextVQA methods, video-language pretraining methods, and video large language models in both accuracy and inference speed. Notably, GAT surpasses the previous state-of-the-art Video TextVQA methods by 3.86\% in accuracy and achieves ten times of faster inference speed than video large language models. The source code is available at https://github.com/zhangyan-ucas/GAT.<br>
<span id='abs_ch'>中文摘要：GAT模型通过聚合文本实例上下文并追踪其时空轨迹，显著提升了视频文本问答的准确性和推理速度，超越了现有最优方法。</span><br>
<span id='abs_en'>English Summary: The GAT model improves Video TextVQA by gathering contextual text instances and tracing their spatio-temporal trajectories, achieving superior accuracy and faster inference than existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1036, <a href='https://arxiv.org/pdf/2508.04149.pdf' target='_blank'>https://arxiv.org/pdf/2508.04149.pdf</a></span>   <span><a href='https://github.com/Difficulty-Based-Preference-Data-Select/Difficulty-Based-Preference-Data-Select' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Qi, Rongwu Xu, Zhijing Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04149">Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Aligning large language models (LLMs) with human preferences is a critical challenge in AI research. While methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they often rely on large, costly preference datasets. The current work lacks methods for high-quality data selection specifically for preference data. In this work, we introduce a novel difficulty-based data selection strategy for preference datasets, grounded in the DPO implicit reward mechanism. By selecting preference data examples with smaller DPO implicit reward gaps, which are indicative of more challenging cases, we improve data efficiency and model alignment. Our approach consistently outperforms five strong baselines across multiple datasets and alignment tasks, achieving superior performance with only 10\% of the original data. This principled, efficient selection method offers a promising solution for scaling LLM alignment with limited resources.<br>
<span id='abs_ch'>中文: 本文提出了一种基于难度的偏好数据选择策略，利用DPO隐式奖励机制筛选更具挑战性的 本，在仅使用10%数据的情况下持续超越多个基线方法，为资源受限的大语言模型对齐提供了高效解决方案。</span><br>
<span id='abs_en'>English: This paper introduces a difficulty-based data selection strategy for preference datasets using DPO's implicit reward mechanism, which consistently outperforms baselines by achieving superior alignment with only 10% of data, offering an efficient solution for LLM alignment with limited resources.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1037, <a href='https://arxiv.org/pdf/2508.04138.pdf' target='_blank'>https://arxiv.org/pdf/2508.04138.pdf</a></span>   <span><a href='https://github.com/hijih/copo-code.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinghang Han, Jiawei Chen, Hang Shao, Hao Ma, Mingcheng Li, Xintian Shen, Lihao Zheng, Wei Chen, Tao Wei, Lihua Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04138">COPO: Consistency-Aware Policy Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning has significantly enhanced the reasoning capabilities of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the introduction of DeepSeek R1 has inspired a surge of interest in leveraging rule-based rewards as a low-cost alternative for computing advantage functions and guiding policy optimization. However, a common challenge observed across many replication and extension efforts is that when multiple sampled responses under a single prompt converge to identical outcomes, whether correct or incorrect, the group-based advantage degenerates to zero. This leads to vanishing gradients and renders the corresponding samples ineffective for learning, ultimately limiting training efficiency and downstream performance. To address this issue, we propose a consistency-aware policy optimization framework that introduces a structured global reward based on outcome consistency, the global loss based on it ensures that, even when model outputs show high intra-group consistency, the training process still receives meaningful learning signals, which encourages the generation of correct and self-consistent reasoning paths from a global perspective. Furthermore, we incorporate an entropy-based soft blending mechanism that adaptively balances local advantage estimation with global optimization, enabling dynamic transitions between exploration and convergence throughout training. Our method introduces several key innovations in both reward design and optimization strategy. We validate its effectiveness through substantial performance gains on multiple mathematical reasoning benchmarks, highlighting the proposed framework's robustness and general applicability. Code of this work has been released at https://github.com/hijih/copo-code.git.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1038, <a href='https://arxiv.org/pdf/2508.04107.pdf' target='_blank'>https://arxiv.org/pdf/2508.04107.pdf</a></span>   <span><a href='https://github.com/jcwang0602/MLLMSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingchao Wang, Zhijian Wu, Dingjiang Huang, Yefeng Zheng, Hong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04107">Unlocking the Potential of MLLMs in Referring Expression Segmentation via a Light-weight Mask Decoder</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reference Expression Segmentation (RES) aims to segment image regions specified by referring expressions and has become popular with the rise of multimodal large models (MLLMs). While MLLMs excel in semantic understanding, their token-generation paradigm struggles with pixel-level dense prediction. Existing RES methods either couple MLLMs with the parameter-heavy Segment Anything Model (SAM) with 632M network parameters or adopt SAM-free lightweight pipelines that sacrifice accuracy. To address the trade-off between performance and cost, we specifically propose MLLMSeg, a novel framework that fully exploits the inherent visual detail features encoded in the MLLM vision encoder without introducing an extra visual encoder. Besides, we propose a detail-enhanced and semantic-consistent feature fusion module (DSFF) that fully integrates the detail-related visual feature with the semantic-related feature output by the large language model (LLM) of MLLM. Finally, we establish a light-weight mask decoder with only 34M network parameters that optimally leverages detailed spatial features from the visual encoder and semantic features from the LLM to achieve precise mask prediction. Extensive experiments demonstrate that our method generally surpasses both SAM-based and SAM-free competitors, striking a better balance between performance and cost. Code is available at https://github.com/jcwang0602/MLLMSeg.<br>
<span id='abs_ch'>中文: 提出的MLLMSeg框架充分利用多模态大模型固有的视觉特征， 需额外视觉编 器即可实现精确的参照表达分割，在性能和成本效率上均优于现有方法。</span><br>
<span id='abs_en'>English: The proposed MLLMSeg framework effectively utilizes the inherent visual features of multimodal large models to achieve precise reference expression segmentation without additional visual encoders, outperforming existing methods in both performance and cost efficiency.Paperid:995,https://arxiv.org/pdf/2508.04106.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1039, <a href='https://arxiv.org/pdf/2508.04080.pdf' target='_blank'>https://arxiv.org/pdf/2508.04080.pdf</a></span>   <span><a href='https://github.com/JinfanTang/GeoSR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinfan Tang, Kunming Wu, Ruifeng Gongxie, Yuya He, Yuankai Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04080">GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent studies have extended the application of large language models (LLMs) to geographic problems, revealing surprising geospatial competence even without explicit spatial supervision. However, LLMs still face challenges in spatial consistency, multi-hop reasoning, and geographic bias. To address these issues, we propose GeoSR, a self-refining agentic reasoning framework that embeds core geographic principles -- most notably Tobler's First Law of Geography -- into an iterative prediction loop. In GeoSR, the reasoning process is decomposed into three collaborating agents: (1) a variable-selection agent that selects relevant covariates from the same location; (2) a point-selection agent that chooses reference predictions at nearby locations generated by the LLM in previous rounds; and (3) a refine agent that coordinates the iterative refinement process by evaluating prediction quality and triggering further rounds when necessary. This agentic loop progressively improves prediction quality by leveraging both spatial dependencies and inter-variable relationships. We validate GeoSR on tasks ranging from physical-world property estimation to socioeconomic prediction. Experimental results show consistent improvements over standard prompting strategies, demonstrating that incorporating geostatistical priors and spatially structured reasoning into LLMs leads to more accurate and equitable geospatial predictions. The code of GeoSR is available at https://github.com/JinfanTang/GeoSR.<br>
<span id='abs_ch'>中文摘要：GeoSR框架通过引入地理学原理和空间推理代理循环，有效提升大语言模型在空间一致性、多跳推理和地理偏差方面的表现，实现更精准的地理空间预测。</span><br>
<span id='abs_en'>English Summary: The GeoSR framework enhances LLMs' geospatial prediction accuracy by integrating geographic principles through an iterative agentic reasoning process that leverages spatial dependencies and variable relationships.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1040, <a href='https://arxiv.org/pdf/2508.04036.pdf' target='_blank'>https://arxiv.org/pdf/2508.04036.pdf</a></span>   <span><a href='https://github.com/TrinhQuocNguyen/CORE-ReID-V2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Trinh Quoc Nguyen, Oky Dicky Ardiansyah Prima, Syahid Al Irfan, Hindriyanto Dwi Purnomo, Radius Tanone
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04036">CORE-ReID V2: Advancing the Domain Adaptation for Object Re-Identification with Optimized Training and Ensemble Fusion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This study presents CORE-ReID V2, an enhanced framework building upon CORE-ReID. The new framework extends its predecessor by addressing Unsupervised Domain Adaptation (UDA) challenges in Person ReID and Vehicle ReID, with further applicability to Object ReID. During pre-training, CycleGAN is employed to synthesize diverse data, bridging image characteristic gaps across different domains. In the fine-tuning, an advanced ensemble fusion mechanism, consisting of the Efficient Channel Attention Block (ECAB) and the Simplified Efficient Channel Attention Block (SECAB), enhances both local and global feature representations while reducing ambiguity in pseudo-labels for target samples. Experimental results on widely used UDA Person ReID and Vehicle ReID datasets demonstrate that the proposed framework outperforms state-of-the-art methods, achieving top performance in Mean Average Precision (mAP) and Rank-k Accuracy (Top-1, Top-5, Top-10). Moreover, the framework supports lightweight backbones such as ResNet18 and ResNet34, ensuring both scalability and efficiency. Our work not only pushes the boundaries of UDA-based Object ReID but also provides a solid foundation for further research and advancements in this domain. Our codes and models are available at https://github.com/TrinhQuocNguyen/CORE-ReID-V2.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1041, <a href='https://arxiv.org/pdf/2508.04025.pdf' target='_blank'>https://arxiv.org/pdf/2508.04025.pdf</a></span>   <span><a href='https://github.com/Fanye12/RecAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Hao, Shuai Wang, Kaiwen Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04025">Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Graphical user interface (GUI) agents have shown promise in automating mobile tasks but still struggle with input redundancy and decision ambiguity. In this paper, we present \textbf{RecAgent}, an uncertainty-aware agent that addresses these issues through adaptive perception. We distinguish two types of uncertainty in GUI navigation: (1) perceptual uncertainty, caused by input redundancy and noise from comprehensive screen information, and (2) decision uncertainty, arising from ambiguous tasks and complex reasoning. To reduce perceptual uncertainty, RecAgent employs a component recommendation mechanism that identifies and focuses on the most relevant UI elements. For decision uncertainty, it uses an interactive module to request user feedback in ambiguous situations, enabling intent-aware decisions. These components are integrated into a unified framework that proactively reduces input complexity and reacts to high-uncertainty cases via human-in-the-loop refinement. Additionally, we propose a dataset called \textbf{ComplexAction} to evaluate the success rate of GUI agents in executing specified single-step actions within complex scenarios. Extensive experiments validate the effectiveness of our approach. The dataset and code will be available at https://github.com/Fanye12/RecAgent.<br>
<span id='abs_ch'>中文摘要：RecAgent是一种不确定性感知的GUI代理，通过自适应感知解决输入冗余和决策模糊问题，利用组件推荐和用户反馈提升移动任务自动化性能。</span><br>
<span id='abs_en'>English Summary: RecAgent is an uncertainty-aware GUI agent that tackles input redundancy and decision ambiguity through adaptive perception, using component recommendations and user feedback to enhance mobile task automation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1042, <a href='https://arxiv.org/pdf/2508.04010.pdf' target='_blank'>https://arxiv.org/pdf/2508.04010.pdf</a></span>   <span><a href='https://github.com/YurunChen/HarmonyGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yurun Chen, Xavier Hu, Yuhan Liu, Keting Yin, Juncheng Li, Zhuosheng Zhang, Shengyu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.04010">HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models enable agents to autonomously perform tasks in open web environments. However, as hidden threats within the web evolve, web agents face the challenge of balancing task performance with emerging risks during long-sequence operations. Although this challenge is critical, current research remains limited to single-objective optimization or single-turn scenarios, lacking the capability for collaborative optimization of both safety and utility in web environments. To address this gap, we propose HarmonyGuard, a multi-agent collaborative framework that leverages policy enhancement and objective optimization to jointly improve both utility and safety. HarmonyGuard features a multi-agent architecture characterized by two fundamental capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent within HarmonyGuard, which automatically extracts and maintains structured security policies from unstructured external documents, while continuously updating policies in response to evolving threats. (2) Dual-Objective Optimization: Based on the dual objectives of safety and utility, the Utility Agent integrated within HarmonyGuard performs the Markovian real-time reasoning to evaluate the objectives and utilizes metacognitive capabilities for their optimization. Extensive evaluations on multiple benchmarks show that HarmonyGuard improves policy compliance by up to 38% and task completion by up to 20% over existing baselines, while achieving over 90% policy compliance across all tasks. Our project is available here: https://github.com/YurunChen/HarmonyGuard.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1043, <a href='https://arxiv.org/pdf/2508.03785.pdf' target='_blank'>https://arxiv.org/pdf/2508.03785.pdf</a></span>   <span><a href='https://github.com/calgo-lab/BGR/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Teodor Chiaburu, Vipin Singh, Frank HauÃer, Felix BieÃmann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03785">SoilNet: A Multimodal Multitask Model for Hierarchical Classification of Soil Horizons</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While recent advances in foundation models have improved the state of the art in many domains, some problems in empirical sciences could not benefit from this progress yet. Soil horizon classification, for instance, remains challenging because of its multimodal and multitask characteristics and a complex hierarchically structured label taxonomy. Accurate classification of soil horizons is crucial for monitoring soil health, which directly impacts agricultural productivity, food security, ecosystem stability and climate resilience. In this work, we propose $\textit{SoilNet}$ - a multimodal multitask model to tackle this problem through a structured modularized pipeline. Our approach integrates image data and geotemporal metadata to first predict depth markers, segmenting the soil profile into horizon candidates. Each segment is characterized by a set of horizon-specific morphological features. Finally, horizon labels are predicted based on the multimodal concatenated feature vector, leveraging a graph-based label representation to account for the complex hierarchical relationships among soil horizons. Our method is designed to address complex hierarchical classification, where the number of possible labels is very large, imbalanced and non-trivially structured. We demonstrate the effectiveness of our approach on a real-world soil profile dataset. All code and experiments can be found in our repository: https://github.com/calgo-lab/BGR/<br>
<span id='abs_ch'>中文: SoilNet是一种多模态多任务模型，通过整合图像数据和地理时态元数据，采用结构化流程实现土壤层精准分类，有效处理复杂层级 签关系以提升土壤健康监测能力。</span><br>
<span id='abs_en'>English: SoilNet is a multimodal multitask model that integrates image data and geotemporal metadata to accurately classify soil horizons through a structured pipeline, addressing complex hierarchical label relationships for improved soil health monitoring.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1044, <a href='https://arxiv.org/pdf/2508.03776.pdf' target='_blank'>https://arxiv.org/pdf/2508.03776.pdf</a></span>   <span><a href='https://github.com/Event-AHU/OpenFusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Zikang Yan, Hao Si, Zhendong Yang, Qingquan Yang, Dengdi Sun, Wanli Lyu, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03776">Revisiting Heat Flux Analysis of Tungsten Monoblock Divertor on EAST using Physics-Informed Neural Network</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Estimating heat flux in the nuclear fusion device EAST is a critically important task. Traditional scientific computing methods typically model this process using the Finite Element Method (FEM). However, FEM relies on grid-based sampling for computation, which is computationally inefficient and hard to perform real-time simulations during actual experiments. Inspired by artificial intelligence-powered scientific computing, this paper proposes a novel Physics-Informed Neural Network (PINN) to address this challenge, significantly accelerating the heat conduction estimation process while maintaining high accuracy. Specifically, given inputs of different materials, we first feed spatial coordinates and time stamps into the neural network, and compute boundary loss, initial condition loss, and physical loss based on the heat conduction equation. Additionally, we sample a small number of data points in a data-driven manner to better fit the specific heat conduction scenario, further enhancing the model's predictive capability. We conduct experiments under both uniform and non-uniform heating conditions on the top surface. Experimental results show that the proposed thermal conduction physics-informed neural network achieves accuracy comparable to the finite element method, while achieving $\times$40 times acceleration in computational efficiency. The dataset and source code will be released on https://github.com/Event-AHU/OpenFusion.<br>
<span id='abs_ch'>本文针对EAST 聚变装置中的热通量估算问题，提出了一种物理信息神经网络方法，在保持与 统有限元法相当精度的同时，将计算效率提升了40倍。</span><br>
<span id='abs_en'>This paper introduces a Physics-Informed Neural Network (PINN) for heat flux estimation in the EAST nuclear fusion device, achieving comparable accuracy to traditional Finite Element Methods while accelerating computation by 40 times.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1045, <a href='https://arxiv.org/pdf/2508.03752.pdf' target='_blank'>https://arxiv.org/pdf/2508.03752.pdf</a></span>   <span><a href='https://github.com/PHPJava666/M3HL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yajun Liu, Zenghui Zhang, Jiang Yue, Weiwei Guo, Dongying Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03752">M$^3$HL: Mutual Mask Mix with High-Low Level Feature Consistency for Semi-Supervised Medical Image Segmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Data augmentation methods inspired by CutMix have demonstrated significant potential in recent semi-supervised medical image segmentation tasks. However, these approaches often apply CutMix operations in a rigid and inflexible manner, while paying insufficient attention to feature-level consistency constraints. In this paper, we propose a novel method called Mutual Mask Mix with High-Low level feature consistency (M$^3$HL) to address the aforementioned challenges, which consists of two key components: 1) M$^3$: An enhanced data augmentation operation inspired by the masking strategy from Masked Image Modeling (MIM), which advances conventional CutMix through dynamically adjustable masks to generate spatially complementary image pairs for collaborative training, thereby enabling effective information fusion between labeled and unlabeled images. 2) HL: A hierarchical consistency regularization framework that enforces high-level and low-level feature consistency between unlabeled and mixed images, enabling the model to better capture discriminative feature representations.Our method achieves state-of-the-art performance on widely adopted medical image segmentation benchmarks including the ACDC and LA datasets. Source code is available at https://github.com/PHPJava666/M3HL<br>
<span id='abs_ch'>中文: 本文提出M$^3$HL方法，通过动态掩 数据增强和分层特征一致性约束，在半监督医学图像分割任务中实现了最先进的性能，并在ACDC和LA数据集上得到验证。</span><br>
<span id='abs_en'>English: This paper introduces M$^3$HL, an enhanced semi-supervised medical image segmentation method that combines dynamic mask-based data augmentation with hierarchical feature consistency to improve performance on benchmarks like ACDC and LA.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1046, <a href='https://arxiv.org/pdf/2508.03742.pdf' target='_blank'>https://arxiv.org/pdf/2508.03742.pdf</a></span>   <span><a href='https://github.com/alibaba-damo-academy/ViSD-Boost' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiwei Cao, Jianpeng Zhang, Zhongyi Shui, Sinuo Wang, Zeli Chen, Xi Li, Le Lu, Xianghua Ye, Tingbo Liang, Qi Zhang, Ling Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03742">Boosting Vision Semantic Density with Anatomy Normality Modeling for Medical Vision-language Pre-training</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision-language pre-training (VLP) has great potential for developing multifunctional and general medical diagnostic capabilities. However, aligning medical images with a low signal-to-noise ratio (SNR) to reports with a high SNR presents a semantic density gap, leading to visual alignment bias. In this paper, we propose boosting vision semantic density to improve alignment effectiveness. On one hand, we enhance visual semantics through disease-level vision contrastive learning, which strengthens the model's ability to differentiate between normal and abnormal samples for each anatomical structure. On the other hand, we introduce an anatomical normality modeling method to model the distribution of normal samples for each anatomy, leveraging VQ-VAE for reconstructing normal vision embeddings in the latent space. This process amplifies abnormal signals by leveraging distribution shifts in abnormal samples, enhancing the model's perception and discrimination of abnormal attributes. The enhanced visual representation effectively captures the diagnostic-relevant semantics, facilitating more efficient and accurate alignment with the diagnostic report. We conduct extensive experiments on two chest CT datasets, CT-RATE and Rad-ChestCT, and an abdominal CT dataset, MedVL-CT69K, and comprehensively evaluate the diagnosis performance across multiple tasks in the chest and abdominal CT scenarios, achieving state-of-the-art zero-shot performance. Notably, our method achieved an average AUC of 84.9% across 54 diseases in 15 organs, significantly surpassing existing methods. Additionally, we demonstrate the superior transfer learning capabilities of our pre-trained model. Code is available at https://github.com/alibaba-damo-academy/ViSD-Boost.<br>
<span id='abs_ch'>中文摘要：本文提出通过疾病级别对比学 和解剖结构正态建模来增强视觉语义密度，从而改进医学视觉语言预训练，在多个CT数据集的零 本诊断任务中实现了最优性能。</span><br>
<span id='abs_en'>English Summary: This paper proposes a method to enhance vision-language pre-training for medical diagnostics by boosting visual semantic density through disease-level contrastive learning and anatomical normality modeling, achieving state-of-the-art zero-shot performance across multiple CT datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1047, <a href='https://arxiv.org/pdf/2508.03741.pdf' target='_blank'>https://arxiv.org/pdf/2508.03741.pdf</a></span>   <span><a href='https://github.com/Linuxin-xxx/LKS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Liu, Qiyang Song, Shaowen Xu, Kerou Zhou, Wenbo Jiang, Xiaoqi Jia, Weijuan Zhang, Heqing Huang, Yakai Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03741">Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) often retain inaccurate or outdated information from pre-training, leading to incorrect predictions or biased outputs during inference. While existing model editing methods can address this challenge, they struggle with editing large amounts of factual information simultaneously and may compromise the general capabilities of the models. In this paper, our empirical study demonstrates that it is feasible to edit the internal representations of LLMs and replace the entities in a manner similar to editing natural language inputs. Based on this insight, we introduce the Latent Knowledge Scalpel (LKS), an LLM editor that manipulates the latent knowledge of specific entities via a lightweight hypernetwork to enable precise and large-scale editing. Experiments conducted on Llama-2 and Mistral show even with the number of simultaneous edits reaching 10,000, LKS effectively performs knowledge editing while preserving the general abilities of the edited LLMs. Code is available at: https://github.com/Linuxin-xxx/LKS.<br>
<span id='abs_ch'>中文: 潜在知识手术刀（LKS）通过操作潜在表征实现了对大型语言模型中事实知识的大规模精准编辑，即使在同时进行上万次修改时仍能保持模型的通用能力。</span><br>
<span id='abs_en'>English: The Latent Knowledge Scalpel (LKS) enables precise, large-scale editing of factual knowledge in LLMs by manipulating latent representations, maintaining model performance even with 10,000 simultaneous edits.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1048, <a href='https://arxiv.org/pdf/2508.03686.pdf' target='_blank'>https://arxiv.org/pdf/2508.03686.pdf</a></span>   <span><a href='https://github.com/open-compass/CompassVerifier' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shudong Liu, Hongwei Liu, Junnan Liu, Linchen Xiao, Songyang Gao, Chengqi Lyu, Yuzhe Gu, Wenwei Zhang, Derek F. Wong, Songyang Zhang, Kai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03686">CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Answer verification is crucial not only for evaluating large language models (LLMs) by matching their unstructured outputs against standard answers, but also serves as the reward model to guide LLM optimization. Most evaluation frameworks rely on regularized matching or employ general LLMs for answer verification, which demands extensive, repetitive customization for regex rules or evaluation prompts. Two fundamental limitations persist in current methodologies: 1) the absence of comprehensive benchmarks that systematically evaluate verification capabilities across different LLMs; and 2) the nascent stage of verifier development, where existing approaches lack both the robustness to handle complex edge cases and the generalizability across different domains. In this work, we develop CompassVerifier, an accurate and robust lightweight verifier model for evaluation and outcome reward. It demonstrates multi-domain competency spanning math, knowledge, and diverse reasoning tasks, with the capability to process various answer types, including multi-subproblems, formulas, and sequence answers, while effectively identifying abnormal/invalid responses. We introduce VerifierBench benchmark comprising model outputs collected from multiple data sources, augmented through manual analysis of metaerror patterns to enhance CompassVerifier. We anticipate that CompassVerifier and VerifierBench will facilitate answer verification, evaluation protocols, and reinforcement learning research. Code and dataset are available at https://github.com/open-compass/CompassVerifier.<br>
<span id='abs_ch'>中文摘要：本文提出了CompassVerifier这一轻量级验证模型，能在多领域准确评估大语言模型输出，并建立VerifierBench基准数据集以推动验证方法和强化学  究。</span><br>
<span id='abs_en'>English Summary: This paper introduces CompassVerifier, a robust lightweight model for verifying LLM outputs across multiple domains, along with the VerifierBench benchmark to advance evaluation and reinforcement learning research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1049, <a href='https://arxiv.org/pdf/2508.03613.pdf' target='_blank'>https://arxiv.org/pdf/2508.03613.pdf</a></span>   <span><a href='https://github.com/Goedel-LM/Goedel-Prover-V2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong Lin, Shange Tang, Bohan Lyu, Ziran Yang, Jui-Hui Chung, Haoyu Zhao, Lai Jiang, Yihan Geng, Jiawei Ge, Jingruo Sun, Jiayun Wu, Jiri Gesi, Ximing Lu, David Acuna, Kaiyu Yang, Hongzhou Lin, Yejin Choi, Danqi Chen, Sanjeev Arora, Chi Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03613">Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce Goedel-Prover-V2, a series of open-source language models that set a new state-of-the-art in automated theorem proving. Built on the standard expert iteration and reinforcement learning pipeline, our approach incorporates three key innovations: (1) Scaffolded data synthesis: We generate synthetic tasks of increasing difficulty to train the model to master increasingly complex theorems; (2) Verifier-guided self-correction: We enable the model to iteratively revise its proofs by leveraging feedback from the Lean compiler; (3) Model averaging: We merge model checkpoints to mitigate the decrease in model output diversity in later stages of training. Our small model, Goedel-Prover-V2-8B, reaches 84.6% pass@32 on MiniF2F and outperforms DeepSeek-Prover-V2-671B under the same metric, despite being 80X smaller. Our flagship model, Goedel-Prover-V2-32B, achieves 88.1% on MiniF2F at pass@32 in standard mode and 90.4% in self-correction mode, outperforming prior SOTA by a large margin. Additionally, our flagship model solves 86 problems on PutnamBench at pass@184, securing the first place among open-source models on the leaderboard, surpassing DeepSeek-Prover-V2-671B's record of solving 47 problems by pass@1024 with a significantly smaller model size and compute budget. At the time of its release (July-August 2025), Goedel-Prover-V2 achieves the strongest overall performance among all open-source theorem provers. It also ranks among the top-performing models--including closed-source systems with publicly reported performance--under a constrained test-time compute budget. Our models, code, and data are released at https://github.com/Goedel-LM/Goedel-Prover-V2.<br>
<span id='abs_ch'>中文: Goedel-Prover-V2系列开源模型通过支架式数据合成和验证器引导自 正等创新技术，在自动定理证明领域实现了最先进性能，其旗舰模型在MiniF2F和PutnamBench基准测试中大幅超越先前系统，同时模型规模显著更小。</span><br>
<span id='abs_en'>English: Goedel-Prover-V2 introduces a series of open-source language models that achieve state-of-the-art performance in automated theorem proving through innovations like scaffolded data synthesis and verifier-guided self-correction, with its flagship model outperforming prior systems on benchmarks like MiniF2F and PutnamBench despite significantly smaller size.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1050, <a href='https://arxiv.org/pdf/2508.03484.pdf' target='_blank'>https://arxiv.org/pdf/2508.03484.pdf</a></span>   <span><a href='https://github.com/horizonsinzqs/SmartGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyao Xu, Dan Zhao, Qingsong Zou, Qing Li, Yong Jiang, Yuhang Wang, Jingyu Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03484">Semantic-aware Graph-guided Behavior Sequences Generation with Large Language Models for Smart Homes</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As smart homes become increasingly prevalent, intelligent models are widely used for tasks such as anomaly detection and behavior prediction. These models are typically trained on static datasets, making them brittle to behavioral drift caused by seasonal changes, lifestyle shifts, or evolving routines. However, collecting new behavior data for retraining is often impractical due to its slow pace, high cost, and privacy concerns. In this paper, we propose SmartGen, an LLM-based framework that synthesizes context-aware user behavior data to support continual adaptation of downstream smart home models. SmartGen consists of four key components. First, we design a Time and Semantic-aware Split module to divide long behavior sequences into manageable, semantically coherent subsequences under dual time-span constraints. Second, we propose Semantic-aware Sequence Compression to reduce input length while preserving representative semantics by clustering behavior mapping in latent space. Third, we introduce Graph-guided Sequence Synthesis, which constructs a behavior relationship graph and encodes frequent transitions into prompts, guiding the LLM to generate data aligned with contextual changes while retaining core behavior patterns. Finally, we design a Two-stage Outlier Filter to identify and remove implausible or semantically inconsistent outputs, aiming to improve the factual coherence and behavioral validity of the generated sequences. Experiments on three real-world datasets demonstrate that SmartGen significantly enhances model performance on anomaly detection and behavior prediction tasks under behavioral drift, with anomaly detection improving by 85.43% and behavior prediction by 70.51% on average. The code is available at https://github.com/horizonsinzqs/SmartGen.<br>
<span id='abs_ch'>中文: SmartGen是一种基于大语言模型的框架，通过生成情境感知的用户行为数据支持智能家居模型的持续适应，在行为漂移情况下将异常检测和预测任务性能分别平均提升85.43%和70.51%。</span><br>
<span id='abs_en'>English: SmartGen, an LLM-based framework, synthesizes context-aware user behavior data to enable continual adaptation of smart home models, significantly improving anomaly detection by 85.43% and behavior prediction by 70.51% under behavioral drift.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1051, <a href='https://arxiv.org/pdf/2508.03475.pdf' target='_blank'>https://arxiv.org/pdf/2508.03475.pdf</a></span>   <span><a href='https://github.com/pranshurastogi29/SemEval-2025-ACL-Multi-and-Crosslingual-Retrieval-using-Bi-encoders' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranshu Rastogi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03475">fact check AI at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-checked Claim Retrieval</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval is approached as a Learning-to-Rank task using a bi-encoder model fine-tuned from a pre-trained transformer optimized for sentence similarity. Training used both the source languages and their English translations for multilingual retrieval and only English translations for cross-lingual retrieval. Using lightweight models with fewer than 500M parameters and training on Kaggle T4 GPUs, the method achieved 92% Success@10 in multilingual and 80% Success@10 in 5th in crosslingual and 10th in multilingual tracks.<br>
<span id='abs_ch'>中文摘要：该 究采用经优化的双编 器转换器模型进行学 排序，在T4 GPU上训练不足5亿参数的轻量模型，实现了多语言检索92%和跨语言检索80%的Success@10指 。</span><br>
<span id='abs_en'>English Summary: The SemEval-2025 Task 7 employs a fine-tuned bi-encoder transformer model for Learning-to-Rank, achieving 92% Success@10 in multilingual and 80% in crosslingual retrieval with efficient sub-500M parameter models trained on T4 GPUs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1052, <a href='https://arxiv.org/pdf/2508.03426.pdf' target='_blank'>https://arxiv.org/pdf/2508.03426.pdf</a></span>   <span><a href='https://github.com/Event-AHU/Medical_Image_Analysis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Futian Wang, Yuhan Qiao, Xiao Wang, Fuling Wang, Yuxiang Zhang, Dengdi Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03426">R2GenKG: Hierarchical Multi-modal Knowledge Graph for LLM-based Radiology Report Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>X-ray medical report generation is one of the important applications of artificial intelligence in healthcare. With the support of large foundation models, the quality of medical report generation has significantly improved. However, challenges such as hallucination and weak disease diagnostic capability still persist. In this paper, we first construct a large-scale multi-modal medical knowledge graph (termed M3KG) based on the ground truth medical report using the GPT-4o. It contains 2477 entities, 3 kinds of relations, 37424 triples, and 6943 disease-aware vision tokens for the CheXpert Plus dataset. Then, we sample it to obtain multi-granularity semantic graphs and use an R-GCN encoder for feature extraction. For the input X-ray image, we adopt the Swin-Transformer to extract the vision features and interact with the knowledge using cross-attention. The vision tokens are fed into a Q-former and retrieved the disease-aware vision tokens using another cross-attention. Finally, we adopt the large language model to map the semantic knowledge graph, input X-ray image, and disease-aware vision tokens into language descriptions. Extensive experiments on multiple datasets fully validated the effectiveness of our proposed knowledge graph and X-ray report generation framework. The source code of this paper will be released on https://github.com/Event-AHU/Medical_Image_Analysis.<br>
<span id='abs_ch'>中文: 本文构建了一个大规模多模态医学知识图谱（M3KG），并提出了一种新框架，通过交叉注意力机制将知识图谱与X射线图像及疾病感知视觉 记相结合，有效提升了AI生成医学报告的准确性并减少了幻觉现象，经多数据集实验充分验证。</span><br>
<span id='abs_en'>English: This paper introduces a large-scale multi-modal medical knowledge graph (M3KG) and a novel framework that integrates it with X-ray images and disease-aware vision tokens using cross-attention mechanisms, significantly enhancing the accuracy and reducing hallucinations in AI-generated medical reports, as validated by extensive experiments.Paperid:1036,https://arxiv.org/pdf/2508.03420.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1053, <a href='https://arxiv.org/pdf/2508.03404.pdf' target='_blank'>https://arxiv.org/pdf/2508.03404.pdf</a></span>   <span><a href='https://github.com/YU-deep/MACT.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinlei Yu, Zhangquan Chen, Yudong Zhang, Shilin Lu, Ruolin Shen, Jiangning Zhang, Xiaobin Hu, Yanwei Fu, Shuicheng Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03404">Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Existing vision-language models (VLMs), whether generalists or specialists, remain constrained by their parameter scale, lack robust self-correction capabilities, and underperform in tasks involving long visual contexts and complex reasoning, resulting in suboptimal performance on document-based tasks. To address this, we propose MACT, a Multi-Agent Collaboration framework with Test-Time scaling, tailored for visual document understanding and visual question answering (VQA). It comprises four distinct small-scale agents, i.e., planning, execution, judgment, and answer agents, with clearly defined roles and effective collaboration. Notably, the judgment agent exclusively verifies correctness and redirects to prior agents for revisions, outperforming conventional correction strategies. To further expand the capability boundaries of the framework, we propose mixed reward modeling that balances agent-specific abilities and global collaboration, as well as agent-wise hybrid test-time scaling, which customizes different scaling strategies for each agent based on their functions. Evaluated on benchmarks spanning both document-based and non-document-based settings, our MACT shows superior performance with a smaller parameter scale without sacrificing the ability of general and mathematical tasks. Especially, it stands out in benchmarks involving long visual contexts and complicated reasoning. The three variants of MACT consistently hold the top three positions in average scores, leading in 13 of the 15 benchmarks. Code will be available at: https://github.com/YU-deep/MACT.git.<br>
<span id='abs_ch'>中文: 提出的MACT框架采用多智能体协作和测试时扩展技术，通过规划、执行、判断和回答四个专门代理的协同工作，在减少参数量的同时，显著提升了视觉文档理解和视觉问答任务的表现，并在多个基准测试中取得领先成绩。</span><br>
<span id='abs_en'>English: The proposed MACT framework employs a multi-agent collaboration system with test-time scaling to enhance visual document understanding and VQA, achieving top performance across multiple benchmarks with fewer parameters by integrating specialized agents for planning, execution, judgment, and answering.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1054, <a href='https://arxiv.org/pdf/2508.03333.pdf' target='_blank'>https://arxiv.org/pdf/2508.03333.pdf</a></span>   <span><a href='https://github.com/magent4aci/CTTS-MM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhende Song, Shengji Tang, Peng Ye, Jiayuan Fan, Lei Bai, Tao Chen, Wanli Ouyang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03333">CTTS: Collective Test-Time Scaling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Test-time scaling (TTS) has emerged as a promising, training-free approach for enhancing large language model (LLM) performance. However, the efficacy of existing methods, such as Best-of-N and Self-Consistency, is fundamentally constrained by the dominant single test-time scaling (STTS) paradigm, which relies on a single LLM agent interacting with a single reward model (SA-SR). Inspired by recent work showing that collective methods can surpass the performance ceiling of individual models, we introduce Collective Test-Time Scaling (CTTS). First, we systematically investigate three primary interaction paradigms of existing multiple models: single-agent-multi-reward (SA-MR), multi-agent-single-reward (MA-SR), and multi-agent-multi-reward (MA-MR). Extensive experiments reveal that the MA-MR paradigm is consistently superior. Based on this finding, we further propose CTTS-MM, a novel framework that operationalizes multi-agent and multi-reward collaboration. CTTS-MM integrates two key technical contributions: (1) for agent collaboration, an Agent Collaboration Search (ACS) that identifies the most effective combination of LLMs from a candidate pool; and (2) for reward model collaboration, a Mixture of Reward Models (MoR) strategy that leverages a Prior Reward model Ensemble Selection (PRES) algorithm to select the optimal ensemble. Evaluations across seven mainstream benchmarks demonstrate that CTTS-MM significantly outperforms leading STTS methods (+4.82% over Best-of-N) and surpasses even flagship proprietary LLMs (+7.06% over GPT-4.1) and open-source LLMs. These results highlight the substantial potential of collective scaling to push the frontier of LLM inference. Code will be released at https://github.com/magent4aci/CTTS-MM.<br>
<span id='abs_ch'>中文: 本文提出集体测试时缩放（CTTS）方法，通过探索多智能体与多奖励模型的协作来增强大语言模型性能，其中CTTS-MM框架在多个基准测试中均展现出优越表现。</span><br>
<span id='abs_en'>English: This paper introduces Collective Test-Time Scaling (CTTS) as a novel approach to enhance large language models by exploring multi-agent and multi-reward-model collaborations, with the proposed CTTS-MM framework demonstrating superior performance across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1055, <a href='https://arxiv.org/pdf/2508.03333.pdf' target='_blank'>https://arxiv.org/pdf/2508.03333.pdf</a></span>   <span><a href='https://github.com/magent4aci/CTTS-MM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhende Song, Shengji Tang, Peng Ye, Jiayuan Fan, Tao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03333">CTTS: Collective Test-Time Scaling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Test-time scaling (TTS) has emerged as a promising research field for enhancing the effectiveness of large language models (LLMs) without extra training. However, most existing approaches, e.g., Best-of-N and Self-Consistency rely on a single agent interacting with a reward model (SA-SR), constrained by limited capabilities of a single test-time scaling (STTS) paradigm. On the other hand, recent works demonstrate that collective-agent methods can break through the upper bound of single-agent systems by orchestrating diverse models. Thus, in this paper, we take a first step towards exploring Collective Test-Time Scaling (CTTS). Consider the different interaction types of single and multiple models, we design three primary paradigms to investigate the optimal paradigm of CTTS: (1) single agent to multiple reward models (SA-MR); (2) multiple agents to single reward model (MA-SR); and (3) multiple agents to multiple reward models (MA-MR). Extensive experiments demonstrate that MA-MR consistently achieves the best performance. Based on this, we propose a novel framework named CTTS-MM that effectively leverages both multi-agent and multi-reward-model collaboration for enhanced inference. Specifically, for multi-agent collaboration, we propose an Agent Collaboration Search (ACS), which searches for the most effective combination of LLM agents from a large candidate pool; for multi-reward-model collaboration, we propose Mixture of Reword Models (MoR), which consists of a curated question pool and a Prior Reward model Ensemble Selection (PRES) to select the optimal combinations of reward models via Pair-wise Reward Ranking (PRR) metric. Experiments across seven mainstream benchmarks demonstrate that the proposed CTTS-MM consistently obtains superior performance. Code will be released at https://github.com/magent4aci/CTTS-MM.<br>
<span id='abs_ch'>中文: 本文提出集体测试时缩放（CTTS）方法，通过探索多智能体与多奖励模型的协作来增强大语言模型性能，其中CTTS-MM框架在多个基准测试中均展现出优越表现。</span><br>
<span id='abs_en'>English: This paper introduces Collective Test-Time Scaling (CTTS) as a novel approach to enhance large language models by exploring multi-agent and multi-reward-model collaborations, with the proposed CTTS-MM framework demonstrating superior performance across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1056, <a href='https://arxiv.org/pdf/2508.03174.pdf' target='_blank'>https://arxiv.org/pdf/2508.03174.pdf</a></span>   <span><a href='https://github.com/InqEduAgent/InqEduAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tian-Fang Zhao, Wen-Xi Yang, Guan Liu, Liang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03174">InqEduAgent: Adaptive AI Learning Partners with Gaussian Process Augmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Collaborative partnership matters in inquiry-oriented education. However, most study partners are selected either rely on experience-based assignments with little scientific planning or build on rule-based machine assistants, encountering difficulties in knowledge expansion and inadequate flexibility. This paper proposes an LLM-empowered agent model for simulating and selecting learning partners tailored to inquiry-oriented learning, named InqEduAgent. Generative agents are designed to capture cognitive and evaluative features of learners in real-world scenarios. Then, an adaptive matching algorithm with Gaussian process augmentation is formulated to identify patterns within prior knowledge. Optimal learning-partner matches are provided for learners facing different exercises. The experimental results show the optimal performance of InqEduAgent in most knowledge-learning scenarios and LLM environment with different levels of capabilities. This study promotes the intelligent allocation of human-based learning partners and the formulation of AI-based learning partners. The code, data, and appendix are publicly available at https://github.com/InqEduAgent/InqEduAgent.<br>
<span id='abs_ch'>中文: 本文提出InqEduAgent模型，利用大语言模型模拟并筛选探究式学 伙伴，通过捕捉学 者特征和自适应匹配算法，在不同知识场景中均展现出最优性能。</span><br>
<span id='abs_en'>English: This paper introduces InqEduAgent, an LLM-powered model that simulates and selects optimal learning partners for inquiry-based education by capturing learner traits and using adaptive matching algorithms, demonstrating superior performance across various knowledge scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1057, <a href='https://arxiv.org/pdf/2508.03174.pdf' target='_blank'>https://arxiv.org/pdf/2508.03174.pdf</a></span>   <span><a href='https://github.com/InqEduAgent/InqEduAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wen-Xi Yang, Tian-Fang Zhao, Guan Liu, Liang Yang, Zi-Tao Liu, Wei-Neng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03174">InqEduAgent: Adaptive AI Learning Partners with Gaussian Process Augmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Collaborative partnership matters in inquiry-oriented education. However, most study partners are selected either rely on experience-based assignments with little scientific planning or build on rule-based machine assistants, encountering difficulties in knowledge expansion and inadequate flexibility. This paper proposes an LLM-empowered agent model for simulating and selecting learning partners tailored to inquiry-oriented learning, named InqEduAgent. Generative agents are designed to capture cognitive and evaluative features of learners in real-world scenarios. Then, an adaptive matching algorithm with Gaussian process augmentation is formulated to identify patterns within prior knowledge. Optimal learning-partner matches are provided for learners facing different exercises. The experimental results show the optimal performance of InqEduAgent in most knowledge-learning scenarios and LLM environment with different levels of capabilities. This study promotes the intelligent allocation of human-based learning partners and the formulation of AI-based learning partners. The code, data, and appendix are publicly available at https://github.com/InqEduAgent/InqEduAgent.<br>
<span id='abs_ch'>中文: 本文提出InqEduAgent模型，利用大语言模型模拟并筛选探究式学 伙伴，通过捕捉学 者特征和自适应匹配算法，在不同知识场景中均展现出最优性能。</span><br>
<span id='abs_en'>English: This paper introduces InqEduAgent, an LLM-powered model that simulates and selects optimal learning partners for inquiry-based education by capturing learner traits and using adaptive matching algorithms, demonstrating superior performance across various knowledge scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1058, <a href='https://arxiv.org/pdf/2508.03167.pdf' target='_blank'>https://arxiv.org/pdf/2508.03167.pdf</a></span>   <span><a href='https://github.com/y0-causal-inference/y0' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Charles Tapley Hoyt, Craig Bakker, Richard J. Callahan, Joseph Cottam, August George, Benjamin M. Gyori, Haley M. Hummel, Nathaniel Merrill, Sara Mohammad Taheri, Pruthvi Prakash Navada, Marc-Antoine Parent, Adam Rupe, Olga Vitek, Jeremy Zucker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03167">Causal identification with $Y_0$</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present the $Y_0$ Python package, which implements causal identification algorithms that apply interventional, counterfactual, and transportability queries to data from (randomized) controlled trials, observational studies, or mixtures thereof. $Y_0$ focuses on the qualitative investigation of causation, helping researchers determine whether a causal relationship can be estimated from available data before attempting to estimate how strong that relationship is. Furthermore, $Y_0$ provides guidance on how to transform the causal query into a symbolic estimand that can be non-parametrically estimated from the available data. $Y_0$ provides a domain-specific language for representing causal queries and estimands as symbolic probabilistic expressions, tools for representing causal graphical models with unobserved confounders, such as acyclic directed mixed graphs (ADMGs), and implementations of numerous identification algorithms from the recent causal inference literature. The $Y_0$ source code can be found under the MIT License at https://github.com/y0-causal-inference/y0 and it can be installed with pip install y0.<br>
<span id='abs_ch'>中文: $Y_0$ Python 包通过干预、反事实和可移植性查询实现 果识别，帮助 究人员使用领域特定语言和图形模型评估 果可估性并将查询转化为符号估计量。</span><br>
<span id='abs_en'>English: The $Y_0$ Python package enables causal identification through interventional, counterfactual, and transportability queries, helping researchers assess causal estimability and transform queries into symbolic estimands using a domain-specific language and graphical models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1059, <a href='https://arxiv.org/pdf/2508.03159.pdf' target='_blank'>https://arxiv.org/pdf/2508.03159.pdf</a></span>   <span><a href='https://github.com/dmis-lab/CoTox' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jueon Park, Yein Park, Minju Song, Soyon Park, Donghyeon Lee, Seungheun Baek, Jaewoo Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03159">CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Drug toxicity remains a major challenge in pharmaceutical development. Recent machine learning models have improved in silico toxicity prediction, but their reliance on annotated data and lack of interpretability limit their applicability. This limits their ability to capture organ-specific toxicities driven by complex biological mechanisms. Large language models (LLMs) offer a promising alternative through step-by-step reasoning and integration of textual data, yet prior approaches lack biological context and transparent rationale. To address this issue, we propose CoTox, a novel framework that integrates LLM with chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox combines chemical structure data, biological pathways, and gene ontology (GO) terms to generate interpretable toxicity predictions through step-by-step reasoning. Using GPT-4o, we show that CoTox outperforms both traditional machine learning and deep learning model. We further examine its performance across various LLMs to identify where CoTox is most effective. Additionally, we find that representing chemical structures with IUPAC names, which are easier for LLMs to understand than SMILES, enhances the model's reasoning ability and improves predictive performance. To demonstrate its practical utility in drug development, we simulate the treatment of relevant cell types with drug and incorporated the resulting biological context into the CoTox framework. This approach allow CoTox to generate toxicity predictions aligned with physiological responses, as shown in case study. This result highlights the potential of LLM-based frameworks to improve interpretability and support early-stage drug safety assessment. The code and prompt used in this work are available at https://github.com/dmis-lab/CoTox.<br>
<span id='abs_ch'>中文: CoTox是一种创新框架，通过将大语言模型与思维链推理相结合，整合化学结构、生物通路和基 本体术语，生成可解释的毒性预测，其性能优于 统模型并提升了药物安全性评估能力。</span><br>
<span id='abs_en'>English: CoTox is a novel framework that integrates large language models with chain-of-thought reasoning, combining chemical structures, biological pathways, and gene ontology terms to generate interpretable toxicity predictions, outperforming traditional models and enhancing drug safety assessment.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1060, <a href='https://arxiv.org/pdf/2508.03127.pdf' target='_blank'>https://arxiv.org/pdf/2508.03127.pdf</a></span>   <span><a href='https://github.com/papersubmit1/landsat30-au' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sai Ma, Zhuang Li, John A Taylor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03127">Landsat30-AU: A Vision-Language Dataset for Australian Landsat Imagery</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision language models (VLMs) that enable natural language interaction with satellite imagery can democratize Earth observation by accelerating expert workflows, making data accessible to non-specialists, and enabling planet-scale automation. However, existing datasets focus mainly on short-term, high-resolution imagery from a limited number of satellites, overlooking low-resolution, multi-satellite, long-term archives, such as Landsat, that are essential for affordable and bias-robust global monitoring. We address this gap with Landsat30-AU, a large-scale vision-language dataset built from 30-meter resolution imagery collected by four Landsat satellites (5, 7, 8, and 9) over Australia, spanning more than 36 years. The dataset includes two components: Landsat30-AU-Cap, containing $196,262$ image-caption pairs, and Landsat30-AU-VQA, comprising 17,725 human-verified visual question answering (VQA) samples across eight remote sensing domains. Both datasets are curated through a bootstrapped pipeline that leverages generic VLMs with iterative refinement and human verification to ensure quality. Our evaluation of eight VLMs on our benchmark reveals that off-the-shelf models struggle to understand satellite imagery. The open-source remote-sensing VLM EarthDial achieves only 0.07 SPIDEr in captioning and a VQA accuracy of 0.48, highlighting the limitations of current approaches. Encouragingly, lightweight fine-tuning of Qwen2.5-VL-7B on Landsat30-AU improves captioning performance from 0.11 to 0.31 SPIDEr and boosts VQA accuracy from 0.74 to 0.87. Code and data are available at https://github.com/papersubmit1/landsat30-au.<br>
<span id='abs_ch'>中文: 视觉语言模型虽能普及地球观测，但现有数据集忽略了对全球监测至关重要的长期多卫星档案；新推出的Landsat30-AU数据集通过提供澳大利亚36年卫星图像及 注，揭示了现有模型的不足，同时证明微调可显著提升性能。</span><br>
<span id='abs_en'>English: Vision language models can democratize Earth observation, but existing datasets overlook critical long-term, multi-satellite archives, which the new Landsat30-AU dataset addresses by providing 36 years of Australian satellite imagery with captions and verified VQA samples, revealing current models' limitations while demonstrating significant improvements through fine-tuning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1061, <a href='https://arxiv.org/pdf/2508.03097.pdf' target='_blank'>https://arxiv.org/pdf/2508.03097.pdf</a></span>   <span><a href='https://github.com/FLAIR-THU/VFLAIR-LLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixuan Gu, Qiufeng Fan, Long Sun, Yang Liu, Xiaojun Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03097">VFLAIR-LLM: A Comprehensive Framework and Benchmark for Split Learning of LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the advancement of Large Language Models (LLMs), LLM applications have expanded into a growing number of fields. However, users with data privacy concerns face limitations in directly utilizing LLM APIs, while private deployments incur significant computational demands. This creates a substantial challenge in achieving secure LLM adaptation under constrained local resources. To address this issue, collaborative learning methods, such as Split Learning (SL), offer a resource-efficient and privacy-preserving solution for adapting LLMs to private domains. In this study, we introduce VFLAIR-LLM (available at https://github.com/FLAIR-THU/VFLAIR-LLM), an extensible and lightweight split learning framework for LLMs, enabling privacy-preserving LLM inference and fine-tuning in resource-constrained environments. Our library provides two LLM partition settings, supporting three task types and 18 datasets. In addition, we provide standard modules for implementing and evaluating attacks and defenses. We benchmark 5 attacks and 9 defenses under various Split Learning for LLM(SL-LLM) settings, offering concrete insights and recommendations on the choice of model partition configurations, defense strategies, and relevant hyperparameters for real-world applications.<br>
<span id='abs_ch'>中文摘要：VFLAIR-LLM框架通过分割学 实现了大语言模型在资源受限环境下的隐私保护适配，提供可配置的模型划分方案，并对安全措施进行全面评估。English Summary: The VFLAIR-LLM framework enables privacy-preserving adaptation of large language models through split learning, providing configurable model partitioning and comprehensive evaluation of security measures for resource-constrained environments.Paperid:1062,https://arxiv.org/pdf/2508.03064.pdfGitHub</span><br>
<span id='abs_en'>English Summary: The VFLAIR-LLM framework enables privacy-preserving adaptation of large language models through split learning, providing configurable model partitioning and comprehensive evaluation of security measures for resource-constrained environments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1062, <a href='https://arxiv.org/pdf/2508.03064.pdf' target='_blank'>https://arxiv.org/pdf/2508.03064.pdf</a></span>   <span><a href='https://github.com/TrinhQuocNguyen/CORE-ReID' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Trinh Quoc Nguyen, Oky Dicky Ardiansyah Prima, Katsuyoshi Hotta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03064">CORE-ReID: Comprehensive Optimization and Refinement through Ensemble fusion in Domain Adaptation for person re-identification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This study introduces a novel framework, "Comprehensive Optimization and Refinement through Ensemble Fusion in Domain Adaptation for Person Re-identification (CORE-ReID)", to address an Unsupervised Domain Adaptation (UDA) for Person Re-identification (ReID). The framework utilizes CycleGAN to generate diverse data that harmonizes differences in image characteristics from different camera sources in the pre-training stage. In the fine-tuning stage, based on a pair of teacher-student networks, the framework integrates multi-view features for multi-level clustering to derive diverse pseudo labels. A learnable Ensemble Fusion component that focuses on fine-grained local information within global features is introduced to enhance learning comprehensiveness and avoid ambiguity associated with multiple pseudo-labels. Experimental results on three common UDAs in Person ReID demonstrate significant performance gains over state-of-the-art approaches. Additional enhancements, such as Efficient Channel Attention Block and Bidirectional Mean Feature Normalization mitigate deviation effects and adaptive fusion of global and local features using the ResNet-based model, further strengthening the framework. The proposed framework ensures clarity in fusion features, avoids ambiguity, and achieves high ac-curacy in terms of Mean Average Precision, Top-1, Top-5, and Top-10, positioning it as an advanced and effective solution for the UDA in Person ReID. Our codes and models are available at https://github.com/TrinhQuocNguyen/CORE-ReID.<br>
<span id='abs_ch'>Chinese: CORE-ReID框架提出了一种新颖的 监督域自适应行人重识别方法，通过CycleGAN生成数据和师生网络集成融合，实现了优于现有方法的性能，其增强的特征清晰度和全面学 效果显著。</span><br>
<span id='abs_en'>English: The CORE-ReID framework introduces a novel unsupervised domain adaptation approach for person re-identification, utilizing CycleGAN-generated data and ensemble fusion with teacher-student networks to achieve superior performance over existing methods through enhanced feature clarity and comprehensive learning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1063, <a href='https://arxiv.org/pdf/2508.03055.pdf' target='_blank'>https://arxiv.org/pdf/2508.03055.pdf</a></span>   <span><a href='https://github.com/hyebin-c/FaceMat.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyebin Cho, Jaehyup Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03055">Uncertainty-Guided Face Matting for Occlusion-Aware Face Transformation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Face filters have become a key element of short-form video content, enabling a wide array of visual effects such as stylization and face swapping. However, their performance often degrades in the presence of occlusions, where objects like hands, hair, or accessories obscure the face. To address this limitation, we introduce the novel task of face matting, which estimates fine-grained alpha mattes to separate occluding elements from facial regions. We further present FaceMat, a trimap-free, uncertainty-aware framework that predicts high-quality alpha mattes under complex occlusions. Our approach leverages a two-stage training pipeline: a teacher model is trained to jointly estimate alpha mattes and per-pixel uncertainty using a negative log-likelihood (NLL) loss, and this uncertainty is then used to guide the student model through spatially adaptive knowledge distillation. This formulation enables the student to focus on ambiguous or occluded regions, improving generalization and preserving semantic consistency. Unlike previous approaches that rely on trimaps or segmentation masks, our framework requires no auxiliary inputs making it well-suited for real-time applications. In addition, we reformulate the matting objective by explicitly treating skin as foreground and occlusions as background, enabling clearer compositing strategies. To support this task, we newly constructed CelebAMat, a large-scale synthetic dataset specifically designed for occlusion-aware face matting. Extensive experiments show that FaceMat outperforms state-of-the-art methods across multiple benchmarks, enhancing the visual quality and robustness of face filters in real-world, unconstrained video scenarios. The source code and CelebAMat dataset are available at https://github.com/hyebin-c/FaceMat.git<br>
<span id='abs_ch'>中文摘要：FaceMat是一种 需辅助输入的新框架，通过预测高质量阿尔法遮罩有效解决面部滤镜中的遮挡问题，显著提升了实时视频应用中的鲁棒性和视觉质量。</span><br>
<span id='abs_en'>English Summary: FaceMat is a novel framework that addresses occlusion challenges in face filters by predicting high-quality alpha mattes without auxiliary inputs, improving robustness and visual quality in real-time video applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1064, <a href='https://arxiv.org/pdf/2508.02753.pdf' target='_blank'>https://arxiv.org/pdf/2508.02753.pdf</a></span>   <span><a href='https://github.com/1327679995/DMSC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan Yang, Jianchao Tang, Zhuo Li, Long Lan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02753">DMSC: Dynamic Multi-Scale Coordination Framework for Time Series Forecasting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Time Series Forecasting (TSF) faces persistent challenges in modeling intricate temporal dependencies across different scales. Despite recent advances leveraging different decomposition operations and novel architectures based on CNN, MLP or Transformer, existing methods still struggle with static decomposition strategies, fragmented dependency modeling, and inflexible fusion mechanisms, limiting their ability to model intricate temporal dependencies. To explicitly solve the mentioned three problems respectively, we propose a novel Dynamic Multi-Scale Coordination Framework (DMSC) with Multi-Scale Patch Decomposition block (EMPD), Triad Interaction Block (TIB) and Adaptive Scale Routing MoE block (ASR-MoE). Specifically, EMPD is designed as a built-in component to dynamically segment sequences into hierarchical patches with exponentially scaled granularities, eliminating predefined scale constraints through input-adaptive patch adjustment. TIB then jointly models intra-patch, inter-patch, and cross-variable dependencies within each layer's decomposed representations. EMPD and TIB are jointly integrated into layers forming a multi-layer progressive cascade architecture, where coarse-grained representations from earlier layers adaptively guide fine-grained feature extraction in subsequent layers via gated pathways. And ASR-MoE dynamically fuses multi-scale predictions by leveraging specialized global and local experts with temporal-aware weighting. Comprehensive experiments on thirteen real-world benchmarks demonstrate that DMSC consistently maintains state-of-the-art (SOTA) performance and superior computational efficiency for TSF tasks. Code is available at https://github.com/1327679995/DMSC.<br>
<span id='abs_ch'>中文: 提出的动态多尺度协调框架（DMSC）通过自适应片段分解和专家融合机制动态建模多尺度依赖关系，在多个基准测试中实现了最先进的时序预测性能。</span><br>
<span id='abs_en'>English: The proposed Dynamic Multi-Scale Coordination Framework (DMSC) addresses limitations in time series forecasting by dynamically modeling multi-scale dependencies through adaptive patch decomposition and specialized fusion mechanisms, achieving state-of-the-art performance across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1065, <a href='https://arxiv.org/pdf/2508.02739.pdf' target='_blank'>https://arxiv.org/pdf/2508.02739.pdf</a></span>   <span><a href='https://github.com/shiyu-coder/Kronos' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Shi, Zongliang Fu, Shuo Chen, Bohan Zhao, Wei Xu, Changshui Zhang, Jian Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02739">Kronos: A Foundation Model for the Language of Financial Markets</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The success of large-scale pre-training paradigm, exemplified by Large Language Models (LLMs), has inspired the development of Time Series Foundation Models (TSFMs). However, their application to financial candlestick (K-line) data remains limited, often underperforming non-pre-trained architectures. Moreover, existing TSFMs often overlook crucial downstream tasks such as volatility prediction and synthetic data generation. To address these limitations, we propose Kronos, a unified, scalable pre-training framework tailored to financial K-line modeling. Kronos introduces a specialized tokenizer that discretizes continuous market information into token sequences, preserving both price dynamics and trade activity patterns. We pre-train Kronos using an autoregressive objective on a massive, multi-market corpus of over 12 billion K-line records from 45 global exchanges, enabling it to learn nuanced temporal and cross-asset representations. Kronos excels in a zero-shot setting across a diverse set of financial tasks. On benchmark datasets, Kronos boosts price series forecasting RankIC by 93% over the leading TSFM and 87% over the best non-pre-trained baseline. It also achieves a 9% lower MAE in volatility forecasting and a 22% improvement in generative fidelity for synthetic K-line sequences. These results establish Kronos as a robust, versatile foundation model for end-to-end financial time series analysis. Our pre-trained model is publicly available at https://github.com/shiyu-coder/Kronos.<br>
<span id='abs_ch'>中文：Kronos 是针对金融K线数据设计的预训练框架，通过创新的 记化处理和大规模训练，在预测、波动率估计和合成数据生成等任务中显著优于现有模型。</span><br>
<span id='abs_en'>English: Kronos is a specialized pre-training framework for financial K-line data that significantly outperforms existing models in forecasting, volatility prediction, and synthetic data generation through its innovative tokenization and large-scale training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1066, <a href='https://arxiv.org/pdf/2508.02694.pdf' target='_blank'>https://arxiv.org/pdf/2508.02694.pdf</a></span>   <span><a href='https://github.com/OPPO-PersonalAI/OAgents' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ningning Wang, Xavier Hu, Pai Liu, He Zhu, Yue Hou, Heyuan Huang, Shengyu Zhang, Jian Yang, Jiaheng Liu, Ge Zhang, Changwang Zhang, Jun Wang, Yuchen Eleanor Jiang, Wangchunshu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02694">Efficient Agents: Building Effective Agents While Reducing Cost</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The remarkable capabilities of Large Language Model (LLM)-driven agents have enabled sophisticated systems to tackle complex, multi-step tasks, but their escalating costs threaten scalability and accessibility. This work presents the first systematic study of the efficiency-effectiveness trade-off in modern agent systems, addressing the critical need for cost-effective designs without sacrificing performance. We investigate three key questions: (1) How much complexity do agentic tasks inherently require? (2) When do additional modules yield diminishing returns? (3) How much efficiency can be gained through the design of efficient agent frameworks? Through an empirical analysis on the GAIA benchmark, we evaluate the impact of LLM backbone selection, agent framework designs, and test-time scaling strategies. Using the cost-of-pass metric, we quantify the efficiency-performance trade-off across these dimensions. Our findings inform the development of Efficient Agents , a novel agent framework that has an optimal complexity to task requirements. Efficient Agents retains 96.7% of the performance of OWL, one leading open-source agent framework, while reducing operational costs from $0.398 to $0.228, resulting in a 28.4% improvement in cost-of-pass. Our work provides actionable insights for designing efficient, high-performing agent systems, advancing the accessibility and sustainability of AI-driven solutions.<br>
<span id='abs_ch'>中文: 本 究提出高效智能体框架，在保持领先系统96.7%性能的同时降低成本28.4%，为平衡AI智能体效率与性能提供了系统性解决方案。</span><br>
<span id='abs_en'>English: This study introduces Efficient Agents, a novel framework that achieves 96.7% performance of leading systems while reducing costs by 28.4%, offering a systematic approach to balance efficiency and effectiveness in AI agent design.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1067, <a href='https://arxiv.org/pdf/2508.02621.pdf' target='_blank'>https://arxiv.org/pdf/2508.02621.pdf</a></span>   <span><a href='https://github.com/yhzhu99/HealthFlow' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinghao Zhu, Yifan Qi, Zixiang Wang, Lei Gu, Dehao Sui, Haoran Hu, Xichen Zhang, Ziyi He, Liantao Ma, Lequan Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02621">HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous Healthcare Research</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The efficacy of AI agents in healthcare research is hindered by their reliance on static, predefined strategies. This creates a critical limitation: agents can become better tool-users but cannot learn to become better strategic planners, a crucial skill for complex domains like healthcare. We introduce HealthFlow, a self-evolving AI agent that overcomes this limitation through a novel meta-level evolution mechanism. HealthFlow autonomously refines its own high-level problem-solving policies by distilling procedural successes and failures into a durable, strategic knowledge base. To anchor our research and facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark featuring complex, realistic health data analysis tasks derived from peer-reviewed clinical research. Our comprehensive experiments demonstrate that HealthFlow's self-evolving approach significantly outperforms state-of-the-art agent frameworks. This work marks a necessary shift from building better tool-users to designing smarter, self-evolving task-managers, paving the way for more autonomous and effective AI for scientific discovery.<br>
<span id='abs_ch'>中文: HealthFlow是一种自我进化的AI智能体，通过元级进化机制自主优化其战略规划能力，在医疗健康 究中显著超越现有框架，推动了自主人工智能的发展。</span><br>
<span id='abs_en'>English: HealthFlow introduces a self-evolving AI agent that autonomously refines its strategic planning through a meta-level evolution mechanism, significantly outperforming existing frameworks and advancing autonomous AI for healthcare research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1068, <a href='https://arxiv.org/pdf/2508.02429.pdf' target='_blank'>https://arxiv.org/pdf/2508.02429.pdf</a></span>   <span><a href='https://github.com/LuoMSen/MLLM-MAC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Miaosen Luo, Jiesen Long, Zequn Li, Yunying Yang, Yuncheng Jiang, Sijie Mai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02429">Multimodal Large Language Models for End-to-End Affective Computing: Benchmarking and Boosting with Generative Knowledge Prompting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal Affective Computing (MAC) aims to recognize and interpret human emotions by integrating information from diverse modalities such as text, video, and audio. Recent advancements in Multimodal Large Language Models (MLLMs) have significantly reshaped the landscape of MAC by offering a unified framework for processing and aligning cross-modal information. However, practical challenges remain, including performance variability across complex MAC tasks and insufficient understanding of how architectural designs and data characteristics impact affective analysis. To address these gaps, we conduct a systematic benchmark evaluation of state-of-the-art open-source MLLMs capable of concurrently processing audio, visual, and textual modalities across multiple established MAC datasets. Our evaluation not only compares the performance of these MLLMs but also provides actionable insights into model optimization by analyzing the influence of model architectures and dataset properties. Furthermore, we propose a novel hybrid strategy that combines generative knowledge prompting with supervised fine-tuning to enhance MLLMs' affective computing capabilities. Experimental results demonstrate that this integrated approach significantly improves performance across various MAC tasks, offering a promising avenue for future research and development in this field. Our code is released on https://github.com/LuoMSen/MLLM-MAC.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1069, <a href='https://arxiv.org/pdf/2508.02411.pdf' target='_blank'>https://arxiv.org/pdf/2508.02411.pdf</a></span>   <span><a href='https://github.com/Event-AHU/Time_Series_Analysis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Hao Si, Fan Zhang, Xiaoya Zhou, Dengdi Sun, Wanli Lyu, Qingquan Yang, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02411">HGTS-Former: Hierarchical HyperGraph Transformer for Multivariate Time Series Analysis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multivariate time series analysis has long been one of the key research topics in the field of artificial intelligence. However, analyzing complex time series data remains a challenging and unresolved problem due to its high dimensionality, dynamic nature, and complex interactions among variables. Inspired by the strong structural modeling capability of hypergraphs, this paper proposes a novel hypergraph-based time series transformer backbone network, termed HGTS-Former, to address the multivariate coupling in time series data. Specifically, given the multivariate time series signal, we first normalize and embed each patch into tokens. Then, we adopt the multi-head self-attention to enhance the temporal representation of each patch. The hierarchical hypergraphs are constructed to aggregate the temporal patterns within each channel and fine-grained relations between different variables. After that, we convert the hyperedge into node features through the EdgeToNode module and adopt the feed-forward network to further enhance the output features. Extensive experiments conducted on two multivariate time series tasks and eight datasets fully validated the effectiveness of our proposed HGTS-Former. The source code will be released on https://github.com/Event-AHU/Time_Series_Analysis.<br>
<span id='abs_ch'>中文: 本文提出HGTS-Former这一基于超图的创新Transformer网络，通过构建分层超图来建模时间序列中的多元耦合关系，在多个数据集上的实验验证了其优越性能。English: This paper introduces HGTS-Former, a novel hypergraph-based transformer network that effectively models multivariate coupling in time series data through hierarchical hypergraph construction and feature enhancement, achieving superior performance across multiple datasets.Paperid:1090,https://arxiv.org/pdf/2508.02401.pdfGitHub</span><br>
<span id='abs_en'>English: This paper introduces HGTS-Former, a novel hypergraph-based transformer network that effectively models multivariate coupling in time series data through hierarchical hypergraph construction and feature enhancement, achieving superior performance across multiple datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1070, <a href='https://arxiv.org/pdf/2508.02401.pdf' target='_blank'>https://arxiv.org/pdf/2508.02401.pdf</a></span>   <span><a href='https://github.com/TUDa-HWAI/CompressKV.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaolin Lin, Jingcun Wang, Olga Kondrateva, Yiyu Shi, Bing Li, Grace Li Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02401">CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in large language models (LLMs) have significantly boosted long-context processing. However, the increasing key-value (KV) cache size poses critical challenges to memory and execution efficiency. Most KV cache compression methods rely on heuristic token eviction using all attention heads in Grouped Query Attention (GQA)-based LLMs. This method ignores the different functionalities of attention heads, leading to the eviction of critical tokens and thus degrades the performance of LLMs.
  To address the issue above, instead of using all the attention heads in GQA-based LLMs to determine important tokens as in the previous work, we first identify the attention heads in each layer that are not only capable of retrieving the initial and final tokens of a prompt, but also capable of retrieving important tokens within the text and attending to their surrounding semantic context. Afterwards, we exploit such heads to determine the important tokens and retain their corresponding KV cache pairs. Furthermore, we analyze the cache eviction error of each layer individually and introduce a layer-adaptive KV cache allocation strategy. Experimental results demonstrate the proposed CompressKV consistently outperforms state-of-the-art approaches under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks. Our code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.<br>
<span id='abs_ch'>中文：提出的CompressKV方法通过选择性利用识别关键令牌的注意力头并采用分层自适应分配策略，改进了KV缓存压缩，在 准基准测试的各种内存预算下均优于现有方法。</span><br>
<span id='abs_en'>English: The proposed CompressKV method improves KV cache compression by selectively using attention heads that identify critical tokens and employing a layer-adaptive allocation strategy, outperforming existing approaches across memory budgets on standard benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1071, <a href='https://arxiv.org/pdf/2508.02343.pdf' target='_blank'>https://arxiv.org/pdf/2508.02343.pdf</a></span>   <span><a href='https://github.com/lwy2020/MicroMix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenyuan Liu, Haoqian Meng, Yilun Luo, Peng Zhang, Xindian Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02343">MicroMix: Efficient Mixed-Precision Quantization with Microscaling Formats for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Quantization significantly accelerates inference in large language models (LLMs) by replacing original high-precision matrices with low-precision counterparts. Recent advances in weight-activation quantization have primarily focused on mapping both weights and activations to the INT4 format. Although the new FP4 Tensor Cores in NVIDIA's Blackwell architecture offer up to 4x speedup over FP16, existing INT4-based kernels fail to fully exploit this capability due to mismatched data formats. To bridge this gap, we propose MicroMix, a co-designed mixed-precision quantization algorithm and matrix multiplication kernel based on Microscaling (MX) data formats. Tailored for the Blackwell architecture, the MicroMix kernel supports arbitrary combinations of MXFP4, MXFP6, and MXFP8 channels, and produces BFloat16 outputs. To achieve a favorable trade-off between accuracy and efficiency for each linear layer, we introduce quantization thresholds that identify activation elements where lower-precision formats (MXFP4 or MXFP6) incur excessive quantization error. Our algorithm selectively allocates higher-precision channels to preserve accuracy while maintaining compute efficiency. MicroMix achieves competitive or superior performance across diverse downstream tasks, including zero-shot and few-shot learning, language modeling, code generation, and mathematical reasoning. On both consumer-grade (RTX 5070Ti laptop) and server-grade (RTX 5090) GPUs, our kernel delivers at least 20% faster execution than TensorRT-FP8. Furthermore, when applied to various Llama and Qwen models, MicroMix consistently improves prefill latency and memory efficiency across a range of batch sizes compared to TensorRT baselines. Our code is available at https://github.com/lwy2020/MicroMix.<br>
<span id='abs_ch'>中文: MicroMix提出了一种协同设计的混合精度量化算法和基于微缩放 式的计算 心，解决了NVIDIA Blackwell架构上的数据 式不匹配问题，在多种任务中实现卓越性能，相比现有基准方案显著提升了执行速度和内存效率。</span><br>
<span id='abs_en'>English: MicroMix introduces a co-designed mixed-precision quantization algorithm and kernel using Microscaling formats to bridge the data format gap on NVIDIA's Blackwell architecture, achieving superior performance across multiple tasks while delivering faster execution and improved efficiency compared to existing baselines.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1072, <a href='https://arxiv.org/pdf/2508.02314.pdf' target='_blank'>https://arxiv.org/pdf/2508.02314.pdf</a></span>   <span><a href='https://github.com/AI4Wireless/LAM4PHY_6G' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajia Guo, Yiming Cui, Shi Jin, Jun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02314">Large AI Models for Wireless Physical Layer</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large artificial intelligence models (LAMs) are transforming wireless physical layer technologies through their robust generalization, multitask processing, and multimodal capabilities. This article reviews recent advancements in LAM applications for physical layer communications, addressing limitations of conventional AI-based approaches. LAM applications are classified into two strategies: leveraging pre-trained LAMs and developing native LAMs designed specifically for physical layer tasks. The motivations and key frameworks of these approaches are comprehensively examined through multiple use cases. Both strategies significantly improve performance and adaptability across diverse wireless scenarios. Future research directions, including efficient architectures, interpretability, standardized datasets, and collaboration between large and small models, are proposed to advance LAM-based physical layer solutions for next-generation communication systems.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1073, <a href='https://arxiv.org/pdf/2508.02292.pdf' target='_blank'>https://arxiv.org/pdf/2508.02292.pdf</a></span>   <span><a href='https://github.com/DVampire/FinWorld' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Zhang, Yilei Zhao, Chuqiao Zong, Xinrun Wang, Bo An
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02292">FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI Research and Deployment</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Financial AI holds great promise for transforming modern finance, with the potential to support a wide range of tasks such as market forecasting, portfolio management, quantitative trading, and automated analysis. However, existing platforms remain limited in task coverage, lack robust multimodal data integration, and offer insufficient support for the training and deployment of large language models (LLMs). In response to these limitations, we present FinWorld, an all-in-one open-source platform that provides end-to-end support for the entire financial AI workflow, from data acquisition to experimentation and deployment. FinWorld distinguishes itself through native integration of heterogeneous financial data, unified support for diverse AI paradigms, and advanced agent automation, enabling seamless development and deployment. Leveraging data from 2 representative markets, 4 stock pools, and over 800 million financial data points, we conduct comprehensive experiments on 4 key financial AI tasks. These experiments systematically evaluate deep learning and reinforcement learning algorithms, with particular emphasis on RL-based finetuning for LLMs and LLM Agents. The empirical results demonstrate that FinWorld significantly enhances reproducibility, supports transparent benchmarking, and streamlines deployment, thereby providing a strong foundation for future research and real-world applications. Code is available at Github~\footnote{https://github.com/DVampire/FinWorld}.<br>
<span id='abs_ch'>中文摘要：FinWorld是一个开源平台，通过整合异构数据、支持多种AI范式及自动化代理，解决了现有金融AI平台任务覆盖不足等问题，为从数据采集到部署的全流程提供端到端支持，并通过大规模实验验证了其卓越性能。</span><br>
<span id='abs_en'>English Summary: FinWorld is an open-source platform that overcomes current financial AI limitations by offering comprehensive workflow support, from data integration to deployment, and enhances research and applications through extensive experiments and benchmarking.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1074, <a href='https://arxiv.org/pdf/2508.02276.pdf' target='_blank'>https://arxiv.org/pdf/2508.02276.pdf</a></span>   <span><a href='https://github.com/gersteinlab/CellForge' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangru Tang, Zhuoyun Yu, Jiapeng Chen, Yan Cui, Daniel Shao, Weixu Wang, Fang Wu, Yuchen Zhuang, Wenqi Shi, Zhi Huang, Arman Cohan, Xihong Lin, Fabian Theis, Smita Krishnaswamy, Mark Gerstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02276">CellForge: Agentic Design of Virtual Cell Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Virtual cell modeling represents an emerging frontier at the intersection of artificial intelligence and biology, aiming to predict quantities such as responses to diverse perturbations quantitatively. However, autonomously building computational models for virtual cells is challenging due to the complexity of biological systems, the heterogeneity of data modalities, and the need for domain-specific expertise across multiple disciplines. Here, we introduce CellForge, an agentic system that leverages a multi-agent framework that transforms presented biological datasets and research objectives directly into optimized computational models for virtual cells. More specifically, given only raw single-cell multi-omics data and task descriptions as input, CellForge outputs both an optimized model architecture and executable code for training virtual cell models and inference. The framework integrates three core modules: Task Analysis for presented dataset characterization and relevant literature retrieval, Method Design, where specialized agents collaboratively develop optimized modeling strategies, and Experiment Execution for automated generation of code. The agents in the Design module are separated into experts with differing perspectives and a central moderator, and have to collaboratively exchange solutions until they achieve a reasonable consensus. We demonstrate CellForge's capabilities in single-cell perturbation prediction, using six diverse datasets that encompass gene knockouts, drug treatments, and cytokine stimulations across multiple modalities. CellForge consistently outperforms task-specific state-of-the-art methods. Overall, CellForge demonstrates how iterative interaction between LLM agents with differing perspectives provides better solutions than directly addressing a modeling challenge. Our code is publicly available at https://github.com/gersteinlab/CellForge.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1075, <a href='https://arxiv.org/pdf/2508.02150.pdf' target='_blank'>https://arxiv.org/pdf/2508.02150.pdf</a></span>   <span><a href='https://github.com/Rainier-rq/verl-if' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyu Ren, Qianyu He, Bowei Zhang, Jie Zeng, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, Fei Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02150">Beyond the Trade-off: Self-Supervised Reinforcement Learning for Reasoning Models' Instruction Following</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reasoning models excel in complex problem solving but exhibit a concerning trade off between reasoning capabilities and instruction following abilities. Existing approaches for improving instruction following rely on stronger external models, creating methodological bottlenecks and practical limitations including increased costs and accessibility constraints. We propose a self-supervised RL framework that leverages reasoning models' own internal signals to improve instruction following capabilities without external supervision. Extensive experiments demonstrate that our framework significantly improves instruction following capabilities while maintaining reasoning performance, offering a scalable and cost-effective approach to enhance instruction following in reasoning models. The data and code are publicly available at https://github.com/Rainier-rq/verl-if.<br>
<span id='abs_ch'>中文摘要：该 究提出一种自监督强化学 框架，利用推理模型内部信号提升其指令遵循能力，在保持推理性能的同时提供了可扩展且经济高效的解决方案。</span><br>
<span id='abs_en'>English Summary: The proposed self-supervised reinforcement learning framework enhances reasoning models' instruction-following capabilities using their internal signals, maintaining reasoning performance while offering a scalable and cost-effective solution.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1076, <a href='https://arxiv.org/pdf/2508.02137.pdf' target='_blank'>https://arxiv.org/pdf/2508.02137.pdf</a></span>   <span><a href='https://github.com/GENTEL-lab/AuroBind' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongyue Zhang, Jiahua Rao, Jie Zhong, Weiqiang Bai, Dongxue Wang, Shaobo Ning, Lifeng Qiao, Sheng Xu, Runze Ma, Will Hua, Jack Xiaoyu Chen, Odin Zhang, Wei Lu, Hanyi Feng, He Yang, Xinchao Shi, Rui Li, Wanli Ouyang, Xinzhu Ma, Jiahao Wang, Jixian Zhang, Jia Duan, Siqi Sun, Jian Zhang, Shuangjia Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02137">Fitness aligned structural modeling enables scalable virtual screening with AuroBind</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Most human proteins remain undrugged, over 96% of human proteins remain unexploited by approved therapeutics. While structure-based virtual screening promises to expand the druggable proteome, existing methods lack atomic-level precision and fail to predict binding fitness, limiting translational impact. We present AuroBind, a scalable virtual screening framework that fine-tunes a custom atomic-level structural model on million-scale chemogenomic data. AuroBind integrates direct preference optimization, self-distillation from high-confidence complexes, and a teacher-student acceleration strategy to jointly predict ligand-bound structures and binding fitness. The proposed models outperform state-of-the-art models on structural and functional benchmarks while enabling 100,000-fold faster screening across ultra-large compound libraries. In a prospective screen across ten disease-relevant targets, AuroBind achieved experimental hit rates of 7-69%, with top compounds reaching sub-nanomolar to picomolar potency. For the orphan GPCRs GPR151 and GPR160, AuroBind identified both agonists and antagonists with success rates of 16-30%, and functional assays confirmed GPR160 modulation in liver and prostate cancer models. AuroBind offers a generalizable framework for structure-function learning and high-throughput molecular screening, bridging the gap between structure prediction and therapeutic discovery.<br>
<span id='abs_ch'>中文摘要：AuroBind是一种可扩展的虚拟筛选框架，通过原子级结构模型预测配体结合结构与结合适应性，在疾病靶点（包括孤儿GPCR）筛选中实现了高实验命中率并鉴定出高效化合物，同时大幅提升了筛选速度。</span><br>
<span id='abs_en'>English Summary: AuroBind is a scalable virtual screening framework that uses atomic-level structural modeling to predict ligand-bound structures and binding fitness, achieving high experimental hit rates and identifying potent compounds for disease targets, including orphan GPCRs, with significantly faster screening speeds.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1077, <a href='https://arxiv.org/pdf/2508.02124.pdf' target='_blank'>https://arxiv.org/pdf/2508.02124.pdf</a></span>   <span><a href='https://github.com/SmallDoges/flash-dmattn' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingze Shi, Yifan Wu, Yiran Peng, Bingheng Wu, Liangdong Wang, Guang Liu, Yuyu Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02124">Trainable Dynamic Mask Sparse Attention</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In large language models, the demand for modeling long contexts is ever-increasing, yet the quadratic complexity of standard self-attention presents a significant bottleneck. While existing sparse attention mechanisms enhance efficiency, they often suffer from limitations such as static patterns and information loss. This paper introduces a Trainable Dynamic Mask Sparse Attention mechanism that addresses these challenges through three key innovations. First, it leverages value vectors to dynamically generate content-aware sparse masks, enabling the model to adaptively identify and focus on crucial information. Second, it implements a position-aware sparse attention computation that effectively skips unnecessary computational regions. Finally, we ensure that the introduced dynamic masks and sparse weights do not obstruct gradients, thereby supporting end-to-end training. This dual-sparsity design allows the model to retain complete information while significantly reducing computational complexity, achieving an excellent balance between efficiency and performance. We validate the performance of Dynamic Mask Attention through comprehensive experiments. Comparative studies demonstrate that our method consistently achieves Pareto dominance across various tasks, including scaling laws, multi-query associative recall, general benchmarks, and needle-in-a-haystack tests, delivering up to 10 times acceleration. These results highlight its capability to effectively balance model efficiency with long-context modeling. Our computational kernel is open-sourced at https://github.com/SmallDoges/flash-dmattn to facilitate further research and application within the community.<br>
<span id='abs_ch'>中文: 本文提出一种可训练动态掩 稀疏注意力机制，通过内容感知的动态掩 和位置感知计算，在保持完整信息的同时显著降低计算复杂度，在多种长上下文任务中实现高达10倍 速和帕累托最优。</span><br>
<span id='abs_en'>English: This paper proposes a Trainable Dynamic Mask Sparse Attention mechanism that uses content-aware dynamic masks and position-aware computation to significantly reduce complexity while maintaining full information, achieving up to 10x acceleration and Pareto dominance across various long-context tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1078, <a href='https://arxiv.org/pdf/2508.02115.pdf' target='_blank'>https://arxiv.org/pdf/2508.02115.pdf</a></span>   <span><a href='https://github.com/still2009/cowardFL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjie Li, Siying Gu, Yiming Li, Kangjie Chen, Zhili Chen, Tianwei Zhang, Shu-Tao Xia, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02115">Coward: Toward Practical Proactive Federated Backdoor Defense via Collision-based Watermark</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Backdoor detection is currently the mainstream defense against backdoor attacks in federated learning (FL), where malicious clients upload poisoned updates that compromise the global model and undermine the reliability of FL deployments. Existing backdoor detection techniques fall into two categories, including passive and proactive ones, depending on whether the server proactively modifies the global model. However, both have inherent limitations in practice: passive defenses are vulnerable to common non-i.i.d. data distributions and random participation of FL clients, whereas current proactive defenses suffer inevitable out-of-distribution (OOD) bias because they rely on backdoor co-existence effects. To address these issues, we introduce a new proactive defense, dubbed Coward, inspired by our discovery of multi-backdoor collision effects, in which consecutively planted, distinct backdoors significantly suppress earlier ones. In general, we detect attackers by evaluating whether the server-injected, conflicting global watermark is erased during local training rather than retained. Our method preserves the advantages of proactive defenses in handling data heterogeneity (\ie, non-i.i.d. data) while mitigating the adverse impact of OOD bias through a revised detection mechanism. Extensive experiments on benchmark datasets confirm the effectiveness of Coward and its resilience to potential adaptive attacks. The code for our method would be available at https://github.com/still2009/cowardFL.<br>
<span id='abs_ch'>中文摘要：联邦学 中的后门检测面临被动防御易受非独立同分布数据影响、主动防御存在分布外偏差的局限，为此提出Coward主动防御方法，利用多后门碰撞效应，通过检测服务器注入的冲突水印在本地训练中是否被擦除来识别攻击者。</span><br>
<span id='abs_en'>English Summary: Backdoor detection in federated learning faces limitations with passive defenses being vulnerable to non-i.i.d. data and proactive ones suffering from out-of-distribution bias, leading to the introduction of Coward, a proactive defense that leverages multi-backdoor collision effects to detect attackers by monitoring the erasure of server-injected watermarks during local training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1079, <a href='https://arxiv.org/pdf/2508.02091.pdf' target='_blank'>https://arxiv.org/pdf/2508.02091.pdf</a></span>   <span><a href='https://github.com/deepreinforce-ai/CRINN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoya Li, Xiaofei Sun, Albert Wang, Chris Shum, Jiwei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02091">CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN's effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN's success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement. Code can be found at https://github.com/deepreinforce-ai/CRINN<br>
<span id='abs_ch'>中文：CRINN提出了一种强化学 方法用于近似最近邻搜索，能在保持精度的同时自动生成更快的实现，并在多个基准测试中取得领先性能。</span><br>
<span id='abs_en'>English: CRINN introduces a reinforcement learning approach to approximate nearest-neighbor search, automatically generating faster implementations while maintaining accuracy and achieving top performance on multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1080, <a href='https://arxiv.org/pdf/2508.02085.pdf' target='_blank'>https://arxiv.org/pdf/2508.02085.pdf</a></span>   <span><a href='https://github.com/JARVIS-Xs/SE-Agent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaye Lin, Yifu Guo, Yuzhen Han, Sen Hu, Ziyi Ni, Licheng Wang, Mingguang Chen, Hongzhang Liu, Ronghao Chen, Yangfan He, Daxin Jiang, Binxing Jiao, Chen Hu, Huacan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02085">SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning with LLM-Based Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Model (LLM)-based agents have recently shown impressive capabilities in complex reasoning and tool use via multi-step interactions with their environments. While these agents have the potential to tackle complicated tasks, their problem-solving process, i.e., agents' interaction trajectory leading to task completion, remains underexploited. These trajectories contain rich feedback that can navigate agents toward the right directions for solving problems correctly. Although prevailing approaches, such as Monte Carlo Tree Search (MCTS), can effectively balance exploration and exploitation, they ignore the interdependence among various trajectories and lack the diversity of search spaces, which leads to redundant reasoning and suboptimal outcomes. To address these challenges, we propose SE-Agent, a Self-Evolution framework that enables Agents to optimize their reasoning processes iteratively. Our approach revisits and enhances former pilot trajectories through three key operations: revision, recombination, and refinement. This evolutionary mechanism enables two critical advantages: (1) it expands the search space beyond local optima by intelligently exploring diverse solution paths guided by previous trajectories, and (2) it leverages cross-trajectory inspiration to efficiently enhance performance while mitigating the impact of suboptimal reasoning paths. Through these mechanisms, SE-Agent achieves continuous self-evolution that incrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench Verified to resolve real-world GitHub issues. Experimental results across five strong LLMs show that integrating SE-Agent delivers up to 55% relative improvement, achieving state-of-the-art performance among all open-source agents on SWE-bench Verified. Our code and demonstration materials are publicly available at https://github.com/JARVIS-Xs/SE-Agent.<br>
<span id='abs_ch'>中文: SE-Agent提出了一种自我进化框架，通过修订、重组和优化轨迹来迭代改进推理过程，在现实任务中实现了高达55%的性能提升，达到顶尖水平。</span><br>
<span id='abs_en'>English: SE-Agent introduces a self-evolution framework that iteratively optimizes reasoning processes by revising, recombining, and refining trajectories, achieving state-of-the-art performance with up to 55% improvement on real-world tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1081, <a href='https://arxiv.org/pdf/2508.01977.pdf' target='_blank'>https://arxiv.org/pdf/2508.01977.pdf</a></span>   <span><a href='https://github.com/Vicentvankor/sun-shine' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fan Gao, Cheng Huang, Nyima Tashi, Yutong Liu, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, Renzeg Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Xiao Feng, Hao Wang, Yongbin Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01977">TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>To address the severe data scarcity in Tibetan, a low-resource language spoken by over six million people, we introduce TIBSTC-CoT, the large-scale, multi-domain Tibetan dataset automatically constructed via chain-of-thought prompting with large language models (LLMs). TIBSTC-CoT establishes a scalable and reproducible framework for dataset creation in low-resource settings, covering diverse domains and reasoning patterns essential for language understanding and generation. Building on this dataset, we develop the Sunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with chain-of-thought capabilities. Trained entirely on TIBSTC-CoT, Sunshine-thinking has demonstrated strong reasoning and generation performance, comparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a significant step toward inclusive AI by enabling high-quality Tibetan language processing through both resource creation and model innovation. All data are available: https://github.com/Vicentvankor/sun-shine.<br>
<span id='abs_ch'>Chinese: 为解决藏语数据稀缺问题，通过大语言模型的思维链提示自动构建了大规模多领域数据集TIBSTC-CoT，并基于此开发了具备思维链能力的藏语大模型Sunshine-thinking系列，其推理和生成性能达到先进水平。</span><br>
<span id='abs_en'>English: To tackle Tibetan's data scarcity, TIBSTC-CoT, a large-scale multi-domain dataset, was created using chain-of-thought prompting with LLMs, leading to the development of the Sunshine-thinking LLM family that demonstrates strong reasoning and generation capabilities comparable to SOTA models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1082, <a href='https://arxiv.org/pdf/2508.01947.pdf' target='_blank'>https://arxiv.org/pdf/2508.01947.pdf</a></span>   <span><a href='https://github.com/sousoura/Inferring-Reward-Machines-and-Transition-Machines-from-POMDP.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuly Wu, Jiamou Liu, Libo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01947">Inferring Reward Machines and Transition Machines from Partially Observable Markov Decision Processes</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Partially Observable Markov Decision Processes (POMDPs) are fundamental to many real-world applications. Although reinforcement learning (RL) has shown success in fully observable domains, learning policies from traces in partially observable environments remains challenging due to non-Markovian observations. Inferring an automaton to handle the non-Markovianity is a proven effective approach, but faces two limitations: 1) existing automaton representations focus only on reward-based non-Markovianity, leading to unnatural problem formulations; 2) inference algorithms face enormous computational costs. For the first limitation, we introduce Transition Machines (TMs) to complement existing Reward Machines (RMs). To develop a unified inference algorithm for both automata types, we propose the Dual Behavior Mealy Machine (DBMM) that subsumes both TMs and RMs. We then introduce DB-RPNI, a passive automata learning algorithm that efficiently infers DBMMs while avoiding the costly reductions required by prior work. We further develop optimization techniques and identify sufficient conditions for inferring the minimal correct automata. Experimentally, our inference method achieves speedups of up to three orders of magnitude over SOTA baselines.<br>
<span id='abs_ch'>中文摘要：针对部分可观测环境中的强化学 挑战，本文提出转移机和统一的双行为米利机模型，并通过DB-RPNI算法实现比现有方法快三个数量级的推理速度，同时保证准确性。</span><br>
<span id='abs_en'>English Summary: Reinforcement learning in partially observable environments is enhanced by introducing Transition Machines and a unified Dual Behavior Mealy Machine, with the DB-RPNI algorithm achieving up to 1000x faster inference while maintaining accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1083, <a href='https://arxiv.org/pdf/2508.01928.pdf' target='_blank'>https://arxiv.org/pdf/2508.01928.pdf</a></span>   <span><a href='https://github.com/SlavkoPrytula/IAUNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaroslav Prytula, Illia Tsiporenko, Ali Zeynalli, Dmytro Fishman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01928">IAUNet: Instance-Aware U-Net</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Instance segmentation is critical in biomedical imaging to accurately distinguish individual objects like cells, which often overlap and vary in size. Recent query-based methods, where object queries guide segmentation, have shown strong performance. While U-Net has been a go-to architecture in medical image segmentation, its potential in query-based approaches remains largely unexplored. In this work, we present IAUNet, a novel query-based U-Net architecture. The core design features a full U-Net architecture, enhanced by a novel lightweight convolutional Pixel decoder, making the model more efficient and reducing the number of parameters. Additionally, we propose a Transformer decoder that refines object-specific features across multiple scales. Finally, we introduce the 2025 Revvity Full Cell Segmentation Dataset, a unique resource with detailed annotations of overlapping cell cytoplasm in brightfield images, setting a new benchmark for biomedical instance segmentation. Experiments on multiple public datasets and our own show that IAUNet outperforms most state-of-the-art fully convolutional, transformer-based, and query-based models and cell segmentation-specific models, setting a strong baseline for cell instance segmentation tasks. Code is available at https://github.com/SlavkoPrytula/IAUNet<br>
<span id='abs_ch'>中文摘要：IAUNet是一种新颖的基于查询的U-Net架构，采用轻量级卷积像 解 器和Transformer解 器，在包括新发布的2025 Revvity全细胞分割数据集在内的多个数据集上实现了生物医学实例分割的最先进性能。</span><br>
<span id='abs_en'>English Summary: IAUNet is a novel query-based U-Net architecture featuring a lightweight convolutional Pixel decoder and a Transformer decoder that achieves state-of-the-art performance in biomedical instance segmentation, as demonstrated on multiple datasets including the newly introduced 2025 Revvity Full Cell Segmentation Dataset.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1084, <a href='https://arxiv.org/pdf/2508.01887.pdf' target='_blank'>https://arxiv.org/pdf/2508.01887.pdf</a></span>   <span><a href='https://github.com/ACMCMC/PDFuzz' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ACMCMC/PDFuzz' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aldan Creo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01887">Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>AI-generated text detectors have become essential tools for maintaining content authenticity, yet their robustness against evasion attacks remains questionable. We present PDFuzz, a novel attack that exploits the discrepancy between visual text layout and extraction order in PDF documents. Our method preserves exact textual content while manipulating character positioning to scramble extraction sequences. We evaluate this approach against the ArguGPT detector using a dataset of human and AI-generated text. Our results demonstrate complete evasion: detector performance drops from (93.6 $\pm$ 1.4) % accuracy and 0.938 $\pm$ 0.014 F1 score to random-level performance ((50.4 $\pm$ 3.2) % accuracy, 0.0 F1 score) while maintaining perfect visual fidelity. Our work reveals a vulnerability in current detection systems that is inherent to PDF document structures and underscores the need for implementing sturdy safeguards against such attacks. We make our code publicly available at https://github.com/ACMCMC/PDFuzz.<br>
<span id='abs_ch'>Chinese: PDFuzz是一种新型规避攻击，通过操纵PDF文档中的字符定位来扰乱文本提取顺序，在保持视觉保真度的同时完全绕过AI生成文本检测器。</span><br>
<span id='abs_en'>English: PDFuzz is a novel evasion attack that manipulates character positioning in PDF documents to scramble text extraction sequences, completely bypassing AI-generated text detectors while preserving visual fidelity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1085, <a href='https://arxiv.org/pdf/2508.01858.pdf' target='_blank'>https://arxiv.org/pdf/2508.01858.pdf</a></span>   <span><a href='https://github.com/Gnonymous/Web-CogReasoner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhan Guo, Cong Guo, Aiwen Sun, Hongliang He, Xinyu Yang, Yue Lu, Yingji Zhang, Xuntao Guo, Dong Zhang, Jianzhuang Liu, Jiang Duan, Yijia Xiao, Liangjian Wen, Hai-Ming Xu, Yong Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01858">Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal large-scale models have significantly advanced the development of web agents, enabling perception and interaction with digital environments akin to human cognition. In this paper, we argue that web agents must first acquire sufficient knowledge to effectively engage in cognitive reasoning. Therefore, we decompose a web agent's capabilities into two essential stages: knowledge content learning and cognitive processes. To formalize this, we propose Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and Procedural. In this framework, knowledge content learning corresponds to the agent's processes of Memorizing and Understanding, which rely on the first two knowledge types, representing the "what" of learning. Conversely, cognitive processes correspond to Exploring, grounded in Procedural knowledge, defining the "how" of reasoning and action. To facilitate knowledge acquisition, we construct the Web-CogDataset, a structured resource curated from 14 real-world websites, designed to systematically instill core knowledge necessary for web agent. This dataset serves as the agent's conceptual grounding-the "nouns" upon which comprehension is built-as well as the basis for learning how to reason and act. Building on this foundation, we operationalize these processes through a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing and training our proposed agent, the Web-CogReasoner. Extensive experimentation reveals its significant superiority over existing models, especially in generalizing to unseen tasks where structured knowledge is decisive. To enable rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation suite designed to assess and compare agent performance across the delineated knowledge domains and cognitive capabilities. Our code and data is open sourced at https://github.com/Gnonymous/Web-CogReasoner<br>
<span id='abs_ch'>中文摘要：本文提出Web-CogKnowledge框架，将网络智能体的能力分解为知识内容学 和认知过程两个阶段，并通过Web-CogReasoner智能体验证了该框架在未见过任务中的卓越泛化能力，其表现显著优于现有模型。</span><br>
<span id='abs_en'>English Summary: This paper introduces the Web-CogKnowledge Framework, which structures web agents' learning into knowledge acquisition and cognitive reasoning stages, and demonstrates its effectiveness through the Web-CogReasoner agent that significantly outperforms existing models, particularly in generalizing to novel tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1086, <a href='https://arxiv.org/pdf/2508.01773.pdf' target='_blank'>https://arxiv.org/pdf/2508.01773.pdf</a></span>   <span><a href='https://github.com/Jiuzhouh/UnPRM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiuzhou Han, Wray Buntine, Ehsan Shareghi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01773">Uncertainty-Based Methods for Automated Process Reward Data Construction and Output Aggregation in Mathematical Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models have demonstrated remarkable capabilities in complex mathematical reasoning tasks, but they inevitably generate errors throughout multi-step solutions. Process-level Reward Models (PRMs) have shown great promise by providing supervision and evaluation at each intermediate step, thereby effectively improving the models' reasoning abilities. However, training effective PRMs requires high-quality process reward data, yet existing methods for constructing such data are often labour-intensive or inefficient. In this paper, we propose an uncertainty-driven framework for automated process reward data construction, encompassing both data generation and annotation processes for PRMs. Additionally, we identify the limitations of both majority vote and PRMs, and introduce two generic uncertainty-aware output aggregation methods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which combine the strengths of majority vote with PRMs. Extensive experiments on ProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the proposed PRM data construction framework, and demonstrate that the two output aggregation methods further improve the mathematical reasoning abilities across diverse PRMs. The code and data will be publicly available at https://github.com/Jiuzhouh/UnPRM.<br>
<span id='abs_ch'>Chinese: 本文提出了一种基于不确定性的自动构建过程奖励数据框架以优化过程奖励模型，并引入两种新型不确定性感知聚合方法，在多个基准测试中显著提升了数学推理能力。</span><br>
<span id='abs_en'>English: This paper introduces an uncertainty-driven framework for automated construction of process reward data to enhance PRMs, along with two novel uncertainty-aware aggregation methods that significantly improve mathematical reasoning across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1087, <a href='https://arxiv.org/pdf/2508.01712.pdf' target='_blank'>https://arxiv.org/pdf/2508.01712.pdf</a></span>   <span><a href='https://github.com/Social-AI-Studio/HateClipSeg.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Wang, Zhuoran Wang, Roy Ka-Wei Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01712">HateClipSeg: A Segment-Level Annotated Dataset for Fine-Grained Hate Video Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Detecting hate speech in videos remains challenging due to the complexity of multimodal content and the lack of fine-grained annotations in existing datasets. We present HateClipSeg, a large-scale multimodal dataset with both video-level and segment-level annotations, comprising over 11,714 segments labeled as Normal or across five Offensive categories: Hateful, Insulting, Sexual, Violence, Self-Harm, along with explicit target victim labels. Our three-stage annotation process yields high inter-annotator agreement (Krippendorff's alpha = 0.817). We propose three tasks to benchmark performance: (1) Trimmed Hateful Video Classification, (2) Temporal Hateful Video Localization, and (3) Online Hateful Video Classification. Results highlight substantial gaps in current models, emphasizing the need for more sophisticated multimodal and temporally aware approaches. The HateClipSeg dataset are publicly available at https://github.com/Social-AI-Studio/HateClipSeg.git.<br>
<span id='abs_ch'>中文摘要：HateClipSeg数据集通过提供包含11,714个片段的细粒度多模态 注，解决了视频仇恨言论检测中的挑战，其基准测试显示现有模型存在显著性能差距，强调了开发先进多模态方法的必要性。</span><br>
<span id='abs_en'>English Summary: The HateClipSeg dataset addresses challenges in video hate speech detection by providing fine-grained multimodal annotations across 11,714 segments, with benchmark tasks revealing significant performance gaps in current models and underscoring the need for advanced multimodal approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1088, <a href='https://arxiv.org/pdf/2508.01711.pdf' target='_blank'>https://arxiv.org/pdf/2508.01711.pdf</a></span>   <span><a href='https://github.com/YangBowenn/GAID' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bowen Yang, Yun Cao, Chen He, Xiaosu Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01711">GAID: Frame-Level Gated Audio-Visual Integration with Directional Perturbation for Text-Video Retrieval</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text-to-video retrieval requires precise alignment between language and temporally rich video signals. Existing methods predominantly exploit visual cues and often overlook complementary audio semantics or adopt coarse fusion strategies, leading to suboptimal multimodal representations. We present GAID, a framework that jointly address this gap via two key components: (i) a Frame-level Gated Fusion (FGF) that adaptively integrates audio and visual features under textual guidance, enabling fine-grained temporal alignment; and (ii) a Directional Adaptive Semantic Perturbation (DASP) that injects structure-aware perturbations into text embeddings, enhancing robustness and discrimination without incurring multi-pass inference. These modules complement each other -- fusion reduces modality gaps while perturbation regularizes cross-modal matching -- yielding more stable and expressive representations. Extensive experiments on MSR-VTT, DiDeMo, LSMDC, and VATEX show consistent state-of-the-art results across all retrieval metrics with notable efficiency gains. Our code is available at https://github.com/YangBowenn/GAID.<br>
<span id='abs_ch'>中文摘要：GAID框架通过文本引导的自适应音视频特征融合和结构化文本嵌入扰动，有效提升了文本-视频检索的精度与鲁棒性，在多个基准测试中均取得最优性能且计算效率显著提高。</span><br>
<span id='abs_en'>English Summary: The GAID framework enhances text-to-video retrieval by adaptively fusing audio-visual features with textual guidance and injecting structured perturbations into text embeddings, achieving state-of-the-art results across multiple benchmarks with improved efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1089, <a href='https://arxiv.org/pdf/2508.01696.pdf' target='_blank'>https://arxiv.org/pdf/2508.01696.pdf</a></span>   <span><a href='https://github.com/liunian-Jay/CoCoA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Jiang, Sendong Zhao, Jianbo Li, Haochun Wang, Lizhe Zhang, Yan Liu, Bing Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01696">Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-Augmented Generation (RAG) has emerged as a promising framework for enhancing the capabilities of Large Language Models (LLMs), especially in knowledge-intensive tasks. Despite its advantages, current RAG methods often struggle to *fully exploit knowledge during generation*. In particular, the synergy between the model's internal parametric knowledge and external retrieved knowledge remains limited. Retrieved contents may sometimes mislead generation, while certain generated content can guide the model toward more accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a framework designed to enhance explicitly synergy over both parametric and retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent RAG framework that first performs conditional knowledge induction and then reasons answers. Building on this, we develop CoCoA, a long-chain training strategy that synthesizes extended multi-agent reasoning trajectories from CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability to explicitly integrate and jointly leverage parametric and retrieved knowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior performance on open-domain and multi-hop QA tasks.<br>
<span id='abs_ch'>Chinese: 提出的协作代理链框架通过多代理推理和长链训练增强了RAG系统中参数化知识与检索知识的协同作用，在问答任务中表现出优越性能。</span><br>
<span id='abs_en'>English: The proposed Collaborative Chain-of-Agents framework enhances synergy between parametric and retrieved knowledge in RAG systems through multi-agent reasoning and long-chain training, achieving superior performance in QA tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1090, <a href='https://arxiv.org/pdf/2508.01696.pdf' target='_blank'>https://arxiv.org/pdf/2508.01696.pdf</a></span>   <span><a href='https://github.com/liunian-Jay/CoCoA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Jiang, Sendong Zhao, Jianbo Li, Haochun Wang, Lizhe Zhang, Yan Liu, Bing Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01696">CoCoA: Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs), especially for knowledge-intensive tasks. Despite its advantages, current RAG methods often struggle to fully exploit knowledge during generation. In particular, the synergy between the model's internal parametric knowledge and external retrieved knowledge remains limited. Retrieved contents may sometimes mislead generation, while certain generated content can guide the model toward more accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a framework designed to enhance explicitly synergy over both parametric and retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent RAG framework that first performs conditional knowledge induction and then reasons answers. Building on this, we develop CoCoA, a long-chain training strategy that synthesizes extended multi-agent reasoning trajectories from CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability to explicitly integrate and jointly leverage parametric and retrieved knowledge. Experimental results demonstrate the superiority of CoCoA in open-domain QA and multi-hop QA.<br>
<span id='abs_ch'>Chinese: 提出的协作代理链框架通过多代理推理和长链训练增强了RAG系统中参数化知识与检索知识的协同作用，在问答任务中表现出优越性能。</span><br>
<span id='abs_en'>English: The proposed Collaborative Chain-of-Agents framework enhances synergy between parametric and retrieved knowledge in RAG systems through multi-agent reasoning and long-chain training, achieving superior performance in QA tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1091, <a href='https://arxiv.org/pdf/2508.01670.pdf' target='_blank'>https://arxiv.org/pdf/2508.01670.pdf</a></span>   <span><a href='https://github.com/jiaqingxie/QCBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqing Xie, Weida Wang, Ben Gao, Zhuo Yang, Haiyuan Wan, Shufei Zhang, Tianfan Fu, Yuqiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01670">QCBench: Evaluating Large Language Models on Domain-Specific Quantitative Chemistry</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Quantitative chemistry is central to modern chemical research, yet the ability of large language models (LLMs) to perform its rigorous, step-by-step calculations remains underexplored. To fill this blank, we propose QCBench, a Quantitative Chemistry oriented benchmark comprising 350 computational chemistry problems across 7 chemistry subfields, which contains analytical chemistry, bio/organic chemistry, general chemistry, inorganic chemistry, physical chemistry, polymer chemistry and quantum chemistry. To systematically evaluate the mathematical reasoning abilities of large language models (LLMs), they are categorized into three tiers: easy, medium, and difficult. Each problem, rooted in realistic chemical scenarios, is structured to prevent heuristic shortcuts and demand explicit numerical reasoning. QCBench enables fine-grained diagnosis of computational weaknesses, reveals model-specific limitations across difficulty levels, and lays the groundwork for future improvements such as domain-adaptive fine-tuning or multi-modal integration. Evaluations on 24 LLMs demonstrate a consistent performance degradation with increasing task complexity, highlighting the current gap between language fluency and scientific computation accuracy. Code for QCBench is available at https://github.com/jiaqingxie/QCBench.<br>
<span id='abs_ch'>中文: 本 究提出QCBench基准测试，涵盖七个化学子领域的350道定量化学题目，用于评估大语言模型的计算推理能力，发现其表现随问题复杂度增 而下降。</span><br>
<span id='abs_en'>English: This study introduces QCBench, a benchmark of 350 quantitative chemistry problems across seven subfields, to evaluate large language models' computational reasoning abilities and reveals their performance declines with increasing problem complexity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1092, <a href='https://arxiv.org/pdf/2508.01647.pdf' target='_blank'>https://arxiv.org/pdf/2508.01647.pdf</a></span>   <span><a href='https://github.com/ManHu2025/DUP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Man Hu, Yahui Ding, Yatao Yang, Liangyu Chen, Yanhao Jia, Shuai Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01647">DUP: Detection-guided Unlearning for Backdoor Purification in Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As backdoor attacks become more stealthy and robust, they reveal critical weaknesses in current defense strategies: detection methods often rely on coarse-grained feature statistics, and purification methods typically require full retraining or additional clean models. To address these challenges, we propose DUP (Detection-guided Unlearning for Purification), a unified framework that integrates backdoor detection with unlearning-based purification. The detector captures feature-level anomalies by jointly leveraging class-agnostic distances and inter-layer transitions. These deviations are integrated through a weighted scheme to identify poisoned inputs, enabling more fine-grained analysis. Based on the detection results, we purify the model through a parameter-efficient unlearning mechanism that avoids full retraining and does not require any external clean model. Specifically, we innovatively repurpose knowledge distillation to guide the student model toward increasing its output divergence from the teacher on detected poisoned samples, effectively forcing it to unlearn the backdoor behavior. Extensive experiments across diverse attack methods and language model architectures demonstrate that DUP achieves superior defense performance in detection accuracy and purification efficacy. Our code is available at https://github.com/ManHu2025/DUP.<br>
<span id='abs_ch'>中文摘要：提出的DUP框架通过特征级异常检测结合基于知识蒸馏的参数高效反学 机制， 需完整重训练或外部干净模型即可有效清除后门。</span><br>
<span id='abs_en'>English Summary: The proposed DUP framework combines backdoor detection using feature-level anomaly analysis with parameter-efficient unlearning through knowledge distillation, effectively eliminating backdoors without full retraining or external clean models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1093, <a href='https://arxiv.org/pdf/2508.01644.pdf' target='_blank'>https://arxiv.org/pdf/2508.01644.pdf</a></span>   <span><a href='https://github.com/PANPANKK/DRKF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peiyuan Jiang, Yao Liu, Qiao Liu, Zongshun Zhang, Jiaye Yang, Lu Liu, Daibing Yao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01644">DRKF: Decoupled Representations with Knowledge Fusion for Multimodal Emotion Recognition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal emotion recognition (MER) aims to identify emotional states by integrating and analyzing information from multiple modalities. However, inherent modality heterogeneity and inconsistencies in emotional cues remain key challenges that hinder performance. To address these issues, we propose a Decoupled Representations with Knowledge Fusion (DRKF) method for MER. DRKF consists of two main modules: an Optimized Representation Learning (ORL) Module and a Knowledge Fusion (KF) Module. ORL employs a contrastive mutual information estimation method with progressive modality augmentation to decouple task-relevant shared representations and modality-specific features while mitigating modality heterogeneity. KF includes a lightweight self-attention-based Fusion Encoder (FE) that identifies the dominant modality and integrates emotional information from other modalities to enhance the fused representation. To handle potential errors from incorrect dominant modality selection under emotionally inconsistent conditions, we introduce an Emotion Discrimination Submodule (ED), which enforces the fused representation to retain discriminative cues of emotional inconsistency. This ensures that even if the FE selects an inappropriate dominant modality, the Emotion Classification Submodule (EC) can still make accurate predictions by leveraging preserved inconsistency information. Experiments show that DRKF achieves state-of-the-art (SOTA) performance on IEMOCAP, MELD, and M3ED. The source code is publicly available at https://github.com/PANPANKK/DRKF.<br>
<span id='abs_ch'>中文: 提出的解耦表征与知识融合（DRKF）方法通过对比学 解耦共享和特定模态特征，并利用融合编 器与情感判别机制整合情感线索，解决了多模态情感识别中的模态异质性和情感不一致问题，在多个基准数据集上实现了最优性能。</span><br>
<span id='abs_en'>English: The proposed Decoupled Representations with Knowledge Fusion (DRKF) method addresses modality heterogeneity and emotional inconsistencies in multimodal emotion recognition by decoupling shared and specific features through contrastive learning and integrating emotional cues via a fusion encoder with an emotion discrimination mechanism, achieving state-of-the-art performance on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1094, <a href='https://arxiv.org/pdf/2508.01554.pdf' target='_blank'>https://arxiv.org/pdf/2508.01554.pdf</a></span>   <span><a href='https://github.com/Yujiaaaaa/PACP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujia Zheng, Tianhao Li, Haotian Huang, Tianyu Zeng, Jingyu Lu, Chuangxin Chu, Yuekai Huang, Ziyou Jiang, Qian Xiong, Yuyao Ge, Mingyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01554">Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Prompt-based adversarial attacks have become an effective means to assess the robustness of large language models (LLMs). However, existing approaches often treat prompts as monolithic text, overlooking their structural heterogeneity-different prompt components contribute unequally to adversarial robustness. Prior works like PromptRobust assume prompts are value-neutral, but our analysis reveals that complex, domain-specific prompts with rich structures have components with differing vulnerabilities. To address this gap, we introduce PromptAnatomy, an automated framework that dissects prompts into functional components and generates diverse, interpretable adversarial examples by selectively perturbing each component using our proposed method, ComPerturb. To ensure linguistic plausibility and mitigate distribution shifts, we further incorporate a perplexity (PPL)-based filtering mechanism. As a complementary resource, we annotate four public instruction-tuning datasets using the PromptAnatomy framework, verified through human review. Extensive experiments across these datasets and five advanced LLMs demonstrate that ComPerturb achieves state-of-the-art attack success rates. Ablation studies validate the complementary benefits of prompt dissection and PPL filtering. Our results underscore the importance of prompt structure awareness and controlled perturbation for reliable adversarial robustness evaluation in LLMs. Code and data are available at https://github.com/Yujiaaaaa/PACP.<br>
<span id='abs_ch'>Chinese Summary: 本文提出PromptAnatomy框架，通过将提示分解为功能组件并采用ComPerturb方法进行选择性扰动，结合困惑度过滤机制保持语言合理性，在多个数据集和大型语言模型上实现了最优的攻击成功率，强调了提示结构认知对对抗鲁棒性评估的重要性。</span><br>
<span id='abs_en'>English Summary: This paper introduces PromptAnatomy, an automated framework that enhances adversarial attack evaluation by dissecting prompts into functional components and selectively perturbing them using ComPerturb, achieving state-of-the-art attack success rates while maintaining linguistic plausibility through perplexity filtering.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1095, <a href='https://arxiv.org/pdf/2508.01490.pdf' target='_blank'>https://arxiv.org/pdf/2508.01490.pdf</a></span>   <span><a href='https://github.com/peng-lab/hescape' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rushin H. Gindra, Giovanni Palla, Mathias Nguyen, Sophia J. Wagner, Manuel Tran, Fabian J Theis, Dieter Saur, Lorin Crawford, Tingying Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01490">A Large-Scale Benchmark of Cross-Modal Learning for Histology and Gene Expression in Spatial Transcriptomics</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spatial transcriptomics enables simultaneous measurement of gene expression and tissue morphology, offering unprecedented insights into cellular organization and disease mechanisms. However, the field lacks comprehensive benchmarks for evaluating multimodal learning methods that leverage both histology images and gene expression data. Here, we present HESCAPE, a large-scale benchmark for cross-modal contrastive pretraining in spatial transcriptomics, built on a curated pan-organ dataset spanning 6 different gene panels and 54 donors. We systematically evaluated state-of-the-art image and gene expression encoders across multiple pretraining strategies and assessed their effectiveness on two downstream tasks: gene mutation classification and gene expression prediction. Our benchmark demonstrates that gene expression encoders are the primary determinant of strong representational alignment, and that gene models pretrained on spatial transcriptomics data outperform both those trained without spatial data and simple baseline approaches. However, downstream task evaluation reveals a striking contradiction: while contrastive pretraining consistently improves gene mutation classification performance, it degrades direct gene expression prediction compared to baseline encoders trained without cross-modal objectives. We identify batch effects as a key factor that interferes with effective cross-modal alignment. Our findings highlight the critical need for batch-robust multimodal learning approaches in spatial transcriptomics. To accelerate progress in this direction, we release HESCAPE, providing standardized datasets, evaluation protocols, and benchmarking tools for the community<br>
<span id='abs_ch'>中文: HESCAPE作为空间转录组学中跨模态对比预训练的大规模基准，揭示了预训练虽能提升基 突变分类性能，但 批次效应干扰而损害基 表达预测，凸显了对批次鲁棒性多模态学 方法的迫切需求。</span><br>
<span id='abs_en'>English: HESCAPE is a comprehensive benchmark for cross-modal contrastive pretraining in spatial transcriptomics, showing that while such pretraining enhances gene mutation classification, it impairs gene expression prediction due to batch effects, underscoring the need for batch-robust multimodal learning methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1096, <a href='https://arxiv.org/pdf/2508.01432.pdf' target='_blank'>https://arxiv.org/pdf/2508.01432.pdf</a></span>   <span><a href='https://github.com/swxkfm/TripTailor' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanzhe Shen, Kaimin Wang, Changze Lv, Xiaoqing Zheng, Xuanjing Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01432">TripTailor: A Real-World Benchmark for Personalized Travel Planning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The continuous evolution and enhanced reasoning capabilities of large language models (LLMs) have elevated their role in complex tasks, notably in travel planning, where demand for personalized, high-quality itineraries is rising. However, current benchmarks often rely on unrealistic simulated data, failing to reflect the differences between LLM-generated and real-world itineraries. Existing evaluation metrics, which primarily emphasize constraints, fall short of providing a comprehensive assessment of the overall quality of travel plans. To address these limitations, we introduce TripTailor, a benchmark designed specifically for personalized travel planning in real-world scenarios. This dataset features an extensive collection of over 500,000 real-world points of interest (POIs) and nearly 4,000 diverse travel itineraries, complete with detailed information, providing a more authentic evaluation framework. Experiments show that fewer than 10\% of the itineraries generated by the latest state-of-the-art LLMs achieve human-level performance. Moreover, we identify several critical challenges in travel planning, including the feasibility, rationality, and personalized customization of the proposed solutions. We hope that TripTailor will drive the development of travel planning agents capable of understanding and meeting user needs while generating practical itineraries. Our code and dataset are available at https://github.com/swxkfm/TripTailor<br>
<span id='abs_ch'>中文: TripTailor基准通过包含超过50万个真实世界兴趣点和近4000条多 化行程的数据集，解决了当前旅行规划评估的不足，实验表明仅有不到10%的最新大型语言模型生成的行程在可行性、合理性和个性化方面达到人类水平。</span><br>
<span id='abs_en'>English: The TripTailor benchmark addresses the limitations of current travel planning evaluations by introducing a dataset with over 500,000 real-world points of interest and nearly 4,000 diverse itineraries, revealing that fewer than 10% of state-of-the-art LLM-generated plans meet human-level performance in feasibility, rationality, and personalization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1097, <a href='https://arxiv.org/pdf/2508.01427.pdf' target='_blank'>https://arxiv.org/pdf/2508.01427.pdf</a></span>   <span><a href='https://github.com/NiceRingNode/SPECTRUM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peirong Zhang, Kai Ding, Lianwen Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01427">Capturing More: Learning Multi-Domain Representations for Robust Online Handwriting Verification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this paper, we propose SPECTRUM, a temporal-frequency synergistic model that unlocks the untapped potential of multi-domain representation learning for online handwriting verification (OHV). SPECTRUM comprises three core components: (1) a multi-scale interactor that finely combines temporal and frequency features through dual-modal sequence interaction and multi-scale aggregation, (2) a self-gated fusion module that dynamically integrates global temporal and frequency features via self-driven balancing. These two components work synergistically to achieve micro-to-macro spectral-temporal integration. (3) A multi-domain distance-based verifier then utilizes both temporal and frequency representations to improve discrimination between genuine and forged handwriting, surpassing conventional temporal-only approaches. Extensive experiments demonstrate SPECTRUM's superior performance over existing OHV methods, underscoring the effectiveness of temporal-frequency multi-domain learning. Furthermore, we reveal that incorporating multiple handwritten biometrics fundamentally enhances the discriminative power of handwriting representations and facilitates verification. These findings not only validate the efficacy of multi-domain learning in OHV but also pave the way for future research in multi-domain approaches across both feature and biometric domains. Code is publicly available at https://github.com/NiceRingNode/SPECTRUM.<br>
<span id='abs_ch'>中文: SPECTRUM是一种时频协同模型，通过多尺度交互和自门控融合整合多领域表征，显著提升了在线笔迹验证的性能，验证了多领域学 的有效性并推动了相关 究发展。English: SPECTRUM is a temporal-frequency synergistic model that enhances online handwriting verification by integrating multi-domain representations through multi-scale interaction and self-gated fusion, outperforming traditional methods and demonstrating the value of multi-domain learning.Paperid:1168,https://arxiv.org/pdf/2508.01412.pdfGitHub</span><br>
<span id='abs_en'>English: SPECTRUM is a temporal-frequency synergistic model that enhances online handwriting verification by integrating multi-domain representations through multi-scale interaction and self-gated fusion, outperforming traditional methods and demonstrating the value of multi-domain learning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1098, <a href='https://arxiv.org/pdf/2508.01401.pdf' target='_blank'>https://arxiv.org/pdf/2508.01401.pdf</a></span>   <span><a href='https://github.com/ahmadrezarm/MedSynth/tree/main' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmad Rezaie Mianroodi, Amirali Rezaie, Niko Grisel Todorov, Cyril Rakovski, Frank Rudzicz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01401">MedSynth: Realistic, Synthetic Medical Dialogue-Note Pairs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Physicians spend significant time documenting clinical encounters, a burden that contributes to professional burnout. To address this, robust automation tools for medical documentation are crucial. We introduce MedSynth -- a novel dataset of synthetic medical dialogues and notes designed to advance the Dialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks. Informed by an extensive analysis of disease distributions, this dataset includes over 10,000 dialogue-note pairs covering over 2000 ICD-10 codes. We demonstrate that our dataset markedly enhances the performance of models in generating medical notes from dialogues, and dialogues from medical notes. The dataset provides a valuable resource in a field where open-access, privacy-compliant, and diverse training data are scarce. Code is available at https://github.com/ahmadrezarm/MedSynth/tree/main and the dataset is available at https://huggingface.co/datasets/Ahmad0067/MedSynth.<br>
<span id='abs_ch'>中文: MedSynth推出包含一万多对医疗对话与记录的人工合成数据集，通过提升对话转记录和记录转对话任务的性能，有效缓解医生职业倦 ，并提供稀缺的合规开放数据资源。</span><br>
<span id='abs_en'>English: MedSynth introduces a synthetic dataset of over 10,000 medical dialogue-note pairs to improve automated documentation, addressing physician burnout by enhancing Dial-2-Note and Note-2-Dial tasks with privacy-compliant data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1099, <a href='https://arxiv.org/pdf/2508.01292.pdf' target='_blank'>https://arxiv.org/pdf/2508.01292.pdf</a></span>   <span><a href='https://github.com/brAIn-science/CoCoLIT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alec Sargood, Lemuel Puglisi, James H. Cole, Neil P. Oxtoby, Daniele RavÃ¬, Daniel C. Alexander
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01292">CoCoLIT: ControlNet-Conditioned Latent Image Translation for MRI to Amyloid PET Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Synthesizing amyloid PET scans from the more widely available and accessible structural MRI modality offers a promising, cost-effective approach for large-scale Alzheimer's Disease (AD) screening. This is motivated by evidence that, while MRI does not directly detect amyloid pathology, it may nonetheless encode information correlated with amyloid deposition that can be uncovered through advanced modeling. However, the high dimensionality and structural complexity of 3D neuroimaging data pose significant challenges for existing MRI-to-PET translation methods. Modeling the cross-modality relationship in a lower-dimensional latent space can simplify the learning task and enable more effective translation. As such, we present CoCoLIT (ControlNet-Conditioned Latent Image Translation), a diffusion-based latent generative framework that incorporates three main innovations: (1) a novel Weighted Image Space Loss (WISL) that improves latent representation learning and synthesis quality; (2) a theoretical and empirical analysis of Latent Average Stabilization (LAS), an existing technique used in similar generative models to enhance inference consistency; and (3) the introduction of ControlNet-based conditioning for MRI-to-PET translation. We evaluate CoCoLIT's performance on publicly available datasets and find that our model significantly outperforms state-of-the-art methods on both image-based and amyloid-related metrics. Notably, in amyloid-positivity classification, CoCoLIT outperforms the second-best method with improvements of +10.5% on the internal dataset and +23.7% on the external dataset. The code and models of our approach are available at https://github.com/brAIn-science/CoCoLIT.<br>
<span id='abs_ch'>中文: CoCoLIT是一种基于扩散模型的潜在生成框架，通过结构MRI合成淀粉 蛋白PET扫描，在图像质量和淀粉 蛋白分类准确性上显著优于现有方法。</span><br>
<span id='abs_en'>English: CoCoLIT is a novel diffusion-based latent generative framework that synthesizes amyloid PET scans from structural MRI, significantly outperforming existing methods in image quality and amyloid classification accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1100, <a href='https://arxiv.org/pdf/2508.01209.pdf' target='_blank'>https://arxiv.org/pdf/2508.01209.pdf</a></span>   <span><a href='https://github.com/SukwonYun/GOODIE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sukwon Yun, Xin Liu, Yunhak Oh, Junseok Lee, Tianlong Chen, Tsuyoshi Murata, Chanyoung Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01209">Oldie but Goodie: Re-illuminating Label Propagation on Graphs with Partially Observed Features</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In real-world graphs, we often encounter missing feature situations where a few or the majority of node features, e.g., sensitive information, are missed. In such scenarios, directly utilizing Graph Neural Networks (GNNs) would yield sub-optimal results in downstream tasks such as node classification. Despite the emergence of a few GNN-based methods attempting to mitigate its missing situation, when only a few features are available, they rather perform worse than traditional structure-based models. To this end, we propose a novel framework that further illuminates the potential of classical Label Propagation (Oldie), taking advantage of Feature Propagation, especially when only a partial feature is available. Now called by GOODIE, it takes a hybrid approach to obtain embeddings from the Label Propagation branch and Feature Propagation branch. To do so, we first design a GNN-based decoder that enables the Label Propagation branch to output hidden embeddings that align with those of the FP branch. Then, GOODIE automatically captures the significance of structure and feature information thanks to the newly designed Structure-Feature Attention. Followed by a novel Pseudo-Label contrastive learning that differentiates the contribution of each positive pair within pseudo-labels originating from the LP branch, GOODIE outputs the final prediction for the unlabeled nodes. Through extensive experiments, we demonstrate that our proposed model, GOODIE, outperforms the existing state-of-the-art methods not only when only a few features are available but also in abundantly available situations. Source code of GOODIE is available at: https://github.com/SukwonYun/GOODIE.<br>
<span id='abs_ch'>中文: GOODIE框架通过结合 签 播和特征 播，并引入创新的注意力机制与对比学 ，有效解决了图中节点特征缺失的问题，在特征稀缺和丰富的情况下均优于现有方法。</span><br>
<span id='abs_en'>English: The proposed GOODIE framework combines Label Propagation and Feature Propagation with a novel attention mechanism and contrastive learning to effectively handle missing node features in graphs, outperforming existing methods in both feature-scarce and feature-rich scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1101, <a href='https://arxiv.org/pdf/2508.01158.pdf' target='_blank'>https://arxiv.org/pdf/2508.01158.pdf</a></span>   <span><a href='https://github.com/BIT-Jack/H2C-lifelong' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/BIT-Jack/H2C-lifelong' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunlong Lin, Zirui Li, Guodong Du, Xiaocong Zhao, Cheng Gong, Xinwei Wang, Chao Lu, Jianwei Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01158">H2C: Hippocampal Circuit-inspired Continual Learning for Lifelong Trajectory Prediction in Autonomous Driving</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Deep learning (DL) has shown state-of-the-art performance in trajectory prediction, which is critical to safe navigation in autonomous driving (AD). However, most DL-based methods suffer from catastrophic forgetting, where adapting to a new distribution may cause significant performance degradation in previously learned ones. Such inability to retain learned knowledge limits their applicability in the real world, where AD systems need to operate across varying scenarios with dynamic distributions. As revealed by neuroscience, the hippocampal circuit plays a crucial role in memory replay, effectively reconstructing learned knowledge based on limited resources. Inspired by this, we propose a hippocampal circuit-inspired continual learning method (H2C) for trajectory prediction across varying scenarios. H2C retains prior knowledge by selectively recalling a small subset of learned samples. First, two complementary strategies are developed to select the subset to represent learned knowledge. Specifically, one strategy maximizes inter-sample diversity to represent the distinctive knowledge, and the other estimates the overall knowledge by equiprobable sampling. Then, H2C updates via a memory replay loss function calculated by these selected samples to retain knowledge while learning new data. Experiments based on various scenarios from the INTERACTION dataset are designed to evaluate H2C. Experimental results show that H2C reduces catastrophic forgetting of DL baselines by 22.71% on average in a task-free manner, without relying on manually informed distributional shifts. The implementation is available at https://github.com/BIT-Jack/H2C-lifelong.<br>
<span id='abs_ch'>中文摘要：针对自动驾驶轨迹预测中深度学 模型面临灾难性遗忘的问题，本文受海马体神经回路启发提出持续学 方法H2C，通过选择性重放已学 本在 需人工 注分布变化的情况下，平均减少基线模型22.71%的遗忘程度，实现跨场景的稳定预测。</span><br>
<span id='abs_en'>English Summary: Deep learning-based trajectory prediction methods for autonomous driving often suffer from catastrophic forgetting when adapting to new scenarios, so this paper proposes a hippocampal circuit-inspired continual learning approach (H2C) that selectively replays learned samples to reduce forgetting by 22.71% while maintaining performance across varying distributions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1102, <a href='https://arxiv.org/pdf/2508.01139.pdf' target='_blank'>https://arxiv.org/pdf/2508.01139.pdf</a></span>   <span><a href='https://github.com/528why/Dataset-Condensation-with-Color-Compensation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huyu Wu, Duo Su, Junjie Hou, Guang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01139">Dataset Condensation with Color Compensation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Dataset condensation always faces a constitutive trade-off: balancing performance and fidelity under extreme compression. Existing methods struggle with two bottlenecks: image-level selection methods (Coreset Selection, Dataset Quantization) suffer from inefficiency condensation, while pixel-level optimization (Dataset Distillation) introduces semantic distortion due to over-parameterization. With empirical observations, we find that a critical problem in dataset condensation is the oversight of color's dual role as an information carrier and a basic semantic representation unit. We argue that improving the colorfulness of condensed images is beneficial for representation learning. Motivated by this, we propose DC3: a Dataset Condensation framework with Color Compensation. After a calibrated selection strategy, DC3 utilizes the latent diffusion model to enhance the color diversity of an image rather than creating a brand-new one. Extensive experiments demonstrate the superior performance and generalization of DC3 that outperforms SOTA methods across multiple benchmarks. To the best of our knowledge, besides focusing on downstream tasks, DC3 is the first research to fine-tune pre-trained diffusion models with condensed datasets. The FID results prove that training networks with our high-quality datasets is feasible without model collapse or other degradation issues. Code and generated data are available at https://github.com/528why/Dataset-Condensation-with-Color-Compensation.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1103, <a href='https://arxiv.org/pdf/2508.01136.pdf' target='_blank'>https://arxiv.org/pdf/2508.01136.pdf</a></span>   <span><a href='https://github.com/weAIDB/DBAIOps/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Zhou, Peng Sun, Xuanhe Zhou, Qianglei Zang, Ji Xu, Tieying Zhang, Guoliang Li, Fan Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01136">DBAIOps: A Reasoning LLM-Enhanced Database Operation and Maintenance System using Knowledge Graphs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The operation and maintenance (O&M) of database systems is critical to ensuring system availability and performance, typically requiring expert experience (e.g., identifying metric-to-anomaly relations) for effective diagnosis and recovery. However, existing automatic database O&M methods, including commercial products, cannot effectively utilize expert experience. On the one hand, rule-based methods only support basic O&M tasks (e.g., metric-based anomaly detection), which are mostly numerical equations and cannot effectively incorporate literal O&M experience (e.g., troubleshooting guidance in manuals). On the other hand, LLM-based methods, which retrieve fragmented information (e.g., standard documents + RAG), often generate inaccurate or generic results. To address these limitations, we present DBAIOps, a novel hybrid database O&M system that combines reasoning LLMs with knowledge graphs to achieve DBA-style diagnosis. First, DBAIOps introduces a heterogeneous graph model for representing the diagnosis experience, and proposes a semi-automatic graph construction algorithm to build that graph from thousands of documents. Second, DBAIOps develops a collection of (800+) reusable anomaly models that identify both directly alerted metrics and implicitly correlated experience and metrics. Third, for each anomaly, DBAIOps proposes a two-stage graph evolution mechanism to explore relevant diagnosis paths and identify missing relations automatically. It then leverages a reasoning LLM (e.g., DeepSeek-R1) to infer root causes and generate clear diagnosis reports for both DBAs and common users. Our evaluation over four mainstream database systems (Oracle, MySQL, PostgreSQL, and DM8) demonstrates that DBAIOps outperforms state-of-the-art baselines, 34.85% and 47.22% higher in root cause and human evaluation accuracy, respectively.<br>
<span id='abs_ch'>中文: DBAIOps是一种结合推理大语言模型与知识图谱的混合数据库运维系统，通过自动识别 本原 并生成清晰报告，实现了专家级诊断，其准确率显著优于现有方法。</span><br>
<span id='abs_en'>English: DBAIOps is a hybrid database O&M system that integrates reasoning LLMs with knowledge graphs to enable expert-style diagnosis, significantly outperforming existing methods in accuracy by automatically identifying root causes and generating clear reports.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1104, <a href='https://arxiv.org/pdf/2508.01055.pdf' target='_blank'>https://arxiv.org/pdf/2508.01055.pdf</a></span>   <span><a href='https://github.com/xuanliugit/FGBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuan Liu, Siru Ouyang, Xianrui Zhong, Jiawei Han, Huimin Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01055">FGBench: A Dataset and Benchmark for Molecular Property Reasoning at Functional Group-Level in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) have gained significant attention in chemistry. However, most existing datasets center on molecular-level property prediction and overlook the role of fine-grained functional group (FG) information. Incorporating FG-level data can provide valuable prior knowledge that links molecular structures with textual descriptions, which can be used to build more interpretable, structure-aware LLMs for reasoning on molecule-related tasks. Moreover, LLMs can learn from such fine-grained information to uncover hidden relationships between specific functional groups and molecular properties, thereby advancing molecular design and drug discovery. Here, we introduce FGBench, a dataset comprising 625K molecular property reasoning problems with functional group information. Functional groups are precisely annotated and localized within the molecule, which ensures the dataset's interoperability thereby facilitating further multimodal applications. FGBench includes both regression and classification tasks on 245 different functional groups across three categories for molecular property reasoning: (1) single functional group impacts, (2) multiple functional group interactions, and (3) direct molecular comparisons. In the benchmark of state-of-the-art LLMs on 7K curated data, the results indicate that current LLMs struggle with FG-level property reasoning, highlighting the need to enhance reasoning capabilities in LLMs for chemistry tasks. We anticipate that the methodology employed in FGBench to construct datasets with functional group-level information will serve as a foundational framework for generating new question-answer pairs, enabling LLMs to better understand fine-grained molecular structure-property relationships. The dataset and evaluation code are available at https://github.com/xuanliugit/FGBench.<br>
<span id='abs_ch'>中文摘要：FGBench推出了一个包含62.5万个分子性质推理问题的数据集，通过整合精细功能基团信息来增强化学领域大语言模型的可解释性和结构感知能力，揭示了现有模型在功能基团层面推理的不足，并为深化分子结构-性质关联理解提供了基础框架。</span><br>
<span id='abs_en'>English Summary: FGBench introduces a dataset with 625K molecular property reasoning problems incorporating fine-grained functional group information to enhance interpretability and structure-awareness in large language models for chemistry, revealing current models' limitations in functional group-level reasoning and providing a framework for improving molecular structure-property understanding.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1105, <a href='https://arxiv.org/pdf/2508.01012.pdf' target='_blank'>https://arxiv.org/pdf/2508.01012.pdf</a></span>   <span><a href='https://github.com/AndyLu666/MCP-EDA-Server' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyi Lu, Hoi Ian Au, Junyao Zhang, Jingyu Pan, Yiting Wang, Ang Li, Jianyi Zhang, Yiran Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01012">AutoEDA: Enabling EDA Flow Automation through Microservice-Based LLM Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Modern Electronic Design Automation (EDA) workflows, especially the RTL-to-GDSII flow, require heavily manual scripting and demonstrate a multitude of tool-specific interactions which limits scalability and efficiency. While LLMs introduces strides for automation, existing LLM solutions require expensive fine-tuning and do not contain standardized frameworks for integration and evaluation. We introduce AutoEDA, a framework for EDA automation that leverages paralleled learning through the Model Context Protocol (MCP) specific for standardized and scalable natural language experience across the entire RTL-to-GDSII flow. AutoEDA limits fine-tuning through structured prompt engineering, implements intelligent parameter extraction and task decomposition, and provides an extended CodeBLEU metric to evaluate the quality of TCL scripts. Results from experiments over five previously curated benchmarks show improvements in automation accuracy and efficiency, as well as script quality when compared to existing methods. AutoEDA is released open-sourced to support reproducibility and the EDA community. Available at: https://github.com/AndyLu666/MCP-EDA-Server<br>
<span id='abs_ch'>中文：AutoEDA是一种创新框架，通过模型上下文协议实现电子设计自动化的 准化自然语言处理，借助结构化提示工程减少微调需求，并采用扩展的CodeBLEU指 提升脚本质量。</span><br>
<span id='abs_en'>English: AutoEDA is a novel framework that automates the Electronic Design Automation workflow by utilizing the Model Context Protocol for standardized natural language processing, reducing the need for fine-tuning through advanced prompt engineering and improving script quality with an extended CodeBLEU metric.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1106, <a href='https://arxiv.org/pdf/2508.00969.pdf' target='_blank'>https://arxiv.org/pdf/2508.00969.pdf</a></span>   <span><a href='https://github.com/Lucas-rbnt/MORPHEUS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas Robinet, Ahmad Berjaoui, Elizabeth Cohen-Jonathan Moyal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00969">Masked Omics Modeling for Multimodal Representation Learning across Histopathology and Molecular Profiles</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Self-supervised learning has driven major advances in computational pathology by enabling models to learn rich representations from hematoxylin and eosin (H&E)-stained cancer tissue. However, histopathology alone often falls short for molecular characterization and understanding clinical outcomes, as important information is contained in high-dimensional omics profiles like transcriptomics, methylomics, or genomics. In this work, we introduce MORPHEUS, a unified transformer-based pre-training framework that encodes both histopathology and multi-omics data into a shared latent space. At its core, MORPHEUS relies on a masked modeling objective applied to randomly selected omics portions, encouraging the model to learn biologically meaningful cross-modal relationships. The same pre-trained network can be applied to histopathology alone or in combination with any subset of omics modalities, seamlessly adapting to the available inputs. Additionally, MORPHEUS enables any-to-any omics generation, enabling one or more omics profiles to be inferred from any subset of modalities, including H&E alone. Pre-trained on a large pan-cancer cohort, MORPHEUS consistently outperforms state-of-the-art methods across diverse modality combinations and tasks, positioning itself as a promising framework for developing multimodal foundation models in oncology. The code is available at: https://github.com/Lucas-rbnt/MORPHEUS<br>
<span id='abs_ch'>MORPHEUS通过自监督学 将病理学与多组学数据整合到共享潜在空间，实现了灵活的多模态分析并在肿瘤学任务中表现卓越。</span><br>
<span id='abs_en'>Self-supervised learning with MORPHEUS integrates histopathology and multi-omics data into a shared latent space, enabling flexible multimodal analysis and superior performance in oncology tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1107, <a href='https://arxiv.org/pdf/2508.00766.pdf' target='_blank'>https://arxiv.org/pdf/2508.00766.pdf</a></span>   <span><a href='https://github.com/Sample-Aware-TTA/Code' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Irene Iele, Francesco Di Feola, Valerio Guarrasi, Paolo Soda
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00766">Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Image-to-image translation has emerged as a powerful technique in medical imaging, enabling tasks such as image denoising and cross-modality conversion. However, it suffers from limitations in handling out-of-distribution samples without causing performance degradation. To address this limitation, we propose a novel Test-Time Adaptation (TTA) framework that dynamically adjusts the translation process based on the characteristics of each test sample. Our method introduces a Reconstruction Module to quantify the domain shift and a Dynamic Adaptation Block that selectively modifies the internal features of a pretrained translation model to mitigate the shift without compromising the performance on in-distribution samples that do not require adaptation. We evaluate our approach on two medical image-to-image translation tasks: low-dose CT denoising and T1 to T2 MRI translation, showing consistent improvements over both the baseline translation model without TTA and prior TTA methods. Our analysis highlights the limitations of the state-of-the-art that uniformly apply the adaptation to both out-of-distribution and in-distribution samples, demonstrating that dynamic, sample-specific adjustment offers a promising path to improve model resilience in real-world scenarios. The code is available at: https://github.com/Sample-Aware-TTA/Code.<br>
<span id='abs_ch'>中文: 本文提出了一种新颖的测试时自适应框架，通过重建模块和动态适应块动态调整图像翻译过程以处理分布外 本，在医学影像任务中展现出优于现有方法的性能且不影响分布内 本。</span><br>
<span id='abs_en'>English: This paper introduces a novel Test-Time Adaptation framework that dynamically adjusts image translation for out-of-distribution samples using a Reconstruction Module and Dynamic Adaptation Block, showing improved performance in medical imaging tasks without compromising in-distribution samples.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1108, <a href='https://arxiv.org/pdf/2508.00701.pdf' target='_blank'>https://arxiv.org/pdf/2508.00701.pdf</a></span>   <span><a href='https://github.com/Zig-HS/D3' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chende Zheng, Ruiqi suo, Chenhao Lin, Zhengyu Zhao, Le Yang, Shuai Liu, Minghui Yang, Cong Wang, Chao Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00701">D3: Training-Free AI-Generated Video Detection Using Second-Order Features</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The evolution of video generation techniques, such as Sora, has made it increasingly easy to produce high-fidelity AI-generated videos, raising public concern over the dissemination of synthetic content. However, existing detection methodologies remain limited by their insufficient exploration of temporal artifacts in synthetic videos. To bridge this gap, we establish a theoretical framework through second-order dynamical analysis under Newtonian mechanics, subsequently extending the Second-order Central Difference features tailored for temporal artifact detection. Building on this theoretical foundation, we reveal a fundamental divergence in second-order feature distributions between real and AI-generated videos. Concretely, we propose Detection by Difference of Differences (D3), a novel training-free detection method that leverages the above second-order temporal discrepancies. We validate the superiority of our D3 on 4 open-source datasets (Gen-Video, VideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo, D3 outperforms the previous best method by 10.39% (absolute) mean Average Precision. Additional experiments on time cost and post-processing operations demonstrate D3's exceptional computational efficiency and strong robust performance. Our code is available at https://github.com/Zig-HS/D3.<br>
<span id='abs_ch'>中文: 本 究提出D3方法，通过利用二阶时序差异 需训练即可有效区分AI生成视频与真实视频，在多个数据集上展现出卓越性能与计算效率。</span><br>
<span id='abs_en'>English: The study introduces D3, a training-free detection method that leverages second-order temporal discrepancies to effectively distinguish AI-generated videos from real ones, demonstrating superior performance and computational efficiency across multiple datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1109, <a href='https://arxiv.org/pdf/2508.00555.pdf' target='_blank'>https://arxiv.org/pdf/2508.00555.pdf</a></span>   <span><a href='https://github.com/yunsaijc/AGILE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiecong Wang, Haoran Li, Hao Peng, Ziqian Zeng, Zihao Wang, Haohua Du, Zhengtao Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00555">Activation-Guided Local Editing for Jailbreaking Attacks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Jailbreaking is an essential adversarial technique for red-teaming these models to uncover and patch security flaws. However, existing jailbreak methods face significant drawbacks. Token-level jailbreak attacks often produce incoherent or unreadable inputs and exhibit poor transferability, while prompt-level attacks lack scalability and rely heavily on manual effort and human ingenuity. We propose a concise and effective two-stage framework that combines the advantages of these approaches. The first stage performs a scenario-based generation of context and rephrases the original malicious query to obscure its harmful intent. The second stage then utilizes information from the model's hidden states to guide fine-grained edits, effectively steering the model's internal representation of the input from a malicious toward a benign one. Extensive experiments demonstrate that this method achieves state-of-the-art Attack Success Rate, with gains of up to 37.74% over the strongest baseline, and exhibits excellent transferability to black-box models. Our analysis further demonstrates that AGILE maintains substantial effectiveness against prominent defense mechanisms, highlighting the limitations of current safeguards and providing valuable insights for future defense development. Our code is available at https://github.com/yunsaijc/AGILE.<br>
<span id='abs_ch'>中文摘要：提出的AGILE框架通过结合基于场景的查询重构和隐藏状态引导编辑，实现了最先进的越狱成功率，同时展现出优秀的可迁移性和防御对抗能力。</span><br>
<span id='abs_en'>English Summary: The proposed AGILE framework combines scenario-based query rephrasing with hidden state-guided editing to achieve state-of-the-art jailbreak effectiveness while demonstrating strong transferability and defense resistance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1110, <a href='https://arxiv.org/pdf/2508.00496.pdf' target='_blank'>https://arxiv.org/pdf/2508.00496.pdf</a></span>   <span><a href='https://github.com/cirmuw/LesiOnTime' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammed Kamran, Maria Bernathova, Raoul Varga, Christian F. Singer, Zsuzsanna Bago-Horvath, Thomas Helbich, Georg Langs, Philipp SeebÃ¶ck
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00496">LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast Lesion Segmentation in Longitudinal DCE-MRI</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced MRI (DCE-MRI) is critical for early cancer detection, especially in high-risk patients. While recent deep learning methods have advanced lesion segmentation, they primarily target large lesions and neglect valuable longitudinal and clinical information routinely used by radiologists. In real-world screening, detecting subtle or emerging lesions requires radiologists to compare across timepoints and consider previous radiology assessments, such as the BI-RADS score. We propose LesiOnTime, a novel 3D segmentation approach that mimics clinical diagnostic workflows by jointly leveraging longitudinal imaging and BIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA) block that dynamically integrates information from previous and current scans; and (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent space alignment for scans with similar radiological assessments, thus embedding domain knowledge into the training process. Evaluated on a curated in-house longitudinal dataset of high-risk patients with DCE-MRI, our approach outperforms state-of-the-art single-timepoint and longitudinal baselines by 5% in terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute complementary performance gains. These results highlight the importance of incorporating temporal and clinical context for reliable early lesion segmentation in real-world breast cancer screening. Our code is publicly available at https://github.com/cirmuw/LesiOnTime<br>
<span id='abs_ch'>中文: LesiOnTime提出了一种融合纵向MRI扫描和BI-RADS评分的3D分割方法，通过时序注意力机制和一致性正则化，在乳腺癌筛查中实现早期小病灶检测的Dice指 提升5%。</span><br>
<span id='abs_en'>English: LesiOnTime introduces a 3D segmentation method that integrates longitudinal MRI scans and BI-RADS scores through temporal attention and consistency regularization, achieving a 5% Dice improvement for early small lesion detection in breast cancer screening.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1111, <a href='https://arxiv.org/pdf/2508.00414.pdf' target='_blank'>https://arxiv.org/pdf/2508.00414.pdf</a></span>   <span><a href='https://github.com/Tencent/CognitiveKernel-Pro' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianqing Fang, Zhisong Zhang, Xiaoyang Wang, Rui Wang, Can Qin, Yuxuan Wan, Jun-Yu Ma, Ce Zhang, Jiaqi Chen, Xiyun Li, Hongming Zhang, Haitao Mi, Dong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00414">Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on a variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-of-the-art results among open-source and free agents. Notably, our 8B-parameter open-source model surpasses previous leading systems such as WebDancer and WebSailor, establishing a new performance standard for accessible, high-capability AI agents. Code is available at https://github.com/Tencent/CognitiveKernel-Pro<br>
<span id='abs_ch'>中文：Cognitive Kernel-Pro 是一个完全开源且基本免费的多模块智能体框架，在GAIA基准测试中取得顶尖性能，通过提升鲁棒性和可及性推动了先进AI智能体的民主化发展。</span><br>
<span id='abs_en'>English: Cognitive Kernel-Pro is a fully open-source and largely free multi-module agent framework that achieves state-of-the-art results on GAIA, democratizing advanced AI agent development with enhanced robustness and accessibility.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1112, <a href='https://arxiv.org/pdf/2508.00413.pdf' target='_blank'>https://arxiv.org/pdf/2508.00413.pdf</a></span>   <span><a href='https://github.com/dc-ai-projects/DC-Gen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyu Chen, Dongyun Zou, Wenkun He, Junsong Chen, Enze Xie, Song Han, Han Cai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00413">DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present DC-AE 1.5, a new family of deep compression autoencoders for high-resolution diffusion models. Increasing the autoencoder's latent channel number is a highly effective approach for improving its reconstruction quality. However, it results in slow convergence for diffusion models, leading to poorer generation quality despite better reconstruction quality. This issue limits the quality upper bound of latent diffusion models and hinders the employment of autoencoders with higher spatial compression ratios. We introduce two key innovations to address this challenge: i) Structured Latent Space, a training-based approach to impose a desired channel-wise structure on the latent space with front latent channels capturing object structures and latter latent channels capturing image details; ii) Augmented Diffusion Training, an augmented diffusion training strategy with additional diffusion training objectives on object latent channels to accelerate convergence. With these techniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling results than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better image generation quality than DC-AE-f32c32 while being 4x faster. Code: https://github.com/dc-ai-projects/DC-Gen.<br>
<span id='abs_ch'>中文摘要：DC-AE 1.5通过结构化潜在空间和增强扩散训练技术，解决了高分辨率扩散模型收敛慢的问题，在提升图像生成质量的同时实现了更快的处理速度。</span><br>
<span id='abs_en'>English Summary: DC-AE 1.5 introduces structured latent space and augmented diffusion training to overcome slow convergence issues in high-resolution diffusion models, achieving both superior image generation quality and faster processing speeds.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1113, <a href='https://arxiv.org/pdf/2508.00395.pdf' target='_blank'>https://arxiv.org/pdf/2508.00395.pdf</a></span>   <span><a href='https://github.com/Ferenas/DAPT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Zhang, Tianfei Zhou, Jiangchao Yao, Ya Zhang, Ivor W. Tsang, Yanfeng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00395">Decouple before Align: Visual Disentanglement Enhances Prompt Tuning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm, has showcased remarkable effectiveness in improving the task-specific transferability of vision-language models. This paper delves into a previously overlooked information asymmetry issue in PT, where the visual modality mostly conveys more context than the object-oriented textual modality. Correspondingly, coarsely aligning these two modalities could result in the biased attention, driving the model to merely focus on the context area. To address this, we propose DAPT, an effective PT framework based on an intuitive decouple-before-align concept. First, we propose to explicitly decouple the visual modality into the foreground and background representation via exploiting coarse-and-fine visual segmenting cues, and then both of these decoupled patterns are aligned with the original foreground texts and the hand-crafted background classes, thereby symmetrically strengthening the modal alignment. To further enhance the visual concentration, we propose a visual pull-push regularization tailored for the foreground-background patterns, directing the original visual representation towards unbiased attention on the region-of-interest object. We demonstrate the power of architecture-free DAPT through few-shot learning, base-to-novel generalization, and data-efficient learning, all of which yield superior performance across prevailing benchmarks. Our code will be released at https://github.com/Ferenas/DAPT.<br>
<span id='abs_ch'>中文: 提示调优作为一种高效的微调方法，虽能提升视觉语言模型性能，但存在视觉与文本信息不对称问题；本文提出的DAPT框架通过解耦视觉前景与背景并分别与文本对齐，有效增强模态对称性和注意力聚焦。</span><br>
<span id='abs_en'>English: Prompt tuning is an efficient fine-tuning method that enhances vision-language models but suffers from visual-textual information asymmetry, which the proposed DAPT framework addresses by decoupling and aligning visual foreground and background with text to improve modal symmetry and attention focus.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1114, <a href='https://arxiv.org/pdf/2508.00383.pdf' target='_blank'>https://arxiv.org/pdf/2508.00383.pdf</a></span>   <span><a href='https://github.com/deepnoid-ai/MVHybrid' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Won June Cho, Hongjun Yoon, Daeky Jeong, Hyeongyeol Lim, Yosep Chong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00383">$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spatial transcriptomics reveals gene expression patterns within tissue context, enabling precision oncology applications such as treatment response prediction, but its high cost and technical complexity limit clinical adoption. Predicting spatial gene expression (biomarkers) from routine histopathology images offers a practical alternative, yet current vision foundation models (VFMs) in pathology based on Vision Transformer (ViT) backbones perform below clinical standards. Given that VFMs are already trained on millions of diverse whole slide images, we hypothesize that architectural innovations beyond ViTs may better capture the low-frequency, subtle morphological patterns correlating with molecular phenotypes. By demonstrating that state space models initialized with negative real eigenvalues exhibit strong low-frequency bias, we introduce $MV_{Hybrid}$, a hybrid backbone architecture combining state space models (SSMs) with ViT. We compare five other different backbone architectures for pathology VFMs, all pretrained on identical colorectal cancer datasets using the DINOv2 self-supervised learning method. We evaluate all pretrained models using both random split and leave-one-study-out (LOSO) settings of the same biomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher correlation than the best-performing ViT and shows 43% smaller performance degradation compared to random split in gene expression prediction, demonstrating superior performance and robustness, respectively. Furthermore, $MV_{Hybrid}$ shows equal or better downstream performance in classification, patch retrieval, and survival prediction tasks compared to that of ViT, showing its promise as a next-generation pathology VFM backbone. Our code is publicly available at: https://github.com/deepnoid-ai/MVHybrid.<br>
<span id='abs_ch'>中文摘要：本 究提出的MV_{Hybrid}混合架构结合状态空间模型与视觉Transformer，在从病理图像预测空间基 表达方面显著优于现有模型，同时在多项临床任务中展现出卓越的鲁棒性和性能表现。</span><br>
<span id='abs_en'>English Summary: The study introduces MV_{Hybrid}, a hybrid architecture combining state space models with Vision Transformers, which significantly outperforms existing models in predicting spatial gene expression from pathology images while demonstrating superior robustness and performance across multiple clinical tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1115, <a href='https://arxiv.org/pdf/2508.00312.pdf' target='_blank'>https://arxiv.org/pdf/2508.00312.pdf</a></span>   <span><a href='https://github.com/Sumutan/GV-VAD.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Suhang Cai, Xiaohao Peng, Chong Wang, Xiaojie Cai, Jiangbo Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00312">GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Video anomaly detection (VAD) plays a critical role in public safety applications such as intelligent surveillance. However, the rarity, unpredictability, and high annotation cost of real-world anomalies make it difficult to scale VAD datasets, which limits the performance and generalization ability of existing models. To address this challenge, we propose a generative video-enhanced weakly-supervised video anomaly detection (GV-VAD) framework that leverages text-conditioned video generation models to produce semantically controllable and physically plausible synthetic videos. These virtual videos are used to augment training data at low cost. In addition, a synthetic sample loss scaling strategy is utilized to control the influence of generated synthetic samples for efficient training. The experiments show that the proposed framework outperforms state-of-the-art methods on UCF-Crime datasets. The code is available at https://github.com/Sumutan/GV-VAD.git.<br>
<span id='abs_ch'>中文: 提出的GV-VAD框架通过生成合成视频来增强训练数据，在UCF-Crime数据集上超越了现有最优方法，提升了视频异常检测性能。</span><br>
<span id='abs_en'>English: The proposed GV-VAD framework enhances video anomaly detection by generating synthetic videos to augment training data, outperforming state-of-the-art methods on the UCF-Crime dataset.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1116, <a href='https://arxiv.org/pdf/2508.00271.pdf' target='_blank'>https://arxiv.org/pdf/2508.00271.pdf</a></span>   <span><a href='https://github.com/qhjqhj00/MetaAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongjin Qian, Zheng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00271">MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this work, we propose MetaAgent, an agentic paradigm inspired by the principle of learning-by-doing, where expertise is developed through hands-on practice and continual self-improvement. MetaAgent starts with a minimal workflow, equipped only with basic reasoning and adaptive help-seeking abilities. When a knowledge gap is encountered, MetaAgent generates natural language help requests, which are routed to the most suitable external tool by a dedicated tool router. As MetaAgent solves tasks, it continually conducts self-reflection and answer verification, distilling actionable experience into concise texts that are dynamically incorporated into future task contexts. Besides, MetaAgent autonomously builds in-house tools and a persistent knowledge base by organizing its tool-use history, further enhancing its ability to retrieve and integrate relevant information We term this continual, data-driven process as \textit{meta tool learning}, through which MetaAgent incrementally refines its reasoning and tool-use strategies, without changing model parameters or requiring further post-training. Evaluated on challenging knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp, MetaAgent consistently outperforms workflow-based baselines and matches or exceeds end-to-end trained agents, demonstrating the promise of self-evolving agentic systems for robust, general-purpose knowledge discovery. We provide our source codes in https://github.com/qhjqhj00/MetaAgent.<br>
<span id='abs_ch'>Chinese: MetaAgent是一种通过实践、自我反思和动态知识整合来自我进化的系统， 需更新模型参数即可在知识发现基准测试中超越现有方法，展现出强大的推理和工具使用能力。</span><br>
<span id='abs_en'>English: MetaAgent is a self-evolving system that enhances its reasoning and tool-use abilities through hands-on practice, self-reflection, and dynamic knowledge integration, outperforming existing methods on knowledge discovery benchmarks without requiring model updates.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1117, <a href='https://arxiv.org/pdf/2508.00155.pdf' target='_blank'>https://arxiv.org/pdf/2508.00155.pdf</a></span>   <span><a href='https://github.com/tomek1911/GEPAR3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomasz SzczepaÅski, Szymon PÅotka, Michal K. Grzeszczyk, Arleta Adamowicz, Piotr Fudalej, PrzemysÅaw Korzeniowski, Tomasz TrzciÅski, Arkadiusz Sitek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00155">GEPAR3D: Geometry Prior-Assisted Learning for 3D Tooth Segmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Tooth segmentation in Cone-Beam Computed Tomography (CBCT) remains challenging, especially for fine structures like root apices, which is critical for assessing root resorption in orthodontics. We introduce GEPAR3D, a novel approach that unifies instance detection and multi-class segmentation into a single step tailored to improve root segmentation. Our method integrates a Statistical Shape Model of dentition as a geometric prior, capturing anatomical context and morphological consistency without enforcing restrictive adjacency constraints. We leverage a deep watershed method, modeling each tooth as a continuous 3D energy basin encoding voxel distances to boundaries. This instance-aware representation ensures accurate segmentation of narrow, complex root apices. Trained on publicly available CBCT scans from a single center, our method is evaluated on external test sets from two in-house and two public medical centers. GEPAR3D achieves the highest overall segmentation performance, averaging a Dice Similarity Coefficient (DSC) of 95.0% (+2.8% over the second-best method) and increasing recall to 95.2% (+9.5%) across all test sets. Qualitative analyses demonstrated substantial improvements in root segmentation quality, indicating significant potential for more accurate root resorption assessment and enhanced clinical decision-making in orthodontics. We provide the implementation and dataset at https://github.com/tomek1911/GEPAR3D.<br>
<span id='abs_ch'>中文: GEPAR3D提出了一种结合统计形状模型与深度分水岭算法的统一检测分割方法，在CBCT影像中实现了95.0%的Dice系数，显著提升了牙 尖端分割精度，为正畸治疗中的牙 吸收评估提供了更可 的解决方案。</span><br>
<span id='abs_en'>English: GEPAR3D introduces a unified deep learning approach combining instance detection and multi-class segmentation with a statistical shape model, achieving superior tooth segmentation performance in CBCT scans with a 95.0% Dice score and significant improvements in root apex delineation for orthodontic applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1118, <a href='https://arxiv.org/pdf/2508.00098.pdf' target='_blank'>https://arxiv.org/pdf/2508.00098.pdf</a></span>   <span><a href='https://github.com/Stress-Aware-Learning/SAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashkan Shakarami, Yousef Yeganeh, Azade Farshad, Lorenzo Nicole, Stefano Ghidoni, Nassir Navab
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00098">Stress-Aware Resilient Neural Training</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper introduces Stress-Aware Learning, a resilient neural training paradigm in which deep neural networks dynamically adjust their optimization behavior - whether under stable training regimes or in settings with uncertain dynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic) Deformation, inspired by structural fatigue in materials science. To instantiate this concept, we propose Plastic Deformation Optimizer, a stress-aware mechanism that injects adaptive noise into model parameters whenever an internal stress signal - reflecting stagnation in training loss and accuracy - indicates persistent optimization difficulty. This enables the model to escape sharp minima and converge toward flatter, more generalizable regions of the loss landscape. Experiments across six architectures, four optimizers, and seven vision benchmarks demonstrate improved robustness and generalization with minimal computational overhead. The code and 3D visuals will be available on GitHub: https://github.com/Stress-Aware-Learning/SAL.<br>
<span id='abs_ch'>中文: 本文提出应力感知学 这一弹性神经训练范式，通过塑性变形优化器向模型参数注入自适应噪声，使模型能够逃离尖锐极小值并收敛至更平坦、泛化能力更强的损失区域，在多种架构和基准测试中展现出卓越的鲁棒性。</span><br>
<span id='abs_en'>English: This paper presents Stress-Aware Learning, a resilient neural training paradigm that uses a Plastic Deformation Optimizer to inject adaptive noise into model parameters, enabling escape from sharp minima and convergence toward flatter, more generalizable loss regions with demonstrated robustness across multiple architectures and benchmarks.Paperid:1240,https://arxiv.org/pdf/2508.00097.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1119, <a href='https://arxiv.org/pdf/2508.00097.pdf' target='_blank'>https://arxiv.org/pdf/2508.00097.pdf</a></span>   <span><a href='https://github.com/XR-Robotics' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhigen Zhao, Liuchuan Yu, Ke Jing, Ning Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00097">XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of Vision-Language-Action models has created an urgent need for large-scale, high-quality robot demonstration datasets. Although teleoperation is the predominant method for data collection, current approaches suffer from limited scalability, complex setup procedures, and suboptimal data quality. This paper presents XRoboToolkit, a cross-platform framework for extended reality based robot teleoperation built on the OpenXR standard. The system features low-latency stereoscopic visual feedback, optimization-based inverse kinematics, and support for diverse tracking modalities including head, controller, hand, and auxiliary motion trackers. XRoboToolkit's modular architecture enables seamless integration across robotic platforms and simulation environments, spanning precision manipulators, mobile robots, and dexterous hands. We demonstrate the framework's effectiveness through precision manipulation tasks and validate data quality by training VLA models that exhibit robust autonomous performance.<br>
<span id='abs_ch'>中文：XRoboToolkit提出了一种基于OpenXR的跨平台扩展现实机器人遥操作框架，具备低延迟反馈和模块化架构，可 缝集成多种机器人平台与仿真环境。</span><br>
<span id='abs_en'>English: XRoboToolkit introduces a cross-platform extended reality framework for scalable, high-quality robot teleoperation using OpenXR, featuring low-latency feedback and modular integration across diverse robotic systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1120, <a href='https://arxiv.org/pdf/2508.00085.pdf' target='_blank'>https://arxiv.org/pdf/2508.00085.pdf</a></span>   <span><a href='https://github.com/raiyaan-abdullah/Motion-Transfer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Raiyaan Abdullah, Jared Claypoole, Michael Cogswell, Ajay Divakaran, Yogesh Rawat
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00085">Punching Bag vs. Punching Person: Motion Transferability in Videos</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Action recognition models demonstrate strong generalization, but can they effectively transfer high-level motion concepts across diverse contexts, even within similar distributions? For example, can a model recognize the broad action "punching" when presented with an unseen variation such as "punching person"? To explore this, we introduce a motion transferability framework with three datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2) Kinetics400-TA; and (3) Something-Something-v2-TA, both adapted from natural video datasets. We evaluate 13 state-of-the-art models on these benchmarks and observe a significant drop in performance when recognizing high-level actions in novel contexts. Our analysis reveals: 1) Multimodal models struggle more with fine-grained unknown actions than with coarse ones; 2) The bias-free Syn-TA proves as challenging as real-world datasets, with models showing greater performance drops in controlled settings; 3) Larger models improve transferability when spatial cues dominate but struggle with intensive temporal reasoning, while reliance on object and background cues hinders generalization. We further explore how disentangling coarse and fine motions can improve recognition in temporally challenging datasets. We believe this study establishes a crucial benchmark for assessing motion transferability in action recognition. Datasets and relevant code: https://github.com/raiyaan-abdullah/Motion-Transfer.<br>
<span id='abs_ch'>中文摘要：本 究提出一个运动可迁移性框架，评估动作识别模型在新情境下泛化高级运动概念的能力，发现模型性能显著下降，并揭示了时序推理和空间偏差对迁移效果的关键影响。</span><br>
<span id='abs_en'>English Summary: This study introduces a motion transferability framework to evaluate how well action recognition models generalize high-level motion concepts across novel contexts, revealing significant performance drops and highlighting challenges in temporal reasoning and spatial bias.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1121, <a href='https://arxiv.org/pdf/2508.00079.pdf' target='_blank'>https://arxiv.org/pdf/2508.00079.pdf</a></span>   <span><a href='https://github.com/areebuzair/PhysicsEval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Oshayer Siddique, J. M Areeb Uzair Alam, Md Jobayer Rahman Rafy, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00079">PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The discipline of physics stands as a cornerstone of human intellect, driving the evolution of technology and deepening our understanding of the fundamental principles of the cosmos. Contemporary literature includes some works centered on the task of solving physics problems - a crucial domain of natural language reasoning. In this paper, we evaluate the performance of frontier LLMs in solving physics problems, both mathematical and descriptive. We also employ a plethora of inference-time techniques and agentic frameworks to improve the performance of the models. This includes the verification of proposed solutions in a cumulative fashion by other, smaller LLM agents, and we perform a comparative analysis of the performance that the techniques entail. There are significant improvements when the multi-agent framework is applied to problems that the models initially perform poorly on. Furthermore, we introduce a new evaluation benchmark for physics problems, ${\rm P{\small HYSICS}E{\small VAL}}$, consisting of 19,609 problems sourced from various physics textbooks and their corresponding correct solutions scraped from physics forums and educational websites. Our code and data are publicly available at https://github.com/areebuzair/PhysicsEval.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1122, <a href='https://arxiv.org/pdf/2508.00047.pdf' target='_blank'>https://arxiv.org/pdf/2508.00047.pdf</a></span>   <span><a href='https://github.com/YYZStart/TriP-LLM.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan-Cheng Yu, Yen-Chieh Ouyang, Chun-An Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00047">TriP-LLM: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Time-series anomaly detection plays a central role across a wide range of application domains. With the increasing proliferation of the Internet of Things (IoT) and smart manufacturing, time-series data has dramatically increased in both scale and dimensionality. This growth has exposed the limitations of traditional statistical methods in handling the high heterogeneity and complexity of such data. Inspired by the recent success of large language models (LLMs) in multimodal tasks across language and vision domains, we propose a novel unsupervised anomaly detection framework: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection (TriP-LLM). TriP-LLM integrates local and global temporal features through a tri-branch design-Patching, Selection, and Global-to encode the input time series into patch-wise tokens, which are then processed by a frozen, pretrained LLM. A lightweight patch-wise decoder reconstructs the input, from which anomaly scores are derived. We evaluate TriP-LLM on several public benchmark datasets using PATE, a recently proposed threshold-free evaluation metric, and conduct all comparisons within a unified open-source framework to ensure fairness. Experimental results show that TriP-LLM consistently outperforms recent state-of-the-art methods across all datasets, demonstrating strong detection capabilities. Furthermore, through extensive ablation studies, we verify the substantial contribution of the LLM to the overall architecture. Compared to LLM-based approaches using Channel Independence (CI) patch processing, TriP-LLM achieves significantly lower memory consumption, making it more suitable for GPU memory-constrained environments. All code and model checkpoints are publicly available on https://github.com/YYZStart/TriP-LLM.git<br>
<span id='abs_ch'>中文: 本文提出TriP-LLM这一新型 监督框架，通过冻结的大型语言模型整合局部与全局时序特征进行时间序列异常检测，在多个基准测试中相比现有最优方法展现出更优性能与更低内存消耗。</span><br>
<span id='abs_en'>English: This paper introduces TriP-LLM, a novel unsupervised framework that leverages a frozen large language model to integrate local and global temporal features for time-series anomaly detection, demonstrating superior performance and lower memory consumption compared to state-of-the-art methods across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1123, <a href='https://arxiv.org/pdf/2508.00047.pdf' target='_blank'>https://arxiv.org/pdf/2508.00047.pdf</a></span>   <span><a href='https://github.com/YYZStart/TriP-LLM.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan-Cheng Yu, Yen-Chieh Ouyang, Chun-An Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00047">TriP-LLM: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Time-series anomaly detection plays a central role across a wide range of application domains. With the increasing proliferation of the Internet of Things (IoT) and smart manufacturing, time-series data has dramatically increased in both scale and dimensionality. This growth has exposed the limitations of traditional statistical methods in handling the high heterogeneity and complexity of such data. Inspired by the recent success of large language models (LLMs) in multimodal tasks across language and vision domains, we propose a novel unsupervised anomaly detection framework: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection (TriP-LLM). TriP-LLM integrates local and global temporal features through a tri-branch design-Patching, Selection, and Global-to encode the input time series into patch-wise tokens, which are then processed by a frozen, pretrained LLM. A lightweight patch-wise decoder reconstructs the input, from which anomaly scores are derived. We evaluate TriP-LLM on several public benchmark datasets using PATE, a recently proposed threshold-free evaluation metric, and conduct all comparisons within a unified open-source framework to ensure fairness. Experimental results show that TriP-LLM consistently outperforms recent state-of-the-art methods across all datasets, demonstrating strong detection capabilities. Furthermore, through extensive ablation studies, we verify the substantial contribution of the LLM to the overall architecture. Compared to LLM-based approaches using Channel Independence (CI) patch processing, TriP-LLM achieves significantly lower memory consumption, making it more suitable for GPU memory-constrained environments. All code and model checkpoints are publicly available on https://github.com/YYZStart/TriP-LLM.git<br>
<span id='abs_ch'>中文: 本文提出TriP-LLM这一新型 监督框架，通过冻结的大型语言模型整合局部与全局时序特征进行时间序列异常检测，在多个基准测试中相比现有最优方法展现出更优性能与更低内存消耗。</span><br>
<span id='abs_en'>English: This paper introduces TriP-LLM, a novel unsupervised framework that leverages a frozen large language model to integrate local and global temporal features for time-series anomaly detection, demonstrating superior performance and lower memory consumption compared to state-of-the-art methods across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1124, <a href='https://arxiv.org/pdf/2508.00017.pdf' target='_blank'>https://arxiv.org/pdf/2508.00017.pdf</a></span>   <span><a href='https://github.com/Generative-Logic/GL/tree/35a111ea9ba53afe051703d6050be0c3923e9724' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikolai Sergeev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00017">Generative Logic: A New Computer Architecture for Deterministic Reasoning and Knowledge Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present Generative Logic (GL), a deterministic architecture that begins from user-supplied axiomatic definitions -- written in a minimalist Mathematical Programming Language (MPL) -- and systematically explores their deductive neighborhood. Definitions are compiled into a distributed grid of simple Logic Blocks (LBs) that exchange messages; any time several expressions unify under an inference rule, a new fact is emitted with full provenance to its sources, yielding replayable, auditable proof graphs.
  A prototype software implementation instantiates the workflow on first-order Peano arithmetic. Starting only from the Peano axioms, GL enumerates candidate implications, applies normalization and type filters, and automatically reconstructs machine-checkable proofs of foundational arithmetic laws including associativity and commutativity of addition, associativity and commutativity of multiplication, and distributivity. Generated proofs export to navigable HTML so that every inference step can be inspected independently.
  We outline a hardware-software co-design path toward massively parallel realizations and describe prospective integration with probabilistic models (e.g., Large Language Models (LLMs)) for autoformalization and conjecture seeding. The Python and MPL code to reproduce the Peano experiments, along with the full HTML proof graphs, are available in the project's GitHub repository at https://github.com/Generative-Logic/GL/tree/35a111ea9ba53afe051703d6050be0c3923e9724 and are permanently archived at https://doi.org/10.5281/zenodo.16408441. We invite community feedback and collaboration.<br>
<span id='abs_ch'>中文摘要：生成逻辑（GL）是一种确定性架构，它将公理化定义编译为逻辑块，系统性地探索演绎邻域，从皮亚诺公理出发自动重建算术基本定律的可验证证明，并生成可追溯的证明图谱。</span><br>
<span id='abs_en'>English Summary: Generative Logic (GL) is a deterministic architecture that compiles axiomatic definitions into logic blocks to systematically explore deductive neighborhoods, generating auditable proof graphs and reconstructing foundational arithmetic laws from Peano axioms.Paperid:1248,https://arxiv.org/pdf/2508.00007.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1125, <a href='https://arxiv.org/pdf/2508.00007.pdf' target='_blank'>https://arxiv.org/pdf/2508.00007.pdf</a></span>   <span><a href='https://github.com/agent-network-protocol' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaowei Chang, Eidan Lin, Chengxuan Yuan, Rizhao Cai, Binbin Chen, Xuan Xie, Yin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00007">Agent Network Protocol Technical White Paper</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the development of large models and autonomous decision-making AI, agents are rapidly becoming the new entities of the internet, following mobile apps. However, existing internet infrastructure is primarily designed for human interaction, creating data silos, unfriendly interfaces, and high collaboration costs among agents, making it difficult to support the needs for large-scale agent interconnection and collaboration. The internet is undergoing a profound transformation, showing four core trends: agents replacing traditional software, universal agent interconnection, native protocol-based connections, and autonomous agent organization and collaboration. To align with these trends, Agent Network Protocol (ANP) proposes a new generation of communication protocols for the Agentic Web. ANP adheres to AI-native design, maintains compatibility with existing internet protocols, adopts a modular composable architecture, follows minimalist yet extensible principles, and enables rapid deployment based on existing infrastructure. Through a three-layer protocol system--identity and encrypted communication layer, meta-protocol negotiation layer, and application protocol layer--ANP. systematically solves the problems of agent identity authentication, dynamic negotiation, and capability discovery interoperability.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1126, <a href='https://arxiv.org/pdf/2507.23784.pdf' target='_blank'>https://arxiv.org/pdf/2507.23784.pdf</a></span>   <span><a href='https://github.com/ExplainableML/sub' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jessica Bader, Leander Girrbach, Stephan Alaniz, Zeynep Akata
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23784">SUB: Benchmarking CBM Generalization via Synthetic Attribute Substitutions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Concept Bottleneck Models (CBMs) and other concept-based interpretable models show great promise for making AI applications more transparent, which is essential in fields like medicine. Despite their success, we demonstrate that CBMs struggle to reliably identify the correct concepts under distribution shifts. To assess the robustness of CBMs to concept variations, we introduce SUB: a fine-grained image and concept benchmark containing 38,400 synthetic images based on the CUB dataset. To create SUB, we select a CUB subset of 33 bird classes and 45 concepts to generate images which substitute a specific concept, such as wing color or belly pattern. We introduce a novel Tied Diffusion Guidance (TDG) method to precisely control generated images, where noise sharing for two parallel denoising processes ensures that both the correct bird class and the correct attribute are generated. This novel benchmark enables rigorous evaluation of CBMs and similar interpretable models, contributing to the development of more robust methods. Our code is available at https://github.com/ExplainableML/sub and the dataset at http://huggingface.co/datasets/Jessica-bader/SUB.<br>
<span id='abs_ch'>Chinese: 概念瓶颈模型（CBMs）在分布变化下难以可 识别正确概念，为此我们引入了包含38,400 合成图像的SUB基准和捆绑扩散引导方法，以严 评估并推动更稳健可解释模型的发展。</span><br>
<span id='abs_en'>English: Concept Bottleneck Models (CBMs) face challenges in accurately identifying concepts under distribution shifts, prompting the development of the SUB benchmark with 38,400 synthetic images and a Tied Diffusion Guidance method to evaluate and enhance their robustness.Paperid:2,https://arxiv.org/pdf/2507.23782.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1127, <a href='https://arxiv.org/pdf/2507.23771.pdf' target='_blank'>https://arxiv.org/pdf/2507.23771.pdf</a></span>   <span><a href='https://github.com/justinkay/coda' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Justin Kay, Grant Van Horn, Subhransu Maji, Daniel Sheldon, Sara Beery
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23771">Consensus-Driven Active Model Selection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The widespread availability of off-the-shelf machine learning models poses a challenge: which model, of the many available candidates, should be chosen for a given data analysis task? This question of model selection is traditionally answered by collecting and annotating a validation dataset -- a costly and time-intensive process. We propose a method for active model selection, using predictions from candidate models to prioritize the labeling of test data points that efficiently differentiate the best candidate. Our method, CODA, performs consensus-driven active model selection by modeling relationships between classifiers, categories, and data points within a probabilistic framework. The framework uses the consensus and disagreement between models in the candidate pool to guide the label acquisition process, and Bayesian inference to update beliefs about which model is best as more information is collected. We validate our approach by curating a collection of 26 benchmark tasks capturing a range of model selection scenarios. CODA outperforms existing methods for active model selection significantly, reducing the annotation effort required to discover the best model by upwards of 70% compared to the previous state-of-the-art. Code and data are available at https://github.com/justinkay/coda.<br>
<span id='abs_ch'>Chinese Summary: CODA提出了一种主动模型选择方法，利用候选模型间的共识与分歧来优先 注数据，相比现有技术将发现最佳模型所需的 注工作量减少了70%以上。</span><br>
<span id='abs_en'>English Summary: CODA introduces an active model selection method that uses consensus and disagreement among candidate models to prioritize data labeling, significantly reducing annotation effort by over 70% compared to existing approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1128, <a href='https://arxiv.org/pdf/2507.23740.pdf' target='_blank'>https://arxiv.org/pdf/2507.23740.pdf</a></span>   <span><a href='https://github.com/idirlab/KGRule2NL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nasim Shirvani-Mahdavi, Devin Wingfield, Amin Ghasemi, Chengkai Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23740">Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Knowledge graphs (KGs) often contain sufficient information to support the inference of new facts. Identifying logical rules not only improves the completeness of a knowledge graph but also enables the detection of potential errors, reveals subtle data patterns, and enhances the overall capacity for reasoning and interpretation. However, the complexity of such rules, combined with the unique labeling conventions of each KG, can make them difficult for humans to understand. In this paper, we explore the potential of large language models to generate natural language explanations for logical rules. Specifically, we extract logical rules using the AMIE 3.5.1 rule discovery algorithm from the benchmark dataset FB15k-237 and two large-scale datasets, FB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, including zero- and few-shot prompting, including variable entity types, and chain-of-thought reasoning. We conduct a comprehensive human evaluation of the generated explanations based on correctness, clarity, and hallucination, and also assess the use of large language models as automatic judges. Our results demonstrate promising performance in terms of explanation correctness and clarity, although several challenges remain for future research. All scripts and data used in this study are publicly available at https://github.com/idirlab/KGRule2NL}{https://github.com/idirlab/KGRule2NL.<br>
<span id='abs_ch'>知识图谱可通过逻辑规则推断新事实，本 究利用大型语言模型为这些规则生成自然语言解释，并通过人工与自动评估检验了其正确性与清晰度。Knowledge graphs can infer new facts through logical rules, and this study uses large language models to generate natural language explanations for these rules, evaluating their correctness and clarity through human and automated assessments.Paperid:6,https://arxiv.org/pdf/2507.23734.pdfGitHubGitHub</span><br>
<span id='abs_en'>Knowledge graphs can infer new facts through logical rules, and this study uses large language models to generate natural language explanations for these rules, evaluating their correctness and clarity through human and automated assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1129, <a href='https://arxiv.org/pdf/2507.23664.pdf' target='_blank'>https://arxiv.org/pdf/2507.23664.pdf</a></span>   <span><a href='https://github.com/wuming29/RAR.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haipeng Liu, Yuxuan Liu, Ting Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23664">Personalized Education with Ranking Alignment Recommendation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Personalized question recommendation aims to guide individual students through questions to enhance their mastery of learning targets. Most previous methods model this task as a Markov Decision Process and use reinforcement learning to solve, but they struggle with efficient exploration, failing to identify the best questions for each student during training. To address this, we propose Ranking Alignment Recommendation (RAR), which incorporates collaborative ideas into the exploration mechanism, enabling more efficient exploration within limited training episodes. Experiments show that RAR effectively improves recommendation performance, and our framework can be applied to any RL-based question recommender. Our code is available in https://github.com/wuming29/RAR.git.<br>
<span id='abs_ch'>中文: 提出的排序对齐推荐（RAR）将协同过滤思想融入强化学 的探索机制，通过更高效的训练显著提升了个性化题目推荐的性能。</span><br>
<span id='abs_en'>English: The proposed Ranking Alignment Recommendation (RAR) integrates collaborative filtering into reinforcement learning exploration to enhance personalized question recommendation by enabling more efficient training and improved performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1130, <a href='https://arxiv.org/pdf/2507.23511.pdf' target='_blank'>https://arxiv.org/pdf/2507.23511.pdf</a></span>   <span><a href='https://github.com/xiaomi-research/mecat' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yadong Niu, Tianzi Wang, Heinrich Dinkel, Xingwei Sun, Jiahao Zhou, Gang Li, Jizhong Liu, Xunying Liu, Junbo Zhang, Jian Luan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23511">MECAT: A Multi-Experts Constructed Benchmark for Fine-Grained Audio Understanding Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While large audio-language models have advanced open-ended audio understanding, they still fall short of nuanced human-level comprehension. This gap persists largely because current benchmarks, limited by data annotations and evaluation metrics, fail to reliably distinguish between generic and highly detailed model outputs. To this end, this work introduces MECAT, a Multi-Expert Constructed Benchmark for Fine-Grained Audio Understanding Tasks. Generated via a pipeline that integrates analysis from specialized expert models with Chain-of-Thought large language model reasoning, MECAT provides multi-perspective, fine-grained captions and open-set question-answering pairs. The benchmark is complemented by a novel metric: DATE (Discriminative-Enhanced Audio Text Evaluation). This metric penalizes generic terms and rewards detailed descriptions by combining single-sample semantic similarity with cross-sample discriminability. A comprehensive evaluation of state-of-the-art audio models is also presented, providing new insights into their current capabilities and limitations. The data and code are available at https://github.com/xiaomi-research/mecat<br>
<span id='abs_ch'>中文摘要：本 究提出了MECAT多专家构建基准及DATE新型评估指 ，通过细粒度音频理解任务解决现有基准的不足，为音频模型的性能评估提供了新视角。</span><br>
<span id='abs_en'>English Summary: This study introduces MECAT, a multi-expert constructed benchmark with a novel DATE metric, to address the limitations of current audio benchmarks by enabling fine-grained evaluation and revealing new insights into audio models' capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1131, <a href='https://arxiv.org/pdf/2507.23440.pdf' target='_blank'>https://arxiv.org/pdf/2507.23440.pdf</a></span>   <span><a href='https://github.com/Mubuky/Self-Foveate' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingzhe Li, Xin Lu, Yanyan Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23440">Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) with instruction following capabilities have demonstrated impressive problem-solving abilities. While synthesizing instructional data from unsupervised text has become a common approach for training such models, conventional methods rely heavily on human effort for data annotation. Although existing automated synthesis paradigms have alleviated this constraint, they still exhibit significant limitations in ensuring adequate diversity and difficulty of synthesized instructions. To address these challenges, we propose Self-Foveate, an innovative LLM-driven method for instruction synthesis. This approach introduces a "Micro-Scatter-Macro" multi-level foveation methodology that effectively guides the LLM to deeply excavate fine-grained information embedded in unsupervised text, thereby enhancing both the diversity and difficulty of synthesized instructions. Comprehensive experiments across multiple unsupervised corpora and diverse model architectures validate the effectiveness and superiority of our proposed method. We publicly release our data and codes: https://github.com/Mubuky/Self-Foveate<br>
<br>
<div id='section'>Paperid: <span id='pid'>1132, <a href='https://arxiv.org/pdf/2507.23370.pdf' target='_blank'>https://arxiv.org/pdf/2507.23370.pdf</a></span>   <span><a href='https://github.com/bytedance/trae-agent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Trae Research Team, Pengfei Gao, Zhao Tian, Xiangxin Meng, Xinchen Wang, Ruida Hu, Yuanan Xiao, Yizhou Liu, Zhao Zhang, Junjie Chen, Cuiyun Gao, Yun Lin, Yingfei Xiong, Chao Peng, Xia Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23370">Trae Agent: An LLM-based Agent for Software Engineering with Test-time Scaling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Software issue resolution is a critical challenge in software engineering and has garnered increasing attention in recent years. With the rapid advancement of large language models (LLMs), substantial progress has been made in addressing real-world software engineering tasks. Recent studies have introduced ensemble reasoning techniques to enhance the performance of LLM-based issue resolution. However, existing prompting-based methods still face limitations in effectively exploring large ensemble spaces and lack the capacity for repository-level understanding, both of which constrain their overall effectiveness. In this paper, we propose Trae Agent, the first agent-based ensemble reasoning approach for repository-level issue resolution. Trae Agent formulates our goal as an optimal solution search problem and addresses two key challenges, i.e., large ensemble spaces and repository-level understanding, through modular agents for generation, pruning, and selection. We conduct extensive experiments using three leading LLMs on the widely-adopted SWE-bench benchmark, comparing Trae Agent against four state-of-the-art ensemble reasoning techniques. Experimental results demonstrate that Trae Agent consistently achieves superior performance, with an average improvement of 10.22% over all baselines in terms of Pass@1. Trae Agent has achieved first place on the SWE-bench Verified leaderboard, with a notable Pass@1 score of 75.20%. We are pleased to release Trae Agent as an open-source project to support the research community, with all resources available at https://github.com/bytedance/trae-agent.<br>
<span id='abs_ch'>中文: 本文提出了首个基于智能体的集成推理方法Trae Agent，通过生成、剪枝和选择模块化智能体解决大型集成空间探索和仓库级理解两大挑战，在SWE-bench基准测试中以75.20%的Pass@1得分实现最优性能，较基线方法平均提升10.22%。</span><br>
<span id='abs_en'>English: This paper introduces Trae Agent, the first agent-based ensemble reasoning approach that addresses software issue resolution by overcoming limitations in exploring large ensemble spaces and achieving repository-level understanding through modular agents, demonstrating superior performance with a 10.22% average improvement and a leading 75.20% Pass@1 score on the SWE-bench benchmark.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1133, <a href='https://arxiv.org/pdf/2507.23315.pdf' target='_blank'>https://arxiv.org/pdf/2507.23315.pdf</a></span>   <span><a href='https://github.com/VineetKumarRakesh/lcnn-opt' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/VineetKumarRakesh/lcnn-opt' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vineet Kumar Rakesh, Soumya Mazumdar, Tapas Samanta, Sarbajit Pal, Amitabha Das
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23315">Impact of Hyperparameter Optimization on the Accuracy of Lightweight Deep Learning Models for Real-Time Image Classification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Lightweight convolutional and transformer-based models have become vital for real-time image classification in resource-constrained applications, such as embedded systems and edge devices. This work analyzes the influence of hyperparameter adjustment on the accuracy and convergence behavior of seven efficient deep learning architectures: EfficientNetV2-S, ConvNeXt-T, MobileViT v2 (XXS/XS/S), MobileNetV3-L, TinyViT-21M, and RepVGG-A2. All models are trained on the ImageNet-1K dataset under consistent training settings, with an emphasis on real-time practicality. An comprehensive ablation study is undertaken to separate the effect of critical hyperparameters, including learning rate schedules, batch sizes, input resolution, data augmentation, regularization approaches, and optimizer choice. To assess appropriateness for real-time applications, each model is assessed not only in terms of Top-1 and Top-5 classification accuracy, but also in terms of inference time, parameter count, model size, and frames-per-second (FPS) on a GPU-accelerated edge deployment simulation. Results demonstrate that cosine learning rate decay and adjustable batch size may greatly boost both accuracy and convergence speed, while keeping low latency and memory cost. Notably, RepVGG-A2 achieves over 80% Top-1 accuracy with efficient inference performance, offering a compelling balance between accuracy and deployment cost for VGG-style models. The results give practical guidance for constructing resource-efficient deep learning models appropriate for real-time image processing pipelines. All code and training logs are publicly accessible at https://github.com/VineetKumarRakesh/lcnn-opt.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1134, <a href='https://arxiv.org/pdf/2507.23257.pdf' target='_blank'>https://arxiv.org/pdf/2507.23257.pdf</a></span>   <span><a href='https://github.com/Lolo1222/IAU' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiawei Liu, Chenwang Wu, Defu Lian, Enhong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23257">Efficient Machine Unlearning via Influence Approximation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Due to growing privacy concerns, machine unlearning, which aims at enabling machine learning models to ``forget" specific training data, has received increasing attention. Among existing methods, influence-based unlearning has emerged as a prominent approach due to its ability to estimate the impact of individual training samples on model parameters without retraining. However, this approach suffers from prohibitive computational overhead arising from the necessity to compute the Hessian matrix and its inverse across all training samples and parameters, rendering it impractical for large-scale models and scenarios involving frequent data deletion requests. This highlights the difficulty of forgetting. Inspired by cognitive science, which suggests that memorizing is easier than forgetting, this paper establishes a theoretical link between memorizing (incremental learning) and forgetting (unlearning). This connection allows machine unlearning to be addressed from the perspective of incremental learning. Unlike the time-consuming Hessian computations in unlearning (forgetting), incremental learning (memorizing) typically relies on more efficient gradient optimization, which supports the aforementioned cognitive theory. Based on this connection, we introduce the Influence Approximation Unlearning (IAU) algorithm for efficient machine unlearning from the incremental perspective. Extensive empirical evaluations demonstrate that IAU achieves a superior balance among removal guarantee, unlearning efficiency, and comparable model utility, while outperforming state-of-the-art methods across diverse datasets and model architectures. Our code is available at https://github.com/Lolo1222/IAU.<br>
<span id='abs_ch'>Chinese: 本文提出了影响近似遗忘（IAU）算法，通过建立增量学 与遗忘之间的理论联系，在保持模型性能的同时高效移除特定训练数据，克服了 统基于影响的遗忘方法存在的计算瓶颈。</span><br>
<span id='abs_en'>English: This paper introduces the Influence Approximation Unlearning (IAU) algorithm, which leverages the connection between incremental learning and unlearning to efficiently remove specific training data from machine learning models while maintaining performance, overcoming the computational challenges of traditional influence-based methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1135, <a href='https://arxiv.org/pdf/2507.23121.pdf' target='_blank'>https://arxiv.org/pdf/2507.23121.pdf</a></span>   <span><a href='https://github.com/ictup/LLM-Chinese-Textual-Disambiguation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinwei Wu, Haojie Li, Hongyu Liu, Xinyu Ji, Ruohan Li, Yule Chen, Yigeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23121">Uncovering the Fragility of Trustworthy LLMs through Chinese Textual Ambiguity</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this work, we study a critical research problem regarding the trustworthiness of large language models (LLMs): how LLMs behave when encountering ambiguous narrative text, with a particular focus on Chinese textual ambiguity. We created a benchmark dataset by collecting and generating ambiguous sentences with context and their corresponding disambiguated pairs, representing multiple possible interpretations. These annotated examples are systematically categorized into 3 main categories and 9 subcategories. Through experiments, we discovered significant fragility in LLMs when handling ambiguity, revealing behavior that differs substantially from humans. Specifically, LLMs cannot reliably distinguish ambiguous text from unambiguous text, show overconfidence in interpreting ambiguous text as having a single meaning rather than multiple meanings, and exhibit overthinking when attempting to understand the various possible meanings. Our findings highlight a fundamental limitation in current LLMs that has significant implications for their deployment in real-world applications where linguistic ambiguity is common, calling for improved approaches to handle uncertainty in language understanding. The dataset and code are publicly available at this GitHub repository: https://github.com/ictup/LLM-Chinese-Textual-Disambiguation.<br>
<span id='abs_ch'>中文摘要：该 究发现大型语言模型在处理中文文本歧义时表现出显著脆弱性，难以区分歧义、过度自信于单一解释且对多种含义过度思考，揭示了其在现实应用中的关键局限性。</span><br>
<span id='abs_en'>English Summary: This research reveals that large language models exhibit significant fragility when processing ambiguous Chinese text, struggling with distinguishing ambiguity, showing overconfidence in single interpretations, and overthinking multiple meanings, highlighting a critical limitation for real-world applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1136, <a href='https://arxiv.org/pdf/2507.22958.pdf' target='_blank'>https://arxiv.org/pdf/2507.22958.pdf</a></span>   <span><a href='https://github.com/Karifannaa/Auto-check-EGE-math' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruslan Khrulev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22958">CHECK-MAT: Checking Hand-Written Mathematical Answers for the Russian Unified State Exam</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper introduces a novel benchmark, EGE-Math Solutions Assessment Benchmark, for evaluating Vision-Language Models (VLMs) on their ability to assess hand-written mathematical solutions. Unlike existing benchmarks that focus on problem solving, our approach centres on understanding student solutions, identifying mistakes, and assigning grades according to fixed criteria. We compile 122 scanned solutions from the Russian Unified State Exam (EGE) together with official expert grades, and evaluate seven modern VLMs from Google, OpenAI, Arcee AI, and Alibaba Cloud in three inference modes. The results reveal current limitations in mathematical reasoning and human-rubric alignment, opening new research avenues in AI-assisted assessment. You can find code in https://github.com/Karifannaa/Auto-check-EGE-math<br>
<span id='abs_ch'>中文: 本文提出EGE-Math解题评估基准，这一新型评估工具专注于对手写数学解题过程进行评分而非解题本身，通过测试七个先进视觉语言模型揭示了当前在数学推理能力方面的局限。</span><br>
<span id='abs_en'>English: This paper presents the EGE-Math Solutions Assessment Benchmark, a novel evaluation tool for Vision-Language Models that focuses on grading handwritten math solutions rather than solving problems, revealing current limitations in mathematical reasoning through testing seven modern VLMs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1137, <a href='https://arxiv.org/pdf/2507.22946.pdf' target='_blank'>https://arxiv.org/pdf/2507.22946.pdf</a></span>   <span><a href='https://github.com/EthanYixuanMi/Smartcourse-Contextual-Advising' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixuan Mi, Yiduo Yu, Yiyi Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22946">SmartCourse: A Contextual AI-Powered Course Advising System for Undergraduates</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present SmartCourse, an integrated course management and AI-driven advising system for undergraduate students (specifically tailored to the Computer Science (CPS) major). SmartCourse addresses the limitations of traditional advising tools by integrating transcript and plan information for student-specific context. The system combines a command-line interface (CLI) and a Gradio web GUI for instructors and students, manages user accounts, course enrollment, grading, and four-year degree plans, and integrates a locally hosted large language model (via Ollama) for personalized course recommendations. It leverages transcript and major plan to offer contextual advice (e.g., prioritizing requirements or retakes). We evaluated the system on 25 representative advising queries and introduced custom metrics: PlanScore, PersonalScore, Lift, and Recall to assess recommendation quality across different context conditions. Experiments show that using full context yields substantially more relevant recommendations than context-omitted modes, confirming the necessity of transcript and plan information for personalized academic advising. SmartCourse thus demonstrates how transcript-aware AI can enhance academic planning.<br>
<span id='abs_ch'>中文: SmartCourse是一个集成课程管理与AI驱动的学业指导系统，通过结合学生成绩单和专业计划提供个性化课程推荐，评估表明完整上下文信息比 上下文模式能显著提升推荐相关性。</span><br>
<span id='abs_en'>English: SmartCourse is an AI-powered academic advising system that integrates student transcripts and degree plans to deliver personalized course recommendations, with evaluations confirming that full contextual data significantly improves recommendation relevance over context-free approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1138, <a href='https://arxiv.org/pdf/2507.22929.pdf' target='_blank'>https://arxiv.org/pdf/2507.22929.pdf</a></span>   <span><a href='https://github.com/ppxy1/EH-Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Pan, Yang Bai, Ke Zou, Yang Zhou, Jun Zhou, Huazhu Fu, Yih-Chung Tham, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22929">EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Medical Large Language Models (MLLMs) play a crucial role in ophthalmic diagnosis, holding significant potential to address vision-threatening diseases. However, their accuracy is constrained by hallucinations stemming from limited ophthalmic knowledge, insufficient visual localization and reasoning capabilities, and a scarcity of multimodal ophthalmic data, which collectively impede precise lesion detection and disease diagnosis. Furthermore, existing medical benchmarks fail to effectively evaluate various types of hallucinations or provide actionable solutions to mitigate them. To address the above challenges, we introduce EH-Benchmark, a novel ophthalmology benchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs' hallucinations based on specific tasks and error types into two primary classes: Visual Understanding and Logical Composition, each comprising multiple subclasses. Given that MLLMs predominantly rely on language-based reasoning rather than visual processing, we propose an agent-centric, three-phase framework, including the Knowledge-Level Retrieval stage, the Task-Level Case Studies stage, and the Result-Level Validation stage. Experimental results show that our multi-agent framework significantly mitigates both types of hallucinations, enhancing accuracy, interpretability, and reliability. Our project is available at https://github.com/ppxy1/EH-Benchmark.<br>
<span id='abs_ch'>中文摘要：医疗大语言模型在眼科诊断中存在 知识不足和多模态数据缺乏导致的幻觉问题，EH-Benchmark通过将幻觉分类为视觉理解与逻辑组合两大类型，并采用包含知识检索与结果验证的三阶段多智能体框架，显著提升了诊断准确性与可 性。</span><br>
<span id='abs_en'>English Summary: Medical Large Language Models (MLLMs) face accuracy limitations in ophthalmology due to hallucinations from insufficient knowledge and multimodal data, which the proposed EH-Benchmark and multi-agent framework effectively mitigate by categorizing and addressing these errors through knowledge retrieval and validation stages.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1139, <a href='https://arxiv.org/pdf/2507.22929.pdf' target='_blank'>https://arxiv.org/pdf/2507.22929.pdf</a></span>   <span><a href='https://github.com/ppxy1/EH-Benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Pan, Yang Bai, Ke Zou, Yang Zhou, Jun Zhou, Huazhu Fu, Yih-Chung Tham, Yong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22929">EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Medical Large Language Models (MLLMs) play a crucial role in ophthalmic diagnosis, holding significant potential to address vision-threatening diseases. However, their accuracy is constrained by hallucinations stemming from limited ophthalmic knowledge, insufficient visual localization and reasoning capabilities, and a scarcity of multimodal ophthalmic data, which collectively impede precise lesion detection and disease diagnosis. Furthermore, existing medical benchmarks fail to effectively evaluate various types of hallucinations or provide actionable solutions to mitigate them. To address the above challenges, we introduce EH-Benchmark, a novel ophthalmology benchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs' hallucinations based on specific tasks and error types into two primary classes: Visual Understanding and Logical Composition, each comprising multiple subclasses. Given that MLLMs predominantly rely on language-based reasoning rather than visual processing, we propose an agent-centric, three-phase framework, including the Knowledge-Level Retrieval stage, the Task-Level Case Studies stage, and the Result-Level Validation stage. Experimental results show that our multi-agent framework significantly mitigates both types of hallucinations, enhancing accuracy, interpretability, and reliability. Our project is available at https://github.com/ppxy1/EH-Benchmark.<br>
<span id='abs_ch'>中文摘要：医疗大语言模型在眼科诊断中存在 知识不足和多模态数据缺乏导致的幻觉问题，EH-Benchmark通过将幻觉分类为视觉理解与逻辑组合两大类型，并采用包含知识检索与结果验证的三阶段多智能体框架，显著提升了诊断准确性与可 性。</span><br>
<span id='abs_en'>English Summary: Medical Large Language Models (MLLMs) face accuracy limitations in ophthalmology due to hallucinations from insufficient knowledge and multimodal data, which the proposed EH-Benchmark and multi-agent framework effectively mitigate by categorizing and addressing these errors through knowledge retrieval and validation stages.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1140, <a href='https://arxiv.org/pdf/2507.22920.pdf' target='_blank'>https://arxiv.org/pdf/2507.22920.pdf</a></span>   <span><a href='https://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jindong Li, Yali Fu, Jiahong Liu, Linxiao Cao, Wei Ji, Menglin Yang, Irwin King, Ming-Hsuan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22920">Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of large language models (LLMs) has intensified the need for effective mechanisms to transform continuous multimodal data into discrete representations suitable for language-based processing. Discrete tokenization, with vector quantization (VQ) as a central approach, offers both computational efficiency and compatibility with LLM architectures. Despite its growing importance, there is a lack of a comprehensive survey that systematically examines VQ techniques in the context of LLM-based systems. This work fills this gap by presenting the first structured taxonomy and analysis of discrete tokenization methods designed for LLMs. We categorize 8 representative VQ variants that span classical and modern paradigms and analyze their algorithmic principles, training dynamics, and integration challenges with LLM pipelines. Beyond algorithm-level investigation, we discuss existing research in terms of classical applications without LLMs, LLM-based single-modality systems, and LLM-based multimodal systems, highlighting how quantization strategies influence alignment, reasoning, and generation performance. In addition, we identify key challenges including codebook collapse, unstable gradient estimation, and modality-specific encoding constraints. Finally, we discuss emerging research directions such as dynamic and task-adaptive quantization, unified tokenization frameworks, and biologically inspired codebook learning. This survey bridges the gap between traditional vector quantization and modern LLM applications, serving as a foundational reference for the development of efficient and generalizable multimodal systems. A continuously updated version is available at: https://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey.<br>
<span id='abs_ch'>中文: 本综述首次系统分析了面向大语言模型的离散 记化向量量化技术，对方法进行分类并探讨融合挑战，同时指明了未来 究方向。</span><br>
<span id='abs_en'>English: This survey provides the first comprehensive analysis of vector quantization techniques for discrete tokenization in large language models, categorizing methods and addressing integration challenges while outlining future research directions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1141, <a href='https://arxiv.org/pdf/2507.22917.pdf' target='_blank'>https://arxiv.org/pdf/2507.22917.pdf</a></span>   <span><a href='https://github.com/kwunhang/TA-RAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kwun Hang Lau, Ruiyuan Zhang, Weijie Shi, Xiaofang Zhou, Xiaojun Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22917">Reading Between the Timelines: RAG for Answering Diachronic Questions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While Retrieval-Augmented Generation (RAG) excels at injecting static, factual knowledge into Large Language Models (LLMs), it exhibits a critical deficit in handling longitudinal queries that require tracking entities and phenomena across time. This blind spot arises because conventional, semantically-driven retrieval methods are not equipped to gather evidence that is both topically relevant and temporally coherent for a specified duration. We address this challenge by proposing a new framework that fundamentally redesigns the RAG pipeline to infuse temporal logic. Our methodology begins by disentangling a user's query into its core subject and its temporal window. It then employs a specialized retriever that calibrates semantic matching against temporal relevance, ensuring the collection of a contiguous evidence set that spans the entire queried period. To enable rigorous evaluation of this capability, we also introduce the Analytical Diachronic Question Answering Benchmark (ADQAB), a challenging evaluation suite grounded in a hybrid corpus of real and synthetic financial news. Empirical results on ADQAB show that our approach yields substantial gains in answer accuracy, surpassing standard RAG implementations by 13% to 27%. This work provides a validated pathway toward RAG systems capable of performing the nuanced, evolutionary analysis required for complex, real-world questions. The dataset and code for this study are publicly available at https://github.com/kwunhang/TA-RAG.<br>
<span id='abs_ch'>Chinese Summary: 本文提出了一种新颖框架，通过引入时间逻辑增强检索增强生成（RAG）系统处理纵向查询的能力，在基准测试中相比 准RAG实现准确率提升13%至27%。</span><br>
<span id='abs_en'>English Summary: This paper introduces a novel framework that enhances Retrieval-Augmented Generation (RAG) by incorporating temporal logic to effectively address longitudinal queries, achieving significant accuracy improvements of 13% to 27% over standard RAG systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1142, <a href='https://arxiv.org/pdf/2507.22853.pdf' target='_blank'>https://arxiv.org/pdf/2507.22853.pdf</a></span>   <span><a href='https://github.com/Tomsawyerhu/APR-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haichuan Hu, Xiaochen Xie, Quanjun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22853">Repair-R1: Better Test Before Repair</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>APR (Automated Program Repair) aims to automatically locate program defects, generate patches and validate the repairs. Existing techniques for APR are often combined with LLMs (Large Language Models), which leverages the code-related knowledge of LLMs to improve repair effectiveness. Current LLM-based APR methods typically utilize test cases only during the inference stage, adopting an iterative approach that performs repair first and validates it through test execution afterward. This conventional paradigm neglects two important aspects: the potential contribution of test cases in the training phase, and the possibility of leveraging testing prior to repair. To address this, we propose Repair-R1, which introduces test cases into the model's training phase and shifts test generation to precede repair. The model is required to first generate discriminative test cases that can distinguish defective behaviors, and then perform repair based on these tests. This enables the model to better locate defects and understand the underlying causes of defects, thereby improving repair effectiveness. We implement Repair-R1 with three different backbone models, using RL (reinforcement learning) to co-optimize test generation and bug repair. Experimental results on four widely adopted benchmarks demonstrate the superiority of Repair-R1. Specially, compared to vanilla models, Repair-R1 improves repair success rate by 2.68\% to 48.29\%, test generation success rate by 16.38\% to 53.28\%, and test coverage by 0.78\% to 53.96\%. We publish the code and weights at https://github.com/Tomsawyerhu/APR-RL and https://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.<br>
<span id='abs_ch'>中文: 提出的Repair-R1方法通过在模型训练阶段引入测试用例并优先进行测试生成，显著提升了自动程序修复的成功率和覆盖率，在多个基准测试中表现优异。</span><br>
<span id='abs_en'>English: The proposed Repair-R1 method enhances automated program repair by incorporating test cases during model training and prioritizing test generation before repair, which significantly improves success rates and coverage across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1143, <a href='https://arxiv.org/pdf/2507.22802.pdf' target='_blank'>https://arxiv.org/pdf/2507.22802.pdf</a></span>   <span><a href='https://github.com/donglihe-hub/FetalCLIP-IQA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongli He, Hu Wang, Mohammad Yaqub
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22802">Advancing Fetal Ultrasound Image Quality Assessment in Low-Resource Settings</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate fetal biometric measurements, such as abdominal circumference, play a vital role in prenatal care. However, obtaining high-quality ultrasound images for these measurements heavily depends on the expertise of sonographers, posing a significant challenge in low-income countries due to the scarcity of trained personnel. To address this issue, we leverage FetalCLIP, a vision-language model pretrained on a curated dataset of over 210,000 fetal ultrasound image-caption pairs, to perform automated fetal ultrasound image quality assessment (IQA) on blind-sweep ultrasound data. We introduce FetalCLIP$_{CLS}$, an IQA model adapted from FetalCLIP using Low-Rank Adaptation (LoRA), and evaluate it on the ACOUSLIC-AI dataset against six CNN and Transformer baselines. FetalCLIP$_{CLS}$ achieves the highest F1 score of 0.757. Moreover, we show that an adapted segmentation model, when repurposed for classification, further improves performance, achieving an F1 score of 0.771. Our work demonstrates how parameter-efficient fine-tuning of fetal ultrasound foundation models can enable task-specific adaptations, advancing prenatal care in resource-limited settings. The experimental code is available at: https://github.com/donglihe-hub/FetalCLIP-IQA.<br>
<span id='abs_ch'>中文: 本 究采用FetalCLIP视觉语言模型，通过低秩适应技术实现胎儿超声图像质量自动评估，以0.771的F1分数展现优异性能，为资源有限地区的产前护理提供了有效解决方案。English: The study introduces FetalCLIP, a vision-language model adapted using Low-Rank Adaptation for automated fetal ultrasound image quality assessment, achieving superior performance with an F1 score of 0.771 and demonstrating potential to enhance prenatal care in resource-limited areas.Paperid:63,https://arxiv.org/pdf/2507.22733.pdfGitHub</span><br>
<span id='abs_en'>English: The study introduces FetalCLIP, a vision-language model adapted using Low-Rank Adaptation for automated fetal ultrasound image quality assessment, achieving superior performance with an F1 score of 0.771 and demonstrating potential to enhance prenatal care in resource-limited areas.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1144, <a href='https://arxiv.org/pdf/2507.22576.pdf' target='_blank'>https://arxiv.org/pdf/2507.22576.pdf</a></span>   <span><a href='https://github.com/glhr/COOkeD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Galadrielle Humblot-Renaux, Gianni Franchi, Sergio Escalera, Thomas B. Moeslund
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22576">COOkeD: Ensemble-based OOD detection in the era of zero-shot CLIP</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Out-of-distribution (OOD) detection is an important building block in trustworthy image recognition systems as unknown classes may arise at test-time. OOD detection methods typically revolve around a single classifier, leading to a split in the research field between the classical supervised setting (e.g. ResNet18 classifier trained on CIFAR100) vs. the zero-shot setting (class names fed as prompts to CLIP). In both cases, an overarching challenge is that the OOD detection performance is implicitly constrained by the classifier's capabilities on in-distribution (ID) data. In this work, we show that given a little open-mindedness from both ends, remarkable OOD detection can be achieved by instead creating a heterogeneous ensemble - COOkeD combines the predictions of a closed-world classifier trained end-to-end on a specific dataset, a zero-shot CLIP classifier, and a linear probe classifier trained on CLIP image features. While bulky at first sight, this approach is modular, post-hoc and leverages the availability of pre-trained VLMs, thus introduces little overhead compared to training a single standard classifier. We evaluate COOkeD on popular CIFAR100 and ImageNet benchmarks, but also consider more challenging, realistic settings ranging from training-time label noise, to test-time covariate shift, to zero-shot shift which has been previously overlooked. Despite its simplicity, COOkeD achieves state-of-the-art performance and greater robustness compared to both classical and CLIP-based OOD detection methods. Code is available at https://github.com/glhr/COOkeD<br>
<span id='abs_ch'>中文: COOkeD提出了一种异构集成方法，融合了闭域分类器、零 本CLIP分类器和线性探针分类器，在多种挑战性场景下实现了最先进的分布外检测性能并展现出更强的鲁棒性。</span><br>
<span id='abs_en'>English: COOkeD introduces a heterogeneous ensemble method combining closed-world, zero-shot CLIP, and linear probe classifiers to achieve state-of-the-art OOD detection performance with enhanced robustness across diverse challenging scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1145, <a href='https://arxiv.org/pdf/2507.22530.pdf' target='_blank'>https://arxiv.org/pdf/2507.22530.pdf</a></span>   <span><a href='https://github.com/scott-yjyang/HRVVS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xincheng Yao, Yijun Yang, Kangwei Guo, Ruiqiang Xiao, Haipeng Zhou, Haisu Tao, Jian Yang, Lei Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22530">HRVVS: A High-resolution Video Vasculature Segmentation Network via Hierarchical Autoregressive Residual Priors</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The segmentation of the hepatic vasculature in surgical videos holds substantial clinical significance in the context of hepatectomy procedures. However, owing to the dearth of an appropriate dataset and the inherently complex task characteristics, few researches have been reported in this domain. To address this issue, we first introduce a high quality frame-by-frame annotated hepatic vasculature dataset containing 35 long hepatectomy videos and 11442 high-resolution frames. On this basis, we propose a novel high-resolution video vasculature segmentation network, dubbed as HRVVS. We innovatively embed a pretrained visual autoregressive modeling (VAR) model into different layers of the hierarchical encoder as prior information to reduce the information degradation generated during the downsampling process. In addition, we designed a dynamic memory decoder on a multi-view segmentation network to minimize the transmission of redundant information while preserving more details between frames. Extensive experiments on surgical video datasets demonstrate that our proposed HRVVS significantly outperforms the state-of-the-art methods. The source code and dataset will be publicly available at \{https://github.com/scott-yjyang/HRVVS}.<br>
<span id='abs_ch'>中文: 本 究提出了一种高分辨率肝血管分割网络（HRVVS）并发布了新 注数据集，通过嵌入预训练VAR模型和动态记忆解 器，在手术视频分析中显著优于现有方法。</span><br>
<span id='abs_en'>English: This study introduces a high-resolution hepatic vasculature segmentation network (HRVVS) and a new annotated dataset, which significantly outperforms existing methods by integrating a pretrained VAR model and a dynamic memory decoder to enhance surgical video analysis.Paperid:77,https://arxiv.org/pdf/2507.22522.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1146, <a href='https://arxiv.org/pdf/2507.22522.pdf' target='_blank'>https://arxiv.org/pdf/2507.22522.pdf</a></span>   <span><a href='https://github.com/wangzy01/ACTIVE-Action-from-Robotic-View' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Wang, Peiming Li, Hong Liu, Zhichao Deng, Can Wang, Jun Liu, Junsong Yuan, Mengyuan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22522">Recognizing Actions from Robotic View for Natural Human-Robot Interaction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Natural Human-Robot Interaction (N-HRI) requires robots to recognize human actions at varying distances and states, regardless of whether the robot itself is in motion or stationary. This setup is more flexible and practical than conventional human action recognition tasks. However, existing benchmarks designed for traditional action recognition fail to address the unique complexities in N-HRI due to limited data, modalities, task categories, and diversity of subjects and environments. To address these challenges, we introduce ACTIVE (Action from Robotic View), a large-scale dataset tailored specifically for perception-centric robotic views prevalent in mobile service robots. ACTIVE comprises 30 composite action categories, 80 participants, and 46,868 annotated video instances, covering both RGB and point cloud modalities. Participants performed various human actions in diverse environments at distances ranging from 3m to 50m, while the camera platform was also mobile, simulating real-world scenarios of robot perception with varying camera heights due to uneven ground. This comprehensive and challenging benchmark aims to advance action and attribute recognition research in N-HRI. Furthermore, we propose ACTIVE-PC, a method that accurately perceives human actions at long distances using Multilevel Neighborhood Sampling, Layered Recognizers, Elastic Ellipse Query, and precise decoupling of kinematic interference from human actions. Experimental results demonstrate the effectiveness of ACTIVE-PC. Our code is available at: https://github.com/wangzy01/ACTIVE-Action-from-Robotic-View.<br>
<span id='abs_ch'>中文摘要：ACTIVE数据集针对自然人机交互中现有基准的不足，通过从移动平台采集包含RGB和点云模态的全面视频数据，覆盖不同距离和环境，旨在推动该领域的 究发展。</span><br>
<span id='abs_en'>English Summary: The ACTIVE dataset is introduced to address the limitations of existing benchmarks in Natural Human-Robot Interaction by providing comprehensive video data with RGB and point cloud modalities, collected from mobile platforms across varying distances and environments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1147, <a href='https://arxiv.org/pdf/2507.22477.pdf' target='_blank'>https://arxiv.org/pdf/2507.22477.pdf</a></span>   <span><a href='https://github.com/Karl1109/LIDAR-Mamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hui Liu, Chen Jia, Fan Shi, Xu Cheng, Mengfei Shi, Xia Xie, Shengyong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22477">LIDAR: Lightweight Adaptive Cue-Aware Fusion Vision Mamba for Multimodal Segmentation of Structural Cracks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Achieving pixel-level segmentation with low computational cost using multimodal data remains a key challenge in crack segmentation tasks. Existing methods lack the capability for adaptive perception and efficient interactive fusion of cross-modal features. To address these challenges, we propose a Lightweight Adaptive Cue-Aware Vision Mamba network (LIDAR), which efficiently perceives and integrates morphological and textural cues from different modalities under multimodal crack scenarios, generating clear pixel-level crack segmentation maps. Specifically, LIDAR is composed of a Lightweight Adaptive Cue-Aware Visual State Space module (LacaVSS) and a Lightweight Dual Domain Dynamic Collaborative Fusion module (LD3CF). LacaVSS adaptively models crack cues through the proposed mask-guided Efficient Dynamic Guided Scanning Strategy (EDG-SS), while LD3CF leverages an Adaptive Frequency Domain Perceptron (AFDP) and a dual-pooling fusion strategy to effectively capture spatial and frequency-domain cues across modalities. Moreover, we design a Lightweight Dynamically Modulated Multi-Kernel convolution (LDMK) to perceive complex morphological structures with minimal computational overhead, replacing most convolutional operations in LIDAR. Experiments on three datasets demonstrate that our method outperforms other state-of-the-art (SOTA) methods. On the light-field depth dataset, our method achieves 0.8204 in F1 and 0.8465 in mIoU with only 5.35M parameters. Code and datasets are available at https://github.com/Karl1109/LIDAR-Mamba.<br>
<span id='abs_ch'>中文: 提出的轻量自适应线索感知视觉Mamba网络（LIDAR）通过自适应感知与融合模块有效整合多模态裂缝特征，以较低计算成本实现了优于现有方法的像 级分割效果。</span><br>
<span id='abs_en'>English: The proposed Lightweight Adaptive Cue-Aware Vision Mamba network (LIDAR) effectively integrates multimodal crack features through adaptive perception and fusion modules, achieving superior pixel-level segmentation with minimal computational cost compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1148, <a href='https://arxiv.org/pdf/2507.22465.pdf' target='_blank'>https://arxiv.org/pdf/2507.22465.pdf</a></span>   <span><a href='https://github.com/ZhengxyFlow/HMHI-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Xiangyu, He Songcheng, Li Wanyun, Li Xiaoqiang, Zhang Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22465">Shallow Features Matter: Hierarchical Memory with Heterogeneous Interaction for Unsupervised Video Object Segmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Unsupervised Video Object Segmentation (UVOS) aims to predict pixel-level masks for the most salient objects in videos without any prior annotations. While memory mechanisms have been proven critical in various video segmentation paradigms, their application in UVOS yield only marginal performance gains despite sophisticated design. Our analysis reveals a simple but fundamental flaw in existing methods: over-reliance on memorizing high-level semantic features. UVOS inherently suffers from the deficiency of lacking fine-grained information due to the absence of pixel-level prior knowledge. Consequently, memory design relying solely on high-level features, which predominantly capture abstract semantic cues, is insufficient to generate precise predictions. To resolve this fundamental issue, we propose a novel hierarchical memory architecture to incorporate both shallow- and high-level features for memory, which leverages the complementary benefits of pixel and semantic information. Furthermore, to balance the simultaneous utilization of the pixel and semantic memory features, we propose a heterogeneous interaction mechanism to perform pixel-semantic mutual interactions, which explicitly considers their inherent feature discrepancies. Through the design of Pixel-guided Local Alignment Module (PLAM) and Semantic-guided Global Integration Module (SGIM), we achieve delicate integration of the fine-grained details in shallow-level memory and the semantic representations in high-level memory. Our Hierarchical Memory with Heterogeneous Interaction Network (HMHI-Net) consistently achieves state-of-the-art performance across all UVOS and video saliency detection benchmarks. Moreover, HMHI-Net consistently exhibits high performance across different backbones, further demonstrating its superiority and robustness. Project page: https://github.com/ZhengxyFlow/HMHI-Net .<br>
<br>
<div id='section'>Paperid: <span id='pid'>1149, <a href='https://arxiv.org/pdf/2507.22418.pdf' target='_blank'>https://arxiv.org/pdf/2507.22418.pdf</a></span>   <span><a href='https://github.com/huynhspm/Data-Uncertainty' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Phi Van Nguyen, Ngoc Huynh Trinh, Duy Minh Lam Nguyen, Phu Loc Nguyen, Quoc Long Tran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22418">Aleatoric Uncertainty Medical Image Segmentation Estimation via Flow Matching</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Quantifying aleatoric uncertainty in medical image segmentation is critical since it is a reflection of the natural variability observed among expert annotators. A conventional approach is to model the segmentation distribution using the generative model, but current methods limit the expression ability of generative models. While current diffusion-based approaches have demonstrated impressive performance in approximating the data distribution, their inherent stochastic sampling process and inability to model exact densities limit their effectiveness in accurately capturing uncertainty. In contrast, our proposed method leverages conditional flow matching, a simulation-free flow-based generative model that learns an exact density, to produce highly accurate segmentation results. By guiding the flow model on the input image and sampling multiple data points, our approach synthesizes segmentation samples whose pixel-wise variance reliably reflects the underlying data distribution. This sampling strategy captures uncertainties in regions with ambiguous boundaries, offering robust quantification that mirrors inter-annotator differences. Experimental results demonstrate that our method not only achieves competitive segmentation accuracy but also generates uncertainty maps that provide deeper insights into the reliability of the segmentation outcomes. The code for this paper is freely available at https://github.com/huynhspm/Data-Uncertainty<br>
<span id='abs_ch'>中文摘要：本 究提出一种基于条件流匹配的方法，通过生成反  注者间差异的多个分割 本来精确量化医学图像分割中的偶然不确定性，既实现了有竞争力的分割精度，又生成了具有深刻见解的不确定性图谱。</span><br>
<span id='abs_en'>English Summary: This study introduces a method using conditional flow matching to accurately quantify aleatoric uncertainty in medical image segmentation by generating multiple segmentation samples that reflect inter-annotator variability, achieving both competitive accuracy and insightful uncertainty maps.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1150, <a href='https://arxiv.org/pdf/2507.22418.pdf' target='_blank'>https://arxiv.org/pdf/2507.22418.pdf</a></span>   <span><a href='https://github.com/huynhspm/Data-Uncertainty' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Phi Van Nguyen, Ngoc Huynh Trinh, Duy Minh Lam Nguyen, Phu Loc Nguyen, Quoc Long Tran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22418">Aleatoric Uncertainty Medical Image Segmentation Estimation via Flow Matching</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Quantifying aleatoric uncertainty in medical image segmentation is critical since it is a reflection of the natural variability observed among expert annotators. A conventional approach is to model the segmentation distribution using the generative model, but current methods limit the expression ability of generative models. While current diffusion-based approaches have demonstrated impressive performance in approximating the data distribution, their inherent stochastic sampling process and inability to model exact densities limit their effectiveness in accurately capturing uncertainty. In contrast, our proposed method leverages conditional flow matching, a simulation-free flow-based generative model that learns an exact density, to produce highly accurate segmentation results. By guiding the flow model on the input image and sampling multiple data points, our approach synthesizes segmentation samples whose pixel-wise variance reliably reflects the underlying data distribution. This sampling strategy captures uncertainties in regions with ambiguous boundaries, offering robust quantification that mirrors inter-annotator differences. Experimental results demonstrate that our method not only achieves competitive segmentation accuracy but also generates uncertainty maps that provide deeper insights into the reliability of the segmentation outcomes. The code for this paper is freely available at https://github.com/huynhspm/Data-Uncertainty<br>
<span id='abs_ch'>中文摘要：本 究提出一种基于条件流匹配的方法，通过生成反  注者间差异的多个分割 本来精确量化医学图像分割中的偶然不确定性，既实现了有竞争力的分割精度，又生成了具有深刻见解的不确定性图谱。</span><br>
<span id='abs_en'>English Summary: This study introduces a method using conditional flow matching to accurately quantify aleatoric uncertainty in medical image segmentation by generating multiple segmentation samples that reflect inter-annotator variability, achieving both competitive accuracy and insightful uncertainty maps.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1151, <a href='https://arxiv.org/pdf/2507.22418.pdf' target='_blank'>https://arxiv.org/pdf/2507.22418.pdf</a></span>   <span><a href='https://github.com/huynhspm/Data-Uncertainty' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Phi Van Nguyen, Ngoc Huynh Trinh, Duy Minh Lam Nguyen, Phu Loc Nguyen, Quoc Long Tran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22418">Aleatoric Uncertainty Medical Image Segmentation Estimation via Flow Matching</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Quantifying aleatoric uncertainty in medical image segmentation is critical since it is a reflection of the natural variability observed among expert annotators. A conventional approach is to model the segmentation distribution using the generative model, but current methods limit the expression ability of generative models. While current diffusion-based approaches have demonstrated impressive performance in approximating the data distribution, their inherent stochastic sampling process and inability to model exact densities limit their effectiveness in accurately capturing uncertainty. In contrast, our proposed method leverages conditional flow matching, a simulation-free flow-based generative model that learns an exact density, to produce highly accurate segmentation results. By guiding the flow model on the input image and sampling multiple data points, our approach synthesizes segmentation samples whose pixel-wise variance reliably reflects the underlying data distribution. This sampling strategy captures uncertainties in regions with ambiguous boundaries, offering robust quantification that mirrors inter-annotator differences. Experimental results demonstrate that our method not only achieves competitive segmentation accuracy but also generates uncertainty maps that provide deeper insights into the reliability of the segmentation outcomes. The code for this paper is freely available at https://github.com/huynhspm/Data-Uncertainty<br>
<span id='abs_ch'>中文摘要：本 究提出一种基于条件流匹配的方法，通过生成反  注者间差异的多个分割 本来精确量化医学图像分割中的偶然不确定性，既实现了有竞争力的分割精度，又生成了具有深刻见解的不确定性图谱。</span><br>
<span id='abs_en'>English Summary: This study introduces a method using conditional flow matching to accurately quantify aleatoric uncertainty in medical image segmentation by generating multiple segmentation samples that reflect inter-annotator variability, achieving both competitive accuracy and insightful uncertainty maps.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1152, <a href='https://arxiv.org/pdf/2507.22411.pdf' target='_blank'>https://arxiv.org/pdf/2507.22411.pdf</a></span>   <span><a href='https://github.com/hyeonseokk/NeedleChain' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyeonseok Moon, Heuiseok Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22411">NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The Needle-in-a-Haystack (NIAH) benchmark is widely used to evaluate Large Language Models' (LLMs) ability to understand long contexts (LC). It evaluates the capability to identify query-relevant context within extensive query-irrelevant passages. Although this method serves as a widely accepted standard for evaluating long-context understanding, our findings suggest it may overestimate the true LC capability of LLMs. We demonstrate that even state-of-the-art models such as GPT-4o struggle to intactly incorporate given contexts made up of solely query-relevant ten sentences. In response, we introduce a novel benchmark, \textbf{NeedleChain}, where the context consists entirely of query-relevant information, requiring the LLM to fully grasp the input to answer correctly. Our benchmark allows for flexible context length and reasoning order, offering a more comprehensive analysis of LLM performance. Additionally, we propose an extremely simple yet compelling strategy to improve LC understanding capability of LLM: ROPE Contraction. Our experiments with various advanced LLMs reveal a notable disparity between their ability to process large contexts and their capacity to fully understand them. Source code and datasets are available at https://github.com/hyeonseokk/NeedleChain<br>
<span id='abs_ch'>中文: "大海捞针"基准可能高估了大语言模型的长上下文理解能力， 为即使先进模型也难以处理纯相关内容，为此我们提出了NeedleChain基准和ROPE收缩策略以更准确地评估和提升性能。</span><br>
<span id='abs_en'>English: The Needle-in-a-Haystack benchmark may overestimate LLMs' long-context understanding, as even advanced models struggle with purely relevant contexts, prompting the introduction of NeedleChain and ROPE Contraction for more accurate evaluation and improvement.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1153, <a href='https://arxiv.org/pdf/2507.22264.pdf' target='_blank'>https://arxiv.org/pdf/2507.22264.pdf</a></span>   <span><a href='https://github.com/Mid-Push/SmartCLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaoan Xie, Lingjing Kong, Yujia Zheng, Yu Yao, Zeyu Tang, Eric P. Xing, Guangyi Chen, Kun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22264">SmartCLIP: Modular Vision-language Alignment with Identification Guarantees</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Contrastive Language-Image Pre-training (CLIP)~\citep{radford2021learning} has emerged as a pivotal model in computer vision and multimodal learning, achieving state-of-the-art performance at aligning visual and textual representations through contrastive learning. However, CLIP struggles with potential information misalignment in many image-text datasets and suffers from entangled representation. On the one hand, short captions for a single image in datasets like MSCOCO may describe disjoint regions in the image, leaving the model uncertain about which visual features to retain or disregard. On the other hand, directly aligning long captions with images can lead to the retention of entangled details, preventing the model from learning disentangled, atomic concepts -- ultimately limiting its generalization on certain downstream tasks involving short prompts.
  In this paper, we establish theoretical conditions that enable flexible alignment between textual and visual representations across varying levels of granularity. Specifically, our framework ensures that a model can not only \emph{preserve} cross-modal semantic information in its entirety but also \emph{disentangle} visual representations to capture fine-grained textual concepts. Building on this foundation, we introduce \ours, a novel approach that identifies and aligns the most relevant visual and textual representations in a modular manner. Superior performance across various tasks demonstrates its capability to handle information misalignment and supports our identification theory. The code is available at https://github.com/Mid-Push/SmartCLIP.<br>
<span id='abs_ch'>中文: CLIP模型在图文数据中存在信息错位和表征  的问题，而本文提出的SmartCLIP框架通过模块化对齐实现了跨模态语义的灵活匹配与解耦表示，显著提升了泛化能力。</span><br>
<span id='abs_en'>English: CLIP faces challenges with information misalignment and entangled representations in image-text datasets, but our proposed SmartCLIP framework enables flexible cross-modal alignment and disentangled visual concepts to improve generalization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1154, <a href='https://arxiv.org/pdf/2507.22171.pdf' target='_blank'>https://arxiv.org/pdf/2507.22171.pdf</a></span>   <span><a href='https://github.com/CjangCjengh/Generic_Persona' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Zhang, Peilin Zhao, Deheng Ye, Hao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22171">Enhancing Jailbreak Attacks on LLMs via Persona Prompts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Jailbreak attacks aim to exploit large language models (LLMs) by inducing them to generate harmful content, thereby revealing their vulnerabilities. Understanding and addressing these attacks is crucial for advancing the field of LLM safety. Previous jailbreak approaches have mainly focused on direct manipulations of harmful intent, with limited attention to the impact of persona prompts. In this study, we systematically explore the efficacy of persona prompts in compromising LLM defenses. We propose a genetic algorithm-based method that automatically crafts persona prompts to bypass LLM's safety mechanisms. Our experiments reveal that: (1) our evolved persona prompts reduce refusal rates by 50-70% across multiple LLMs, and (2) these prompts demonstrate synergistic effects when combined with existing attack methods, increasing success rates by 10-20%. Our code and data are available at https://github.com/CjangCjengh/Generic_Persona.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1155, <a href='https://arxiv.org/pdf/2507.22086.pdf' target='_blank'>https://arxiv.org/pdf/2507.22086.pdf</a></span>   <span><a href='https://github.com/typybench/typybench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Honghua Dong, Jiacheng Yang, Xun Deng, Yuhe Jiang, Gennady Pekhimenko, Fan Long, Xujie Si
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22086">TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Type inference for dynamic languages like Python is a persistent challenge in software engineering. While large language models (LLMs) have shown promise in code understanding, their type inference capabilities remain underexplored. We introduce TypyBench, a benchmark designed to evaluate LLMs' type inference across entire Python repositories. TypyBench features two novel metrics: TypeSim, which captures nuanced semantic relationships between predicted and ground truth types, and TypeCheck, which assesses type consistency across codebases. Our evaluation of various LLMs on a curated dataset of 50 high-quality Python repositories reveals that, although LLMs achieve decent TypeSim scores, they struggle with complex nested types and exhibit significant type consistency errors. These findings suggest that future research should shift focus from improving type similarity to addressing repository-level consistency. TypyBench provides a foundation for this new direction, offering insights into model performance across different type complexities and usage contexts. Our code and data are available at https://github.com/typybench/typybench.<br>
<span id='abs_ch'>中文摘要：TypyBench是一个评估大语言模型在Python仓库中类型推断能力的新基准，发现模型虽在类型相似度上表现良好，但在处理复杂嵌套类型和保持代 库一致性方面存在显著不足。</span><br>
<span id='abs_en'>English Summary: TypyBench is a new benchmark for evaluating large language models' type inference in Python repositories, revealing their strengths in type similarity but weaknesses in handling complex nested types and maintaining consistency across codebases.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1156, <a href='https://arxiv.org/pdf/2507.22025.pdf' target='_blank'>https://arxiv.org/pdf/2507.22025.pdf</a></span>   <span><a href='https://github.com/KDEGroup/UI-AGILE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuquan Lian, Yuhang Wu, Jia Ma, Yifan Ding, Zihan Song, Bingqi Chen, Xiawu Zheng, Hui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22025">UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The emergence of Multimodal Large Language Models (MLLMs) has driven significant advances in Graphical User Interface (GUI) agent capabilities. Nevertheless, existing GUI agent training and inference techniques still suffer from a dilemma for reasoning designs, ineffective reward, and visual noise. To address these issues, we introduce UI-AGILE for enhancing GUI agents at both training and inference. For training, we propose a suite of improvements to the Supervised Fine-Tuning (SFT) process: 1) a continuous reward function to incentivize high-precision grounding; 2) a ``Simple Thinking'' reward to balance planning with speed and grounding accuracy; and 3) a cropping-based resampling strategy to mitigate the sparse reward problem and improve learning on complex tasks. For inference, we present decomposed grounding with selection to dramatically improve grounding accuracy on high-resolution displays by breaking the image into smaller, manageable parts. Experiments show that UI-AGILE achieves the state-of-the-art grounding performance on two benchmarks ScreenSpot-Pro and ScreenSpot-v2 while it also exhibits strong general agent capabilities. For instance, using both our training and inference enhancement methods brings 23\% grounding accuracy improvement over the best baseline on ScreenSpot-Pro. We provide the code in https://github.com/KDEGroup/UI-AGILE.<br>
<span id='abs_ch'>中文：UI-AGILE通过连续奖励和裁剪策略优化训练，并采用分解式定位改进推理，在基准测试中实现了图形用户界面代理的最先进性能。</span><br>
<span id='abs_en'>English: UI-AGILE introduces training enhancements like continuous rewards and cropping strategies, along with inference improvements through decomposed grounding, achieving state-of-the-art GUI agent performance on benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1157, <a href='https://arxiv.org/pdf/2507.21905.pdf' target='_blank'>https://arxiv.org/pdf/2507.21905.pdf</a></span>   <span><a href='https://github.com/SumSubstance/Deepfake-Detectors-in-the-Wild' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Viacheslav Pirogov, Maksim Artemev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21905">Evaluating Deepfake Detectors in the Wild</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Deepfakes powered by advanced machine learning models present a significant and evolving threat to identity verification and the authenticity of digital media. Although numerous detectors have been developed to address this problem, their effectiveness has yet to be tested when applied to real-world data. In this work we evaluate modern deepfake detectors, introducing a novel testing procedure designed to mimic real-world scenarios for deepfake detection. Using state-of-the-art deepfake generation methods, we create a comprehensive dataset containing more than 500,000 high-quality deepfake images. Our analysis shows that detecting deepfakes still remains a challenging task. The evaluation shows that in fewer than half of the deepfake detectors tested achieved an AUC score greater than 60%, with the lowest being 50%. We demonstrate that basic image manipulations, such as JPEG compression or image enhancement, can significantly reduce model performance. All code and data are publicly available at https://github.com/SumSubstance/Deepfake-Detectors-in-the-Wild.<br>
<span id='abs_ch'>中文: 现代深度伪 检测器在现实场景中表现不佳，仅不到半数检测器的AUC超过60%，且简单的图像处理会大幅降低其检测性能。</span><br>
<span id='abs_en'>English: Modern deepfake detectors struggle in real-world scenarios, with fewer than half achieving over 60% AUC and basic image manipulations significantly degrading their performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1158, <a href='https://arxiv.org/pdf/2507.21875.pdf' target='_blank'>https://arxiv.org/pdf/2507.21875.pdf</a></span>   <span><a href='https://github.com/GkikasStefanos/Tiny-BioMoE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefanos Gkikas, Ioannis Kyprakis, Manolis Tsiknakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21875">Tiny-BioMoE: a Lightweight Embedding Model for Biosignal Analysis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Pain is a complex and pervasive condition that affects a significant portion of the population. Accurate and consistent assessment is essential for individuals suffering from pain, as well as for developing effective management strategies in a healthcare system. Automatic pain assessment systems enable continuous monitoring, support clinical decision-making, and help minimize patient distress while mitigating the risk of functional deterioration. Leveraging physiological signals offers objective and precise insights into a person's state, and their integration in a multimodal framework can further enhance system performance. This study has been submitted to the Second Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN). The proposed approach introduces Tiny-BioMoE, a lightweight pretrained embedding model for biosignal analysis. Trained on 4.4 million biosignal image representations and consisting of only 7.3 million parameters, it serves as an effective tool for extracting high-quality embeddings for downstream tasks. Extensive experiments involving electrodermal activity, blood volume pulse, respiratory signals, peripheral oxygen saturation, and their combinations highlight the model's effectiveness across diverse modalities in automatic pain recognition tasks. The model's architecture (code) and weights are available at https://github.com/GkikasStefanos/Tiny-BioMoE.<br>
<span id='abs_ch'>中文摘要：本 究提出Tiny-BioMoE轻量级预训练模型，通过多模态生物信号实验验证了其在自动疼痛识别任务中的有效性。</span><br>
<span id='abs_en'>English Summary: This study introduces Tiny-BioMoE, a lightweight pretrained model for biosignal analysis that demonstrates strong performance in automatic pain recognition across multiple physiological signals through extensive experiments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1159, <a href='https://arxiv.org/pdf/2507.21873.pdf' target='_blank'>https://arxiv.org/pdf/2507.21873.pdf</a></span>   <span><a href='https://github.com/raffaelepojer/NeSy-for-graph-data' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Raffaele Pojer, Andrea Passerini, Kim G. Larsen, Manfred Jaeger
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21873">A Neuro-Symbolic Approach for Probabilistic Reasoning on Graph Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Graph neural networks (GNNs) excel at predictive tasks on graph-structured data but often lack the ability to incorporate symbolic domain knowledge and perform general reasoning. Relational Bayesian Networks (RBNs), in contrast, enable fully generative probabilistic modeling over graph-like structures and support rich symbolic knowledge and probabilistic inference. This paper presents a neuro-symbolic framework that seamlessly integrates GNNs into RBNs, combining the learning strength of GNNs with the flexible reasoning capabilities of RBNs.
  We develop two implementations of this integration: one compiles GNNs directly into the native RBN language, while the other maintains the GNN as an external component. Both approaches preserve the semantics and computational properties of GNNs while fully aligning with the RBN modeling paradigm. We also propose a maximum a-posteriori (MAP) inference method for these neuro-symbolic models.
  To demonstrate the framework's versatility, we apply it to two distinct problems. First, we transform a GNN for node classification into a collective classification model that explicitly models homo- and heterophilic label patterns, substantially improving accuracy. Second, we introduce a multi-objective network optimization problem in environmental planning, where MAP inference supports complex decision-making. Both applications include new publicly available benchmark datasets.
  This work introduces a powerful and coherent neuro-symbolic approach to graph data, bridging learning and reasoning in ways that enable novel applications and improved performance across diverse tasks.<br>
<span id='abs_ch'>Chinese: 本文提出了一种神经符号框架，将图神经网络（GNN）集成到关系贝叶斯网络（RBN）中，结合GNN的学 能力与RBN的概率推理优势，在多种图结构任务中实现了创新应用和性能提升。</span><br>
<span id='abs_en'>English: This paper introduces a neuro-symbolic framework that integrates graph neural networks (GNNs) into relational Bayesian networks (RBNs), combining GNNs' learning capabilities with RBNs' probabilistic reasoning to enable novel applications and improved performance across diverse graph-based tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1160, <a href='https://arxiv.org/pdf/2507.21848.pdf' target='_blank'>https://arxiv.org/pdf/2507.21848.pdf</a></span>   <span><a href='https://github.com/ZhangXJ199/EDGE-GRPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingjian Zhang, Siwei Wen, Wenjun Wu, Lei Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21848">EDGE-GRPO: Entropy-Driven GRPO with Guided Error Correction for Advantage Diversity</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have made remarkable progress in enhancing step-by-step reasoning through reinforcement learning. However, the Group Relative Policy Optimization (GRPO) algorithm, which relies on sparse reward rules, often encounters the issue of identical rewards within groups, leading to the advantage collapse problem. Existing works typically address this challenge from two perspectives: enforcing model reflection to enhance response diversity, and introducing internal feedback to augment the training signal (advantage). In this work, we begin by analyzing the limitations of model reflection and investigating the policy entropy of responses at the fine-grained sample level. Based on our experimental findings, we propose the EDGE-GRPO algorithm, which adopts \textbf{E}ntropy-\textbf{D}riven Advantage and \textbf{G}uided \textbf{E}rror Correction to effectively mitigate the problem of advantage collapse. Extensive experiments on several main reasoning benchmarks demonstrate the effectiveness and superiority of our approach. It is available at https://github.com/ZhangXJ199/EDGE-GRPO.<br>
<span id='abs_ch'>中文: EDGE-GRPO算法通过引入熵驱动的优势计算和引导式错误修正，有效解决了大语言模型推理中的优势崩溃问题，在多个基准测试中展现出卓越性能。</span><br>
<span id='abs_en'>English: The EDGE-GRPO algorithm effectively mitigates the advantage collapse problem in LLM reasoning by incorporating entropy-driven advantage calculation and guided error correction, demonstrating superior performance across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1161, <a href='https://arxiv.org/pdf/2507.21802.pdf' target='_blank'>https://arxiv.org/pdf/2507.21802.pdf</a></span>   <span><a href='https://github.com/Tencent-Hunyuan/MixGRPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, Zhao Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21802">MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although GRPO substantially enhances flow matching models in human preference alignment of image generation, methods such as FlowGRPO still exhibit inefficiency due to the necessity of sampling and optimizing over all denoising steps specified by the Markov Decision Process (MDP). In this paper, we propose $\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed sampling strategies through the integration of stochastic differential equations (SDE) and ordinary differential equations (ODE). This streamlines the optimization process within the MDP to improve efficiency and boost performance. Specifically, MixGRPO introduces a sliding window mechanism, using SDE sampling and GRPO-guided optimization only within the window, while applying ODE sampling outside. This design confines sampling randomness to the time-steps within the window, thereby reducing the optimization overhead, and allowing for more focused gradient updates to accelerate convergence. Additionally, as time-steps beyond the sliding window are not involved in optimization, higher-order solvers are supported for sampling. So we present a faster variant, termed $\textbf{MixGRPO-Flash}$, which further improves training efficiency while achieving comparable performance. MixGRPO exhibits substantial gains across multiple dimensions of human preference alignment, outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50% lower training time. Notably, MixGRPO-Flash further reduces training time by 71%. Codes and models are available at $\href{https://github.com/Tencent-Hunyuan/MixGRPO}{MixGRPO}$.<br>
<span id='abs_ch'>中文: MixGRPO采用滑动窗口混合采 策略，结合SDE和ODE仅优化部分去噪步骤，在图像生成的人类偏好对齐中显著提升了训练效率和性能。English: MixGRPO introduces a mixed sampling strategy with a sliding window that combines SDE and ODE to optimize only specific denoising steps, significantly improving training efficiency and performance in human preference alignment for image generation.Paperid:121,https://arxiv.org/pdf/2507.21799.pdfGitHub</span><br>
<span id='abs_en'>English: MixGRPO introduces a mixed sampling strategy with a sliding window that combines SDE and ODE to optimize only specific denoising steps, significantly improving training efficiency and performance in human preference alignment for image generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1162, <a href='https://arxiv.org/pdf/2507.21802.pdf' target='_blank'>https://arxiv.org/pdf/2507.21802.pdf</a></span>   <span><a href='https://github.com/Tencent-Hunyuan/MixGRPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, Zhao Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21802">MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although GRPO substantially enhances flow matching models in human preference alignment of image generation, methods such as FlowGRPO still exhibit inefficiency due to the necessity of sampling and optimizing over all denoising steps specified by the Markov Decision Process (MDP). In this paper, we propose $\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed sampling strategies through the integration of stochastic differential equations (SDE) and ordinary differential equations (ODE). This streamlines the optimization process within the MDP to improve efficiency and boost performance. Specifically, MixGRPO introduces a sliding window mechanism, using SDE sampling and GRPO-guided optimization only within the window, while applying ODE sampling outside. This design confines sampling randomness to the time-steps within the window, thereby reducing the optimization overhead, and allowing for more focused gradient updates to accelerate convergence. Additionally, as time-steps beyond the sliding window are not involved in optimization, higher-order solvers are supported for sampling. So we present a faster variant, termed $\textbf{MixGRPO-Flash}$, which further improves training efficiency while achieving comparable performance. MixGRPO exhibits substantial gains across multiple dimensions of human preference alignment, outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50% lower training time. Notably, MixGRPO-Flash further reduces training time by 71%. Codes and models are available at $\href{https://github.com/Tencent-Hunyuan/MixGRPO}{MixGRPO}$.<br>
<span id='abs_ch'>中文: MixGRPO采用滑动窗口混合采 策略，结合SDE和ODE仅优化部分去噪步骤，在图像生成的人类偏好对齐中显著提升了训练效率和性能。English: MixGRPO introduces a mixed sampling strategy with a sliding window that combines SDE and ODE to optimize only specific denoising steps, significantly improving training efficiency and performance in human preference alignment for image generation.Paperid:121,https://arxiv.org/pdf/2507.21799.pdfGitHub</span><br>
<span id='abs_en'>English: MixGRPO introduces a mixed sampling strategy with a sliding window that combines SDE and ODE to optimize only specific denoising steps, significantly improving training efficiency and performance in human preference alignment for image generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1163, <a href='https://arxiv.org/pdf/2507.21802.pdf' target='_blank'>https://arxiv.org/pdf/2507.21802.pdf</a></span>   <span><a href='https://github.com/Tencent-Hunyuan/MixGRPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, Zhao Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21802">MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although GRPO substantially enhances flow matching models in human preference alignment of image generation, methods such as FlowGRPO still exhibit inefficiency due to the necessity of sampling and optimizing over all denoising steps specified by the Markov Decision Process (MDP). In this paper, we propose $\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed sampling strategies through the integration of stochastic differential equations (SDE) and ordinary differential equations (ODE). This streamlines the optimization process within the MDP to improve efficiency and boost performance. Specifically, MixGRPO introduces a sliding window mechanism, using SDE sampling and GRPO-guided optimization only within the window, while applying ODE sampling outside. This design confines sampling randomness to the time-steps within the window, thereby reducing the optimization overhead, and allowing for more focused gradient updates to accelerate convergence. Additionally, as time-steps beyond the sliding window are not involved in optimization, higher-order solvers are supported for sampling. So we present a faster variant, termed $\textbf{MixGRPO-Flash}$, which further improves training efficiency while achieving comparable performance. MixGRPO exhibits substantial gains across multiple dimensions of human preference alignment, outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50% lower training time. Notably, MixGRPO-Flash further reduces training time by 71%. Codes and models are available at $\href{https://github.com/Tencent-Hunyuan/MixGRPO}{MixGRPO}$.<br>
<span id='abs_ch'>中文: MixGRPO采用滑动窗口混合采 策略，结合SDE和ODE仅优化部分去噪步骤，在图像生成的人类偏好对齐中显著提升了训练效率和性能。English: MixGRPO introduces a mixed sampling strategy with a sliding window that combines SDE and ODE to optimize only specific denoising steps, significantly improving training efficiency and performance in human preference alignment for image generation.Paperid:121,https://arxiv.org/pdf/2507.21799.pdfGitHub</span><br>
<span id='abs_en'>English: MixGRPO introduces a mixed sampling strategy with a sliding window that combines SDE and ODE to optimize only specific denoising steps, significantly improving training efficiency and performance in human preference alignment for image generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1164, <a href='https://arxiv.org/pdf/2507.21799.pdf' target='_blank'>https://arxiv.org/pdf/2507.21799.pdf</a></span>   <span><a href='https://github.com/rfcrate/RF_CRATE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xie Zhang, Yina Wang, Chenshu Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21799">Unlocking Interpretability for RF Sensing: A Complex-Valued White-Box Transformer</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The empirical success of deep learning has spurred its application to the radio-frequency (RF) domain, leading to significant advances in Deep Wireless Sensing (DWS). However, most existing DWS models function as black boxes with limited interpretability, which hampers their generalizability and raises concerns in security-sensitive physical applications. In this work, inspired by the remarkable advances of white-box transformers, we present RF-CRATE, the first mathematically interpretable deep network architecture for RF sensing, grounded in the principles of complex sparse rate reduction. To accommodate the unique RF signals, we conduct non-trivial theoretical derivations that extend the original real-valued white-box transformer to the complex domain. By leveraging the CR-Calculus framework, we successfully construct a fully complex-valued white-box transformer with theoretically derived self-attention and residual multi-layer perceptron modules. Furthermore, to improve the model's ability to extract discriminative features from limited wireless data, we introduce Subspace Regularization, a novel regularization strategy that enhances feature diversity, resulting in an average performance improvement of 19.98% across multiple sensing tasks. We extensively evaluate RF-CRATE against seven baselines with multiple public and self-collected datasets involving different RF signals. The results show that RF-CRATE achieves performance on par with thoroughly engineered black-box models, while offering full mathematical interpretability. More importantly, by extending CRATE to the complex domain, RF-CRATE yields substantial improvements, achieving an average classification gain of 5.08% and reducing regression error by 10.34% across diverse sensing tasks compared to CRATE. RF-CRATE is fully open-sourced at: https://github.com/rfcrate/RF_CRATE.<br>
<span id='abs_ch'>中文: 本文提出了首个数学可解释的射频 感深度网络RF-CRATE，通过将白盒Transformer扩展至复数域，在保持与黑盒模型相当性能的同时实现了完全可解释性，并借助子空间正则化显著提升了特征提取能力。</span><br>
<span id='abs_en'>English: This paper introduces RF-CRATE, the first mathematically interpretable deep network for RF sensing that extends white-box transformers to the complex domain, achieving performance comparable to black-box models while offering full interpretability and improved feature extraction through subspace regularization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1165, <a href='https://arxiv.org/pdf/2507.21638.pdf' target='_blank'>https://arxiv.org/pdf/2507.21638.pdf</a></span>   <span><a href='https://github.com/assistive-autonomy/assistax' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Leonard Hinckeldey, Elliot Fosong, Elle Miller, Rimvydas Rubavicius, Trevor McInroe, Patricia Wollstadt, Christiane B. Wiebel-Herboth, Subramanian Ramamoorthy, Stefano V. Albrecht
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21638">Assistax: A Hardware-Accelerated Reinforcement Learning Benchmark for Assistive Robotics</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The development of reinforcement learning (RL) algorithms has been largely driven by ambitious challenge tasks and benchmarks. Games have dominated RL benchmarks because they present relevant challenges, are inexpensive to run and easy to understand. While games such as Go and Atari have led to many breakthroughs, they often do not directly translate to real-world embodied applications. In recognising the need to diversify RL benchmarks and addressing complexities that arise in embodied interaction scenarios, we introduce Assistax: an open-source benchmark designed to address challenges arising in assistive robotics tasks. Assistax uses JAX's hardware acceleration for significant speed-ups for learning in physics-based simulations. In terms of open-loop wall-clock time, Assistax runs up to $370\times$ faster when vectorising training runs compared to CPU-based alternatives. Assistax conceptualises the interaction between an assistive robot and an active human patient using multi-agent RL to train a population of diverse partner agents against which an embodied robotic agent's zero-shot coordination capabilities can be tested. Extensive evaluation and hyperparameter tuning for popular continuous control RL and MARL algorithms provide reliable baselines and establish Assistax as a practical benchmark for advancing RL research for assistive robotics. The code is available at: https://github.com/assistive-autonomy/assistax.<br>
<span id='abs_ch'>中文: Assistax是一个基于JAX硬件 速的开源基准测试平台，通过多智能体强化学 模拟辅助机器人与人类互动，其训练速度比CPU方案快370倍，旨在推动辅助机器人领域的强化学  究。</span><br>
<span id='abs_en'>English: Assistax is a new open-source benchmark using JAX-accelerated physics simulations to advance reinforcement learning for assistive robotics, featuring multi-agent training and 370× faster performance than CPU alternatives.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1166, <a href='https://arxiv.org/pdf/2507.21588.pdf' target='_blank'>https://arxiv.org/pdf/2507.21588.pdf</a></span>   <span><a href='https://github.com/ENJOY-Yin-jiong/PHP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiong Yin, Liang Li, Jiehua Zhang, Yuhan Gao, Chenggang Yan, Xichun Sheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21588">Progressive Homeostatic and Plastic Prompt Tuning for Audio-Visual Multi-Task Incremental Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Audio-visual multi-task incremental learning aims to continuously learn from multiple audio-visual tasks without the need for joint training on all tasks. The challenge of the problem is how to preserve the old task knowledge while facilitating the learning of new task with previous experiences. To address these challenges, we introduce a three-stage Progressive Homeostatic and Plastic audio-visual prompt (PHP) method. In the shallow phase, we design the task-shared modality aggregating adapter to foster cross-task and cross-modal audio-visual representation learning to enhance shared understanding between tasks. In the middle phase, we propose the task-specific modality-shared dynamic generating adapter, which constructs prompts that are tailored to individual tasks while remaining general across modalities, which balances the models ability to retain knowledge against forgetting with its potential for versatile multi-task transferability. In the deep phase, we introduce the task-specific modality-independent prompts to further refine the understand ability by targeting individual information for each task and modality. By incorporating these three phases, PHP retains task-specific prompts while adapting shared parameters for new tasks to effectively balance knowledge sharing and specificity. Our method achieves SOTA performance in different orders of four tasks (AVE, AVVP, AVS and AVQA). Our code can be available at https://github.com/ENJOY-Yin-jiong/PHP.<br>
<span id='abs_ch'>中文: 提出的渐进式稳态与可塑性（PHP）方法通过三个阶段的提示机制，在视听多任务增量学 中平衡知识保留与迁移，实现了最先进的性能。</span><br>
<span id='abs_en'>English: The proposed Progressive Homeostatic and Plastic (PHP) method enables effective audio-visual multi-task incremental learning by balancing knowledge retention and transfer across tasks through three specialized prompt phases, achieving state-of-the-art performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1167, <a href='https://arxiv.org/pdf/2507.21585.pdf' target='_blank'>https://arxiv.org/pdf/2507.21585.pdf</a></span>   <span><a href='https://github.com/Lumos0507/SafeDriveRAG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Ye, Mengshi Qi, Zhaohong Liu, Liang Liu, Huadong Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21585">SafeDriveRAG: Towards Safe Autonomous Driving with Knowledge Graph-based Retrieval-Augmented Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this work, we study how vision-language models (VLMs) can be utilized to enhance the safety for the autonomous driving system, including perception, situational understanding, and path planning. However, existing research has largely overlooked the evaluation of these models in traffic safety-critical driving scenarios. To bridge this gap, we create the benchmark (SafeDrive228K) and propose a new baseline based on VLM with knowledge graph-based retrieval-augmented generation (SafeDriveRAG) for visual question answering (VQA). Specifically, we introduce SafeDrive228K, the first large-scale multimodal question-answering benchmark comprising 228K examples across 18 sub-tasks. This benchmark encompasses a diverse range of traffic safety queries, from traffic accidents and corner cases to common safety knowledge, enabling a thorough assessment of the comprehension and reasoning abilities of the models. Furthermore, we propose a plug-and-play multimodal knowledge graph-based retrieval-augmented generation approach that employs a novel multi-scale subgraph retrieval algorithm for efficient information retrieval. By incorporating traffic safety guidelines collected from the Internet, this framework further enhances the model's capacity to handle safety-critical situations. Finally, we conduct comprehensive evaluations on five mainstream VLMs to assess their reliability in safety-sensitive driving tasks. Experimental results demonstrate that integrating RAG significantly improves performance, achieving a +4.73% gain in Traffic Accidents tasks, +8.79% in Corner Cases tasks and +14.57% in Traffic Safety Commonsense across five mainstream VLMs, underscoring the potential of our proposed benchmark and methodology for advancing research in traffic safety. Our source code and data are available at https://github.com/Lumos0507/SafeDriveRAG.<br>
<span id='abs_ch'>中文: 本 究提出了首个用于评估视觉语言模型在交通安全关键场景下性能的大规模多模态基准SafeDrive228K，并开发了基于知识图谱的检索增强生成方法SafeDriveRAG，该方法显著提升了模型在多种安全任务中的表现。</span><br>
<span id='abs_en'>English: This study introduces SafeDrive228K, the first large-scale multimodal benchmark for evaluating vision-language models in traffic safety-critical scenarios, and proposes SafeDriveRAG, a knowledge graph-based retrieval-augmented generation method that significantly enhances model performance across various safety tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1168, <a href='https://arxiv.org/pdf/2507.21503.pdf' target='_blank'>https://arxiv.org/pdf/2507.21503.pdf</a></span>   <span><a href='https://github.com/DSTTSD/MoHoBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanxu Zhu, Shitong Duan, Xiangxu Zhang, Jitao Sang, Peng Zhang, Tun Lu, Xiao Zhou, Jing Yao, Xiaoyuan Yi, Xing Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21503">MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently Multimodal Large Language Models (MLLMs) have achieved considerable advancements in vision-language tasks, yet produce potentially harmful or untrustworthy content. Despite substantial work investigating the trustworthiness of language models, MMLMs' capability to act honestly, especially when faced with visually unanswerable questions, remains largely underexplored. This work presents the first systematic assessment of honesty behaviors across various MLLMs. We ground honesty in models' response behaviors to unanswerable visual questions, define four representative types of such questions, and construct MoHoBench, a large-scale MMLM honest benchmark, consisting of 12k+ visual question samples, whose quality is guaranteed by multi-stage filtering and human verification. Using MoHoBench, we benchmarked the honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our findings show that: (1) most models fail to appropriately refuse to answer when necessary, and (2) MMLMs' honesty is not solely a language modeling issue, but is deeply influenced by visual information, necessitating the development of dedicated methods for multimodal honesty alignment. Therefore, we implemented initial alignment methods using supervised and preference learning to improve honesty behavior, providing a foundation for future work on trustworthy MLLMs. Our data and code can be found at https://github.com/DSTTSD/MoHoBench.<br>
<span id='abs_ch'>中文: 本 究首次提出MoHoBench基准，系统评估多模态大语言模型在面对视觉不可答问题时的诚实性，发现多数模型 法恰当拒答且视觉信息显著影响诚实表现，进而提出了改进的对齐方法。English: This study introduces MoHoBench, the first benchmark to systematically evaluate the honesty of Multimodal Large Language Models (MLLMs) when faced with unanswerable visual questions, revealing that most models fail to refuse appropriately and that visual information significantly impacts honesty, leading to proposed alignment methods for improvement.Paperid:135,https://arxiv.org/pdf/2507.21494.pdfGitHub</span><br>
<span id='abs_en'>English: This study introduces MoHoBench, the first benchmark to systematically evaluate the honesty of Multimodal Large Language Models (MLLMs) when faced with unanswerable visual questions, revealing that most models fail to refuse appropriately and that visual information significantly impacts honesty, leading to proposed alignment methods for improvement.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1169, <a href='https://arxiv.org/pdf/2507.21419.pdf' target='_blank'>https://arxiv.org/pdf/2507.21419.pdf</a></span>   <span><a href='https://github.com/pan-xi/GovRelBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiquan Wang, Yi Chen, Shang Zeng, Yun Bian, Zhe Cui
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21419">GovRelBench:A Benchmark for Government Domain Relevance</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current evaluations of LLMs in the government domain primarily focus on safety considerations in specific scenarios, while the assessment of the models' own core capabilities, particularly domain relevance, remains insufficient. To address this gap, we propose GovRelBench, a benchmark specifically designed for evaluating the core capabilities of LLMs in the government domain. GovRelBench consists of government domain prompts and a dedicated evaluation tool, GovRelBERT. During the training process of GovRelBERT, we introduce the SoftGovScore method: this method trains a model based on the ModernBERT architecture by converting hard labels to soft scores, enabling it to accurately compute the text's government domain relevance score. This work aims to enhance the capability evaluation framework for large models in the government domain, providing an effective tool for relevant research and practice. Our code and dataset are available at https://github.com/pan-xi/GovRelBench.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1170, <a href='https://arxiv.org/pdf/2507.21391.pdf' target='_blank'>https://arxiv.org/pdf/2507.21391.pdf</a></span>   <span><a href='https://github.com/sjz5202/LLaVA-Reward' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shijie Zhou, Ruiyi Zhang, Huaisheng Zhu, Branislav Kveton, Yufan Zhou, Jiuxiang Gu, Jian Chen, Changyou Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21391">Multimodal LLMs as Customized Reward Models for Text-to-Image Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce LLaVA-Reward, an efficient reward model designed to automatically evaluate text-to-image (T2I) generations across multiple perspectives, leveraging pretrained multimodal large language models (MLLMs). Existing MLLM-based approaches require instruction-following data for supervised fine-tuning and evaluate generation quality on analyzing text response, which is time-consuming and difficult to train. To address this problem, we propose LLaVA-Reward, which directly utilizes the hidden states of MLLMs given text-image pairs. To enhance the bidirectional interaction between visual and textual representations in decoder-only MLLMs, we further propose adding a Skip-connection Cross Attention (SkipCA) module. This design enhances text-image correlation reasoning by connecting early-layer visual features with later-layer hidden representations. In addition, LLaVA-Reward supports different types of preference data for efficient fine-tuning, including paired preference data and unpaired data. We train LLaVA-Reward on four evaluation perspectives: text-image alignment, fidelity/artifact, safety, and overall ranking. Empirical results demonstrate that LLaVA-Reward outperforms conventional and MLLM-based methods in generating human-aligned scores for automatic evaluations and inference-time scaling in text-to-image generations.<br>
<span id='abs_ch'>中文: LLaVA-Reward是一种高效的奖励模型，利用预训练多模态大语言模型的隐藏状态和跳跃连接交叉注意力模块，从多角度自动评估文生图生成质量，在自动评估和推理扩展方面优于现有方法。English: LLaVA-Reward is an efficient reward model that uses pretrained multimodal large language models to automatically evaluate text-to-image generations from multiple perspectives by leveraging hidden states and a Skip-connection Cross Attention module for improved visual-textual reasoning.Paperid:144,https://arxiv.org/pdf/2507.21358.pdfGitHub</span><br>
<span id='abs_en'>English: LLaVA-Reward is an efficient reward model that uses pretrained multimodal large language models to automatically evaluate text-to-image generations from multiple perspectives by leveraging hidden states and a Skip-connection Cross Attention module for improved visual-textual reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1171, <a href='https://arxiv.org/pdf/2507.21340.pdf' target='_blank'>https://arxiv.org/pdf/2507.21340.pdf</a></span>   <span><a href='https://github.com/ibm/struct-text' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Satyananda Kashyap, Sola Shirai, Nandana Mihindukulasooriya, Horst Samulowitz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21340">StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Extracting structured information from text, such as key-value pairs that could augment tabular data, is quite useful in many enterprise use cases. Although large language models (LLMs) have enabled numerous automated pipelines for converting natural language into structured formats, there is still a lack of benchmarks for evaluating their extraction quality, especially in specific domains or focused documents specific to a given organization. Building such benchmarks by manual annotations is labour-intensive and limits the size and scalability of the benchmarks. In this work, we present StructText, an end-to-end framework for automatically generating high-fidelity benchmarks for key-value extraction from text using existing tabular data. It uses available tabular data as structured ground truth, and follows a two-stage ``plan-then-execute'' pipeline to synthetically generate corresponding natural-language text. To ensure alignment between text and structured source, we introduce a multi-dimensional evaluation strategy that combines (a) LLM-based judgments on factuality, hallucination, and coherence and (b) objective extraction metrics measuring numeric and temporal accuracy. We evaluated the proposed method on 71,539 examples across 49 datasets. Results reveal that while LLMs achieve strong factual accuracy and avoid hallucination, they struggle with narrative coherence in producing extractable text. Notably, models presume numerical and temporal information with high fidelity yet this information becomes embedded in narratives that resist automated extraction. We release a framework, including datasets, evaluation tools, and baseline extraction systems, to support continued research.<br>
<span id='abs_ch'>中文: 本文提出StructText框架，通过自动生成高质量基准来评估文本中的键值对提取，解决了评估方法缺乏可扩展性的问题，并揭示了大语言模型虽保持事实准确性但在生成文本的叙事连贯性方面存在不足。</span><br>
<span id='abs_en'>English: This paper introduces StructText, an automated framework for generating high-fidelity benchmarks to evaluate key-value extraction from text, addressing the lack of scalable evaluation methods while revealing that LLMs maintain factual accuracy but struggle with narrative coherence in generated text.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1172, <a href='https://arxiv.org/pdf/2507.21260.pdf' target='_blank'>https://arxiv.org/pdf/2507.21260.pdf</a></span>   <span><a href='https://github.com/amartya21/Adam-PnP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Amartya Banerjee, Xingyu Xu, Caroline MoosmÃ¼ller, Harlin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21260">Adaptive Multimodal Protein Plug-and-Play with Diffusion-Based Priors</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In an inverse problem, the goal is to recover an unknown parameter (e.g., an image) that has typically undergone some lossy or noisy transformation during measurement. Recently, deep generative models, particularly diffusion models, have emerged as powerful priors for protein structure generation. However, integrating noisy experimental data from multiple sources to guide these models remains a significant challenge. Existing methods often require precise knowledge of experimental noise levels and manually tuned weights for each data modality. In this work, we introduce Adam-PnP, a Plug-and-Play framework that guides a pre-trained protein diffusion model using gradients from multiple, heterogeneous experimental sources. Our framework features an adaptive noise estimation scheme and a dynamic modality weighting mechanism integrated into the diffusion process, which reduce the need for manual hyperparameter tuning. Experiments on complex reconstruction tasks demonstrate significantly improved accuracy using Adam-PnP.<br>
<span id='abs_ch'>Chinese Summary: Adam-PnP是一种即插即用框架，通过自适应噪声估计和动态多源实验数据 权机制引导蛋白质扩散模型，显著减少了人工参数调整需求并提升了结构重建精度。</span><br>
<span id='abs_en'>English Summary: Adam-PnP is a Plug-and-Play framework that enhances protein structure reconstruction by guiding diffusion models with adaptive noise estimation and dynamic weighting of multiple experimental data sources, reducing manual tuning while improving accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1173, <a href='https://arxiv.org/pdf/2507.21206.pdf' target='_blank'>https://arxiv.org/pdf/2507.21206.pdf</a></span>   <span><a href='https://github.com/SafeRL-Lab/agentic-web' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingxuan Yang, Mulei Ma, Yuxuan Huang, Huacan Chai, Chenyu Gong, Haoran Geng, Yuanjian Zhou, Ying Wen, Meng Fang, Muhao Chen, Shangding Gu, Ming Jin, Costas Spanos, Yang Yang, Pieter Abbeel, Dawn Song, Weinan Zhang, Jun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21206">Agentic Web: Weaving the Next Web with AI Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The emergence of AI agents powered by large language models (LLMs) marks a pivotal shift toward the Agentic Web, a new phase of the internet defined by autonomous, goal-driven interactions. In this paradigm, agents interact directly with one another to plan, coordinate, and execute complex tasks on behalf of users. This transition from human-driven to machine-to-machine interaction allows intent to be delegated, relieving users from routine digital operations and enabling a more interactive, automated web experience. In this paper, we present a structured framework for understanding and building the Agentic Web. We trace its evolution from the PC and Mobile Web eras and identify the core technological foundations that support this shift. Central to our framework is a conceptual model consisting of three key dimensions: intelligence, interaction, and economics. These dimensions collectively enable the capabilities of AI agents, such as retrieval, recommendation, planning, and collaboration. We analyze the architectural and infrastructural challenges involved in creating scalable agentic systems, including communication protocols, orchestration strategies, and emerging paradigms such as the Agent Attention Economy. We conclude by discussing the potential applications, societal risks, and governance issues posed by agentic systems, and outline research directions for developing open, secure, and intelligent ecosystems shaped by both human intent and autonomous agent behavior. A continuously updated collection of relevant studies for agentic web is available at: https://github.com/SafeRL-Lab/agentic-web.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1174, <a href='https://arxiv.org/pdf/2507.21184.pdf' target='_blank'>https://arxiv.org/pdf/2507.21184.pdf</a></span>   <span><a href='https://github.com/linhaowei1/SLD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haowei Lin, Xiangyu Wang, Jianzhu Ma, Yitao Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21184">EvoSLD: Automated Neural Scaling Law Discovery With Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Scaling laws are fundamental mathematical relationships that predict how neural network performance evolves with changes in variables such as model size, dataset size, and computational resources. Traditionally, discovering these laws requires extensive human expertise and manual experimentation. We introduce EvoSLD, an automated framework for Scaling Law Discovery (SLD) that leverages evolutionary algorithms guided by Large Language Models (LLMs) to co-evolve symbolic expressions and their optimization routines. Formulated to handle scaling variables, control variables, and response metrics across diverse experimental settings, EvoSLD searches for parsimonious, universal functional forms that minimize fitting errors on grouped data subsets. Evaluated on five real-world scenarios from recent literature, EvoSLD rediscovers exact human-derived laws in two cases and surpasses them in others, achieving up to orders-of-magnitude reductions in normalized mean squared error on held-out test sets. Compared to baselines like symbolic regression and ablated variants, EvoSLD demonstrates superior accuracy, interpretability, and efficiency, highlighting its potential to accelerate AI research. Code is available at https://github.com/linhaowei1/SLD.<br>
<span id='abs_ch'>中文: 本文提出的SLDAgent通过协同优化自主发现扩展定律，首次证明人工智能生成的定律在各项任务中均能超越人工推导的对应定律，在预测精度和实际应用方面展现出显著优势。</span><br>
<span id='abs_en'>English: This paper introduces SLDAgent, an evolution-based agent that autonomously discovers scaling laws through co-optimization, demonstrating for the first time that AI-generated laws consistently outperform human-derived counterparts in accuracy and practical utility across diverse tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1175, <a href='https://arxiv.org/pdf/2507.21167.pdf' target='_blank'>https://arxiv.org/pdf/2507.21167.pdf</a></span>   <span><a href='https://github.com/MLrollIT/ChartM3Our' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/MLrollIT/ChartM3' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/yaolinli/VCE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Donglu Yang, Liang Zhang, Zihao Yue, Liangyu Chen, Yichen Xu, Wenxuan Wang, Qin Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21167">ChartM$^3$: Benchmarking Chart Editing with Multimodal Instructions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Charts are a fundamental visualization format widely used in data analysis across research and industry. While enabling users to edit charts based on high-level intentions is of great practical value, existing methods primarily rely on natural language instructions, which are often too ambiguous to support fine-grained editing. In this work, we introduce a novel paradigm for multimodal chart editing, where user intent is expressed through a combination of natural language and visual indicators that explicitly highlight the elements to be modified. To support this paradigm, we present Chart$\text{M}^3$, a new benchmark for Multimodal chart editing with Multi-level complexity and Multi-perspective evaluation. Chart$\text{M}^3$ contains 1,000 samples spanning four levels of editing difficulty. Each sample includes triplets in the form of (chart, code, multimodal instructions). To comprehensively evaluate chart editing models, Chart$\text{M}^3$ provides metrics that assess both visual appearance and code correctness. Our benchmark reveals significant limitations in current multimodal large language models (MLLMs), including GPT-4o, particularly in their ability to interpret and act on visual indicators. To address this, we construct Chart$\text{M}^3$-Train, a large-scale training set with 24,000 multimodal chart editing samples. Fine-tuning MLLMs on this dataset leads to substantial improvements, demonstrating the importance of multimodal supervision in building practical chart editing systems. Our datasets, codes, and evaluation tools are available at https://github.com/MLrollIT/ChartM3. %https://github.com/MLrollIT/ChartM3Our datasets, codes, and evaluation tools are available at https://github.com/yaolinli/VCE.<br>
<span id='abs_ch'>中文: 本文提出了一种结合自然语言与视觉 记的多模态图表编辑新范式，建立了ChartM3基准进行评估，并证明在专业数据集上微调多模态大模型能显著提升其视觉编辑指令的解析能力。</span><br>
<span id='abs_en'>English: This paper introduces a multimodal chart editing paradigm combining natural language and visual indicators, presents the ChartM3 benchmark for evaluation, and demonstrates that fine-tuning MLLMs on a specialized dataset significantly improves performance in interpreting visual editing instructions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1176, <a href='https://arxiv.org/pdf/2507.21164.pdf' target='_blank'>https://arxiv.org/pdf/2507.21164.pdf</a></span>   <span><a href='https://github.com/Nicolas-Pinon/uad_ocsvm_guided_repr_learning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicolas Pinon, Carole Lartizien
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21164">OCSVM-Guided Representation Learning for Unsupervised Anomaly Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Unsupervised anomaly detection (UAD) aims to detect anomalies without labeled data, a necessity in many machine learning applications where anomalous samples are rare or not available. Most state-of-the-art methods fall into two categories: reconstruction-based approaches, which often reconstruct anomalies too well, and decoupled representation learning with density estimators, which can suffer from suboptimal feature spaces. While some recent methods attempt to couple feature learning and anomaly detection, they often rely on surrogate objectives, restrict kernel choices, or introduce approximations that limit their expressiveness and robustness. To address this challenge, we propose a novel method that tightly couples representation learning with an analytically solvable one-class SVM (OCSVM), through a custom loss formulation that directly aligns latent features with the OCSVM decision boundary. The model is evaluated on two tasks: a new benchmark based on MNIST-C, and a challenging brain MRI subtle lesion detection task. Unlike most methods that focus on large, hyperintense lesions at the image level, our approach succeeds to target small, non-hyperintense lesions, while we evaluate voxel-wise metrics, addressing a more clinically relevant scenario. Both experiments evaluate a form of robustness to domain shifts, including corruption types in MNIST-C and scanner/age variations in MRI. Results demonstrate performance and robustness of our proposed mode,highlighting its potential for general UAD and real-world medical imaging applications. The source code is available at https://github.com/Nicolas-Pinon/uad_ocsvm_guided_repr_learning<br>
<span id='abs_ch'>中文摘要：本文提出了一种新颖的 监督异常检测方法，通过自定义损失函数将表征学 与一类支持向量机紧密结合，在领域偏移基准测试和医学影像任务中检测细微异常方面展现出卓越的性能和鲁棒性。</span><br>
<span id='abs_en'>English Summary: This paper introduces a novel unsupervised anomaly detection method that tightly couples representation learning with a one-class SVM through a custom loss function, demonstrating superior performance and robustness in detecting subtle anomalies across domain-shifted benchmarks and medical imaging tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1177, <a href='https://arxiv.org/pdf/2507.21138.pdf' target='_blank'>https://arxiv.org/pdf/2507.21138.pdf</a></span>   <span><a href='https://github.com/inworld-ai/tts' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Oleg Atamanenko, Anna Chalova, Joseph Coombes, Nikki Cope, Phillip Dang, Zhifeng Deng, Jimmy Du, Michael Ermolenko, Feifan Fan, Yufei Feng, Cheryl Fichter, Pavel Filimonov, Louis Fischer, Kylan Gibbs, Valeria Gusarova, Pavel Karpik, Andreas Assad Kottner, Ian Lee, Oliver Louie, Jasmine Mai, Mikhail Mamontov, Suri Mao, Nurullah Morshed, Igor Poletaev, Florin Radu, Dmytro Semernia, Evgenii Shingarev, Vikram Sivaraja, Peter Skirko, Rinat Takhautdinov, Robert Villahermosa, Jean Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21138">TTS-1 Technical Report</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce Inworld TTS-1, a set of two Transformer-based autoregressive text-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters and is designed for utmost quality and expressiveness in demanding applications. TTS-1 is our most efficient model, with 1.6B parameters, built for real-time speech synthesis and on-device use cases. By scaling train-time compute and applying a sequential process of pre-training, fine-tuning, and RL-alignment of the speech-language model (SpeechLM) component, both models achieve state-of-the-art performance on a variety of benchmarks, demonstrating exceptional quality relying purely on in-context learning of the speaker's voice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech with low latency, and support 11 languages with fine-grained emotional control and non-verbal vocalizations through audio markups. We additionally open-source our training and modeling code under an MIT license.<br>
<span id='abs_ch'>中文: Inworld TTS-1推出两款基于Transformer的语音合成模型，其中88亿参数的TTS-1-Max面向高质量应用，16亿参数的TTS-1适用于实时场景，通过先进训练方法实现顶尖性能，支持48kHz多语言语音合成与精细情感控制。</span><br>
<span id='abs_en'>English: Inworld TTS-1 introduces two Transformer-based TTS models, with the 8.8B-parameter TTS-1-Max for high-quality applications and the 1.6B-parameter TTS-1 for real-time use, both achieving state-of-the-art performance through advanced training and supporting 48kHz multilingual speech with emotional control.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1178, <a href='https://arxiv.org/pdf/2507.21125.pdf' target='_blank'>https://arxiv.org/pdf/2507.21125.pdf</a></span>   <span><a href='https://github.com/AryaAftab/RATE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Karan Mirhosseini, Arya Aftab, Alireza Sheikh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21125">RATE: An LLM-Powered Retrieval Augmented Generation Technology-Extraction Pipeline</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In an era of radical technology transformations, technology maps play a crucial role in enhancing decision making. These maps heavily rely on automated methods of technology extraction. This paper introduces Retrieval Augmented Technology Extraction (RATE), a Large Language Model (LLM) based pipeline for automated technology extraction from scientific literature. RATE combines Retrieval Augmented Generation (RAG) with multi-definition LLM-based validation. This hybrid method results in high recall in candidate generation alongside with high precision in candidate filtering. While the pipeline is designed to be general and widely applicable, we demonstrate its use on 678 research articles focused on Brain-Computer Interfaces (BCIs) and Extended Reality (XR) as a case study. Consequently, The validated technology terms by RATE were mapped into a co-occurrence network, revealing thematic clusters and structural features of the research landscape. For the purpose of evaluation, a gold standard dataset of technologies in 70 selected random articles had been curated by the experts. In addition, a technology extraction model based on Bidirectional Encoder Representations of Transformers (BERT) was used as a comparative method. RATE achieved F1-score of 91.27%, Significantly outperforming BERT with F1-score of 53.73%. Our findings highlight the promise of definition-driven LLM methods for technology extraction and mapping. They also offer new insights into emerging trends within the BCI-XR field. The source code is available https://github.com/AryaAftab/RATE<br>
<span id='abs_ch'>中文: 本文提出RATE框架，通过结合检索增强生成与多定义验证的LLM技术提取方法，在脑机接口与扩展现实案例中实现91.27%的F1值，显著优于BERT模型，为技术图谱构建提供新方案。</span><br>
<span id='abs_en'>English: This paper introduces RATE, an LLM-based pipeline that combines retrieval-augmented generation with multi-definition validation to achieve high-precision automated technology extraction from scientific literature, significantly outperforming BERT with a 91.27% F1-score in BCI-XR case studies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1179, <a href='https://arxiv.org/pdf/2507.21120.pdf' target='_blank'>https://arxiv.org/pdf/2507.21120.pdf</a></span>   <span><a href='https://github.com/ArtAICare/Affect-aware-CDR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bereket A. Yilma, Luis A. Leiva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21120">Affect-aware Cross-Domain Recommendation for Art Therapy via Music Preference Elicitation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Art Therapy (AT) is an established practice that facilitates emotional processing and recovery through creative expression. Recently, Visual Art Recommender Systems (VA RecSys) have emerged to support AT, demonstrating their potential by personalizing therapeutic artwork recommendations. Nonetheless, current VA RecSys rely on visual stimuli for user modeling, limiting their ability to capture the full spectrum of emotional responses during preference elicitation. Previous studies have shown that music stimuli elicit unique affective reflections, presenting an opportunity for cross-domain recommendation (CDR) to enhance personalization in AT. Since CDR has not yet been explored in this context, we propose a family of CDR methods for AT based on music-driven preference elicitation. A large-scale study with 200 users demonstrates the efficacy of music-driven preference elicitation, outperforming the classic visual-only elicitation approach. Our source code, data, and models are available at https://github.com/ArtAICare/Affect-aware-CDR<br>
<span id='abs_ch'>中文: 艺术疗法通过基于音乐驱动偏好诱导的跨领域推荐方法得到增强，该方法在捕捉情感反应方面优于 统的仅视觉方法。</span><br>
<span id='abs_en'>English: Art therapy is enhanced by cross-domain recommendation methods that use music-driven preference elicitation, which outperforms traditional visual-only approaches in capturing emotional responses.</span>Paperid:161,https://arxiv.org/pdf/2507.21083.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1180, <a href='https://arxiv.org/pdf/2507.21083.pdf' target='_blank'>https://arxiv.org/pdf/2507.21083.pdf</a></span>   <span><a href='https://github.com/bardolfranck/llm-responses-viewer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Franck Bardol
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21083">ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models like GPT-4 adjust their responses not only based on the question asked, but also on how it is emotionally phrased. We systematically vary the emotional tone of 156 prompts - spanning controversial and everyday topics - and analyze how it affects model responses. Our findings show that GPT-4 is three times less likely to respond negatively to a negatively framed question than to a neutral one. This suggests a "rebound" bias where the model overcorrects, often shifting toward neutrality or positivity. On sensitive topics (e.g., justice or politics), this effect is even more pronounced: tone-based variation is suppressed, suggesting an alignment override. We introduce concepts like the "tone floor" - a lower bound in response negativity - and use tone-valence transition matrices to quantify behavior. Visualizations based on 1536-dimensional embeddings confirm semantic drift based on tone. Our work highlights an underexplored class of biases driven by emotional framing in prompts, with implications for AI alignment and trust. Code and data are available at: https://github.com/bardolfranck/llm-responses-viewer<br>
<br>
<div id='section'>Paperid: <span id='pid'>1181, <a href='https://arxiv.org/pdf/2507.21035.pdf' target='_blank'>https://arxiv.org/pdf/2507.21035.pdf</a></span>   <span><a href='https://github.com/Liu-Hy/GenoMAS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyang Liu, Yijiang Li, Haohan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21035">GenoMAS: A Multi-Agent Framework for Scientific Discovery via Code-Driven Gene Expression Analysis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Gene expression analysis holds the key to many biomedical discoveries, yet extracting insights from raw transcriptomic data remains formidable due to the complexity of multiple large, semi-structured files and the need for extensive domain expertise. Current automation approaches are often limited by either inflexible workflows that break down in edge cases or by fully autonomous agents that lack the necessary precision for rigorous scientific inquiry. GenoMAS charts a different course by presenting a team of LLM-based scientists that integrates the reliability of structured workflows with the adaptability of autonomous agents. GenoMAS orchestrates six specialized LLM agents through typed message-passing protocols, each contributing complementary strengths to a shared analytic canvas. At the heart of GenoMAS lies a guided-planning framework: programming agents unfold high-level task guidelines into Action Units and, at each juncture, elect to advance, revise, bypass, or backtrack, thereby maintaining logical coherence while bending gracefully to the idiosyncrasies of genomic data.
  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation of 89.13% for data preprocessing and an F$_1$ of 60.48% for gene identification, surpassing the best prior art by 10.61% and 16.85% respectively. Beyond metrics, GenoMAS surfaces biologically plausible gene-phenotype associations corroborated by the literature, all while adjusting for latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.<br>
<span id='abs_ch'>中文: GenoMAS 通过整合结构化工作流与自主代理的LLM科学家团队，解决了当前基 表达分析自动化的局限，在基准测试中表现优异，并能发现生物学上合理的基 -表型关联。English: GenoMAS introduces a team of LLM-based scientists that combines structured workflows with autonomous agents to overcome the limitations of current automation in gene expression analysis, achieving superior performance on benchmarks and uncovering biologically plausible gene-phenotype associations.Paperid:164,https://arxiv.org/pdf/2507.21017.pdfGitHub</span><br>
<span id='abs_en'>English: GenoMAS introduces a team of LLM-based scientists that combines structured workflows with autonomous agents to overcome the limitations of current automation in gene expression analysis, achieving superior performance on benchmarks and uncovering biologically plausible gene-phenotype associations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1182, <a href='https://arxiv.org/pdf/2507.21017.pdf' target='_blank'>https://arxiv.org/pdf/2507.21017.pdf</a></span>   <span><a href='https://github.com/sunblaze-ucb/mirage-bench.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weichen Zhang, Yiyou Sun, Pohao Huang, Jiayue Pu, Heyue Lin, Dawn Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21017">MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Hallucinations pose critical risks for large language model (LLM)-based agents, often manifesting as hallucinative actions resulting from fabricated or misinterpreted information within the cognitive context. While recent studies have exposed such failures, existing evaluations remain fragmented and lack a principled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions in Risky AGEnt settings--the first unified benchmark for eliciting and evaluating hallucinations in interactive LLM-agent scenarios. We begin by introducing a three-part taxonomy to address agentic hallucinations: actions that are unfaithful to (i) task instructions, (ii) execution history, or (iii) environment observations. To analyze, we first elicit such failures by performing a systematic audit of existing agent benchmarks, then synthesize test cases using a snapshot strategy that isolates decision points in deterministic and reproducible manners. To evaluate hallucination behaviors, we adopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware prompts, enabling scalable, high-fidelity assessment of agent actions without enumerating full action spaces. MIRAGE-Bench provides actionable insights on failure modes of LLM agents and lays the groundwork for principled progress in mitigating hallucinations in interactive environments.<br>
<span id='abs_ch'>中文摘要：MIRAGE-Bench是首个用于评估交互式LLM智能体幻觉的统一基准，通过三要 分类法和系统化测试方法，有效识别任务执行与环境交互中的错误行为。</span><br>
<span id='abs_en'>English Summary: MIRAGE-Bench is the first unified benchmark for evaluating hallucinations in interactive LLM agents, featuring a three-part taxonomy and systematic testing methodology to identify failures in task execution and environment interaction.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1183, <a href='https://arxiv.org/pdf/2507.21004.pdf' target='_blank'>https://arxiv.org/pdf/2507.21004.pdf</a></span>   <span><a href='https://github.com/fanglioc/Compositional_Function_Networks' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21004">Compositional Function Networks: A High-Performance Alternative to Deep Neural Networks with Built-in Interpretability</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Deep Neural Networks (DNNs) deliver impressive performance but their black-box nature limits deployment in high-stakes domains requiring transparency. We introduce Compositional Function Networks (CFNs), a novel framework that builds inherently interpretable models by composing elementary mathematical functions with clear semantics. Unlike existing interpretable approaches that are limited to simple additive structures, CFNs support diverse compositional patterns -- sequential, parallel, and conditional -- enabling complex feature interactions while maintaining transparency. A key innovation is that CFNs are fully differentiable, allowing efficient training through standard gradient descent. We demonstrate CFNs' versatility across multiple domains, from symbolic regression to image classification with deep hierarchical networks. Our empirical evaluation shows CFNs achieve competitive performance against black-box models (96.24% accuracy on CIFAR-10) while outperforming state-of-the-art interpretable models like Explainable Boosting Machines. By combining the hierarchical expressiveness and efficient training of deep learning with the intrinsic interpretability of well-defined mathematical functions, CFNs offer a powerful framework for applications where both performance and accountability are paramount.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1184, <a href='https://arxiv.org/pdf/2507.20994.pdf' target='_blank'>https://arxiv.org/pdf/2507.20994.pdf</a></span>   <span><a href='https://github.com/listen0425/Security-Tensors' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shen Li, Liuyi Yao, Wujia Niu, Lan Zhang, Yaliang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20994">Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety to Vision in LVLM</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large visual-language models (LVLMs) integrate aligned large language models (LLMs) with visual modules to process multimodal inputs. However, the safety mechanisms developed for text-based LLMs do not naturally extend to visual modalities, leaving LVLMs vulnerable to harmful image inputs. To address this cross-modal safety gap, we introduce security tensors - trainable input vectors applied during inference through either the textual or visual modality. These tensors transfer textual safety alignment to visual processing without modifying the model's parameters. They are optimized using a curated dataset containing (i) malicious image-text pairs requiring rejection, (ii) contrastive benign pairs with text structurally similar to malicious queries, with the purpose of being contrastive examples to guide visual reliance, and (iii) general benign samples preserving model functionality. Experimental results demonstrate that both textual and visual security tensors significantly enhance LVLMs' ability to reject diverse harmful visual inputs while maintaining near-identical performance on benign tasks. Further internal analysis towards hidden-layer representations reveals that security tensors successfully activate the language module's textual "safety layers" in visual inputs, thereby effectively extending text-based safety to the visual modality.<br>
<span id='abs_ch'>中文摘要：安全 量作为可训练的输入向量被引入，能将文本安全机制迁移到大型视觉语言模型的视觉处理中，有效增强其拒绝有害视觉输入的能力，同时保持良性任务性能。</span><br>
<span id='abs_en'>English Summary: Security tensors are introduced as trainable input vectors that transfer textual safety mechanisms to visual processing in large visual-language models, effectively enhancing their ability to reject harmful visual inputs while preserving performance on benign tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1185, <a href='https://arxiv.org/pdf/2507.20987.pdf' target='_blank'>https://arxiv.org/pdf/2507.20987.pdf</a></span>   <span><a href='https://github.com/deepreasonings/WholeBodyBenchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinhan Di, Kristin Qi, Pengqian Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20987">JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech Generation Version 1</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in diffusion-based video generation have enabled photo-realistic short clips, but current methods still struggle to achieve multi-modal consistency when jointly generating whole-body motion and natural speech. Current approaches lack comprehensive evaluation frameworks that assess both visual and audio quality, and there are insufficient benchmarks for region-specific performance analysis. To address these gaps, we introduce the Joint Whole-Body Talking Avatar and Speech Generation Version I(JWB-DH-V1), comprising a large-scale multi-modal dataset with 10,000 unique identities across 2 million video samples, and an evaluation protocol for assessing joint audio-video generation of whole-body animatable avatars. Our evaluation of SOTA models reveals consistent performance disparities between face/hand-centric and whole-body performance, which incidates essential areas for future research. The dataset and evaluation tools are publicly available at https://github.com/deepreasonings/WholeBodyBenchmark.<br>
<span id='abs_ch'>中文: 本 究推出了JWB-DH-V1数据集和评估框架，旨在解决全身动作与语音联合生成中的不足，并通过性能差异揭示了未来 究的关键方向。</span><br>
<span id='abs_en'>English: The study introduces JWB-DH-V1, a comprehensive dataset and evaluation framework to address gaps in joint whole-body motion and speech generation, revealing performance disparities that highlight key research areas.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1186, <a href='https://arxiv.org/pdf/2507.20930.pdf' target='_blank'>https://arxiv.org/pdf/2507.20930.pdf</a></span>   <span><a href='https://github.com/pegasi-ai/shield' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Likun Tan, Kuan-Wei Huang, Kevin Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20930">FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Hallucinations in large language models pose a critical challenge for applications requiring factual reliability, particularly in high-stakes domains such as finance. This work presents an effective approach for detecting and editing factually incorrect content in model-generated responses based on the provided context. Given a user-defined domain-specific error taxonomy, we construct a synthetic dataset by inserting tagged errors into financial question-answering corpora and then fine-tune four language models, Phi-4, Phi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8% improvement in binary F1 score and a 30% gain in overall detection performance compared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having only 4 billion parameters, maintains competitive performance with just a 2% drop in binary detection and a 0.1% decline in overall detection compared to OpenAI-o3. Our work provides a practical solution for detecting and editing factual inconsistencies in financial text generation while introducing a generalizable framework that can enhance the trustworthiness and alignment of large language models across diverse applications beyond finance. Our code and data are available at https://github.com/pegasi-ai/shield.<br>
<span id='abs_ch'>中文: 本 究提出一种基于合成金融数据集微调模型的方法，用于检测和修正大语言模型中的事实性错误，显著提升了性能，并为增强模型可信度提供了可推广的框架。</span><br>
<span id='abs_en'>English: This study introduces a method to detect and edit factual inaccuracies in large language models by fine-tuning models like Phi-4 on a synthetic financial dataset, achieving significant performance gains and offering a generalizable framework for improving model reliability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1187, <a href='https://arxiv.org/pdf/2507.20923.pdf' target='_blank'>https://arxiv.org/pdf/2507.20923.pdf</a></span>   <span><a href='https://github.com/langkhachhoha/MPaGE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minh Hieu Ha, Hung Phan, Tung Duy Doan, Tung Dao, Dao Tran, Huynh Thi Thanh Binh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20923">Pareto-Grid-Guided Large Language Models for Fast and High-Quality Heuristics Design in Multi-Objective Combinatorial Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multi-objective combinatorial optimization problems (MOCOP) frequently arise in practical applications that require the simultaneous optimization of conflicting objectives. Although traditional evolutionary algorithms can be effective, they typically depend on domain knowledge and repeated parameter tuning, limiting flexibility when applied to unseen MOCOP instances. Recently, integration of Large Language Models (LLMs) into evolutionary computation has opened new avenues for automatic heuristic generation, using their advanced language understanding and code synthesis capabilities. Nevertheless, most existing approaches predominantly focus on single-objective tasks, often neglecting key considerations such as runtime efficiency and heuristic diversity in multi-objective settings. To bridge this gap, we introduce Multi-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a novel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO) framework that leverages LLMs and Pareto Front Grid (PFG) technique. By partitioning the objective space into grids and retaining top-performing candidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize heuristics with semantically distinct logical structures during variation, thus promoting diversity and mitigating redundancy within the population. Through extensive evaluations, MPaGE demonstrates superior performance over existing LLM-based frameworks, and achieves competitive results to traditional Multi-objective evolutionary algorithms (MOEAs), with significantly faster runtime. Our code is available at: https://github.com/langkhachhoha/MPaGE.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1188, <a href='https://arxiv.org/pdf/2507.20880.pdf' target='_blank'>https://arxiv.org/pdf/2507.20880.pdf</a></span>   <span><a href='https://github.com/declare-lab/jamify' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Renhang Liu, Chia-Yu Hung, Navonil Majumder, Taylor Gautreaux, Amir Ali Bagherzadeh, Chuan Li, Dorien Herremans, Soujanya Poria
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20880">JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability and Aesthetic Alignment</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion and flow-matching models have revolutionized automatic text-to-audio generation in recent times. These models are increasingly capable of generating high quality and faithful audio outputs capturing to speech and acoustic events. However, there is still much room for improvement in creative audio generation that primarily involves music and songs. Recent open lyrics-to-song models, such as, DiffRhythm, ACE-Step, and LeVo, have set an acceptable standard in automatic song generation for recreational use. However, these models lack fine-grained word-level controllability often desired by musicians in their workflows. To the best of our knowledge, our flow-matching-based JAM is the first effort toward endowing word-level timing and duration control in song generation, allowing fine-grained vocal control. To enhance the quality of generated songs to better align with human preferences, we implement aesthetic alignment through Direct Preference Optimization, which iteratively refines the model using a synthetic dataset, eliminating the need or manual data annotations. Furthermore, we aim to standardize the evaluation of such lyrics-to-song models through our public evaluation dataset JAME. We show that JAM outperforms the existing models in terms of the music-specific attributes.<br>
<span id='abs_ch'>中文：JAM模型通过流匹配和美学对齐技术，在歌曲生成中实现了词级时序与时长控制，其公开评估数据集JAME建立了 准化评测体系，在音乐特定属性上超越了现有模型。</span><br>
<span id='abs_en'>English: The JAM model introduces word-level timing and duration control in song generation using flow-matching and aesthetic alignment, outperforming existing models in music-specific attributes while standardizing evaluation with its public dataset JAME.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1189, <a href='https://arxiv.org/pdf/2507.20836.pdf' target='_blank'>https://arxiv.org/pdf/2507.20836.pdf</a></span>   <span><a href='https://github.com/jakobsnl/RAGTruth_Xtended' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jakob Snel, Seong Joon Oh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20836">First Hallucination Tokens Are Different from Conditional Ones</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Hallucination, the generation of untruthful content, is one of the major concerns regarding foundational models. Detecting hallucinations at the token level is vital for real-time filtering and targeted correction, yet the variation of hallucination signals within token sequences is not fully understood. Leveraging the RAGTruth corpus with token-level annotations and reproduced logits, we analyse how these signals depend on a token's position within hallucinated spans, contributing to an improved understanding of token-level hallucination. Our results show that the first hallucinated token carries a stronger signal and is more detectable than conditional tokens. We release our analysis framework, along with code for logit reproduction and metric computation at https://github.com/jakobsnl/RAGTruth_Xtended.<br>
<span id='abs_ch'>中文: 大型语言模型常产生幻觉，利用RAGTruth语料库的 究发现，首个幻觉 记的检测率远高于后续 记，这一结构特性在不同模型中均保持一致。</span><br>
<span id='abs_en'>English: Large Language Models often produce hallucinations, and a study using the RAGTruth corpus reveals that the first hallucinated token is significantly more detectable than subsequent ones, a pattern consistent across models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1190, <a href='https://arxiv.org/pdf/2507.20836.pdf' target='_blank'>https://arxiv.org/pdf/2507.20836.pdf</a></span>   <span><a href='https://github.com/jakobsnl/RAGTruth_Xtended' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jakob Snel, Seong Joon Oh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20836">First Hallucination Tokens Are Different from Conditional Ones</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) hallucinate, and detecting these cases is key to ensuring trust. While many approaches address hallucination detection at the response or span level, recent work explores token-level detection, enabling more fine-grained intervention. However, the distribution of hallucination signal across sequences of hallucinated tokens remains unexplored. We leverage token-level annotations from the RAGTruth corpus and find that the first hallucinated token is far more detectable than later ones. This structural property holds across models, suggesting that first hallucination tokens play a key role in token-level hallucination detection. Our code is available at https://github.com/jakobsnl/RAGTruth_Xtended.<br>
<span id='abs_ch'>中文: 大型语言模型常产生幻觉，利用RAGTruth语料库的 究发现，首个幻觉 记的检测率远高于后续 记，这一结构特性在不同模型中均保持一致。</span><br>
<span id='abs_en'>English: Large Language Models often produce hallucinations, and a study using the RAGTruth corpus reveals that the first hallucinated token is significantly more detectable than subsequent ones, a pattern consistent across models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1191, <a href='https://arxiv.org/pdf/2507.20746.pdf' target='_blank'>https://arxiv.org/pdf/2507.20746.pdf</a></span>   <span><a href='https://github.com/2ephyrus/AR-LIF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyu Huang, Wei Meng, Quan Liu, Kun Chen, Li Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20746">AR-LIF: Adaptive reset leaky integrate-and-fire neuron for spiking neural networks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spiking neural networks offer low energy consumption due to their event-driven nature. Beyond binary spike outputs, their intrinsic floating-point dynamics merit greater attention. Neuronal threshold levels and reset modes critically determine spike count and timing. Hard reset cause information loss, while soft reset apply uniform treatment to neurons. To address these issues, we design an adaptive reset neuron that establishes relationships between inputs, outputs, and reset, while integrating a simple yet effective threshold adjustment strategy. Experimental results demonstrate that our method achieves excellent performance while maintaining lower energy consumption. In particular, it attains state-of-the-art accuracy on Tiny-ImageNet and CIFAR10-DVS. Codes are available at https://github.com/2ephyrus/AR-LIF.<br>
<span id='abs_ch'>Chinese: 该自适应重置神经元通过建立输入-输出-重置关系并整合阈值调节策略，解决了脉冲神经网络中的信息丢失和统一处理问题，在Tiny-ImageNet等数据集上以低能耗实现了最优精度。</span><br>
<span id='abs_en'>English: The proposed adaptive reset neuron addresses information loss and uniform treatment issues in spiking neural networks by establishing input-output-reset relationships and incorporating a threshold adjustment strategy, achieving state-of-the-art accuracy on datasets like Tiny-ImageNet with low energy consumption.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1192, <a href='https://arxiv.org/pdf/2507.20745.pdf' target='_blank'>https://arxiv.org/pdf/2507.20745.pdf</a></span>   <span><a href='https://github.com/Lucenova/ReSoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Zhu, Haiwen Diao, Shang Gao, Jiazuo Yu, Jiawen Zhu, Yunzhi Zhuge, Shuai Hao, Xu Jia, Lu Zhang, Ying Zhang, Huchuan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20745">Regularizing Subspace Redundancy of Low-Rank Adaptation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Low-Rank Adaptation (LoRA) and its variants have delivered strong capability in Parameter-Efficient Transfer Learning (PETL) by minimizing trainable parameters and benefiting from reparameterization. However, their projection matrices remain unrestricted during training, causing high representation redundancy and diminishing the effectiveness of feature adaptation in the resulting subspaces. While existing methods mitigate this by manually adjusting the rank or implicitly applying channel-wise masks, they lack flexibility and generalize poorly across various datasets and architectures. Hence, we propose ReSoRA, a method that explicitly models redundancy between mapping subspaces and adaptively Regularizes Subspace redundancy of Low-Rank Adaptation. Specifically, it theoretically decomposes the low-rank submatrices into multiple equivalent subspaces and systematically applies de-redundancy constraints to the feature distributions across different projections. Extensive experiments validate that our proposed method consistently facilitates existing state-of-the-art PETL methods across various backbones and datasets in vision-language retrieval and standard visual classification benchmarks. Besides, as a training supervision, ReSoRA can be seamlessly integrated into existing approaches in a plug-and-play manner, with no additional inference costs. Code is publicly available at: https://github.com/Lucenova/ReSoRA.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1193, <a href='https://arxiv.org/pdf/2507.20630.pdf' target='_blank'>https://arxiv.org/pdf/2507.20630.pdf</a></span>   <span><a href='https://github.com/liaolea/TransPrune' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ao Li, Yuxiang Duan, Jinghui Zhang, Congbo Ma, Yutong Xie, Gustavo Carneiro, Mohammad Yaqub, Hu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20630">TransPrune: Token Transition Pruning for Efficient Large Vision-Language Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Vision-Language Models (LVLMs) have advanced multimodal learning but face high computational costs due to the large number of visual tokens, motivating token pruning to improve inference efficiency. The key challenge lies in identifying which tokens are truly important. Most existing approaches rely on attention-based criteria to estimate token importance. However, they inherently suffer from certain limitations, such as positional bias. In this work, we explore a new perspective on token importance based on token transitions in LVLMs. We observe that the transition of token representations provides a meaningful signal of semantic information. Based on this insight, we propose TransPrune, a training-free and efficient token pruning method. Specifically, TransPrune progressively prunes tokens by assessing their importance through a combination of Token Transition Variation (TTV)-which measures changes in both the magnitude and direction of token representations-and Instruction-Guided Attention (IGA), which measures how strongly the instruction attends to image tokens via attention. Extensive experiments demonstrate that TransPrune achieves comparable multimodal performance to original LVLMs, such as LLaVA-v1.5 and LLaVA-Next, across eight benchmarks, while reducing inference TFLOPs by more than half. Moreover, TTV alone can serve as an effective criterion without relying on attention, achieving performance comparable to attention-based methods. The code will be made publicly available upon acceptance of the paper at https://github.com/liaolea/TransPrune.<br>
<span id='abs_ch'>Chinese: TransPrune是一种 需训练的令牌剪枝方法，通过结合令牌转换变化和指令引导注意力来评估重要性，逐步剪除冗余令牌，在保持多模态性能的同时将推理计算量减半。</span><br>
<span id='abs_en'>English: TransPrune is a training-free token pruning method that enhances inference efficiency in Large Vision-Language Models by progressively removing tokens based on Token Transition Variation and Instruction-Guided Attention, achieving comparable performance with over 50% reduction in computational costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1194, <a href='https://arxiv.org/pdf/2507.20536.pdf' target='_blank'>https://arxiv.org/pdf/2507.20536.pdf</a></span>   <span><a href='https://github.com/SHI-Labs/T2I-Copilot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chieh-Yun Chen, Min Shi, Gong Zhang, Humphrey Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20536">T2I-Copilot: A Training-Free Multi-Agent Text-to-Image System for Enhanced Prompt Interpretation and Interactive Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text-to-Image (T2I) generative models have revolutionized content creation but remain highly sensitive to prompt phrasing, often requiring users to repeatedly refine prompts multiple times without clear feedback. While techniques such as automatic prompt engineering, controlled text embeddings, denoising, and multi-turn generation mitigate these issues, they offer limited controllability, or often necessitate additional training, restricting the generalization abilities. Thus, we introduce T2I-Copilot, a training-free multi-agent system that leverages collaboration between (Multimodal) Large Language Models to automate prompt phrasing, model selection, and iterative refinement. This approach significantly simplifies prompt engineering while enhancing generation quality and text-image alignment compared to direct generation. Specifically, T2I-Copilot consists of three agents: (1) Input Interpreter, which parses the input prompt, resolves ambiguities, and generates a standardized report; (2) Generation Engine, which selects the appropriate model from different types of T2I models and organizes visual and textual prompts to initiate generation; and (3) Quality Evaluator, which assesses aesthetic quality and text-image alignment, providing scores and feedback for potential regeneration. T2I-Copilot can operate fully autonomously while also supporting human-in-the-loop intervention for fine-grained control. On GenAI-Bench, using open-source generation models, T2I-Copilot achieves a VQA score comparable to commercial models RecraftV3 and Imagen 3, surpasses FLUX1.1-pro by 6.17% at only 16.59% of its cost, and outperforms FLUX.1-dev and SD 3.5 Large by 9.11% and 6.36%. Code will be released at: https://github.com/SHI-Labs/T2I-Copilot.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1195, <a href='https://arxiv.org/pdf/2507.20243.pdf' target='_blank'>https://arxiv.org/pdf/2507.20243.pdf</a></span>   <span><a href='https://github.com/BruthYU/protein-se3' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lang Yu, Zhangyang Gao, Cheng Tan, Qin Chen, Jie Zhou, Liang He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20243">Protein-SE(3): Benchmarking SE(3)-based Generative Models for Protein Structure Design</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>SE(3)-based generative models have shown great promise in protein geometry modeling and effective structure design. However, the field currently lacks a modularized benchmark to enable comprehensive investigation and fair comparison of different methods. In this paper, we propose Protein-SE(3), a new benchmark based on a unified training framework, which comprises protein scaffolding tasks, integrated generative models, high-level mathematical abstraction, and diverse evaluation metrics. Recent advanced generative models designed for protein scaffolding, from multiple perspectives like DDPM (Genie1 and Genie2), Score Matching (FrameDiff and RfDiffusion) and Flow Matching (FoldFlow and FrameFlow) are integrated into our framework. All integrated methods are fairly investigated with the same training dataset and evaluation metrics. Furthermore, we provide a high-level abstraction of the mathematical foundations behind the generative models, enabling fast prototyping of future algorithms without reliance on explicit protein structures. Accordingly, we release the first comprehensive benchmark built upon unified training framework for SE(3)-based protein structure design, which is publicly accessible at https://github.com/BruthYU/protein-se3.<br>
<span id='abs_ch'>中文：本文提出了Protein-SE(3)基准，为基于SE(3)的蛋白质结构生成模型建立了统一训练框架下的模块化评估体系，整合了多种先进方法并提供了高层数学抽象以支持未来算法快速开发。</span><br>
<span id='abs_en'>English: The paper introduces Protein-SE(3), a modular benchmark for fair comparison of SE(3)-based generative models in protein structure design, integrating diverse methods and providing mathematical abstraction for future algorithm development.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1196, <a href='https://arxiv.org/pdf/2507.20174.pdf' target='_blank'>https://arxiv.org/pdf/2507.20174.pdf</a></span>   <span><a href='https://github.com/kong13661/LRR-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Kong, Jinhao Duan, Kaidi Xu, Zhenhua Guo, Xiaofeng Zhu, Xiaoshuang Shi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20174">LRR-Bench: Left, Right or Rotate? Vision-Language models Still Struggle With Spatial Understanding Tasks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Real-world applications, such as autonomous driving and humanoid robot manipulation, require precise spatial perception. However, it remains underexplored how Vision-Language Models (VLMs) recognize spatial relationships and perceive spatial movement. In this work, we introduce a spatial evaluation pipeline and construct a corresponding benchmark. Specifically, we categorize spatial understanding into two main types: absolute spatial understanding, which involves querying the absolute spatial position (e.g., left, right) of an object within an image, and 3D spatial understanding, which includes movement and rotation. Notably, our dataset is entirely synthetic, enabling the generation of test samples at a low cost while also preventing dataset contamination. We conduct experiments on multiple state-of-the-art VLMs and observe that there is significant room for improvement in their spatial understanding abilities. Explicitly, in our experiments, humans achieve near-perfect performance on all tasks, whereas current VLMs attain human-level performance only on the two simplest tasks. For the remaining tasks, the performance of VLMs is distinctly lower than that of humans. In fact, the best-performing Vision-Language Models even achieve near-zero scores on multiple tasks. The dataset and code are available on https://github.com/kong13661/LRR-Bench.<br>
<span id='abs_ch'>Chinese: 本 究引入了一个合成基准来评估视觉语言模型的空间理解能力，发现尽管人类在所有任务中表现出色，但现有模型明显落后，尤其在复杂空间推理方面。</span><br>
<span id='abs_en'>English: This study introduces a synthetic benchmark to evaluate Vision-Language Models' spatial understanding, revealing that while humans excel across tasks, current VLMs significantly lag, especially in complex spatial reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1197, <a href='https://arxiv.org/pdf/2507.20156.pdf' target='_blank'>https://arxiv.org/pdf/2507.20156.pdf</a></span>   <span><a href='https://github.com/daulettoibazar/Compact_VLM_Filter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Daulet Toibazar, Kesen Wang, Sherif Mohamed, Abdulaziz Al-Badawi, Abdulrahman Alfulayt, Pedro J. Moreno
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20156">Trust the Model: Compact VLMs as In-Context Judges for Image-Text Data Quality</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision-language models (VLMs) extend the conventional large language models by integrating visual data, enabling richer multimodal reasoning and significantly broadens the practical applications of AI. However, including visual inputs also brings new challenges in maintaining data quality. Empirical evidence consistently shows that carefully curated and representative training examples often yield superior results compared to simply increasing the quantity of data. Inspired by this observation, we introduce a streamlined data filtration framework that employs a compact VLM, fine-tuned on a high-quality image-caption annotated dataset. This model effectively evaluates and filters potential training samples based on caption and image quality and alignment. Unlike previous approaches, which typically add auxiliary filtration modules on top of existing full-scale VLMs, our method exclusively utilizes the inherent evaluative capability of a purpose-built small VLM. This strategy eliminates the need for extra modules and reduces training overhead. Our lightweight model efficiently filters out inaccurate, noisy web data, improving image-text alignment and caption linguistic fluency. Experimental results show that datasets underwent high-precision filtration using our compact VLM perform on par with, or even surpass, larger and noisier datasets gathered through high-volume web crawling. Thus, our method provides a lightweight yet robust solution for building high-quality vision-language training corpora. \\ \textbf{Availability and implementation:} Our compact VLM filtration model, training data, utility scripts, and Supplementary data (Appendices) are freely available at https://github.com/daulettoibazar/Compact_VLM_Filter.<br>
<span id='abs_ch'>中文: 本 究提出了一种轻量级数据过滤框架，利用紧凑型视觉语言模型过滤网络噪声数据以提升训练质量，在降低计算开销的同时，其过滤后的数据集性能可媲美甚至优于大规模采集数据。</span><br>
<span id='abs_en'>English: This study introduces a lightweight data filtration framework using a compact vision-language model to enhance training data quality by filtering noisy web data, achieving performance comparable to or better than larger datasets while reducing computational overhead.Paperid:218,https://arxiv.org/pdf/2507.20148.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1198, <a href='https://arxiv.org/pdf/2507.20145.pdf' target='_blank'>https://arxiv.org/pdf/2507.20145.pdf</a></span>   <span><a href='https://github.com/wangk0b/Multi_Agentic_QA_Long_Doc.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kesen Wang, Daulet Toibazar, Abdulrahman Alfulayt, Abdulaziz S. Albadawi, Ranya A. Alkahtani, Asma A. Ibrahim, Haneen A. Alhomoud, Sherif Mohamed, Pedro J. Moreno
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20145">Multi-Agent Interactive Question Generation Framework for Long Document Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Document Understanding (DU) in long-contextual scenarios with complex layouts remains a significant challenge in vision-language research. Although Large Vision-Language Models (LVLMs) excel at short-context DU tasks, their performance declines in long-context settings. A key limitation is the scarcity of fine-grained training data, particularly for low-resource languages such as Arabic. Existing state-of-the-art techniques rely heavily on human annotation, which is costly and inefficient. We propose a fully automated, multi-agent interactive framework to generate long-context questions efficiently. Our approach efficiently generates high-quality single- and multi-page questions for extensive English and Arabic documents, covering hundreds of pages across diverse domains. This facilitates the development of LVLMs with enhanced long-context understanding ability. Experimental results in this work have shown that our generated English and Arabic questions (\textbf{AraEngLongBench}) are quite challenging to major open- and close-source LVLMs. The code and data proposed in this work can be found in https://github.com/wangk0b/Multi_Agentic_QA_Long_Doc.git. Sample Question and Answer (QA) pairs and structured system prompts can be found in the Appendix.<br>
<span id='abs_ch'>中文: 本文提出了一种全自动多智能体交互框架，能够高效生成长篇英文和阿拉伯文文档的复杂问答数据，解决了细粒度训练数据稀缺问题，显著提升了大规模视觉语言模型的长文档理解能力。</span><br>
<span id='abs_en'>English: This paper introduces an automated multi-agent framework that generates challenging long-context questions for English and Arabic documents, addressing the scarcity of training data and enhancing large vision-language models' document understanding capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1199, <a href='https://arxiv.org/pdf/2507.20144.pdf' target='_blank'>https://arxiv.org/pdf/2507.20144.pdf</a></span>   <span><a href='https://github.com/liuzy0708/Awesome-OL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zeyi Liu, Songqiao Hu, Pengyu Han, Jiaming Liu, Xiao He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20144">Awesome-OL: An Extensible Toolkit for Online Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In recent years, online learning has attracted increasing attention due to its adaptive capability to process streaming and non-stationary data. To facilitate algorithm development and practical deployment in this area, we introduce Awesome-OL, an extensible Python toolkit tailored for online learning research. Awesome-OL integrates state-of-the-art algorithm, which provides a unified framework for reproducible comparisons, curated benchmark datasets, and multi-modal visualization. Built upon the scikit-multiflow open-source infrastructure, Awesome-OL emphasizes user-friendly interactions without compromising research flexibility or extensibility. The source code is publicly available at: https://github.com/liuzy0708/Awesome-OL.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1200, <a href='https://arxiv.org/pdf/2507.20136.pdf' target='_blank'>https://arxiv.org/pdf/2507.20136.pdf</a></span>   <span><a href='https://github.com/Breezelled/KDD-Cup-2025-Meta-CRAG-MM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Baiyu Chen, Wilson Wongso, Xiaoqian Hu, Yue Tan, Flora Salim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20136">Multi-Stage Verification-Centric Framework for Mitigating Hallucination in Multi-Modal RAG</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents the technical solution developed by team CRUISE for the KDD Cup 2025 Meta Comprehensive RAG Benchmark for Multi-modal, Multi-turn (CRAG-MM) challenge. The challenge aims to address a critical limitation of modern Vision Language Models (VLMs): their propensity to hallucinate, especially when faced with egocentric imagery, long-tail entities, and complex, multi-hop questions. This issue is particularly problematic in real-world applications where users pose fact-seeking queries that demand high factual accuracy across diverse modalities. To tackle this, we propose a robust, multi-stage framework that prioritizes factual accuracy and truthfulness over completeness. Our solution integrates a lightweight query router for efficiency, a query-aware retrieval and summarization pipeline, a dual-pathways generation and a post-hoc verification. This conservative strategy is designed to minimize hallucinations, which incur a severe penalty in the competition's scoring metric. Our approach achieved 3rd place in Task 1, demonstrating the effectiveness of prioritizing answer reliability in complex multi-modal RAG systems. Our implementation is available at https://github.com/Breezelled/KDD-Cup-2025-Meta-CRAG-MM .<br>
<span id='abs_ch'>中文: 本文介绍了CRUISE团队针对KDD Cup 2025多模态对话基准挑战提出的解决方案，该方案通过查询路由、检索、双路径生成和后验验证的多阶段框架，优先保证事实准确性，有效减少了视觉语言模型的幻觉问题，最终获得任务第三名。English: This paper introduces CRUISE team's third-place winning solution for the KDD Cup 2025 CRAG-MM challenge, featuring a multi-stage framework that prioritizes factual accuracy through query routing, retrieval, dual-path generation, and verification to minimize hallucinations in Vision Language Models.Paperid:222,https://arxiv.org/pdf/2507.20104.pdfGitHub</span><br>
<span id='abs_en'>English: This paper introduces CRUISE team's third-place winning solution for the KDD Cup 2025 CRAG-MM challenge, featuring a multi-stage framework that prioritizes factual accuracy through query routing, retrieval, dual-path generation, and verification to minimize hallucinations in Vision Language Models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1201, <a href='https://arxiv.org/pdf/2507.20059.pdf' target='_blank'>https://arxiv.org/pdf/2507.20059.pdf</a></span>   <span><a href='https://github.com/ritaranx/RAG_in_the_Wild' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/ritaranx/RAG_in_the_Wild' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ran Xu, Yuchen Zhuang, Yue Yu, Haoyu Wang, Wenqi Shi, Carl Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20059">RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge Retrieval Augmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved at inference time. While RAG demonstrates strong performance on benchmarks largely derived from general-domain corpora like Wikipedia, its effectiveness under realistic, diverse retrieval scenarios remains underexplored. We evaluated RAG systems using MassiveDS, a large-scale datastore with mixture of knowledge, and identified critical limitations: retrieval mainly benefits smaller models, rerankers add minimal value, and no single retrieval source consistently excels. Moreover, current LLMs struggle to route queries across heterogeneous knowledge sources. These findings highlight the need for adaptive retrieval strategies before deploying RAG in real-world settings. Our code and data can be found at https://github.com/ritaranx/RAG_in_the_Wild.<br>
<span id='abs_ch'>中文: RAG通过外部知识增强大语言模型，但在多 化现实场景中效果有限，如对大模型提升不足、跨异构知识源的查询路由困难，需开发自适应检索策略才能实际应用。</span><br>
<span id='abs_en'>English: RAG enhances LLMs with external knowledge but faces limitations in diverse real-world scenarios, such as limited benefits for larger models and poor query routing across heterogeneous sources, necessitating adaptive strategies before deployment.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1202, <a href='https://arxiv.org/pdf/2507.20016.pdf' target='_blank'>https://arxiv.org/pdf/2507.20016.pdf</a></span>   <span><a href='https://github.com/junkangLiu0/FedSWA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liu junkang, Yuanyuan Liu, Fanhua Shang, Hongying Liu, Jin Liu, Wei Feng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20016">FedSWA: Improving Generalization in Federated Learning with Highly Heterogeneous Data via Momentum-Based Stochastic Controlled Weight Averaging</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>For federated learning (FL) algorithms such as FedSAM, their generalization capability is crucial for real-word applications. In this paper, we revisit the generalization problem in FL and investigate the impact of data heterogeneity on FL generalization. We find that FedSAM usually performs worse than FedAvg in the case of highly heterogeneous data, and thus propose a novel and effective federated learning algorithm with Stochastic Weight Averaging (called \texttt{FedSWA}), which aims to find flatter minima in the setting of highly heterogeneous data. Moreover, we introduce a new momentum-based stochastic controlled weight averaging FL algorithm (\texttt{FedMoSWA}), which is designed to better align local and global models.
  Theoretically, we provide both convergence analysis and generalization bounds for \texttt{FedSWA} and \texttt{FedMoSWA}. We also prove that the optimization and generalization errors of \texttt{FedMoSWA} are smaller than those of their counterparts, including FedSAM and its variants. Empirically, experimental results on CIFAR10/100 and Tiny ImageNet demonstrate the superiority of the proposed algorithms compared to their counterparts. Open source code at: https://github.com/junkangLiu0/FedSWA.<br>
<span id='abs_ch'>中文摘要：本文针对联邦学 中数据高度异构的问题，提出了FedSWA和FedMoSWA两种新算法，通过随机权重平均技术寻找更平坦的最小值，在理论和实验上均证明其比现有方法具有更优的泛化性能。</span><br>
<span id='abs_en'>English Summary: This paper proposes two novel federated learning algorithms, FedSWA and FedMoSWA, which utilize stochastic weight averaging to find flatter minima and improve generalization performance under highly heterogeneous data conditions, demonstrating superior theoretical and empirical results compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1203, <a href='https://arxiv.org/pdf/2507.20008.pdf' target='_blank'>https://arxiv.org/pdf/2507.20008.pdf</a></span>   <span><a href='https://github.com/padmavathi026/Smart-Fare-Prediction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Padmavathi Moorthy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20008">Robust Taxi Fare Prediction Under Noisy Conditions: A Comparative Study of GAT, TimesNet, and XGBoost</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Precise fare prediction is crucial in ride-hailing platforms and urban mobility systems. This study examines three machine learning models-Graph Attention Networks (GAT), XGBoost, and TimesNet to evaluate their predictive capabilities for taxi fares using a real-world dataset comprising over 55 million records. Both raw (noisy) and denoised versions of the dataset are analyzed to assess the impact of data quality on model performance. The study evaluated the models along multiple axes, including predictive accuracy, calibration, uncertainty estimation, out-of-distribution (OOD) robustness, and feature sensitivity. We also explore pre-processing strategies, including KNN imputation, Gaussian noise injection, and autoencoder-based denoising. The study reveals critical differences between classical and deep learning models under realistic conditions, offering practical guidelines for building robust and scalable models in urban fare prediction systems.<br>
<span id='abs_ch'>中文: 本 究通过超过5500万条真实数据评估了GAT、XGBoost和TimesNet三种机器学 模型在出租车费预测中的表现，从准确性、鲁棒性和数据质量多维度对比分析，为城市交通系统提供了实用的建模指导。</span><br>
<span id='abs_en'>English: This study evaluates three machine learning models—GAT, XGBoost, and TimesNet—for taxi fare prediction using a large real-world dataset, analyzing their performance across accuracy, robustness, and data quality while providing practical guidelines for urban mobility systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1204, <a href='https://arxiv.org/pdf/2507.19956.pdf' target='_blank'>https://arxiv.org/pdf/2507.19956.pdf</a></span>   <span><a href='https://github.com/MedARC-AI/algonauts2025' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cesar Kadir Torrico Villanueva, Jiaxin Cindy Tu, Mihir Tripathy, Connor Lane, Rishab Iyer, Paul S. Scotti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19956">Predicting Brain Responses To Natural Movies With Multimodal LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present MedARC's team solution to the Algonauts 2025 challenge. Our pipeline leveraged rich multimodal representations from various state-of-the-art pretrained models across video (V-JEPA2), speech (Whisper), text (Llama 3.2), vision-text (InternVL3), and vision-text-audio (Qwen2.5-Omni). These features extracted from the models were linearly projected to a latent space, temporally aligned to the fMRI time series, and finally mapped to cortical parcels through a lightweight encoder comprising a shared group head plus subject-specific residual heads. We trained hundreds of model variants across hyperparameter settings, validated them on held-out movies and assembled ensembles targeted to each parcel in each subject. Our final submission achieved a mean Pearson's correlation of 0.2085 on the test split of withheld out-of-distribution movies, placing our team in fourth place for the competition. We further discuss a last-minute optimization that would have raised us to second place. Our results highlight how combining features from models trained in different modalities, using a simple architecture consisting of shared-subject and single-subject components, and conducting comprehensive model selection and ensembling improves generalization of encoding models to novel movie stimuli. All code is available on GitHub.<br>
<span id='abs_ch'>中文: MedARC团队在Algonauts 2025挑战赛中通过整合多模态预训练模型特征，采用共享组头与个体残差头的轻量编 器架构，结合大规模超参数调优与集成学 ，在测试集上获得0.2085平均皮尔逊相关系数，最终位列第四。</span><br>
<span id='abs_en'>English: MedARC's fourth-place solution for the Algonauts 2025 challenge combined multimodal features from state-of-the-art models using a lightweight encoder with shared and subject-specific components, achieving strong generalization through extensive hyperparameter tuning and ensemble methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1205, <a href='https://arxiv.org/pdf/2507.19950.pdf' target='_blank'>https://arxiv.org/pdf/2507.19950.pdf</a></span>   <span><a href='https://github.com/zhengcy-lambo/RARE.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengyu Zheng, Jin Huang, Honghua Chen, Mingqiang Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19950">RARE: Refine Any Registration of Pairwise Point Clouds via Zero-Shot Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent research leveraging large-scale pretrained diffusion models has demonstrated the potential of using diffusion features to establish semantic correspondences in images. Inspired by advancements in diffusion-based techniques, we propose a novel zero-shot method for refining point cloud registration algorithms. Our approach leverages correspondences derived from depth images to enhance point feature representations, eliminating the need for a dedicated training dataset. Specifically, we first project the point cloud into depth maps from multiple perspectives and extract implicit knowledge from a pretrained diffusion network as depth diffusion features. These features are then integrated with geometric features obtained from existing methods to establish more accurate correspondences between point clouds. By leveraging these refined correspondences, our approach achieves significantly improved registration accuracy. Extensive experiments demonstrate that our method not only enhances the performance of existing point cloud registration techniques but also exhibits robust generalization capabilities across diverse datasets. Codes are available at https://github.com/zhengcy-lambo/RARE.git.<br>
<span id='abs_ch'>中文: 本文提出一种零 本方法，通过将预训练模型的深度扩散特征与 何特征相结合来优化点云配准， 需训练数据即可显著提升配准精度和泛化能力。</span><br>
<span id='abs_en'>English: This paper introduces a zero-shot method that refines point cloud registration by integrating depth diffusion features from pretrained models with geometric features, achieving enhanced accuracy and generalization without requiring training data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1206, <a href='https://arxiv.org/pdf/2507.19898.pdf' target='_blank'>https://arxiv.org/pdf/2507.19898.pdf</a></span>   <span><a href='https://github.com/LIST-LUXEMBOURG/ts-insight' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Parsa Vares, Ãloi Durant, Jun Pang, Nicolas MÃ©doc, Mohammad Ghoniem
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19898">TS-Insight: Visualizing Thompson Sampling for Verification and XAI</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Thompson Sampling (TS) and its variants are powerful Multi-Armed Bandit algorithms used to balance exploration and exploitation strategies in active learning. Yet, their probabilistic nature often turns them into a "black box", hindering debugging and trust. We introduce TS-Insight, a visual analytics tool explicitly designed to shed light on the internal decision mechanisms of Thompson Sampling-based algorithms, for model developers. It comprises multiple plots, tracing for each arm the evolving posteriors, evidence counts, and sampling outcomes, enabling the verification, diagnosis, and explainability of exploration/exploitation dynamics. This tool aims at fostering trust and facilitating effective debugging and deployment in complex binary decision-making scenarios especially in sensitive domains requiring interpretable decision-making.<br>
<span id='abs_ch'>中文: TS-Insight是一款可视化分析工具，通过多图展示汤普森采 算法的内部决策机制，增强信任并促进在敏感领域中的有效调试。</span><br>
<span id='abs_en'>English: TS-Insight is a visual analytics tool that reveals the internal decision mechanisms of Thompson Sampling algorithms through multiple plots, enhancing trust and enabling effective debugging in sensitive domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1207, <a href='https://arxiv.org/pdf/2507.19891.pdf' target='_blank'>https://arxiv.org/pdf/2507.19891.pdf</a></span>   <span><a href='https://github.com/earl-juanico/rca' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Drandreb Earl O. Juanico, Rowel O. Atienza, Jeffrey Kenneth Go
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19891">Interpretable Open-Vocabulary Referring Object Detection with Reverse Contrast Attention</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We propose Reverse Contrast Attention (RCA), a plug-in method that enhances object localization in vision-language transformers without retraining. RCA reweights final-layer attention by suppressing extremes and amplifying mid-level activations to let semantically relevant but subdued tokens guide predictions. We evaluate it on Open Vocabulary Referring Object Detection (OV-RefOD), introducing FitAP, a confidence-free average precision metric based on IoU and box area. RCA improves FitAP in 11 out of 15 open-source VLMs, with gains up to $+26.6\%$. Effectiveness aligns with attention sharpness and fusion timing; while late-fusion models benefit consistently, models like $\texttt{DeepSeek-VL2}$ also improve, pointing to capacity and disentanglement as key factors. RCA offers both interpretability and performance gains for multimodal transformers. Codes and dataset are available from https://github.com/earl-juanico/rca<br>
<span id='abs_ch'>Chinese Summary: 反向对比注意力（RCA）是一种 需重新训练的即插即用方法，通过重新 权注意力机制增强中层激活来提升视觉语言Transformer中的目 定位能力，在开放词汇参考目 检测任务中最高可实现26.6%的性能提升。</span><br>
<span id='abs_en'>English Summary: Reverse Contrast Attention (RCA) is a plug-in method that improves object localization in vision-language transformers by reweighting attention to enhance mid-level activations, achieving performance gains up to 26.6% on Open Vocabulary Referring Object Detection without requiring retraining.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1208, <a href='https://arxiv.org/pdf/2507.19849.pdf' target='_blank'>https://arxiv.org/pdf/2507.19849.pdf</a></span>   <span><a href='https://github.com/dongguanting/ARPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanting Dong, Hangyu Mao, Kai Ma, Licheng Bao, Yifei Chen, Zhongyuan Wang, Zhongxia Chen, Jiazhen Du, Huiyang Wang, Fuzheng Zhang, Guorui Zhou, Yutao Zhu, Ji-Rong Wen, Zhicheng Dou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19849">Agentic Reinforced Policy Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large-scale reinforcement learning with verifiable rewards (RLVR) has demonstrated its effectiveness in harnessing the potential of large language models (LLMs) for single-turn reasoning tasks. In realistic reasoning scenarios, LLMs can often utilize external tools to assist in task-solving processes. However, current RL algorithms inadequately balance the models' intrinsic long-horizon reasoning capabilities and their proficiency in multi-turn tool interactions. To bridge this gap, we propose Agentic Reinforced Policy Optimization (ARPO), a novel agentic RL algorithm tailored for training multi-turn LLM-based agents. Through preliminary experiments, we observe that LLMs tend to exhibit highly uncertain behavior, characterized by an increase in the entropy distribution of generated tokens, immediately following interactions with external tools. Motivated by this observation, ARPO incorporates an entropy-based adaptive rollout mechanism, dynamically balancing global trajectory sampling and step-level sampling, thereby promoting exploration at steps with high uncertainty after tool usage. By integrating an advantage attribution estimation, ARPO enables LLMs to internalize advantage differences in stepwise tool-use interactions. Our experiments across 13 challenging benchmarks in computational reasoning, knowledge reasoning, and deep search domains demonstrate ARPO's superiority over trajectory-level RL algorithms. Remarkably, ARPO achieves improved performance using only half of the tool-use budget required by existing methods, offering a scalable solution for aligning LLM-based agents with real-time dynamic environments. Our code and datasets are released at https://github.com/dongguanting/ARPO<br>
<span id='abs_ch'>中文: ARPO是一种创新的强化学 算法，通过基于熵的自适应执行机制动态平衡探索与利用，显著提升大语言模型在多轮工具交互中的表现，在多个推理基准测试中以更少的工具使用预算实现了更优性能。</span><br>
<span id='abs_en'>English: ARPO is a novel reinforcement learning algorithm that enhances large language models' performance in multi-turn tool interactions by dynamically balancing exploration and exploitation through an entropy-based adaptive rollout mechanism, achieving superior results with reduced tool-use budgets across various reasoning benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1209, <a href='https://arxiv.org/pdf/2507.19749.pdf' target='_blank'>https://arxiv.org/pdf/2507.19749.pdf</a></span>   <span><a href='https://github.com/HomuraT/ASPBench' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/HomuraT/ASPBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lin Ren, Guohui Xiao, Guilin Qi, Yishuai Geng, Haohan Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19749">Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Answer Set Programming (ASP) is a powerful paradigm for non-monotonic reasoning. Recently, large language models (LLMs) have demonstrated promising capabilities in logical reasoning. Despite this potential, current evaluations of LLM capabilities in ASP are often limited. Existing works normally employ overly simplified ASP programs, do not support negation, disjunction, or multiple answer sets. Furthermore, there is a lack of benchmarks that introduce tasks specifically designed for ASP solving. To bridge this gap, we introduce ASPBench, a comprehensive ASP benchmark, including three ASP specific tasks: ASP entailment, answer set verification, and answer set computation. Our extensive evaluations on ASPBench reveal that while 14 state-of-the-art LLMs, including \emph{deepseek-r1}, \emph{o4-mini}, and \emph{gemini-2.5-flash-thinking}, perform relatively well on the first two simpler tasks, they struggle with answer set computation, which is the core of ASP solving. These findings offer insights into the current limitations of LLMs in ASP solving. This highlights the need for new approaches that integrate symbolic reasoning capabilities more effectively. The code and dataset are available at https://github.com/HomuraT/ASPBench.<br>
<span id='abs_ch'>中文: ASPBench这一新基准测试表明，尽管大语言模型在ASP蕴含和验证等简单任务上表现尚可，但在 心的答案集计算任务上存在明显不足，凸显了 强符号推理能力融合的必要性。</span><br>
<span id='abs_en'>English: ASPBench, a new benchmark for Answer Set Programming, reveals that while large language models handle simpler tasks like entailment and verification, they struggle with the core task of answer set computation, highlighting the need for better integration of symbolic reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1210, <a href='https://arxiv.org/pdf/2507.19737.pdf' target='_blank'>https://arxiv.org/pdf/2507.19737.pdf</a></span>   <span><a href='https://github.com/tsinghua-fib-lab/DisasterMobLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinzhou Tang, Huandong Wang, Xiaochen Fan, Yong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19737">Predicting Human Mobility in Disasters via LLM-Enhanced Cross-City Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The vulnerability of cities to natural disasters has increased with urbanization and climate change, making it more important to predict human mobility in the disaster scenarios for downstream tasks including location-based early disaster warning and pre-allocating rescue resources, etc. However, existing human mobility prediction models are mainly designed for normal scenarios, and fail to adapt to disaster scenarios due to the shift of human mobility patterns under disaster. To address this issue, we introduce \textbf{DisasterMobLLM}, a mobility prediction framework for disaster scenarios that can be integrated into existing deep mobility prediction methods by leveraging LLMs to model the mobility intention and transferring the common knowledge of how different disasters affect mobility intentions between cities. This framework utilizes a RAG-Enhanced Intention Predictor to forecast the next intention, refines it with an LLM-based Intention Refiner, and then maps the intention to an exact location using an Intention-Modulated Location Predictor. Extensive experiments illustrate that DisasterMobLLM can achieve a 32.8\% improvement in terms of Acc@1 and a 35.0\% improvement in terms of the F1-score of predicting immobility compared to the baselines. The code is available at https://github.com/tsinghua-fib-lab/DisasterMobLLM.<br>
<span id='abs_ch'>中文总结：DisasterMobLLM是一种创新框架，通过利用大语言模型模拟移动意图并迁移跨城市灾害知识，显著提升了自然灾害场景下人类移动预测的准确性。English Summary: DisasterMobLLM is a novel framework that leverages large language models to significantly improve human mobility prediction during natural disasters by modeling mobility intentions and transferring cross-city disaster knowledge.Paperid:252,https://arxiv.org/pdf/2507.19730.pdfGitHub</span><br>
<span id='abs_en'>English Summary: DisasterMobLLM is a novel framework that leverages large language models to significantly improve human mobility prediction during natural disasters by modeling mobility intentions and transferring cross-city disaster knowledge.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1211, <a href='https://arxiv.org/pdf/2507.19730.pdf' target='_blank'>https://arxiv.org/pdf/2507.19730.pdf</a></span>   <span><a href='https://github.com/Ruchtech/uQRPCA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liyang Wang, Shiqian Wu, Shun Fang, Qile Zhu, Jiaxin Wu, Sos Again
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19730">Quaternion-Based Robust PCA for Efficient Moving Target Detection and Background Recovery in Color Videos</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Moving target detection is a challenging computer vision task aimed at generating accurate segmentation maps in diverse in-the-wild color videos captured by static cameras. If backgrounds and targets can be simultaneously extracted and recombined, such synthetic data can significantly enrich annotated in-the-wild datasets and enhance the generalization ability of deep models. Quaternion-based RPCA (QRPCA) is a promising unsupervised paradigm for color image processing. However, in color video processing, Quaternion Singular Value Decomposition (QSVD) incurs high computational costs, and rank-1 quaternion matrix fails to yield rank-1 color channels. In this paper, we reduce the computational complexity of QSVD to o(1) by utilizing a quaternion Riemannian manifold. Furthermor, we propose the universal QRPCA (uQRPCA) framework, which achieves a balance in simultaneously segmenting targets and recovering backgrounds from color videos. Moreover, we expand to uQRPCA+ by introducing the Color Rank-1 Batch (CR1B) method to further process and obtain the ideal low-rank background across color channels. Experiments demonstrate our uQRPCA+ achieves State Of The Art (SOTA) performance on moving target detection and background recovery tasks compared to existing open-source methods. Our implementation is publicly available on GitHub at https://github.com/Ruchtech/uQRPCA<br>
<span id='abs_ch'>中文: 本文提出的uQRPCA+框架通过四元数黎曼流形降低计算复杂度，并采用颜色秩一批处理方法，在彩色视频运动目 检测和背景恢复任务中实现了最先进的性能。</span><br>
<span id='abs_en'>English: This paper introduces the uQRPCA+ framework, which leverages quaternion Riemannian manifolds to reduce computational complexity and employs the Color Rank-1 Batch method to achieve state-of-the-art performance in moving target detection and background recovery from color videos.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1212, <a href='https://arxiv.org/pdf/2507.19694.pdf' target='_blank'>https://arxiv.org/pdf/2507.19694.pdf</a></span>   <span><a href='https://github.com/farukalpay/ordinal-folding-index/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Faruk Alpay, Hamdi Alakkad, Bugra Kilictas, Taylan Alpay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19694">Ultracoarse Equilibria and Ordinal-Folding Dynamics in Operator-Algebraic Models of Infinite Multi-Agent Games</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We develop an operator algebraic framework for infinite games with a continuum of agents and prove that regret based learning dynamics governed by a noncommutative continuity equation converge to a unique quantal response equilibrium under mild regularity assumptions. The framework unifies functional analysis, coarse geometry and game theory by assigning to every game a von Neumann algebra that represents collective strategy evolution. A reflective regret operator within this algebra drives the flow of strategy distributions and its fixed point characterises equilibrium. We introduce the ordinal folding index, a computable ordinal valued metric that measures the self referential depth of the dynamics, and show that it bounds the transfinite time needed for convergence, collapsing to zero on coarsely amenable networks. The theory yields new invariant subalgebra rigidity results, establishes existence and uniqueness of envy free and maximin share allocations in continuum economies, and links analytic properties of regret flows with empirical stability phenomena in large language models. These contributions supply a rigorous mathematical foundation for large scale multi agent systems and demonstrate the utility of ordinal metrics for equilibrium selection.<br>
<span id='abs_ch'>中文摘要：本 究提出了一个针对连续体智能体 限博弈的算子代数框架，证明了基于遗憾的学 动态会收敛到唯一的量子响应均衡，并为大规模多智能体系统建立了严 的数学基础。</span><br>
<span id='abs_en'>English Summary: This study introduces an operator algebraic framework for infinite games with a continuum of agents, demonstrating that regret-based learning converges to a unique quantal response equilibrium and providing new mathematical foundations for large-scale multi-agent systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1213, <a href='https://arxiv.org/pdf/2507.19543.pdf' target='_blank'>https://arxiv.org/pdf/2507.19543.pdf</a></span>   <span><a href='https://github.com/emiliamazzo/WARPP/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Emilia Mazzolenis, Ruirui Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19543">Agent WARPP: Workflow Adherence via Runtime Parallel Personalization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) are increasingly applied in task-oriented dialogue (TOD) systems but often struggle with long, conditional workflows that involve external tool calls and depend on user-specific information. We present Workflow Adherence via Runtime Parallel Personalization, or WARPP, a training-free, modular framework that combines multi-agent orchestration with runtime personalization to improve workflow adherence in LLM-based systems. By dynamically pruning conditional branches based on user attributes, the framework reduces reasoning overhead and narrows tool selection at runtime. WARPP deploys a parallelized architecture where a dedicated Personalizer agent operates alongside modular, domain-specific agents to dynamically tailor execution paths in real time. The framework is evaluated across five representative user intents of varying complexity within three domains: banking, flights, and healthcare. Our evaluation leverages synthetic datasets and LLM-powered simulated users to test scenarios with conditional dependencies. Our results demonstrate that WARPP outperforms both the non-personalized method and the ReAct baseline, achieving increasingly larger gains in parameter fidelity and tool accuracy as intent complexity grows, while also reducing average token usage, without any additional training.<br>
<span id='abs_ch'>中文摘要：WARPP框架通过多智能体协同与实时个性化相结合， 需额外训练即可动态优化基于大语言模型的任务型对话系统的工作流执行效果。</span><br>
<span id='abs_en'>English summary: The WARPP framework enhances LLM-based task-oriented dialogue systems by integrating multi-agent orchestration with runtime personalization, dynamically optimizing workflow execution without requiring additional training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1214, <a href='https://arxiv.org/pdf/2507.19525.pdf' target='_blank'>https://arxiv.org/pdf/2507.19525.pdf</a></span>   <span><a href='https://github.com/cure-lab/MMCircuitEval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenchen Zhao, Zhengyuan Shi, Xiangyu Wen, Chengjie Liu, Yi Liu, Yunhao Zhou, Yuxiang Zhao, Hefei Feng, Yinan Zhu, Gwok-Waa Wan, Xin Cheng, Weiyu Chen, Yongqi Fu, Chujie Chen, Chenhao Xue, Guangyu Sun, Ying Wang, Yibo Lin, Jun Yang, Ning Xu, Xi Wang, Qiang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19525">MMCircuitEval: A Comprehensive Multimodal Circuit-Focused Benchmark for Evaluating LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The emergence of multimodal large language models (MLLMs) presents promising opportunities for automation and enhancement in Electronic Design Automation (EDA). However, comprehensively evaluating these models in circuit design remains challenging due to the narrow scope of existing benchmarks. To bridge this gap, we introduce MMCircuitEval, the first multimodal benchmark specifically designed to assess MLLM performance comprehensively across diverse EDA tasks. MMCircuitEval comprises 3614 meticulously curated question-answer (QA) pairs spanning digital and analog circuits across critical EDA stages - ranging from general knowledge and specifications to front-end and back-end design. Derived from textbooks, technical question banks, datasheets, and real-world documentation, each QA pair undergoes rigorous expert review for accuracy and relevance. Our benchmark uniquely categorizes questions by design stage, circuit type, tested abilities (knowledge, comprehension, reasoning, computation), and difficulty level, enabling detailed analysis of model capabilities and limitations. Extensive evaluations reveal significant performance gaps among existing LLMs, particularly in back-end design and complex computations, highlighting the critical need for targeted training datasets and modeling approaches. MMCircuitEval provides a foundational resource for advancing MLLMs in EDA, facilitating their integration into real-world circuit design workflows. Our benchmark is available at https://github.com/cure-lab/MMCircuitEval.<br>
<span id='abs_ch'>中文：MMCircuitEval基准的推出旨在全面评估多模态大语言模型在电子设计自动化中的表现，揭示了显著的性能差距，并为提升这些模型在电路设计中的应用提供了基础资源。English: The MMCircuitEval benchmark is introduced to comprehensively evaluate multimodal large language models in Electronic Design Automation, revealing significant performance gaps and providing a foundational resource for advancing these models in circuit design.Paperid:257,https://arxiv.org/pdf/2507.19523.pdfGitHub</span><br>
<span id='abs_en'>English: The MMCircuitEval benchmark is introduced to comprehensively evaluate multimodal large language models in Electronic Design Automation, revealing significant performance gaps and providing a foundational resource for advancing these models in circuit design.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1215, <a href='https://arxiv.org/pdf/2507.19523.pdf' target='_blank'>https://arxiv.org/pdf/2507.19523.pdf</a></span>   <span><a href='https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyu Su, Xiner Li, Yuchao Lin, Ziqian Xie, Degui Zhi, Shuiwang Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19523">Language Models for Controllable DNA Sequence Design</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We consider controllable DNA sequence design, where sequences are generated by conditioning on specific biological properties. While language models (LMs) such as GPT and BERT have achieved remarkable success in natural language generation, their application to DNA sequence generation remains largely underexplored. In this work, we introduce ATGC-Gen, an Automated Transformer Generator for Controllable Generation, which leverages cross-modal encoding to integrate diverse biological signals. ATGC-Gen is instantiated with both decoder-only and encoder-only transformer architectures, allowing flexible training and generation under either autoregressive or masked recovery objectives. We evaluate ATGC-Gen on representative tasks including promoter and enhancer sequence design, and further introduce a new dataset based on ChIP-Seq experiments for modeling protein binding specificity. Our experiments demonstrate that ATGC-Gen can generate fluent, diverse, and biologically relevant sequences aligned with the desired properties. Compared to prior methods, our model achieves notable improvements in controllability and functional relevance, highlighting the potential of language models in advancing programmable genomic design. The source code is released at (https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen).<br>
<br>
<div id='section'>Paperid: <span id='pid'>1216, <a href='https://arxiv.org/pdf/2507.19492.pdf' target='_blank'>https://arxiv.org/pdf/2507.19492.pdf</a></span>   <span><a href='https://github.com/SD122025/ChartGen/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jovana Kondic, Pengyuan Li, Dhiraj Joshi, Zexue He, Shafiq Abedin, Jennifer Sun, Ben Wiesel, Eli Schwartz, Ahmed Nassar, Bo Wu, Assaf Arbelle, Aude Oliva, Dan Gutfreund, Leonid Karlinsky, Rogerio Feris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19492">ChartGen: Scaling Chart Understanding Via Code-Guided Synthetic Chart Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chart-to-code reconstruction -- the task of recovering executable plotting scripts from chart images -- provides important insights into a model's ability to ground data visualizations in precise, machine-readable form. Yet many existing multimodal benchmarks largely focus primarily on answering questions about charts or summarizing them. To bridge this gap, we present ChartGen, a fully-automated pipeline for code-guided synthetic chart generation. Starting from seed chart images, ChartGen (i) prompts a vision-language model (VLM) to reconstruct each image into a python script, and (ii) iteratively augments that script with a code-oriented large language model (LLM). Using ChartGen, we create 222.5K unique chart-image code pairs from 13K seed chart images, and present an open-source synthetic chart dataset covering 27 chart types, 11 plotting libraries, and multiple data modalities (image, code, text, CSV, DocTags). From this corpus, we curate a held-out chart-to-code evaluation subset of 4.3K chart image-code pairs, and evaluate six open-weight VLMs (3B - 26B parameters), highlighting substantial room for progress. We release the pipeline, prompts, and the dataset to help accelerate efforts towards robust chart understanding and vision-conditioned code generation: https://github.com/SD122025/ChartGen/<br>
<span id='abs_ch'>中文：本文提出了ChartGen，一个自动化生成图表-代 对的流程，旨在填补多模态基准在图表到代 重建任务上的空白，通过创建包含多种图表类型和绘图库的数据集，并评估了多个视觉语言模型的性能。</span><br>
<span id='abs_en'>English: This paper introduces ChartGen, an automated pipeline that generates synthetic chart-image code pairs to advance chart-to-code reconstruction, addressing a gap in multimodal benchmarks by creating a comprehensive dataset and evaluating several vision-language models.Paperid:259,https://arxiv.org/pdf/2507.19478.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1217, <a href='https://arxiv.org/pdf/2507.19304.pdf' target='_blank'>https://arxiv.org/pdf/2507.19304.pdf</a></span>   <span><a href='https://github.com/IbrahimUWA/MuStD.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Ibrahim, Naveed Akhtar, Haitian Wang, Saeed Anwar, Ajmal Mian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19304">Multistream Network for LiDAR and Camera-based 3D Object Detection in Outdoor Scenes</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fusion of LiDAR and RGB data has the potential to enhance outdoor 3D object detection accuracy. To address real-world challenges in outdoor 3D object detection, fusion of LiDAR and RGB input has started gaining traction. However, effective integration of these modalities for precise object detection task still remains a largely open problem. To address that, we propose a MultiStream Detection (MuStD) network, that meticulously extracts task-relevant information from both data modalities. The network follows a three-stream structure. Its LiDAR-PillarNet stream extracts sparse 2D pillar features from the LiDAR input while the LiDAR-Height Compression stream computes Bird's-Eye View features. An additional 3D Multimodal stream combines RGB and LiDAR features using UV mapping and polar coordinate indexing. Eventually, the features containing comprehensive spatial, textural and geometric information are carefully fused and fed to a detection head for 3D object detection. Our extensive evaluation on the challenging KITTI Object Detection Benchmark using public testing server at https://www.cvlibs.net/datasets/kitti/eval_object_detail.php?&result=d162ec699d6992040e34314d19ab7f5c217075e0 establishes the efficacy of our method by achieving new state-of-the-art or highly competitive results in different categories while remaining among the most efficient methods. Our code will be released through MuStD GitHub repository at https://github.com/IbrahimUWA/MuStD.git<br>
<span id='abs_ch'>Chinese: 提出的多流检测（MuStD）网络通过三流架构有效融合激光雷达与RGB数据，在KITTI基准测试中实现了最优的三维物体检测性能，同时保持高效运行。</span><br>
<span id='abs_en'>English: The proposed MultiStream Detection (MuStD) network effectively fuses LiDAR and RGB data through a three-stream architecture to achieve state-of-the-art 3D object detection performance on the KITTI benchmark while maintaining high efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1218, <a href='https://arxiv.org/pdf/2507.19247.pdf' target='_blank'>https://arxiv.org/pdf/2507.19247.pdf</a></span>   <span><a href='https://github.com/asiresearch/lm-theory' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19247">A Markov Categorical Framework for Language Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Autoregressive language models achieve remarkable performance, yet a unified theory explaining their internal mechanisms, how training shapes their representations, and enables complex behaviors, remains elusive. We introduce a new analytical framework that models the single-step generation process as a composition of information-processing stages using the language of Markov categories. This compositional perspective provides a unified mathematical language to connect three critical aspects of language modeling that are typically studied in isolation: the training objective, the geometry of the learned representation space, and practical model capabilities. First, our framework provides a precise information-theoretic rationale for the success of multi-token prediction methods like speculative decoding, quantifying the information surplus a model's hidden state contains about tokens beyond the immediate next one. Second, we clarify how the standard negative log-likelihood (NLL) objective compels the model to learn not just the next word, but also the data's intrinsic conditional uncertainty, a process we formalize using categorical entropy. Our central result shows that, under a linear-softmax head with bounded features, minimizing NLL induces spectral alignment: the learned representation space aligns with the eigenspectrum of a predictive similarity operator. This work presents a powerful new lens for understanding how information flows through a model and how the training objective shapes its internal geometry.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1219, <a href='https://arxiv.org/pdf/2507.19234.pdf' target='_blank'>https://arxiv.org/pdf/2507.19234.pdf</a></span>   <span><a href='https://github.com/GeminiLight/virne' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianfu Wang, Liwei Deng, Xi Chen, Junyang Wang, Huiguo He, Leilei Ding, Wei Wu, Qilin Fan, Hui Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19234">Virne: A Comprehensive Benchmark for Deep RL-based Network Resource Allocation in NFV</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Resource allocation (RA) is critical to efficient service deployment in Network Function Virtualization (NFV), a transformative networking paradigm. Recently, deep Reinforcement Learning (RL)-based methods have been showing promising potential to address this complexity. However, the lack of a systematic benchmarking framework and thorough analysis hinders the exploration of emerging networks and the development of more robust algorithms while causing inconsistent evaluation. In this paper, we introduce Virne, a comprehensive benchmarking framework for the NFV-RA problem, with a focus on supporting deep RL-based methods. Virne provides customizable simulations for diverse network scenarios, including cloud, edge, and 5G environments. It also features a modular and extensible implementation pipeline that supports over 30 methods of various types, and includes practical evaluation perspectives beyond effectiveness, such as scalability, generalization, and scalability. Furthermore, we conduct in-depth analysis through extensive experiments to provide valuable insights into performance trade-offs for efficient implementation and offer actionable guidance for future research directions. Overall, with its diverse simulations, rich implementations, and extensive evaluation capabilities, Virne could serve as a comprehensive benchmark for advancing NFV-RA methods and deep RL applications. The code is publicly available at https://github.com/GeminiLight/virne.<br>
<span id='abs_ch'>中文: 本文介绍了Virne，一个全面的NFV资源分配基准框架，支持深度强化学 方法，提供可定制模拟和广泛评估功能，以推动该领域的 究进展。</span><br>
<span id='abs_en'>English: This paper introduces Virne, a comprehensive benchmarking framework for NFV resource allocation that supports deep reinforcement learning methods with customizable simulations and extensive evaluation capabilities to advance research in the field.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1220, <a href='https://arxiv.org/pdf/2507.19201.pdf' target='_blank'>https://arxiv.org/pdf/2507.19201.pdf</a></span>   <span><a href='https://github.com/lixinHUST/Gated-Conditional-Diffusion-Model/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Li, Kaixiang Yang, Qiang Li, Zhiwei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19201">Joint Holistic and Lesion Controllable Mammogram Synthesis via Gated Conditional Diffusion Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Mammography is the most commonly used imaging modality for breast cancer screening, driving an increasing demand for deep-learning techniques to support large-scale analysis. However, the development of accurate and robust methods is often limited by insufficient data availability and a lack of diversity in lesion characteristics. While generative models offer a promising solution for data synthesis, current approaches often fail to adequately emphasize lesion-specific features and their relationships with surrounding tissues. In this paper, we propose Gated Conditional Diffusion Model (GCDM), a novel framework designed to jointly synthesize holistic mammogram images and localized lesions. GCDM is built upon a latent denoising diffusion framework, where the noised latent image is concatenated with a soft mask embedding that represents breast, lesion, and their transitional regions, ensuring anatomical coherence between them during the denoising process. To further emphasize lesion-specific features, GCDM incorporates a gated conditioning branch that guides the denoising process by dynamically selecting and fusing the most relevant radiomic and geometric properties of lesions, effectively capturing their interplay. Experimental results demonstrate that GCDM achieves precise control over small lesion areas while enhancing the realism and diversity of synthesized mammograms. These advancements position GCDM as a promising tool for clinical applications in mammogram synthesis. Our code is available at https://github.com/lixinHUST/Gated-Conditional-Diffusion-Model/<br>
<span id='abs_ch'>Chinese Summary: 门控条件扩散模型（GCDM）是一种创新框架，通过潜在去噪过程结合软掩 嵌入和门控调节分支，能够同时合成完整的乳腺X光图像和局部病灶，有效增强病灶特征与周围组织的解剖一致性。</span><br>
<span id='abs_en'>English Summary: The Gated Conditional Diffusion Model (GCDM) is a novel framework that synthesizes both complete mammogram images and localized lesions by using a latent denoising process with soft mask embeddings and a gated conditioning branch to enhance lesion-specific features and anatomical coherence.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1221, <a href='https://arxiv.org/pdf/2507.19132.pdf' target='_blank'>https://arxiv.org/pdf/2507.19132.pdf</a></span>   <span><a href='https://github.com/OS-Copilot/OS-Map' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuetian Chen, Yinghao Chen, Xinfeng Yuan, Zhuo Peng, Lu Chen, Yuekeng Li, Zhoujia Zhang, Yingqian Huang, Leyan Huang, Jiaqing Liang, Tianbao Xie, Zhiyong Wu, Qiushi Sun, Biqing Qi, Bowen Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19132">OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Computer-using agents have shown strong potential to boost human productivity and enable new application forms across platforms. While recent advances have led to usable applications, existing benchmarks fail to account for the internal task heterogeneity and the corresponding agent capabilities, as well as their alignment with actual user demands-hindering both targeted capability development and the reliable transition of research progress into practical deployment. To bridge the gap, we present OS-MAP, a benchmark for daily computer-using automation that organizes its 416 realistic tasks across 15 applications along two key dimensions: a five-level taxonomy of automation and a generalization scope derived from a real-world user demand hierarchy. To enable fine-grained analysis of required capabilities and alignment with real-world scenarios, OS-MAP evaluates agents along two dimensions: automation level across a five-level taxonomy, and generalization scope across a demand hierarchy. This design captures varying levels of required agent autonomy and generalization, forming a performance-generalization evaluation matrix for structured and comprehensive assessment. Experiments show that even State-of-the-Art agents with VLM backbones struggle with higher-level tasks involving perception, reasoning, and coordination-highlighting the need for a deeper understanding of current strengths and limitations to drive the future progress in computer-using agents research and deployment. All code, environments, baselines, and data are publicly available at https://github.com/OS-Copilot/OS-Map.<br>
<span id='abs_ch'>中文: 计算机使用代理虽能提升生产力，但现有基准未能匹配实际需求， 此我们推出OS-MAP基准，通过自动化分级和泛化范围评估代理能力，揭示其在高阶任务中的不足，以推动 究与应用发展。</span><br>
<span id='abs_en'>English: Computer-using agents show promise for productivity but face challenges in aligning capabilities with real-world tasks, prompting the introduction of OS-MAP, a benchmark that evaluates agents across automation levels and generalization scopes to address these gaps and guide future development.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1222, <a href='https://arxiv.org/pdf/2507.19089.pdf' target='_blank'>https://arxiv.org/pdf/2507.19089.pdf</a></span>   <span><a href='https://github.com/ShuhaoLii/RoadDiff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuhao Li, Weidong Yang, Yue Cui, Xiaoxing Liu, Lingkai Meng, Lipeng Ma, Fan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.19089">Fine-Grained Traffic Inference from Road to Lane via Spatio-Temporal Graph Node Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fine-grained traffic management and prediction are fundamental to key applications such as autonomous driving, lane change guidance, and traffic signal control. However, obtaining lane-level traffic data has become a critical bottleneck for data-driven models due to limitations in the types and number of sensors and issues with the accuracy of tracking algorithms. To address this, we propose the Fine-grained Road Traffic Inference (FRTI) task, which aims to generate more detailed lane-level traffic information using limited road data, providing a more energy-efficient and cost-effective solution for precise traffic management. This task is abstracted as the first scene of the spatio-temporal graph node generation problem. We designed a two-stage framework--RoadDiff--to solve the FRTI task. solve the FRTI task. This framework leverages the Road-Lane Correlation Autoencoder-Decoder and the Lane Diffusion Module to fully utilize the limited spatio-temporal dependencies and distribution relationships of road data to accurately infer fine-grained lane traffic states. Based on existing research, we designed several baseline models with the potential to solve the FRTI task and conducted extensive experiments on six datasets representing different road conditions to validate the effectiveness of the RoadDiff model in addressing the FRTI task. The relevant datasets and code are available at https://github.com/ShuhaoLii/RoadDiff.<br>
<span id='abs_ch'>Chinese: 本 究提出了细粒度道路交通推断（FRTI）任务，并设计了两阶段RoadDiff框架，通过利用有限道路数据高效生成详细车道级交通信息，在六个数据集上的大量实验验证了其有效性。</span><br>
<span id='abs_en'>English: The study introduces the Fine-grained Road Traffic Inference (FRTI) task and proposes a two-stage RoadDiff framework to generate detailed lane-level traffic data efficiently using limited road information, validated through extensive experiments on six datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1223, <a href='https://arxiv.org/pdf/2507.18929.pdf' target='_blank'>https://arxiv.org/pdf/2507.18929.pdf</a></span>   <span><a href='https://github.com/cccccj-03/MGHFT_ACMMM2025' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Chen, Yuxuan Hu, Haifeng Lu, Wei Wang, Min Yang, Chengming Li, Xiping Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18929">MGHFT: Multi-Granularity Hierarchical Fusion Transformer for Cross-Modal Sticker Emotion Recognition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although pre-trained visual models with text have demonstrated strong capabilities in visual feature extraction, sticker emotion understanding remains challenging due to its reliance on multi-view information, such as background knowledge and stylistic cues. To address this, we propose a novel multi-granularity hierarchical fusion transformer (MGHFT), with a multi-view sticker interpreter based on Multimodal Large Language Models. Specifically, inspired by the human ability to interpret sticker emotions from multiple views, we first use Multimodal Large Language Models to interpret stickers by providing rich textual context via multi-view descriptions. Then, we design a hierarchical fusion strategy to fuse the textual context into visual understanding, which builds upon a pyramid visual transformer to extract both global and local sticker features at multiple stages. Through contrastive learning and attention mechanisms, textual features are injected at different stages of the visual backbone, enhancing the fusion of global- and local-granularity visual semantics with textual guidance. Finally, we introduce a text-guided fusion attention mechanism to effectively integrate the overall multimodal features, enhancing semantic understanding. Extensive experiments on 2 public sticker emotion datasets demonstrate that MGHFT significantly outperforms existing sticker emotion recognition approaches, achieving higher accuracy and more fine-grained emotion recognition. Compared to the best pre-trained visual models, our MGHFT also obtains an obvious improvement, 5.4% on F1 and 4.0% on accuracy. The code is released at https://github.com/cccccj-03/MGHFT_ACMMM2025.<br>
<span id='abs_ch'>中文摘要：提出的多粒度分层融合Transformer（MGHFT）通过多模态大语言模型实现表情包的多视角解读，并采用分层策略将文本信息与多阶段视觉特征融合，在表情情感识别任务中显著优于现有方法。</span><br>
<span id='abs_en'>English Summary: The proposed Multi-Granularity Hierarchical Fusion Transformer (MGHFT) leverages multimodal large language models to interpret stickers through multi-view descriptions and hierarchically fuses textual context with visual features, achieving superior performance in sticker emotion recognition compared to existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1224, <a href='https://arxiv.org/pdf/2507.18897.pdf' target='_blank'>https://arxiv.org/pdf/2507.18897.pdf</a></span>   <span><a href='https://github.com/opendilab/HH-Codec' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongkun Xue, Yazhe Niu, Shuai Hu, Zixin Yin, Yongqiang Yao, Jing Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18897">HH-Codec: High Compression High-fidelity Discrete Neural Codec for Spoken Language Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Discrete speech tokenization is a fundamental component in speech codecs. However, in large-scale speech-to-speech systems, the complexity of parallel streams from multiple quantizers and the computational cost of high-time-dimensional codecs pose significant challenges. In this paper, we introduce HH-Codec, a neural codec that achieves extreme compression at 24 tokens per second for 24 kHz audio while relying on single-quantizer inference. Our approach involves a carefully designed Vector Quantization space for Spoken Language Modeling, optimizing compression efficiency while minimizing information loss. Building on this, we propose an asymmetric encoder-decoder architecture (Audio-VQ-Mel-Audio) that leverages dual supervision and progressive training to enhance reconstruction stability and fidelity. HH-Codec achieves state-of-the-art performance in speech reconstruction with an ultra-low bandwidth of 0.3 kbps. We further evaluate its effectiveness in codebook utilization and generative model adaptation, with extensive ablations validating the necessity of each module. HH-Codec is available at https://github.com/opendilab/HH-Codec.<br>
<span id='abs_ch'>Chinese: HH-Codec 提出了一种采用单量化器设计和非对称编 器-解 器架构的神经语音编解 器，以每秒24个令牌和0.3 kbps的超低带宽实现了最先进的语音压缩与高保真重建。</span><br>
<span id='abs_en'>English: HH-Codec introduces a neural speech codec using a single-quantizer design and an asymmetric encoder-decoder architecture, achieving state-of-the-art compression at 24 tokens per second with 0.3 kbps bandwidth for high-fidelity speech reconstruction.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1225, <a href='https://arxiv.org/pdf/2507.18848.pdf' target='_blank'>https://arxiv.org/pdf/2507.18848.pdf</a></span>   <span><a href='https://github.com/ubc-tea/PTCMIL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Beidi Zhao, SangMook Kim, Hao Chen, Chen Zhou, Zu-hua Gao, Gang Wang, Xiaoxiao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18848">PTCMIL: Multiple Instance Learning via Prompt Token Clustering for Whole Slide Image Analysis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multiple Instance Learning (MIL) has advanced WSI analysis but struggles with the complexity and heterogeneity of WSIs. Existing MIL methods face challenges in aggregating diverse patch information into robust WSI representations. While ViTs and clustering-based approaches show promise, they are computationally intensive and fail to capture task-specific and slide-specific variability. To address these limitations, we propose PTCMIL, a novel Prompt Token Clustering-based ViT for MIL aggregation. By introducing learnable prompt tokens into the ViT backbone, PTCMIL unifies clustering and prediction tasks in an end-to-end manner. It dynamically aligns clustering with downstream tasks, using projection-based clustering tailored to each WSI, reducing complexity while preserving patch heterogeneity. Through token merging and prototype-based pooling, PTCMIL efficiently captures task-relevant patterns. Extensive experiments on eight datasets demonstrate its superior performance in classification and survival analysis tasks, outperforming state-of-the-art methods. Systematic ablation studies confirm its robustness and strong interpretability. The code is released at https://github.com/ubc-tea/PTCMIL.<br>
<span id='abs_ch'>中文: PTCMIL提出了一种基于提示令牌聚类的视觉变换器，通过动态对齐聚类与下游任务，在降低计算复杂度的同时有效捕获任务相关模式，在WSI分析中展现出卓越性能。</span><br>
<span id='abs_en'>English: PTCMIL introduces a prompt token clustering-based Vision Transformer that dynamically aligns clustering with downstream tasks, achieving superior performance in WSI analysis through efficient task-relevant pattern capture and reduced computational complexity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1226, <a href='https://arxiv.org/pdf/2507.18838.pdf' target='_blank'>https://arxiv.org/pdf/2507.18838.pdf</a></span>   <span><a href='https://github.com/biomedia-mira/flow-ssn' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabio De Sousa Ribeiro, Omar Todd, Charles Jones, Avinash Kori, Raghav Mehta, Ben Glocker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18838">Flow Stochastic Segmentation Networks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce the Flow Stochastic Segmentation Network (Flow-SSN), a generative segmentation model family featuring discrete-time autoregressive and modern continuous-time flow variants. We prove fundamental limitations of the low-rank parameterisation of previous methods and show that Flow-SSNs can estimate arbitrarily high-rank pixel-wise covariances without assuming the rank or storing the distributional parameters. Flow-SSNs are also more efficient to sample from than standard diffusion-based segmentation models, thanks to most of the model capacity being allocated to learning the base distribution of the flow, constituting an expressive prior. We apply Flow-SSNs to challenging medical imaging benchmarks and achieve state-of-the-art results. Code available: https://github.com/biomedia-mira/flow-ssn.<br>
<span id='abs_ch'>中文摘要：Flow-SSN模型系列通过离散和连续时间变体克服了先前方法的秩限制，同时借助表达能力强的先验分布实现高效采 ，在医学影像基准测试中取得了最先进的结果。</span><br>
<span id='abs_en'>English Summary: The Flow-SSN model family introduces generative segmentation with discrete and continuous-time variants that overcome previous methods' rank limitations while enabling efficient sampling through an expressive prior, achieving state-of-the-art results on medical imaging benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1227, <a href='https://arxiv.org/pdf/2507.18742.pdf' target='_blank'>https://arxiv.org/pdf/2507.18742.pdf</a></span>   <span><a href='https://github.com/vicgalle/specification-self-correction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>VÃ­ctor Gallego
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18742">Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Language models (LMs) are susceptible to in-context reward hacking, where they exploit flaws in tainted or faulty written specifications or rubrics to achieve high scores without fulfilling the user's true intent. We introduce Specification Self-Correction (SSC), a novel, test-time framework that enables an LM to identify and correct flaws within its own guiding specification. SSC employs a multi-step inference process where the model first generates a response based on a potentially tainted specification, critiques its output, and then revises the specification itself to remove the exploitable loophole. A final, more robust response is then generated using this self-corrected specification. Across experiments spanning creative writing and agentic coding tasks with several LMs, we demonstrate that while models initially game tainted specifications in 50-70\% of cases, the SSC process reduces this vulnerability by over 90\%. This dynamic repair occurs at inference time, requires no weight modification, and leads to more robustly aligned model behavior. Code at https://github.com/vicgalle/specification-self-correction .<br>
<span id='abs_ch'>中文: 语言模型会利用有缺陷的规范来获取高分却未满足用户真实意图，而提出的规范自 正框架能让模型在推理时识别并修正这些缺陷， 需调整权重即可将漏洞减少90%以上。</span><br>
<span id='abs_en'>English: Language models can exploit flawed specifications to achieve high scores without meeting user intent, but the proposed Specification Self-Correction framework enables them to identify and correct these flaws at inference time, reducing vulnerability by over 90% without weight modifications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1228, <a href='https://arxiv.org/pdf/2507.18584.pdf' target='_blank'>https://arxiv.org/pdf/2507.18584.pdf</a></span>   <span><a href='https://github.com/Krueske/AQuilt' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaopeng Ke, Hexuan Deng, Xuebo Liu, Jun Rao, Zhenxi Song, Jun Yu, Min Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18584">AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite the impressive performance of large language models (LLMs) in general domains, they often underperform in specialized domains. Existing approaches typically rely on data synthesis methods and yield promising results by using unlabeled data to capture domain-specific features. However, these methods either incur high computational costs or suffer from performance limitations, while also demonstrating insufficient generalization across different tasks. To address these challenges, we propose AQuilt, a framework for constructing instruction-tuning data for any specialized domains from corresponding unlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic, and Task type. By incorporating logic and inspection, we encourage reasoning processes and self-inspection to enhance model performance. Moreover, customizable task instructions enable high-quality data generation for any task. As a result, we construct a dataset of 703k examples to train a powerful data synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3 while utilizing just 17% of the production cost. Further analysis demonstrates that our generated data exhibits higher relevance to downstream tasks. Source code, models, and scripts are available at https://github.com/Krueske/AQuilt.<br>
<span id='abs_ch'>Chinese Summary: AQuilt框架通过从未 注数据中构建指令调优数据，以仅17%的成本实现了与DeepSeek-V3相当的性能，同时在下游任务中展现出更高的数据相关性。</span><br>
<span id='abs_en'>English Summary: The AQuilt framework efficiently generates high-quality instruction-tuning data from unlabeled domain-specific data, achieving performance comparable to DeepSeek-V3 at just 17% of the cost while demonstrating superior task relevance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1229, <a href='https://arxiv.org/pdf/2507.18577.pdf' target='_blank'>https://arxiv.org/pdf/2507.18577.pdf</a></span>   <span><a href='https://github.com/FinFM/Awesome-FinFMs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liyuan Chen, Shuoling Liu, Jiangpeng Yan, Xiaoyu Wang, Henglin Liu, Chuang Li, Kecheng Jiao, Jixuan Ying, Yang Veronica Liu, Qiang Yang, Xiu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18577">Advancing Financial Engineering with Foundation Models: Progress, Applications, and Challenges</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The advent of foundation models (FMs) - large-scale pre-trained models with strong generalization capabilities - has opened new frontiers for financial engineering. While general-purpose FMs such as GPT-4 and Gemini have demonstrated promising performance in tasks ranging from financial report summarization to sentiment-aware forecasting, many financial applications remain constrained by unique domain requirements such as multimodal reasoning, regulatory compliance, and data privacy. These challenges have spurred the emergence of Financial Foundation Models (FFMs) - a new class of models explicitly designed for finance. This survey presents a comprehensive overview of FFMs, with a taxonomy spanning three key modalities: Financial Language Foundation Models (FinLFMs), Financial Time-Series Foundation Models (FinTSFMs), and Financial Visual-Language Foundation Models (FinVLFMs). We review their architectures, training methodologies, datasets, and real-world applications. Furthermore, we identify critical challenges in data availability, algorithmic scalability, and infrastructure constraints, and offer insights into future research opportunities. We hope this survey serves as both a comprehensive reference for understanding FFMs and a practical roadmap for future innovation. An updated collection of FFM-related publications and resources will be maintained on our website https://github.com/FinFM/Awesome-FinFMs.<br>
<span id='abs_ch'>Chinese: 基础模型正在革新金融工程，催生了专门应对多模态推理和监管合规等金融领域挑战的金融基础模型，本综述系统梳理了其架构分类、应用场景及未来 究方向，为领域发展提供路线图。</span><br>
<span id='abs_en'>English: Foundation models are revolutionizing financial engineering by enabling specialized Financial Foundation Models that address domain-specific challenges like multimodal reasoning and regulatory compliance, with this survey providing a comprehensive taxonomy and analysis of their architectures, applications, and future research directions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1230, <a href='https://arxiv.org/pdf/2507.18552.pdf' target='_blank'>https://arxiv.org/pdf/2507.18552.pdf</a></span>   <span><a href='https://github.com/cdx-cindy/VideoMind' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Baoyao Yang, Wanyun Li, Dixin Chen, Junxiang Chen, Wenbin Yao, Haifeng Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18552">VideoMind: An Omni-Modal Video Dataset with Intent Grounding for Deep-Cognitive Video Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper introduces VideoMind, a video-centric omni-modal dataset designed for deep video content cognition and enhanced multi-modal feature representation. The dataset comprises 103K video samples (3K reserved for testing), each paired with audio and systematically detailed textual descriptions. Specifically, every video and its audio is described across three hierarchical layers (factual, abstract, and intent), progressing from surface to depth. It contains over 22 million words, averaging ~225 words per sample. VideoMind's key distinction from existing datasets is its provision of intent expressions, which require contextual integration across the entire video and are not directly observable. These deep-cognitive expressions are generated using a Chain-of-Thought (COT) approach, prompting the mLLM through step-by-step reasoning. Each description includes annotations for subject, place, time, event, action, and intent, supporting downstream recognition tasks. Crucially, we establish a gold-standard benchmark with 3,000 manually validated samples for evaluating deep-cognitive video understanding. We design hybrid-cognitive retrieval experiments, scored by multi-level retrieval metrics, to appropriately assess deep video comprehension. Evaluation results for models (e.g., InternVideo, VAST, UMT-L) are released. VideoMind serves as a powerful benchmark for fine-grained cross-modal alignment and advances fields requiring in-depth video understanding, such as emotion and intent recognition. The data is publicly available on GitHub, HuggingFace, and OpenDataLab, https://github.com/cdx-cindy/VideoMind.<br>
<span id='abs_ch'>中文: 本文介绍了VideoMind视频数据集，该数据集通过链式思维方法生成包含意图表达的多层次文本描述，为深度视频理解和跨模态对齐任务提供了重要基准。</span><br>
<span id='abs_en'>English: This paper presents VideoMind, a comprehensive video dataset with audio and multi-layered textual descriptions that uniquely includes intent expressions generated via Chain-of-Thought reasoning, serving as a benchmark for deep video understanding and cross-modal alignment tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1231, <a href='https://arxiv.org/pdf/2507.18546.pdf' target='_blank'>https://arxiv.org/pdf/2507.18546.pdf</a></span>   <span><a href='https://github.com/fastino-ai/GLiNER2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Urchade Zaratiana, Gil Pasternak, Oliver Boyd, George Hurn-Maloney, Ash Lewis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18546">GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Information extraction (IE) is fundamental to numerous NLP applications, yet existing solutions often require specialized models for different tasks or rely on computationally expensive large language models. We present GLiNER2, a unified framework that enhances the original GLiNER architecture to support named entity recognition, text classification, and hierarchical structured data extraction within a single efficient model. Built pretrained transformer encoder architecture, GLiNER2 maintains CPU efficiency and compact size while introducing multi-task composition through an intuitive schema-based interface. Our experiments demonstrate competitive performance across extraction and classification tasks with substantial improvements in deployment accessibility compared to LLM-based alternatives. We release GLiNER2 as an open-source pip-installable library with pre-trained models and documentation at https://github.com/fastino-ai/GLiNER2.<br>
<span id='abs_ch'>Chinese: GLiNER2 是一个统一且高效的框架，能在单一模型中支持命名实体识别、文本分类等多种自然语言处理任务，相比专用模型或基于大语言模型的方案，它性能优异且部署便捷。</span><br>
<span id='abs_en'>English: GLiNER2 is a unified and efficient framework that supports multiple NLP tasks like named entity recognition and text classification within a single model, offering competitive performance and easy deployment compared to specialized or LLM-based solutions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1232, <a href='https://arxiv.org/pdf/2507.18512.pdf' target='_blank'>https://arxiv.org/pdf/2507.18512.pdf</a></span>   <span><a href='https://github.com/CEA-LIST/SAEshareConcepts' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>ClÃ©ment Cornet, Romaric BesanÃ§on, HervÃ© Le Borgne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18512">Explaining How Visual, Textual and Multimodal Encoders Share Concepts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Sparse autoencoders (SAEs) have emerged as a powerful technique for extracting human-interpretable features from neural networks activations. Previous works compared different models based on SAE-derived features but those comparisons have been restricted to models within the same modality. We propose a novel indicator allowing quantitative comparison of models across SAE features, and use it to conduct a comparative study of visual, textual and multimodal encoders. We also propose to quantify the Comparative Sharedness of individual features between different classes of models. With these two new tools, we conduct several studies on 21 encoders of the three types, with two significantly different sizes, and considering generalist and domain specific datasets. The results allow to revisit previous studies at the light of encoders trained in a multimodal context and to quantify to which extent all these models share some representations or features. They also suggest that visual features that are specific to VLMs among vision encoders are shared with text encoders, highlighting the impact of text pretraining. The code is available at https://github.com/CEA-LIST/SAEshareConcepts<br>
<span id='abs_ch'>中文摘要：稀疏自编 器通过新型量化指 和特征共享度评估，实现了跨模态模型的比较 究，发现多模态视觉模型中的特定视觉特征与文本编 器共享，体现了文本预训练的影响。</span><br>
<span id='abs_en'>English Summary: Sparse autoencoders enable cross-modal model comparisons through a novel quantitative indicator and comparative sharedness metric, revealing that visual features in multimodal models overlap with text encoders due to pretraining influences.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1233, <a href='https://arxiv.org/pdf/2507.18323.pdf' target='_blank'>https://arxiv.org/pdf/2507.18323.pdf</a></span>   <span><a href='https://github.com/bakqui/semi-seg-ecg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Minje Park, Jeonghwa Lim, Taehyung Yu, Sunghoon Joo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18323">SemiSegECG: A Multi-Dataset Benchmark for Semi-Supervised Semantic Segmentation in ECG Delineation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Electrocardiogram (ECG) delineation, the segmentation of meaningful waveform features, is critical for clinical diagnosis. Despite recent advances using deep learning, progress has been limited by the scarcity of publicly available annotated datasets. Semi-supervised learning presents a promising solution by leveraging abundant unlabeled ECG data. In this study, we present SemiSegECG, the first systematic benchmark for semi-supervised semantic segmentation (SemiSeg) in ECG delineation. We curated and unified multiple public datasets, including previously underused sources, to support robust and diverse evaluation. We adopted five representative SemiSeg algorithms from computer vision, implemented them on two different architectures: the convolutional network and the transformer, and evaluated them in two different settings: in-domain and cross-domain. Additionally, we propose ECG-specific training configurations and augmentation strategies and introduce a standardized evaluation framework. Our results show that the transformer outperforms the convolutional network in semi-supervised ECG delineation. We anticipate that SemiSegECG will serve as a foundation for advancing semi-supervised ECG delineation methods and will facilitate further research in this domain.<br>
<span id='abs_ch'>中文：SemiSegECG首次建立了心电描记半监督语义分割的系统基准，通过整合多源数据和 准化评估框架，证明了基于Transformer的模型在半监督心电波形分割中优于卷积网络。</span><br>
<span id='abs_en'>English: SemiSegECG introduces the first systematic benchmark for semi-supervised semantic segmentation in ECG delineation, demonstrating transformer-based models' superiority over convolutional networks while providing unified datasets and evaluation frameworks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1234, <a href='https://arxiv.org/pdf/2507.18262.pdf' target='_blank'>https://arxiv.org/pdf/2507.18262.pdf</a></span>   <span><a href='https://github.com/scy-v/ReSem3D' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyu Su, Weiwei Shang, Chen Qian, Fei Zhang, Shuang Cong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18262">ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Semantics-driven 3D spatial constraints align highlevel semantic representations with low-level action spaces, facilitating the unification of task understanding and execution in robotic manipulation. The synergistic reasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation Models (VFMs) enables cross-modal 3D spatial constraint construction. Nevertheless, existing methods have three key limitations: (1) coarse semantic granularity in constraint modeling, (2) lack of real-time closed-loop planning, (3) compromised robustness in semantically diverse environments. To address these challenges, we propose ReSem3D, a unified manipulation framework for semantically diverse environments, leveraging the synergy between VFMs and MLLMs to achieve fine-grained visual grounding and dynamically constructs hierarchical 3D spatial constraints for real-time manipulation. Specifically, the framework is driven by hierarchical recursive reasoning in MLLMs, which interact with VFMs to automatically construct 3D spatial constraints from natural language instructions and RGB-D observations in two stages: part-level extraction and region-level refinement. Subsequently, these constraints are encoded as real-time optimization objectives in joint space, enabling reactive behavior to dynamic disturbances. Extensive simulation and real-world experiments are conducted in semantically rich household and sparse chemical lab environments. The results demonstrate that ReSem3D performs diverse manipulation tasks under zero-shot conditions, exhibiting strong adaptability and generalization. Code and videos are available at https://github.com/scy-v/ReSem3D and https://resem3d.github.io.<br>
<span id='abs_ch'>中文: ReSem3D框架通过多模态AI模型的协同作用，从自然语言指令构建精细化的3D空间约束，实现在多 化环境中的实时自适应机器人操作。</span><br>
<span id='abs_en'>English: ReSem3D is a robotic manipulation framework that leverages multimodal AI models to create fine-grained 3D spatial constraints from natural language, enabling real-time adaptive task execution in diverse environments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1235, <a href='https://arxiv.org/pdf/2507.18112.pdf' target='_blank'>https://arxiv.org/pdf/2507.18112.pdf</a></span>   <span><a href='https://github.com/xiaovhua/tenvoo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Binghua Li, Ziqing Chang, Tong Liang, Chao Li, Toshihisa Tanaka, Shigeki Aoki, Qibin Zhao, Zhe Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18112">Parameter-Efficient Fine-Tuning of 3D DDPM for MRI Image Generation Using Tensor Networks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We address the challenge of parameter-efficient fine-tuning (PEFT) for three-dimensional (3D) U-Net-based denoising diffusion probabilistic models (DDPMs) in magnetic resonance imaging (MRI) image generation. Despite its practical significance, research on parameter-efficient representations of 3D convolution operations remains limited. To bridge this gap, we propose Tensor Volumetric Operator (TenVOO), a novel PEFT method specifically designed for fine-tuning DDPMs with 3D convolutional backbones. Leveraging tensor network modeling, TenVOO represents 3D convolution kernels with lower-dimensional tensors, effectively capturing complex spatial dependencies during fine-tuning with few parameters. We evaluate TenVOO on three downstream brain MRI datasets-ADNI, PPMI, and BraTS2021-by fine-tuning a DDPM pretrained on 59,830 T1-weighted brain MRI scans from the UK Biobank. Our results demonstrate that TenVOO achieves state-of-the-art performance in multi-scale structural similarity index measure (MS-SSIM), outperforming existing approaches in capturing spatial dependencies while requiring only 0.3% of the trainable parameters of the original model. Our code is available at: https://github.com/xiaovhua/tenvoo<br>
<span id='abs_ch'>中文: 我们提出TenVOO方法，针对三维U-Net扩散模型的参数高效微调，通过 量网络以少量参数表示卷积 ，在脑部MRI数据集上实现了最优性能。</span><br>
<span id='abs_en'>English: We propose TenVOO, a parameter-efficient fine-tuning method for 3D U-Net-based DDPMs in MRI generation that uses tensor networks to represent convolution kernels with fewer parameters while achieving superior performance on brain MRI datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1236, <a href='https://arxiv.org/pdf/2507.18082.pdf' target='_blank'>https://arxiv.org/pdf/2507.18082.pdf</a></span>   <span><a href='https://github.com/HealthX-Lab/TextSAM-EUS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pascal Spiegler, Taha Koleilat, Arash Harirpoush, Corey S. Miller, Hassan Rivaz, Marta Kersten-Oertel, Yiming Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18082">TextSAM-EUS: Text Prompt Learning for SAM to Accurately Segment Pancreatic Tumor in Endoscopic Ultrasound</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Pancreatic cancer carries a poor prognosis and relies on endoscopic ultrasound (EUS) for targeted biopsy and radiotherapy. However, the speckle noise, low contrast, and unintuitive appearance of EUS make segmentation of pancreatic tumors with fully supervised deep learning (DL) models both error-prone and dependent on large, expert-curated annotation datasets. To address these challenges, we present TextSAM-EUS, a novel, lightweight, text-driven adaptation of the Segment Anything Model (SAM) that requires no manual geometric prompts at inference. Our approach leverages text prompt learning (context optimization) through the BiomedCLIP text encoder in conjunction with a LoRA-based adaptation of SAM's architecture to enable automatic pancreatic tumor segmentation in EUS, tuning only 0.86% of the total parameters. On the public Endoscopic Ultrasound Database of the Pancreas, TextSAM-EUS with automatic prompts attains 82.69% Dice and 85.28% normalized surface distance (NSD), and with manual geometric prompts reaches 83.10% Dice and 85.70% NSD, outperforming both existing state-of-the-art (SOTA) supervised DL models and foundation models (e.g., SAM and its variants). As the first attempt to incorporate prompt learning in SAM-based medical image segmentation, TextSAM-EUS offers a practical option for efficient and robust automatic EUS segmentation. Code is available at https://github.com/HealthX-Lab/TextSAM-EUS .<br>
<span id='abs_ch'>中文摘要：TextSAM-EUS是一种轻量级的文本驱动改进模型， 需手动 何提示即可实现内镜超声中胰腺肿瘤的自动分割，在仅调整极少量参数的情况下，其性能超越了现有最先进方法。</span><br>
<span id='abs_en'>English Summary: TextSAM-EUS is a lightweight, text-adapted version of the Segment Anything Model that enables automatic pancreatic tumor segmentation in endoscopic ultrasound without manual prompts, achieving superior performance over existing methods while tuning only a minimal fraction of parameters.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1237, <a href='https://arxiv.org/pdf/2507.18059.pdf' target='_blank'>https://arxiv.org/pdf/2507.18059.pdf</a></span>   <span><a href='https://github.com/liyheng/MAGPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yueheng Li, Guangming Xie, Zongqing Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18059">Multi-Agent Guided Policy Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Due to practical constraints such as partial observability and limited communication, Centralized Training with Decentralized Execution (CTDE) has become the dominant paradigm in cooperative Multi-Agent Reinforcement Learning (MARL). However, existing CTDE methods often underutilize centralized training or lack theoretical guarantees. We propose Multi-Agent Guided Policy Optimization (MAGPO), a novel framework that better leverages centralized training by integrating centralized guidance with decentralized execution. MAGPO uses an auto-regressive joint policy for scalable, coordinated exploration and explicitly aligns it with decentralized policies to ensure deployability under partial observability. We provide theoretical guarantees of monotonic policy improvement and empirically evaluate MAGPO on 43 tasks across 6 diverse environments. Results show that MAGPO consistently outperforms strong CTDE baselines and matches or surpasses fully centralized approaches, offering a principled and practical solution for decentralized multi-agent learning. Our code and experimental data can be found in https://github.com/liyheng/MAGPO.<br>
<span id='abs_ch'>中文摘要：本文提出MAGPO这一新型CTDE框架，通过将集中式指导与分散执行相结合，在保证理论性能提升的同时实现了更有效的多智能体协同学 ，在多个测试环境中显著优于现有方法。</span><br>
<span id='abs_en'>English Summary: The paper introduces MAGPO, a novel CTDE framework that enhances centralized training with guided policy optimization to ensure scalable coordination and theoretical guarantees, outperforming existing methods across diverse environments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1238, <a href='https://arxiv.org/pdf/2507.18043.pdf' target='_blank'>https://arxiv.org/pdf/2507.18043.pdf</a></span>   <span><a href='https://github.com/duykhuongnguyen/GrAInS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18043">GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Inference-time steering methods offer a lightweight alternative to fine-tuning large language models (LLMs) and vision-language models (VLMs) by modifying internal activations at test time without updating model weights. However, most existing approaches rely on fixed, global intervention vectors, overlook the causal influence of individual input tokens, and fail to leverage informative gradients from the model's logits, particularly in multimodal settings where visual and textual inputs contribute unevenly. To address these limitations, we introduce GrAInS, an inference-time steering approach that operates across both language-only and vision-language models and tasks. GrAInS uses contrastive, gradient-based attribution via Integrated Gradients to identify the top-k most influential tokens, both positively and negatively attributed based on their contribution to preferred versus dispreferred outputs. These tokens are then used to construct directional steering vectors that capture semantic shifts from undesirable to desirable behavior. During inference, GrAInS adjusts hidden activations at transformer layers guided by token-level attribution signals, and normalizes activations to preserve representational scale. This enables fine-grained, interpretable, and modular control over model behavior, without retraining or auxiliary supervision. Empirically, GrAInS consistently outperforms both fine-tuning and existing steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514 with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all while preserving the model's fluency and general capabilities.<br>
<span id='abs_ch'>中文: GrAInS提出了一种新颖的推理时引导方法，通过基于梯度的归 分析动态调整模型激活， 需重新训练即可在提升真实性、降低幻觉方面实现显著性能突 。</span><br>
<span id='abs_en'>English: GrAInS introduces a novel inference-time steering method that uses gradient-based token attribution to dynamically adjust model activations, achieving significant performance improvements in truthfulness and reduced hallucinations without retraining.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1239, <a href='https://arxiv.org/pdf/2507.17977.pdf' target='_blank'>https://arxiv.org/pdf/2507.17977.pdf</a></span>   <span><a href='https://github.com/ruid7181/GA-sklearn' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Deng, Ziqi Li, Mingshu Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17977">Improving the Computational Efficiency and Explainability of GeoAggregator</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate modeling and explaining geospatial tabular data (GTD) are critical for understanding geospatial phenomena and their underlying processes. Recent work has proposed a novel transformer-based deep learning model named GeoAggregator (GA) for this purpose, and has demonstrated that it outperforms other statistical and machine learning approaches. In this short paper, we further improve GA by 1) developing an optimized pipeline that accelerates the dataloading process and streamlines the forward pass of GA to achieve better computational efficiency; and 2) incorporating a model ensembling strategy and a post-hoc model explanation function based on the GeoShapley framework to enhance model explainability. We validate the functionality and efficiency of the proposed strategies by applying the improved GA model to synthetic datasets. Experimental results show that our implementation improves the prediction accuracy and inference speed of GA compared to the original implementation. Moreover, explanation experiments indicate that GA can effectively captures the inherent spatial effects in the designed synthetic dataset. The complete pipeline has been made publicly available for community use (https://github.com/ruid7181/GA-sklearn).<br>
<span id='abs_ch'>Chinese: 本 究通过优化数据管道并引入集成策略和基于GeoShapley的解释功能，提升了GeoAggregator模型的预测精度、推理速度和可解释性，在合成数据集上验证了其有效性。</span><br>
<span id='abs_en'>English: The study enhances the GeoAggregator model by optimizing its data pipeline and incorporating ensemble strategies with GeoShapley-based explanations, resulting in improved accuracy, speed, and interpretability on synthetic datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1240, <a href='https://arxiv.org/pdf/2507.17744.pdf' target='_blank'>https://arxiv.org/pdf/2507.17744.pdf</a></span>   <span><a href='https://github.com/stdstu12/YUME' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaofeng Mao, Shaoheng Lin, Zhen Li, Chuanhao Li, Wenshuo Peng, Tong He, Jiangmiao Pang, Mingmin Chi, Yu Qiao, Kaipeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17744">Yume: An Interactive World Generation Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present a preview version of \method, which creates a dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce a well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer~(MVDT) with a memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset \sekai to train \method, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on https://github.com/stdstu12/YUME. Yume will update monthly to achieve its original goal. Project page: https://stdstu12.github.io/YUME-Project/.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1241, <a href='https://arxiv.org/pdf/2507.17731.pdf' target='_blank'>https://arxiv.org/pdf/2507.17731.pdf</a></span>   <span><a href='https://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Li, Zhichen Zeng, Xiao Lin, Feihao Fang, Yanru Qu, Zhe Xu, Zhining Liu, Xuying Ning, Tianxin Wei, Ge Liu, Hanghang Tong, Jingrui He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17731">Flow Matching Meets Biology and Life Science: A Survey</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Over the past decade, advances in generative modeling, such as generative adversarial networks, masked autoencoders, and diffusion models, have significantly transformed biological research and discovery, enabling breakthroughs in molecule design, protein generation, drug discovery, and beyond. At the same time, biological applications have served as valuable testbeds for evaluating the capabilities of generative models. Recently, flow matching has emerged as a powerful and efficient alternative to diffusion-based generative modeling, with growing interest in its application to problems in biology and life sciences. This paper presents the first comprehensive survey of recent developments in flow matching and its applications in biological domains. We begin by systematically reviewing the foundations and variants of flow matching, and then categorize its applications into three major areas: biological sequence modeling, molecule generation and design, and peptide and protein generation. For each, we provide an in-depth review of recent progress. We also summarize commonly used datasets and software tools, and conclude with a discussion of potential future directions. The corresponding curated resources are available at https://github.com/Violet24K/Awesome-Flow-Matching-Meets-Biology.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1242, <a href='https://arxiv.org/pdf/2507.17539.pdf' target='_blank'>https://arxiv.org/pdf/2507.17539.pdf</a></span>   <span><a href='https://github.com/MeteorElf/FundusExpert' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyao Liu, Diping Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17539">Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal large language models (MLLMs) demonstrate significant potential in the field of medical diagnosis. However, they face critical challenges in specialized domains such as ophthalmology, particularly the fragmentation of annotation granularity and inconsistencies in clinical reasoning logic, which hinder precise cross-modal understanding. This paper introduces FundusExpert, an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning capabilities, along with FundusGen, a dataset constructed through the intelligent Fundus-Engine system. Fundus-Engine automates localization and leverages MLLM-based semantic expansion to integrate global disease classification, local object detection, and fine-grained feature analysis within a single fundus image. Additionally, by constructing a clinically aligned cognitive chain, it guides the model to generate interpretable reasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen, achieves the best performance in ophthalmic question-answering tasks, surpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in zero-shot report generation tasks, achieving a clinical consistency of 77.0%, significantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling law between data quality and model capability ($L \propto N^{0.068}$), demonstrating that the cognitive alignment annotations in FundusGen enhance data utilization efficiency. By integrating region-level localization with diagnostic reasoning chains, our work develops a scalable, clinically-aligned MLLM and explores a pathway toward bridging the visual-language gap in specific MLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.<br>
<span id='abs_ch'>中文: FundusExpert是一种专为眼科设计的先进多模态大语言模型，通过整合定位与诊断推理能力，有效解决了 注粒度碎片化和临床逻辑不一致的问题，在医学任务中表现出卓越性能。</span><br>
<span id='abs_en'>English: FundusExpert is an advanced multimodal large language model designed for ophthalmology, integrating positioning and diagnostic reasoning to achieve superior performance in medical tasks by addressing annotation fragmentation and clinical reasoning inconsistencies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1243, <a href='https://arxiv.org/pdf/2507.17472.pdf' target='_blank'>https://arxiv.org/pdf/2507.17472.pdf</a></span>   <span><a href='https://github.com/junhua/bgm-han' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhua Liu, Roy Ka-Wei Lee, Kwan Hui Lim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17472">BGM-HAN: A Hierarchical Attention Network for Accurate and Fair Decision Assessment on Semi-Structured Profiles</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Human decision-making in high-stakes domains often relies on expertise and heuristics, but is vulnerable to hard-to-detect cognitive biases that threaten fairness and long-term outcomes. This work presents a novel approach to enhancing complex decision-making workflows through the integration of hierarchical learning alongside various enhancements. Focusing on university admissions as a representative high-stakes domain, we propose BGM-HAN, an enhanced Byte-Pair Encoded, Gated Multi-head Hierarchical Attention Network, designed to effectively model semi-structured applicant data. BGM-HAN captures multi-level representations that are crucial for nuanced assessment, improving both interpretability and predictive performance. Experimental results on real admissions data demonstrate that our proposed model significantly outperforms both state-of-the-art baselines from traditional machine learning to large language models, offering a promising framework for augmenting decision-making in domains where structure, context, and fairness matter. Source code is available at: https://github.com/junhua/bgm-han.<br>
<span id='abs_ch'>中文: 本 究提出了BGM-HAN分层注意力网络，通过建模多层级数据表征来提升大学招生等高风险领域的决策质量，有效改善公平性和预测准确性。</span><br>
<span id='abs_en'>English: This study introduces BGM-HAN, a hierarchical attention network that enhances decision-making in high-stakes domains like university admissions by modeling multi-level data representations to improve fairness and predictive accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1244, <a href='https://arxiv.org/pdf/2507.17297.pdf' target='_blank'>https://arxiv.org/pdf/2507.17297.pdf</a></span>   <span><a href='https://github.com/theMoro/dcase25task4' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tobias Morocutti, Jonathan Greif, Paul Primus, Florian Schmid, Gerhard Widmer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17297">On Temporal Guidance and Iterative Refinement in Audio Source Separation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spatial semantic segmentation of sound scenes (S5) involves the accurate identification of active sound classes and the precise separation of their sources from complex acoustic mixtures. Conventional systems rely on a two-stage pipeline - audio tagging followed by label-conditioned source separation - but are often constrained by the absence of fine-grained temporal information critical for effective separation. In this work, we address this limitation by introducing a novel approach for S5 that enhances the synergy between the event detection and source separation stages. Our key contributions are threefold. First, we fine-tune a pre-trained Transformer to detect active sound classes. Second, we utilize a separate instance of this fine-tuned Transformer to perform sound event detection (SED), providing the separation module with detailed, time-varying guidance. Third, we implement an iterative refinement mechanism that progressively enhances separation quality by recursively reusing the separator's output from previous iterations. These advancements lead to significant improvements in both audio tagging and source separation performance, as demonstrated by our system's second-place finish in Task 4 of the DCASE Challenge 2025. Our implementation and model checkpoints are available in our GitHub repository: https://github.com/theMoro/dcase25task4 .<br>
<span id='abs_ch'>中文摘要：本 究提出了一种新颖的空间语义声音分割方法，通过微调Transformer进行声音事件检测并结合迭代优化机制，显著提升了音频分类和声源分离性能，并在DCASE 2025挑战赛中验证了其有效性。</span><br>
<span id='abs_en'>English Summary: This study introduces a novel approach for spatial semantic sound segmentation that integrates a fine-tuned Transformer for sound event detection with an iterative refinement mechanism, significantly improving both audio classification and source separation performance as demonstrated in the DCASE Challenge 2025.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1245, <a href='https://arxiv.org/pdf/2507.17224.pdf' target='_blank'>https://arxiv.org/pdf/2507.17224.pdf</a></span>   <span><a href='https://github.com/IgarashiAkatuki/HuiduRep' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Cao, Zishuo Feng, Wei Shi, Jicong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17224">HuiduRep: A Robust Self-Supervised Framework for Learning Neural Representations from Extracellular Recordings</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Extracellular recordings are transient voltage fluctuations in the vicinity of neurons, serving as a fundamental modality in neuroscience for decoding brain activity at single-neuron resolution. Spike sorting, the process of attributing each detected spike to its corresponding neuron, is a pivotal step in brain sensing pipelines. However, it remains challenging under low signal-to-noise ratio (SNR), electrode drift, and cross-session variability. In this paper, we propose HuiduRep, a robust self-supervised representation learning framework that extracts discriminative and generalizable features from extracellular recordings. By integrating contrastive learning with a denoising autoencoder, HuiduRep learns latent representations robust to noise and drift. With HuiduRep, we develop a spike sorting pipeline that clusters spike representations without ground truth labels. Experiments on hybrid and real-world datasets demonstrate that HuiduRep achieves strong robustness. Furthermore, the pipeline significantly outperforms state-of-the-art tools such as KiloSort4 and MountainSort5 on accuracy and precision on diverse datasets. These findings demonstrate the potential of self-supervised spike representation learning as a foundational tool for robust and generalizable processing of extracellular recordings. Code is available at: https://github.com/IgarashiAkatuki/HuiduRep<br>
<span id='abs_ch'>中文: 本文提出的HuiduRep自监督框架能从细胞外记录中学 鲁棒的尖峰表征， 需 签即可实现精确的尖峰分类，其性能显著优于KiloSort4和MountainSort5等现有工具。English: This paper introduces HuiduRep, a self-supervised framework that learns robust spike representations from extracellular recordings, enabling accurate spike sorting without labels and outperforming existing tools like KiloSort4 and MountainSort5.Paperid:392,https://arxiv.org/pdf/2507.17192.pdfGitHub</span><br>
<span id='abs_en'>English: This paper introduces HuiduRep, a self-supervised framework that learns robust spike representations from extracellular recordings, enabling accurate spike sorting without labels and outperforming existing tools like KiloSort4 and MountainSort5.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1246, <a href='https://arxiv.org/pdf/2507.17178.pdf' target='_blank'>https://arxiv.org/pdf/2507.17178.pdf</a></span>   <span><a href='https://github.com/zjukg/SKA-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqiang Liu, Enpei Niu, Yin Hua, Mengshu Sun, Lei Liang, Huajun Chen, Wen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17178">SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although large language models (LLMs) have made significant progress in understanding Structured Knowledge (SK) like KG and Table, existing evaluations for SK understanding are non-rigorous (i.e., lacking evaluations of specific capabilities) and focus on a single type of SK. Therefore, we aim to propose a more comprehensive and rigorous structured knowledge understanding benchmark to diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a Structured Knowledge Augmented QA Benchmark that encompasses four widely used structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a three-stage pipeline to construct SKA-Bench instances, which includes a question, an answer, positive knowledge units, and noisy knowledge units. To evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we expand the instances into four fundamental ability testbeds: Noise Robustness, Order Insensitivity, Information Integration, and Negative Rejection. Empirical evaluations on 8 representative LLMs, including the advanced DeepSeek-R1, indicate that existing LLMs still face significant challenges in understanding structured knowledge, and their performance is influenced by factors such as the amount of noise, the order of knowledge units, and hallucination phenomenon. Our dataset and code are available at https://github.com/zjukg/SKA-Bench.<br>
<span id='abs_ch'>中文: 本文提出SKA-Bench这一结构化知识理解基准，通过四种知识形式和四项基础能力测试评估大语言模型，发现现有模型在处理噪声、顺序敏感性和幻觉现象方面仍面临重大挑战。</span><br>
<span id='abs_en'>English: This paper introduces SKA-Bench, a comprehensive benchmark for evaluating large language models' structured knowledge understanding across four knowledge forms and four fundamental abilities, revealing that current models still struggle significantly with noise, order sensitivity, and hallucinations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1247, <a href='https://arxiv.org/pdf/2507.17178.pdf' target='_blank'>https://arxiv.org/pdf/2507.17178.pdf</a></span>   <span><a href='https://github.com/zjukg/SKA-Bench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqiang Liu, Enpei Niu, Yin Hua, Mengshu Sun, Lei Liang, Huajun Chen, Wen Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17178">SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Although large language models (LLMs) have made significant progress in understanding Structured Knowledge (SK) like KG and Table, existing evaluations for SK understanding are non-rigorous (i.e., lacking evaluations of specific capabilities) and focus on a single type of SK. Therefore, we aim to propose a more comprehensive and rigorous structured knowledge understanding benchmark to diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a Structured Knowledge Augmented QA Benchmark that encompasses four widely used structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a three-stage pipeline to construct SKA-Bench instances, which includes a question, an answer, positive knowledge units, and noisy knowledge units. To evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we expand the instances into four fundamental ability testbeds: Noise Robustness, Order Insensitivity, Information Integration, and Negative Rejection. Empirical evaluations on 8 representative LLMs, including the advanced DeepSeek-R1, indicate that existing LLMs still face significant challenges in understanding structured knowledge, and their performance is influenced by factors such as the amount of noise, the order of knowledge units, and hallucination phenomenon. Our dataset and code are available at https://github.com/zjukg/SKA-Bench.<br>
<span id='abs_ch'>中文: 本文提出SKA-Bench这一结构化知识理解基准，通过四种知识形式和四项基础能力测试评估大语言模型，发现现有模型在处理噪声、顺序敏感性和幻觉现象方面仍面临重大挑战。</span><br>
<span id='abs_en'>English: This paper introduces SKA-Bench, a comprehensive benchmark for evaluating large language models' structured knowledge understanding across four knowledge forms and four fundamental abilities, revealing that current models still struggle significantly with noise, order sensitivity, and hallucinations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1248, <a href='https://arxiv.org/pdf/2507.17152.pdf' target='_blank'>https://arxiv.org/pdf/2507.17152.pdf</a></span>   <span><a href='https://github.com/LinFunster/JAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangze Lin, Ying He, Fei Yu, Hong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17152">JAM: Keypoint-Guided Joint Prediction after Classification-Aware Marginal Proposal for Multi-Agent Interaction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Predicting the future motion of road participants is a critical task in autonomous driving. In this work, we address the challenge of low-quality generation of low-probability modes in multi-agent joint prediction. To tackle this issue, we propose a two-stage multi-agent interactive prediction framework named \textit{keypoint-guided joint prediction after classification-aware marginal proposal} (JAM). The first stage is modeled as a marginal prediction process, which classifies queries by trajectory type to encourage the model to learn all categories of trajectories, providing comprehensive mode information for the joint prediction module. The second stage is modeled as a joint prediction process, which takes the scene context and the marginal proposals from the first stage as inputs to learn the final joint distribution. We explicitly introduce key waypoints to guide the joint prediction module in better capturing and leveraging the critical information from the initial predicted trajectories. We conduct extensive experiments on the real-world Waymo Open Motion Dataset interactive prediction benchmark. The results show that our approach achieves competitive performance. In particular, in the framework comparison experiments, the proposed JAM outperforms other prediction frameworks and achieves state-of-the-art performance in interactive trajectory prediction. The code is available at https://github.com/LinFunster/JAM to facilitate future research.<br>
<span id='abs_ch'>Chinese: 本文提出JAM双阶段框架，通过先对轨迹类型分类确保模式覆盖，再利用关键路径点优化联合预测，在Waymo数据集上实现了交互式轨迹预测的最优性能。</span><br>
<span id='abs_en'>English: This paper introduces JAM, a two-stage framework that enhances multi-agent trajectory prediction by first classifying trajectory types for comprehensive mode coverage and then using key waypoints to refine joint predictions, achieving state-of-the-art results on the Waymo dataset.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1249, <a href='https://arxiv.org/pdf/2507.17135.pdf' target='_blank'>https://arxiv.org/pdf/2507.17135.pdf</a></span>   <span><a href='https://github.com/Ting-Justin-Jiang/sada-icml' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ting Jiang, Yixiao Wang, Hancheng Ye, Zishan Shao, Jingwei Sun, Jingyang Zhang, Zekai Chen, Jianyi Zhang, Yiran Chen, Hai Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17135">SADA: Stability-guided Adaptive Diffusion Acceleration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion models have achieved remarkable success in generative tasks but suffer from high computational costs due to their iterative sampling process and quadratic attention costs. Existing training-free acceleration strategies that reduce per-step computation cost, while effectively reducing sampling time, demonstrate low faithfulness compared to the original baseline. We hypothesize that this fidelity gap arises because (a) different prompts correspond to varying denoising trajectory, and (b) such methods do not consider the underlying ODE formulation and its numerical solution. In this paper, we propose Stability-guided Adaptive Diffusion Acceleration (SADA), a novel paradigm that unifies step-wise and token-wise sparsity decisions via a single stability criterion to accelerate sampling of ODE-based generative models (Diffusion and Flow-matching). For (a), SADA adaptively allocates sparsity based on the sampling trajectory. For (b), SADA introduces principled approximation schemes that leverage the precise gradient information from the numerical ODE solver. Comprehensive evaluations on SD-2, SDXL, and Flux using both EDM and DPM++ solvers reveal consistent $\ge 1.8\times$ speedups with minimal fidelity degradation (LPIPS $\leq 0.10$ and FID $\leq 4.5$) compared to unmodified baselines, significantly outperforming prior methods. Moreover, SADA adapts seamlessly to other pipelines and modalities: It accelerates ControlNet without any modifications and speeds up MusicLDM by $1.8\times$ with $\sim 0.01$ spectrogram LPIPS.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1250, <a href='https://arxiv.org/pdf/2507.17083.pdf' target='_blank'>https://arxiv.org/pdf/2507.17083.pdf</a></span>   <span><a href='https://github.com/DzpLab/SDGOCC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zaipeng Duan, Chenxu Dang, Xuzhong Hu, Pei An, Junfeng Ding, Jie Zhan, Yunbiao Xu, Jie Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17083">SDGOCC: Semantic and Depth-Guided Bird's-Eye View Transformation for 3D Multimodal Occupancy Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal 3D occupancy prediction has garnered significant attention for its potential in autonomous driving. However, most existing approaches are single-modality: camera-based methods lack depth information, while LiDAR-based methods struggle with occlusions. Current lightweight methods primarily rely on the Lift-Splat-Shoot (LSS) pipeline, which suffers from inaccurate depth estimation and fails to fully exploit the geometric and semantic information of 3D LiDAR points. Therefore, we propose a novel multimodal occupancy prediction network called SDG-OCC, which incorporates a joint semantic and depth-guided view transformation coupled with a fusion-to-occupancy-driven active distillation. The enhanced view transformation constructs accurate depth distributions by integrating pixel semantics and co-point depth through diffusion and bilinear discretization. The fusion-to-occupancy-driven active distillation extracts rich semantic information from multimodal data and selectively transfers knowledge to image features based on LiDAR-identified regions. Finally, for optimal performance, we introduce SDG-Fusion, which uses fusion alone, and SDG-KL, which integrates both fusion and distillation for faster inference. Our method achieves state-of-the-art (SOTA) performance with real-time processing on the Occ3D-nuScenes dataset and shows comparable performance on the more challenging SurroundOcc-nuScenes dataset, demonstrating its effectiveness and robustness. The code will be released at https://github.com/DzpLab/SDGOCC.<br>
<span id='abs_ch'>Chinese: 提出的SDG-OCC网络通过联合语义和深度引导的视角变换与主动蒸馏技术，克服了现有多模态3D 据预测方法的局限性，在基准数据集上实现了最先进的性能并支持实时处理。</span><br>
<span id='abs_en'>English: The proposed SDG-OCC network introduces a joint semantic and depth-guided view transformation with active distillation to overcome limitations in existing multimodal 3D occupancy prediction methods, achieving state-of-the-art performance on benchmark datasets while enabling real-time processing.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1251, <a href='https://arxiv.org/pdf/2507.17015.pdf' target='_blank'>https://arxiv.org/pdf/2507.17015.pdf</a></span>   <span><a href='https://github.com/apple/ml-agent-evaluator' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Arduin Findeis, Floris Weers, Guoli Yin, Ke Ye, Ruoming Pang, Tom Gunter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.17015">Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Pairwise preferences over model responses are widely collected to evaluate and provide feedback to large language models (LLMs). Given two alternative model responses to the same input, a human or AI annotator selects the "better" response. This approach can provide feedback for domains where other hard-coded metrics are difficult to obtain (e.g., chat response quality), thereby helping model evaluation or training. However, for some domains high-quality pairwise comparisons can be tricky to obtain - from AI and humans. For example, for responses with many factual statements, annotators may disproportionately weigh writing quality rather than underlying facts. In this work, we explore augmenting standard AI annotator systems with additional tools to improve performance on three challenging response domains: long-form factual, math and code tasks. We propose a tool-using agentic system to provide higher quality feedback on these domains. Our system uses web-search and code execution to ground itself based on external validation, independent of the LLM's internal knowledge and biases. We provide extensive experimental results evaluating our method across the three targeted response domains as well as general annotation tasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as three new datasets for domains with saturated pre-existing datasets. Our results indicate that external tools can indeed improve performance in many, but not all, cases. More generally, our experiments highlight the sensitivity of performance to simple parameters (e.g., prompt) and the need for improved (non-saturated) annotator benchmarks. We share our code at https://github.com/apple/ml-agent-evaluator.<br>
<span id='abs_ch'>中文: 本 究提出了一种利用工具的系统，通过结合网络搜索和代 执行来增强AI 注器，以改进在长文本事实、数学和代 等挑战性领域的成对偏好评估，结果表明外部工具在许多情况下能提升性能，同时强调了改进 注基准的必要性。</span><br>
<span id='abs_en'>English: This study introduces a tool-using agentic system that enhances AI annotators with web-search and code execution to improve pairwise preference evaluations for challenging domains like long-form factual, math, and code tasks, showing that external tools boost performance in many cases while highlighting the need for better benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1252, <a href='https://arxiv.org/pdf/2507.16829.pdf' target='_blank'>https://arxiv.org/pdf/2507.16829.pdf</a></span>   <span><a href='https://github.com/geektoni/mitigating-harm-recsys' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Giovanni De Toni, Erasmo Purificato, Emilia GÃ³mez, Bruno Lepri, Andrea Passerini, Cristian Consonni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16829">You Don't Bring Me Flowers: Mitigating Unwanted Recommendations Through Conformal Risk Control</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recommenders are significantly shaping online information consumption. While effective at personalizing content, these systems increasingly face criticism for propagating irrelevant, unwanted, and even harmful recommendations. Such content degrades user satisfaction and contributes to significant societal issues, including misinformation, radicalization, and erosion of user trust. Although platforms offer mechanisms to mitigate exposure to undesired content, these mechanisms are often insufficiently effective and slow to adapt to users' feedback. This paper introduces an intuitive, model-agnostic, and distribution-free method that uses conformal risk control to provably bound unwanted content in personalized recommendations by leveraging simple binary feedback on items. We also address a limitation of traditional conformal risk control approaches, i.e., the fact that the recommender can provide a smaller set of recommended items, by leveraging implicit feedback on consumed items to expand the recommendation set while ensuring robust risk mitigation. Our experimental evaluation on data coming from a popular online video-sharing platform demonstrates that our approach ensures an effective and controllable reduction of unwanted recommendations with minimal effort. The source code is available here: https://github.com/geektoni/mitigating-harm-recsys.<br>
<span id='abs_ch'>中文: 本文提出一种与模型 关的方法，利用合规风险控制和二元用户反馈来可 地限制个性化推荐中的不良内容，通过最少投入有效减少有害建议。</span><br>
<span id='abs_en'>English: This paper presents a model-agnostic method using conformal risk control to provably limit unwanted content in personalized recommendations through binary user feedback, effectively reducing harmful suggestions with minimal effort.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1253, <a href='https://arxiv.org/pdf/2507.16812.pdf' target='_blank'>https://arxiv.org/pdf/2507.16812.pdf</a></span>   <span><a href='https://github.com/GAIR-NLP/MegaScience;' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Run-Ze Fan, Zengzhi Wang, Pengfei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16812">MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Scientific reasoning is critical for developing AI scientists and supporting human researchers in advancing the frontiers of natural science discovery. However, the open-source community has primarily focused on mathematics and coding while neglecting the scientific domain, largely due to the absence of open, large-scale, high-quality, verifiable scientific reasoning datasets. To bridge this gap, we first present TextbookReasoning, an open dataset featuring truthful reference answers extracted from 12k university-level scientific textbooks, comprising 650k reasoning questions spanning 7 scientific disciplines. We further introduce MegaScience, a large-scale mixture of high-quality open-source datasets totaling 1.25 million instances, developed through systematic ablation studies that evaluate various data selection methodologies to identify the optimal subset for each publicly available scientific dataset. Meanwhile, we build a comprehensive evaluation system covering diverse subjects and question types across 15 benchmarks, incorporating comprehensive answer extraction strategies to ensure accurate evaluation metrics. Our experiments demonstrate that our datasets achieve superior performance and training efficiency with more concise response lengths compared to existing open-source scientific datasets. Furthermore, we train Llama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which significantly outperform the corresponding official instruct models in average performance. In addition, MegaScience exhibits greater effectiveness for larger and stronger models, suggesting a scaling benefit for scientific tuning. We release our data curation pipeline, evaluation system, datasets, and seven trained models to the community to advance scientific reasoning research.<br>
<span id='abs_ch'>中文: 本 究推出了TextbookReasoning和MegaScience两个开放数据集，旨在填补高质量科学推理资源的空白，这些数据集在多个基准测试和基础模型上显著提升了性能与训练效率。</span><br>
<span id='abs_en'>English: This study introduces TextbookReasoning and MegaScience, two open datasets designed to address the scarcity of high-quality scientific reasoning resources, which significantly enhance model performance and training efficiency across multiple benchmarks and base models.Paperid:414,https://arxiv.org/pdf/2507.16802.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1254, <a href='https://arxiv.org/pdf/2507.16768.pdf' target='_blank'>https://arxiv.org/pdf/2507.16768.pdf</a></span>   <span><a href='https://github.com/wrran/wgrammar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ran Wang, Xiaoxuan Liu, Hao Ren, Gang Chen, Fanchao Qi, Maosong Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16768">WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Structured decoding enables large language models (LLMs) to generate outputs in formats required by downstream systems, such as HTML or JSON. However, existing methods suffer from efficiency bottlenecks due to grammar compilation, state tracking, and mask creation. We observe that many real-world tasks embed strong prior knowledge about output structure. Leveraging this, we propose a decomposition of constraints into static and dynamic components -- precompiling static structures offline and instantiating dynamic arguments at runtime using grammar snippets. Instead of relying on pushdown automata, we employ a compositional set of operators to model regular formats, achieving lower transition latency. We introduce wgrammar, a lightweight decoding engine that integrates domain-aware simplification, constraint decomposition, and mask caching, achieving up to 250x speedup over existing systems. wgrammar's source code is publicly available at https://github.com/wrran/wgrammar.<br>
<span id='abs_ch'>中文: 提出的wgrammar引擎通过将约束分解为静态和动态组件，利用预编译结构和语法片段 速结构化解 ，相比现有方法实现了高达250倍的 速效果。</span><br>
<span id='abs_en'>English: The proposed wgrammar engine accelerates structured decoding by decomposing constraints into static and dynamic components, using precompiled structures and grammar snippets to achieve up to 250x speedup over existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1255, <a href='https://arxiv.org/pdf/2507.16725.pdf' target='_blank'>https://arxiv.org/pdf/2507.16725.pdf</a></span>   <span><a href='https://github.com/SwordFaith/RAVine' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yilong Xu, Xiang Long, Zhi Zheng, Jinhua Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16725">RAVine: Reality-Aligned Evaluation for Agentic Search</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Agentic search, as a more autonomous and adaptive paradigm of retrieval augmentation, is driving the evolution of intelligent search systems. However, existing evaluation frameworks fail to align well with the goals of agentic search. First, the complex queries commonly used in current benchmarks often deviate from realistic user search scenarios. Second, prior approaches tend to introduce noise when extracting ground truth for end-to-end evaluations, leading to distorted assessments at a fine-grained level. Third, most current frameworks focus solely on the quality of final answers, neglecting the evaluation of the iterative process inherent to agentic search. To address these limitations, we propose RAVine -- a Reality-Aligned eValuation framework for agentic LLMs with search. RAVine targets multi-point queries and long-form answers that better reflect user intents, and introduces an attributable ground truth construction strategy to enhance the accuracy of fine-grained evaluation. Moreover, RAVine examines model's interaction with search tools throughout the iterative process, and accounts for factors of efficiency. We benchmark a series of models using RAVine and derive several insights, which we hope will contribute to advancing the development of agentic search systems. The code and datasets are available at https://github.com/SwordFaith/RAVine.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1256, <a href='https://arxiv.org/pdf/2507.16696.pdf' target='_blank'>https://arxiv.org/pdf/2507.16696.pdf</a></span>   <span><a href='https://github.com/jianganbai/FISHER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pingyi Fan, Anbai Jiang, Shuwei Zhang, Zhiqiang Lv, Bing Han, Xinhu Zheng, Wenrui Liang, Junjie Li, Wei-Qiang Zhang, Yanmin Qian, Xie Chen, Cheng Lu, Jia Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16696">FISHER: A Foundation Model for Multi-Modal Industrial Signal Comprehensive Representation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the rapid deployment of SCADA systems, how to effectively analyze industrial signals and detect abnormal states is an urgent need for the industry. Due to the significant heterogeneity of these signals, which we summarize as the M5 problem, previous works only focus on small sub-problems and employ specialized models, failing to utilize the synergies between modalities and the powerful scaling law. However, we argue that the M5 signals can be modeled in a unified manner due to the intrinsic similarity. As a result, we propose FISHER, a Foundation model for multi-modal Industrial Signal compreHEnsive Representation. To support arbitrary sampling rates, FISHER considers the increment of sampling rate as the concatenation of sub-band information. Specifically, FISHER takes the STFT sub-band as the modeling unit and adopts a teacher student SSL framework for pre-training. We also develop the RMIS benchmark, which evaluates the representations of M5 industrial signals on multiple health management tasks. Compared with top SSL models, FISHER showcases versatile and outstanding capabilities with a general performance gain up to 5.03%, along with much more efficient scaling curves. We also investigate the scaling law on downstream tasks and derive potential avenues for future works. FISHER is now open-sourced on https://github.com/jianganbai/FISHER<br>
<span id='abs_ch'>中文: 随着SCADA系统的快速部署，工业信号分析需求日益迫切；为此提出的FISHER基础模型通过子带建模和师生自监督框架，在多模态工业信号表征上实现最高5.03%的性能提升，展现出卓越的泛化能力。</span><br>
<span id='abs_en'>English: The rapid expansion of SCADA systems necessitates effective analysis of heterogeneous industrial signals, leading to the development of FISHER, a unified foundation model that leverages a teacher-student SSL framework and sub-band processing to achieve versatile performance gains of up to 5.03% over specialized models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1257, <a href='https://arxiv.org/pdf/2507.16579.pdf' target='_blank'>https://arxiv.org/pdf/2507.16579.pdf</a></span>   <span><a href='https://github.com/xiaojiao929/PHMDiff' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaojiao Xiao, Qinmin Vivian Hu, Guanghui Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16579">Pyramid Hierarchical Masked Diffusion Model for Imaging Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Medical image synthesis plays a crucial role in clinical workflows, addressing the common issue of missing imaging modalities due to factors such as extended scan times, scan corruption, artifacts, patient motion, and intolerance to contrast agents. The paper presents a novel image synthesis network, the Pyramid Hierarchical Masked Diffusion Model (PHMDiff), which employs a multi-scale hierarchical approach for more detailed control over synthesizing high-quality images across different resolutions and layers. Specifically, this model utilizes randomly multi-scale high-proportion masks to speed up diffusion model training, and balances detail fidelity and overall structure. The integration of a Transformer-based Diffusion model process incorporates cross-granularity regularization, modeling the mutual information consistency across each granularity's latent spaces, thereby enhancing pixel-level perceptual accuracy. Comprehensive experiments on two challenging datasets demonstrate that PHMDiff achieves superior performance in both the Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM), highlighting its capability to produce high-quality synthesized images with excellent structural integrity. Ablation studies further confirm the contributions of each component. Furthermore, the PHMDiff model, a multi-scale image synthesis framework across and within medical imaging modalities, shows significant advantages over other methods. The source code is available at https://github.com/xiaojiao929/PHMDiff<br>
<span id='abs_ch'>中文摘要：本文提出PHMDiff模型，通过多尺度掩 扩散和跨粒度正则化方法，在 速训练的同时提升了医学图像合成的质量与结构完整性，在多个数据集上表现出优越性能。</span><br>
<span id='abs_en'>English Summary: The paper introduces PHMDiff, a pyramid hierarchical masked diffusion model that accelerates training and enhances medical image synthesis quality by employing multi-scale masked diffusion and cross-granularity regularization, achieving superior performance on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1258, <a href='https://arxiv.org/pdf/2507.16535.pdf' target='_blank'>https://arxiv.org/pdf/2507.16535.pdf</a></span>   <span><a href='https://github.com/whiteinblue/EarthCrafter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shang Liu, Chenjie Cao, Chaohui Yu, Wen Qian, Jing Wang, Fan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16535">EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent Diffusion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite the remarkable developments achieved by recent 3D generation works, scaling these methods to geographic extents, such as modeling thousands of square kilometers of Earth's surface, remains an open challenge. We address this through a dual innovation in data infrastructure and model architecture. First, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date, consisting of 50k curated scenes (each measuring 600m x 600m) captured across the U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene provides pose-annotated multi-view images, depth maps, normals, semantic segmentation, and camera poses, with explicit quality control to ensure terrain diversity. Building on this foundation, we propose EarthCrafter, a tailored framework for large-scale 3D Earth generation via sparse-decoupled latent diffusion. Our architecture separates structural and textural generation: 1) Dual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D Gaussian Splats (2DGS) into compact latent spaces, largely alleviating the costly computation suffering from vast geographic scales while preserving critical information. 2) We propose condition-aware flow matching models trained on mixed inputs (semantics, images, or neither) to flexibly model latent geometry and texture features independently. Extensive experiments demonstrate that EarthCrafter performs substantially better in extremely large-scale generation. The framework further supports versatile applications, from semantic-guided urban layout generation to unconditional terrain synthesis, while maintaining geographic plausibility through our rich data priors from Aerial-Earth3D. Our project page is available at https://whiteinblue.github.io/earthcrafter/<br>
<span id='abs_ch'>中文摘要：本 究提出了迄今最大的三维航拍数据集Aerial-Earth3D，并开发了EarthCrafter框架，通过稀疏解耦的潜在扩散技术实现大规模三维地球生成，在提升计算效率的同时保持了地理合理性。</span><br>
<span id='abs_en'>English Summary: The study introduces Aerial-Earth3D, the largest 3D aerial dataset, and EarthCrafter, a novel framework using sparse-decoupled latent diffusion to enable large-scale 3D Earth generation with enhanced computational efficiency and geographic accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1259, <a href='https://arxiv.org/pdf/2507.16533.pdf' target='_blank'>https://arxiv.org/pdf/2507.16533.pdf</a></span>   <span><a href='https://github.com/automl/ConfigurableOptimizer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abhash Kumar Jha, Shakiba Moradian, Arjun Krishnakumar, Martin Rapp, Frank Hutter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16533">confopt: A Library for Implementation and Evaluation of Gradient-based One-Shot NAS Methods</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Gradient-based one-shot neural architecture search (NAS) has significantly reduced the cost of exploring architectural spaces with discrete design choices, such as selecting operations within a model. However, the field faces two major challenges. First, evaluations of gradient-based NAS methods heavily rely on the DARTS benchmark, despite the existence of other available benchmarks. This overreliance has led to saturation, with reported improvements often falling within the margin of noise. Second, implementations of gradient-based one-shot NAS methods are fragmented across disparate repositories, complicating fair and reproducible comparisons and further development. In this paper, we introduce Configurable Optimizer (confopt), an extensible library designed to streamline the development and evaluation of gradient-based one-shot NAS methods. Confopt provides a minimal API that makes it easy for users to integrate new search spaces, while also supporting the decomposition of NAS optimizers into their core components. We use this framework to create a suite of new DARTS-based benchmarks, and combine them with a novel evaluation protocol to reveal a critical flaw in how gradient-based one-shot NAS methods are currently assessed. The code can be found at https://github.com/automl/ConfigurableOptimizer.<br>
<span id='abs_ch'>中文: 本文介绍了可配置优化器（confopt），这是一个可扩展的库，旨在解决基于梯度的单次神经架构搜索中对DARTS基准的过度依赖和实现碎片化问题，通过支持新搜索空间的便捷集成和分解NAS优化器 心组件，同时利用新型基准和评估协议揭示了当前评估方法中的关键缺陷。</span><br>
<span id='abs_en'>English: This paper introduces Configurable Optimizer (confopt), an extensible library that addresses the overreliance on the DARTS benchmark and fragmented implementations in gradient-based one-shot neural architecture search by enabling easy integration of new search spaces and decomposing NAS optimizers, while also revealing a critical flaw in current evaluation methods through novel benchmarks and protocols.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1260, <a href='https://arxiv.org/pdf/2507.16524.pdf' target='_blank'>https://arxiv.org/pdf/2507.16524.pdf</a></span>   <span><a href='https://github.com/bjshuyuan/Spatial-3D-LLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyan Wang, Zeju Li, Yifan Xu, Jiaxing Qi, Zhifei Yang, Ruifei Ma, Xiangde Liu, Chao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16524">Spatial 3D-LLM: Exploring Spatial Awareness in 3D Vision-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>New era has unlocked exciting possibilities for extending Large Language Models (LLMs) to tackle 3D vision-language tasks. However, most existing 3D multimodal LLMs (MLLMs) rely on compressing holistic 3D scene information or segmenting independent objects to perform these tasks, which limits their spatial awareness due to insufficient representation of the richness inherent in 3D scenes. To overcome these limitations, we propose Spatial 3D-LLM, a 3D MLLM specifically designed to enhance spatial awareness for 3D vision-language tasks by enriching the spatial embeddings of 3D scenes. Spatial 3D-LLM integrates an LLM backbone with a progressive spatial awareness scheme that progressively captures spatial information as the perception field expands, generating location-enriched 3D scene embeddings to serve as visual prompts. Furthermore, we introduce two novel tasks: 3D object distance measurement and 3D layout editing, and construct a 3D instruction dataset, MODEL, to evaluate the model's spatial awareness capabilities. Experimental results demonstrate that Spatial 3D-LLM achieves state-of-the-art performance across a wide range of 3D vision-language tasks, revealing the improvements stemmed from our progressive spatial awareness scheme of mining more profound spatial information. Our code is available at https://github.com/bjshuyuan/Spatial-3D-LLM.<br>
<span id='abs_ch'>中文: 提出的Spatial 3D-LLM通过渐进式空间嵌入增强3D视觉语言任务的空间感知能力，并借助新任务和专用数据集验证了其领先性能。</span><br>
<span id='abs_en'>English: The proposed Spatial 3D-LLM enhances spatial awareness in 3D vision-language tasks through progressive spatial embeddings and achieves state-of-the-art performance, as validated by novel tasks and a dedicated dataset.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1261, <a href='https://arxiv.org/pdf/2507.16514.pdf' target='_blank'>https://arxiv.org/pdf/2507.16514.pdf</a></span>   <span><a href='https://github.com/aiben-ch/EESE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junying Wang, Zicheng Zhang, Yijin Guo, Farong Wen, Ye Shen, Yingji Liang, Yalun Wu, Wenzhe Li, Chunyi Li, Zijian Chen, Qi Jia, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16514">The Ever-Evolving Science Exam</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As foundation models grow rapidly in capability and deployment, evaluating their scientific understanding becomes increasingly critical. Existing science benchmarks have made progress towards broad **Range**, wide **Reach**, and high **Rigor**, yet they often face two major challenges: **data leakage risks** that compromise benchmarking validity, and **evaluation inefficiency** due to large-scale testing. To address these issues, we introduce the **Ever-Evolving Science Exam (EESE)**, a dynamic benchmark designed to reliably assess scientific capabilities in foundation models. Our approach consists of two components: 1) a non-public **EESE-Pool** with over 100K expertly constructed science instances (question-answer pairs) across 5 disciplines and 500+ subfields, built through a multi-stage pipeline ensuring **Range**, **Reach**, and **Rigor**, 2) a periodically updated 500-instance subset **EESE**, sampled and validated to enable leakage-resilient, low-overhead evaluations. Experiments on 32 open- and closed-source models demonstrate that EESE effectively differentiates the strengths and weaknesses of models in scientific fields and cognitive dimensions. Overall, EESE provides a robust, scalable, and forward-compatible solution for science benchmark design, offering a realistic measure of how well foundation models handle science questions. The project page is at: https://github.com/aiben-ch/EESE.<br>
<span id='abs_ch'>Chinese: 本文提出的“持续演进科学考试（EESE）”是一个动态基准，通过构建大规模非公开题库和定期更新子集，有效解决数据泄露和评估效率问题，为评估基础模型的科学能力提供了可 、可扩展的解决方案。</span><br>
<span id='abs_en'>English: The Ever-Evolving Science Exam (EESE) is introduced as a dynamic benchmark to reliably evaluate the scientific capabilities of foundation models, addressing data leakage and inefficiency issues through a large, non-public question pool and periodic updates for leakage-resilient, low-overhead assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1262, <a href='https://arxiv.org/pdf/2507.16514.pdf' target='_blank'>https://arxiv.org/pdf/2507.16514.pdf</a></span>   <span><a href='https://github.com/aiben-ch/EESE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junying Wang, Zicheng Zhang, Yijin Guo, Farong Wen, Ye Shen, Yingji Liang, Yalun Wu, Wenzhe Li, Chunyi Li, Zijian Chen, Qi Jia, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16514">The Ever-Evolving Science Exam</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As foundation models grow rapidly in capability and deployment, evaluating their scientific understanding becomes increasingly critical. Existing science benchmarks have made progress towards broad Range, wide Reach, and high Rigor, yet they often face two major challenges: data leakage risks that compromise benchmarking validity, and evaluation inefficiency due to large-scale testing. To address these issues, we introduce the Ever-Evolving Science Exam (EESE), a dynamic benchmark designed to reliably assess scientific capabilities in foundation models. Our approach consists of two components: 1) a non-public EESE-Pool with over 100K expertly constructed science instances (question-answer pairs) across 5 disciplines and 500+ subfields, built through a multi-stage pipeline ensuring Range, Reach, and Rigor, 2) a periodically updated 500-instance subset EESE, sampled and validated to enable leakage-resilient, low-overhead evaluations. Experiments on 32 open- and closed-source models demonstrate that EESE effectively differentiates the strengths and weaknesses of models in scientific fields and cognitive dimensions. Overall, EESE provides a robust, scalable, and forward-compatible solution for science benchmark design, offering a realistic measure of how well foundation models handle science questions. The project page is at: https://github.com/aiben-ch/EESE.<br>
<span id='abs_ch'>Chinese: 本文提出的“持续演进科学考试（EESE）”是一个动态基准，通过构建大规模非公开题库和定期更新子集，有效解决数据泄露和评估效率问题，为评估基础模型的科学能力提供了可 、可扩展的解决方案。</span><br>
<span id='abs_en'>English: The Ever-Evolving Science Exam (EESE) is introduced as a dynamic benchmark to reliably evaluate the scientific capabilities of foundation models, addressing data leakage and inefficiency issues through a large, non-public question pool and periodic updates for leakage-resilient, low-overhead assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1263, <a href='https://arxiv.org/pdf/2507.16514.pdf' target='_blank'>https://arxiv.org/pdf/2507.16514.pdf</a></span>   <span><a href='https://github.com/aiben-ch/EESE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junying Wang, Zicheng Zhang, Yijin Guo, Farong Wen, Ye Shen, Yingji Liang, Yalun Wu, Wenzhe Li, Chunyi Li, Zijian Chen, Qi Jia, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16514">The Ever-Evolving Science Exam</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As foundation models grow rapidly in capability and deployment, evaluating their scientific understanding becomes increasingly critical. Existing science benchmarks have made progress towards broad Range, wide Reach, and high Rigor, yet they often face two major challenges: data leakage risks that compromise benchmarking validity, and evaluation inefficiency due to large-scale testing. To address these issues, we introduce the Ever-Evolving Science Exam (EESE), a dynamic benchmark designed to reliably assess scientific capabilities in foundation models. Our approach consists of two components: 1) a non-public EESE-Pool with over 100K expertly constructed science instances (question-answer pairs) across 5 disciplines and 500+ subfields, built through a multi-stage pipeline ensuring Range, Reach, and Rigor, 2) a periodically updated 500-instance subset EESE, sampled and validated to enable leakage-resilient, low-overhead evaluations. Experiments on 32 open- and closed-source models demonstrate that EESE effectively differentiates the strengths and weaknesses of models in scientific fields and cognitive dimensions. Overall, EESE provides a robust, scalable, and forward-compatible solution for science benchmark design, offering a realistic measure of how well foundation models handle science questions. The project page is at: https://github.com/aiben-ch/EESE.<br>
<span id='abs_ch'>Chinese: 本文提出的“持续演进科学考试（EESE）”是一个动态基准，通过构建大规模非公开题库和定期更新子集，有效解决数据泄露和评估效率问题，为评估基础模型的科学能力提供了可 、可扩展的解决方案。</span><br>
<span id='abs_en'>English: The Ever-Evolving Science Exam (EESE) is introduced as a dynamic benchmark to reliably evaluate the scientific capabilities of foundation models, addressing data leakage and inefficiency issues through a large, non-public question pool and periodic updates for leakage-resilient, low-overhead assessments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1264, <a href='https://arxiv.org/pdf/2507.16347.pdf' target='_blank'>https://arxiv.org/pdf/2507.16347.pdf</a></span>   <span><a href='https://github.com/streetcorner/HPGNN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yumeng Wang, Zengyi Wo, Wenjun Wang, Xingcheng Fu, Minglai Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16347">Leveraging Personalized PageRank and Higher-Order Topological Structures for Heterophily Mitigation in Graph Neural Networks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Graph Neural Networks (GNNs) excel in node classification tasks but often assume homophily, where connected nodes share similar labels. This assumption does not hold in many real-world heterophilic graphs. Existing models for heterophilic graphs primarily rely on pairwise relationships, overlooking multi-scale information from higher-order structures. This leads to suboptimal performance, particularly under noise from conflicting class information across nodes. To address these challenges, we propose HPGNN, a novel model integrating Higher-order Personalized PageRank with Graph Neural Networks. HPGNN introduces an efficient high-order approximation of Personalized PageRank (PPR) to capture long-range and multi-scale node interactions. This approach reduces computational complexity and mitigates noise from surrounding information. By embedding higher-order structural information into convolutional networks, HPGNN effectively models key interactions across diverse graph dimensions. Extensive experiments on benchmark datasets demonstrate HPGNN's effectiveness. The model achieves better performance than five out of seven state-of-the-art methods on heterophilic graphs in downstream tasks while maintaining competitive performance on homophilic graphs. HPGNN's ability to balance multi-scale information and robustness to noise makes it a versatile solution for real-world graph learning challenges. Codes are available at https://github.com/streetcorner/HPGNN.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1265, <a href='https://arxiv.org/pdf/2507.16347.pdf' target='_blank'>https://arxiv.org/pdf/2507.16347.pdf</a></span>   <span><a href='https://github.com/streetcorner/HPGNN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yumeng Wang, Zengyi Wo, Wenjun Wang, Xingcheng Fu, Minglai Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16347">Leveraging Personalized PageRank and Higher-Order Topological Structures for Heterophily Mitigation in Graph Neural Networks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Graph Neural Networks (GNNs) excel in node classification tasks but often assume homophily, where connected nodes share similar labels. This assumption does not hold in many real-world heterophilic graphs. Existing models for heterophilic graphs primarily rely on pairwise relationships, overlooking multi-scale information from higher-order structures. This leads to suboptimal performance, particularly under noise from conflicting class information across nodes. To address these challenges, we propose HPGNN, a novel model integrating Higher-order Personalized PageRank with Graph Neural Networks. HPGNN introduces an efficient high-order approximation of Personalized PageRank (PPR) to capture long-range and multi-scale node interactions. This approach reduces computational complexity and mitigates noise from surrounding information. By embedding higher-order structural information into convolutional networks, HPGNN effectively models key interactions across diverse graph dimensions. Extensive experiments on benchmark datasets demonstrate HPGNN's effectiveness. The model achieves better performance than five out of seven state-of-the-art methods on heterophilic graphs in downstream tasks while maintaining competitive performance on homophilic graphs. HPGNN's ability to balance multi-scale information and robustness to noise makes it a versatile solution for real-world graph learning challenges. Codes are available at https://github.com/streetcorner/HPGNN.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1266, <a href='https://arxiv.org/pdf/2507.16280.pdf' target='_blank'>https://arxiv.org/pdf/2507.16280.pdf</a></span>   <span><a href='https://github.com/GAIR-NLP/ResearcherBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianze Xu, Pengrui Lu, Lyumanshan Ye, Xiangkun Hu, Pengfei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16280">ResearcherBench: Evaluating Deep AI Research Systems on the Frontiers of Scientific Inquiry</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The emergence of deep research systems presents significant capabilities in problem-solving, extending from basic queries to sophisticated research tasks. However, existing benchmarks primarily evaluate these systems as agents for web retrieval and report generation, overlooking their potential to discover novel insights on the frontiers of scientific research. To address this gap, we introduce ResearcherBench, the first benchmark focused on evaluating the capabilities of these advanced, agentic systems - which we refer to as Deep AI Research Systems (DARS) - on frontier AI scientific questions. We compiled a dataset of 65 research questions expertly selected from real-world scientific scenarios such as laboratory discussions and interviews, spanning 35 different AI subjects and categorized into three types: technical details, literature review, and open consulting. Our dual evaluation framework combines rubric assessment, which uses expert-designed criteria to evaluate insight quality, with factual assessment, which measures citation accuracy (faithfulness) and coverage (groundedness). We evaluated several leading commercial DARS and baseline systems. Results show that OpenAI Deep Research and Gemini Deep Research significantly outperform other systems, with particular strength in open-ended consulting questions. Such capabilities represent a meaningful step toward AI self-improvement, aligning with the vision of ASI for AI. We open-source ResearcherBench to provide a standardized platform for promoting the development of next-generation AI research assistants, hoping to foster a new perspective in AI research evaluation for a novel pattern of scientific collaboration: https://github.com/GAIR-NLP/ResearcherBench.<br>
<span id='abs_ch'>中文摘要：ResearcherBench是首个针对前沿科学问题评估深度AI 究系统（DARS）的基准，通过专家设计的评估 准与事实准确性测量相结合，推动AI 究能力向自主科学发现发展。English Summary: ResearcherBench is introduced as the first benchmark to evaluate Deep AI Research Systems (DARS) on frontier scientific questions, combining expert-designed rubric assessments with factual accuracy measurements to advance AI research capabilities.Paperid:447,https://arxiv.org/pdf/2507.16251.pdfGitHub</span><br>
<span id='abs_en'>English Summary: ResearcherBench is introduced as the first benchmark to evaluate Deep AI Research Systems (DARS) on frontier scientific questions, combining expert-designed rubric assessments with factual accuracy measurements to advance AI research capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1267, <a href='https://arxiv.org/pdf/2507.16251.pdf' target='_blank'>https://arxiv.org/pdf/2507.16251.pdf</a></span>   <span><a href='https://github.com/vvangfaye/HoliTracer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Wang, Bo Dang, Wanchun Li, Wei Chen, Yansheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16251">HoliTracer: Holistic Vectorization of Geographic Objects from Large-Size Remote Sensing Imagery</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the increasing resolution of remote sensing imagery (RSI), large-size RSI has emerged as a vital data source for high-precision vector mapping of geographic objects. Existing methods are typically constrained to processing small image patches, which often leads to the loss of contextual information and produces fragmented vector outputs. To address these, this paper introduces HoliTracer, the first framework designed to holistically extract vectorized geographic objects from large-size RSI. In HoliTracer, we enhance segmentation of large-size RSI using the Context Attention Net (CAN), which employs a local-to-global attention mechanism to capture contextual dependencies. Furthermore, we achieve holistic vectorization through a robust pipeline that leverages the Mask Contour Reformer (MCR) to reconstruct polygons and the Polygon Sequence Tracer (PST) to trace vertices. Extensive experiments on large-size RSI datasets, including buildings, water bodies, and roads, demonstrate that HoliTracer outperforms state-of-the-art methods. Our code and data are available in https://github.com/vvangfaye/HoliTracer.<br>
<span id='abs_ch'>中文：HoliTracer是一种创新框架，通过上下文注意力网络增强分割，并采用稳健流程实现精确矢量化，能够从大幅遥感影像中整体提取地理对象矢量，性能优于现有方法。English: HoliTracer is a novel framework that holistically extracts vectorized geographic objects from large-size remote sensing imagery using Context Attention Net for enhanced segmentation and a robust pipeline for accurate vectorization, outperforming existing methods.<br>Paperid:448,https://arxiv.org/pdf/2507.16240.pdfGitHub</span><br>
<span id='abs_en'>English: HoliTracer is a novel framework that holistically extracts vectorized geographic objects from large-size remote sensing imagery using Context Attention Net for enhanced segmentation and a robust pipeline for accurate vectorization, outperforming existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1268, <a href='https://arxiv.org/pdf/2507.16213.pdf' target='_blank'>https://arxiv.org/pdf/2507.16213.pdf</a></span>   <span><a href='https://github.com/xiangwentao666/MVP-LM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wentao Xiang, Haoxian Tan, Cong Wei, Yujie Zhong, Dengjie Li, Yujiu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.16213">Advancing Visual Large Language Model for Multi-granular Versatile Perception</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Perception is a fundamental task in the field of computer vision, encompassing a diverse set of subtasks that can be systematically categorized into four distinct groups based on two dimensions: prediction type and instruction type. Notably, existing researches often focus solely on a limited subset of these potential combinations, which constrains their applicability and versatility across various contexts. In response to this challenge, we present MVP-LM, a Multi-granular and Versatile Perception framework incorporating Visual Large Language Model. Our framework is designed to integrate both word-based and sentence-based perception tasks alongside box and mask predictions within a single architecture. MVP-LM features an innovative multi-granularity decoder in conjunction with a CoT-inspired dataset unification strategy, enabling seamless supervised fine-tuning across a wide spectrum of tasks, including but not limited to panoptic segmentation, detection, grounding, and referring expression segmentation. Furthermore, we introduce a query enhancement strategy aimed at harnessing the decoding and generative capabilities inherent in VLLMs. Extensive experiments conducted across a range of benchmarks in both word-based and sentence-based perception tasks substantiate the efficacy of our framework. The code will be available at https://github.com/xiangwentao666/MVP-LM.<br>
<span id='abs_ch'>中文: 该摘要提出了MVP-LM框架，通过多粒度解 器和数据集统一策略，将基于词语和句子的感知任务与边界框及掩 预测整合于单一架构，有效提升了计算机视觉任务的通用性和性能表现。</span><br>
<span id='abs_en'>English: The abstract introduces MVP-LM, a unified framework that integrates word- and sentence-based perception tasks with box and mask predictions using a multi-granularity decoder and dataset unification strategy to enhance versatility across computer vision applications.Paperid:451,https://arxiv.org/pdf/2507.16200.pdfGitHubGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1269, <a href='https://arxiv.org/pdf/2507.15897.pdf' target='_blank'>https://arxiv.org/pdf/2507.15897.pdf</a></span>   <span><a href='https://github.com/Ugness/ReDi_discrete' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaehoon Yoo, Wonjung Kim, Seunghoon Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15897">ReDi: Rectified Discrete Flow</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Discrete Flow-based Models (DFMs) are powerful generative models for high-quality discrete data but typically suffer from slow sampling speeds due to their reliance on iterative decoding processes. This reliance on a multi-step process originates from the factorization approximation of DFMs, which is necessary for handling high-dimensional data. In this paper, we rigorously characterize the approximation error from factorization using Conditional Total Correlation (TC), which depends on the coupling. To reduce the Conditional TC and enable efficient few-step generation, we propose Rectified Discrete Flow (ReDi), a novel iterative method that reduces factorization error by rectifying the coupling between source and target distributions. We theoretically prove that each ReDi step guarantees a monotonic decreasing Conditional TC, ensuring its convergence. Empirically, ReDi significantly reduces Conditional TC and enables few-step generation. Moreover, we demonstrate that the rectified couplings are well-suited for training efficient one-step models on image generation. ReDi offers a simple and theoretically grounded approach for tackling the few-step challenge, providing a new perspective on efficient discrete data synthesis. Code is available at https://github.com/Ugness/ReDi_discrete<br>
<span id='abs_ch'>中文摘要：ReDi是一种通过修正耦合关系来减少离散流模型 式分解误差的新方法，能够实现高效少步生成并确保收敛性。</span><br>
<span id='abs_en'>English Summary: ReDi is a novel method that reduces factorization error in Discrete Flow-based Models by rectifying couplings, enabling efficient few-step generation with guaranteed convergence.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1270, <a href='https://arxiv.org/pdf/2507.15889.pdf' target='_blank'>https://arxiv.org/pdf/2507.15889.pdf</a></span>   <span><a href='https://github.com/NoahVl/Dr-Boot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Noah van der Vleuten
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15889">Dr. Boot: Bootstrapping Program Synthesis Language Models to Perform Repairing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Language models for program synthesis are usually trained and evaluated on programming competition datasets (MBPP, APPS). However, these datasets are limited in size and quality, while these language models are extremely data hungry. Additionally, the language models have a misaligned program synthesis process compared to humans. While humans iteratively develop code with the help of a compiler, most program synthesis models currently produce code in one go. To solve these issues, we introduce a bootstrapping algorithm for program synthesis, that supports teaching models how to repair. We show that bootstrapping consistently outperforms regular fine-tuning. Compared to other work, our bootstrapped model performs on par with fine-tuned models that are 68\% larger. Notably, bootstrapping with repairing also improves non-repairing performance compared to regular bootstrapping during inference. However, on our models, repairing during inference is likely inferior to simply sampling the same number of solutions. Furthermore, we find that there are issues with the example test cases in the training portion of the APPS dataset that are valuable to the community, as many repairing and reinforcement learning methods rely on them.<br>
<span id='abs_ch'>Chinese: 提出的程序合成引导算法通过教授代 修复来提升模型性能，其效果优于 准微调，并能以更小模型实现相当成果，但推理时的修复策略相比多次采 存在局限性。</span><br>
<span id='abs_en'>English: The proposed bootstrapping algorithm for program synthesis enhances model performance by teaching code repair, outperforming standard fine-tuning and achieving comparable results with significantly smaller models, though inference-time repair shows limitations compared to multiple sampling.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1271, <a href='https://arxiv.org/pdf/2507.15867.pdf' target='_blank'>https://arxiv.org/pdf/2507.15867.pdf</a></span>   <span><a href='https://github.com/jhnwu3/RDMA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>John Wu, Adam Cross, Jimeng Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15867">RDMA: Cost Effective Agent-Driven Rare Disease Discovery within Electronic Health Record Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Rare diseases affect 1 in 10 Americans, yet standard ICD coding systems fail to capture these conditions in electronic health records (EHR), leaving crucial information buried in clinical notes. Current approaches struggle with medical abbreviations, miss implicit disease mentions, raise privacy concerns with cloud processing, and lack clinical reasoning abilities. We present Rare Disease Mining Agents (RDMA), a framework that mirrors how medical experts identify rare disease patterns in EHR. RDMA connects scattered clinical observations that together suggest specific rare conditions. By handling clinical abbreviations, recognizing implicit disease patterns, and applying contextual reasoning locally on standard hardware, RDMA reduces privacy risks while improving F1 performance by upwards of 30\% and decreasing inferences costs 10-fold. This approach helps clinicians avoid the privacy risk of using cloud services while accessing key rare disease information from EHR systems, supporting earlier diagnosis for rare disease patients. Available at https://github.com/jhnwu3/RDMA.<br>
<span id='abs_ch'>中文：RDMA框架通过本地处理电子健康记录中的临床笔记，提升罕见疾病检测的准确性，在保护隐私的同时将F1性能提高30%以上并降低十倍推理成本，有助于实现早期诊断。</span><br>
<span id='abs_en'>English: The RDMA framework improves rare disease detection in EHRs by processing clinical notes locally to enhance privacy, increasing F1 performance by over 30% and cutting inference costs tenfold while enabling earlier diagnoses.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1272, <a href='https://arxiv.org/pdf/2507.15852.pdf' target='_blank'>https://arxiv.org/pdf/2507.15852.pdf</a></span>   <span><a href='https://github.com/OpenIXCLab/SeC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhixiong Zhang, Shuangrui Ding, Xiaoyi Dong, Songxin He, Jianfan Lin, Junsong Tang, Yuhang Zang, Yuhang Cao, Dahua Lin, Jiaqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15852">SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Video Object Segmentation (VOS) is a core task in computer vision, requiring models to track and segment target objects across video frames. Despite notable advances with recent efforts, current techniques still lag behind human capabilities in handling drastic visual variations, occlusions, and complex scene changes. This limitation arises from their reliance on appearance matching, neglecting the human-like conceptual understanding of objects that enables robust identification across temporal dynamics. Motivated by this gap, we propose Segment Concept (SeC), a concept-driven segmentation framework that shifts from conventional feature matching to the progressive construction and utilization of high-level, object-centric representations. SeC employs Large Vision-Language Models (LVLMs) to integrate visual cues across diverse frames, constructing robust conceptual priors. During inference, SeC forms a comprehensive semantic representation of the target based on processed frames, realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively balances LVLM-based semantic reasoning with enhanced feature matching, dynamically adjusting computational efforts based on scene complexity. To rigorously assess VOS methods in scenarios demanding high-level conceptual reasoning and robust semantic understanding, we introduce the Semantic Complex Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160 manually annotated multi-scenario videos designed to challenge models with substantial appearance variations and dynamic scene transformations. In particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS, establishing a new state-of-the-art in concept-aware video object segmentation.<br>
<span id='abs_ch'>中文: 提出的Segment Concept (SeC)框架通过从外观匹配转向概念驱动推理，利用大型视觉语言模型构建鲁棒的对象表征，并在新的SeCVOS基准测试中实现了最先进的性能。</span><br>
<span id='abs_en'>English: The proposed Segment Concept (SeC) framework advances video object segmentation by shifting from appearance-based matching to concept-driven reasoning, leveraging large vision-language models to construct robust object representations and achieving state-of-the-art performance on the new SeCVOS benchmark.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1273, <a href='https://arxiv.org/pdf/2507.15844.pdf' target='_blank'>https://arxiv.org/pdf/2507.15844.pdf</a></span>   <span><a href='https://github.com/zju-real/hbpo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangke Lyu, Linjuan Wu, Yuchen Yan, Xingyu Wu, Hao Li, Yongliang Shen, Peisheng Jiang, Weiming Lu, Jun Xiao, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15844">Hierarchical Budget Policy Optimization for Adaptive Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large reasoning models achieve remarkable performance through extensive chain-of-thought generation, yet they suffer from a critical inefficiency: applying uniformly extensive reasoning regardless of problem complexity. We present Hierarchical Budget Policy Optimization (HBPO), a reinforcement learning framework that enables models to learn problem-specific reasoning depths without sacrificing capability. Unlike existing approaches that impose rigid constraints or rely on discrete mode selection, HBPO partitions the exploration space into budget-constrained hierarchies (512-2560 tokens), each with differentiated reward structures that preserve both efficiency incentives and reasoning capabilities. This design addresses a fundamental challenge in efficient reasoning training: traditional length penalties systematically bias models away from necessary long reasoning paths, causing exploration space collapse. Through hierarchical sampling and budget-aware rewards, HBPO maintains exploration diversity while teaching models to recognize when extended deliberation is warranted. Extensive experiments demonstrate that HBPO reduces average token usage by up to 60.6% while improving accuracy by 3.14% across four reasoning benchmarks. Most notably, HBPO exhibits emergent adaptive behavior where models automatically adjust reasoning depth based on problem complexity. Our results suggest that reasoning efficiency and capability are not inherently conflicting, and can be simultaneously optimized through appropriately structured hierarchical training that preserves exploration diversity.<br>
<span id='abs_ch'>中文: HBPO框架通过分层预算策略让模型 据问题复杂度自适应调整推理深度，在四大基准测试中实现最高60.6%的令牌节省，同时准确率提升3.14%。</span><br>
<span id='abs_en'>English: HBPO is a reinforcement learning framework that enables models to adaptively adjust reasoning depth based on problem complexity, achieving up to 60.6% token reduction while improving accuracy by 3.14% across benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1274, <a href='https://arxiv.org/pdf/2507.15765.pdf' target='_blank'>https://arxiv.org/pdf/2507.15765.pdf</a></span>   <span><a href='https://github.com/QIcita/HDF_DFER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, Meng Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15765">Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Dynamic Facial Expression Recognition (DFER) plays a critical role in affective computing and human-computer interaction. Although existing methods achieve comparable performance, they inevitably suffer from performance degradation under sample heterogeneity caused by multi-source data and individual expression variability. To address these challenges, we propose a novel framework, called Heterogeneity-aware Distributional Framework (HDF), and design two plug-and-play modules to enhance time-frequency modeling and mitigate optimization imbalance caused by hard samples. Specifically, the Time-Frequency Distributional Attention Module (DAM) captures both temporal consistency and frequency robustness through a dual-branch attention design, improving tolerance to sequence inconsistency and visual style shifts. Then, based on gradient sensitivity and information bottleneck principles, an adaptive optimization module Distribution-aware Scaling Module (DSM) is introduced to dynamically balance classification and contrastive losses, enabling more stable and discriminative representation learning. Extensive experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF significantly improves both recognition accuracy and robustness. Our method achieves superior weighted average recall (WAR) and unweighted average recall (UAR) while maintaining strong generalization across diverse and imbalanced scenarios. Codes are released at https://github.com/QIcita/HDF_DFER.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1275, <a href='https://arxiv.org/pdf/2507.15758.pdf' target='_blank'>https://arxiv.org/pdf/2507.15758.pdf</a></span>   <span><a href='https://github.com/zju-real/lapoProject:https://zju-real.github.io/lapo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyu Wu, Yuchen Yan, Shangke Lyu, Linjuan Wu, Yiwen Qiu, Yongliang Shen, Weiming Lu, Jian Shao, Jun Xiao, Yueting Zhuang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15758">LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large reasoning models have achieved remarkable performance through extended chain-of-thought sequences, yet this computational freedom leads to excessive token generation even for simple problems. We present Length-Adaptive Policy Optimization (LAPO), a novel framework that transforms reasoning length control from an external constraint into an intrinsic model capability. Unlike existing approaches that impose rigid limits or rely on post-hoc interventions, LAPO enables models to internalize an understanding of appropriate reasoning depth through a two-stage reinforcement learning process. In the first stage, models learn natural reasoning patterns by discovering the statistical distribution of successful solution lengths. The second stage leverages these patterns as meta-cognitive guidance, embedding them directly within the model's reasoning context to ensure inference-time flexibility. Experiments on mathematical reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9% while improving accuracy by 2.3%. Our analysis reveals that models trained with LAPO develop emergent abilities to allocate computational resources based on problem complexity, achieving efficient reasoning without sacrificing quality.<br>
<span id='abs_ch'>中文摘要：LAPO通过两阶段强化学 使模型内化推理深度控制能力，在数学推理基准测试中实现40.9%的令牌使用减少和2.3%的准确率提升，展现出 据问题复杂度自主分配计算资源的新兴能力。English Summary: LAPO is a reinforcement learning framework that enables models to intrinsically control reasoning length by learning optimal solution patterns, reducing token usage by 40.9% while improving accuracy by 2.3% through adaptive computational allocation.Paperid:472,https://arxiv.org/pdf/2507.15752.pdfGitHub</span><br>
<span id='abs_en'>English Summary: LAPO is a reinforcement learning framework that enables models to intrinsically control reasoning length by learning optimal solution patterns, reducing token usage by 40.9% while improving accuracy by 2.3% through adaptive computational allocation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1276, <a href='https://arxiv.org/pdf/2507.15752.pdf' target='_blank'>https://arxiv.org/pdf/2507.15752.pdf</a></span>   <span><a href='https://github.com/nerchio/Human_Chatbot-Generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruizhe Zhu, Hao Zhu, Yaxuan Li, Syang Zhou, Shijing Cai, Malgorzata Lazuka, Elliott Ash
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15752">DialogueForge: LLM Simulation of Human-Chatbot Dialogue</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Collecting human-chatbot dialogues typically demands substantial manual effort and is time-consuming, which limits and poses challenges for research on conversational AI. In this work, we propose DialogueForge - a framework for generating AI-simulated conversations in human-chatbot style. To initialize each generated conversation, DialogueForge uses seed prompts extracted from real human-chatbot interactions. We test a variety of LLMs to simulate the human chatbot user, ranging from state-of-the-art proprietary models to small-scale open-source LLMs, and generate multi-turn dialogues tailored to specific tasks. In addition, we explore fine-tuning techniques to enhance the ability of smaller models to produce indistinguishable human-like dialogues. We evaluate the quality of the simulated conversations and compare different models using the UniEval and GTEval evaluation protocols. Our experiments show that large proprietary models (e.g., GPT-4o) generally outperform others in generating more realistic dialogues, while smaller open-source models (e.g., Llama, Mistral) offer promising performance with greater customization. We demonstrate that the performance of smaller models can be significantly improved by employing supervised fine-tuning techniques. Nevertheless, maintaining coherent and natural long-form human-like dialogues remains a common challenge across all models.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1277, <a href='https://arxiv.org/pdf/2507.15617.pdf' target='_blank'>https://arxiv.org/pdf/2507.15617.pdf</a></span>   <span><a href='https://github.com/edlowther/automated-epidemiology' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>David Bann, Ed Lowther, Liam Wright, Yevgeniya Kovalchuk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15617">Why can't Epidemiology be automated (yet)?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in artificial intelligence (AI) - particularly generative AI - present new opportunities to accelerate, or even automate, epidemiological research. Unlike disciplines based on physical experimentation, a sizable fraction of Epidemiology relies on secondary data analysis and thus is well-suited for such augmentation. Yet, it remains unclear which specific tasks can benefit from AI interventions or where roadblocks exist. Awareness of current AI capabilities is also mixed. Here, we map the landscape of epidemiological tasks using existing datasets - from literature review to data access, analysis, writing up, and dissemination - and identify where existing AI tools offer efficiency gains. While AI can increase productivity in some areas such as coding and administrative tasks, its utility is constrained by limitations of existing AI models (e.g. hallucinations in literature reviews) and human systems (e.g. barriers to accessing datasets). Through examples of AI-generated epidemiological outputs, including fully AI-generated papers, we demonstrate that recently developed agentic systems can now design and execute epidemiological analysis, albeit to varied quality (see https://github.com/edlowther/automated-epidemiology). Epidemiologists have new opportunities to empirically test and benchmark AI systems; realising the potential of AI will require two-way engagement between epidemiologists and engineers.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1278, <a href='https://arxiv.org/pdf/2507.15577.pdf' target='_blank'>https://arxiv.org/pdf/2507.15577.pdf</a></span>   <span><a href='https://github.com/hugocarlesso/GeMix' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hugo Carlesso, Maria Eliza Patulea, Moncef Garouani, Radu Tudor Ionescu, Josiane Mothe
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15577">GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Mixup has become a popular augmentation strategy for image classification, yet its naive pixel-wise interpolation often produces unrealistic images that can hinder learning, particularly in high-stakes medical applications. We propose GeMix, a two-stage framework that replaces heuristic blending with a learned, label-aware interpolation powered by class-conditional GANs. First, a StyleGAN2-ADA generator is trained on the target dataset. During augmentation, we sample two label vectors from Dirichlet priors biased toward different classes and blend them via a Beta-distributed coefficient. Then, we condition the generator on this soft label to synthesize visually coherent images that lie along a continuous class manifold. We benchmark GeMix on the large-scale COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101, EfficientNet-B0). When combined with real data, our method increases macro-F1 over traditional mixup for all backbones, reducing the false negative rate for COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup, delivering stronger regularization and greater semantic fidelity, without disrupting existing training pipelines. We publicly release our code at https://github.com/hugocarlesso/GeMix to foster reproducibility and further research.<br>
<span id='abs_ch'>中文摘要：GeMix提出了一种基于类别条件生成对抗网络的两阶段框架，通过生成具有语义连续性的逼真插值图像，在COVID-19检测任务中全面优于 统混合增强方法，显著降低了假阴性率并保持即插即用的特性。English Summary: GeMix introduces a two-stage framework using class-conditional GANs to generate realistic, label-aware interpolated images for medical image classification, outperforming traditional mixup methods across multiple backbones on COVID-19 detection while reducing false negatives.Paperid:480,https://arxiv.org/pdf/2507.15576.pdfGitHub</span><br>
<span id='abs_en'>English Summary: GeMix introduces a two-stage framework using class-conditional GANs to generate realistic, label-aware interpolated images for medical image classification, outperforming traditional mixup methods across multiple backbones on COVID-19 detection while reducing false negatives.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1279, <a href='https://arxiv.org/pdf/2507.15524.pdf' target='_blank'>https://arxiv.org/pdf/2507.15524.pdf</a></span>   <span><a href='https://github.com/simonsejse/RARE-UNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Simon Winther Albertsen, Hjalte Svaneborg BjÃ¸rnstrup, Mostafa Mehdipour Ghazi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15524">RARE-UNet: Resolution-Aligned Routing Entry for Adaptive Medical Image Segmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate segmentation is crucial for clinical applications, but existing models often assume fixed, high-resolution inputs and degrade significantly when faced with lower-resolution data in real-world scenarios. To address this limitation, we propose RARE-UNet, a resolution-aware multi-scale segmentation architecture that dynamically adapts its inference path to the spatial resolution of the input. Central to our design are multi-scale blocks integrated at multiple encoder depths, a resolution-aware routing mechanism, and consistency-driven training that aligns multi-resolution features with full-resolution representations. We evaluate RARE-UNet on two benchmark brain imaging tasks for hippocampus and tumor segmentation. Compared to standard UNet, its multi-resolution augmented variant, and nnUNet, our model achieves the highest average Dice scores of 0.84 and 0.65 across resolution, while maintaining consistent performance and significantly reduced inference time at lower resolutions. These results highlight the effectiveness and scalability of our architecture in achieving resolution-robust segmentation. The codes are available at: https://github.com/simonsejse/RARE-UNet.<br>
<span id='abs_ch'>Chinese: RARE-UNet是一种分辨率自适应的分割架构，能 据输入分辨率动态调整推理路径，相比现有模型在不同分辨率下均获得更高的Dice分数并显著减少推理时间。</span><br>
<span id='abs_en'>English: RARE-UNet is a resolution-aware segmentation architecture that dynamically adapts to input resolutions, achieving superior Dice scores and faster inference across varied resolutions compared to existing models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1280, <a href='https://arxiv.org/pdf/2507.15518.pdf' target='_blank'>https://arxiv.org/pdf/2507.15518.pdf</a></span>   <span><a href='https://github.com/HAMLET-2025/HAMLET' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sizhou Chen, Shufan Jiang, Chi Zhang, Xiao-Lei Zhang, Xuelong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15518">HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Creating an immersive and interactive theatrical experience is a long-term goal in the field of interactive narrative. The emergence of large language model (LLM) is providing a new path to achieve this goal. However, existing LLM-based drama generation methods often result in AI agents that lack initiative and cannot interact with the physical environment. Furthermore, these methods typically require detailed user input to drive the drama. These limitations reduce the interactivity and immersion of online real-time performance. To address the above challenges, we propose HAMLET, a multi-agent framework focused on drama creation and online performance. Given a simple topic, the framework generates a narrative blueprint, guiding the subsequent improvisational performance. During the online performance, each actor is given an autonomous mind. This means that actors can make independent decisions based on their own background, goals, and emotional state. In addition to conversations with other actors, their decisions can also change the state of scene props through actions such as opening a letter or picking up a weapon. The change is then broadcast to other related actors, updating what they know and care about, which in turn influences their next action. To evaluate the quality of drama performance, we designed an evaluation method to assess three primary aspects, including character performance, narrative quality, and interaction experience. The experimental evaluation shows that HAMLET can create expressive and coherent theatrical experiences. Our code, dataset and models are available at https://github.com/HAMLET-2025/HAMLET.<br>
<span id='abs_ch'>中文: HAMLET是一个多智能体框架，通过赋予演员自主决策能力并与角色及场景道具互动，生成具有表现力和连贯性的沉浸式戏剧体验，提升了线上演出的互动性。</span><br>
<span id='abs_en'>English: HAMLET is a multi-agent framework that generates immersive theatrical experiences by enabling autonomous actors to make decisions and interact with both characters and physical props, enhancing interactivity and coherence in online performances.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1281, <a href='https://arxiv.org/pdf/2507.15507.pdf' target='_blank'>https://arxiv.org/pdf/2507.15507.pdf</a></span>   <span><a href='https://github.com/JohannesAck/OffPolicyCorrectedRewardModeling' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Johannes Ackermann, Takashi Ishida, Masashi Sugiyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15507">Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning from Human Feedback (RLHF) allows us to train models, such as language models (LMs), to follow complex human preferences. In RLHF for LMs, we first train an LM using supervised fine-tuning, sample pairs of responses, obtain human feedback, and use the resulting data to train a reward model (RM). RL methods are then used to train the LM to maximize the reward given by the RM. As training progresses, the responses generated by the LM no longer resemble the responses seen by the RM during training, leading to the RM becoming inaccurate. The score given by the RM keeps increasing, but the learned behavior no longer matches the human preferences. This issue is known as overoptimization. We investigate overoptimization from the point of view of distribution shift and show that the shift results in an inconsistent estimate of the RM parameters, leading to an inconsistent estimate of the policy gradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which iteratively off-policy corrects the RM using importance weighting, without requiring new labels or samples. This results in a more accurate RM, which empirically leads to an improved final policy. We validate our approach in experiments with summarization and chatbot datasets and show that it performs significantly better than standard RLHF methods and baselines. Our implementation is available at https://github.com/JohannesAck/OffPolicyCorrectedRewardModeling<br>
<span id='abs_ch'>中文摘要：基于人类反馈的强化学 （RLHF）训练语言模型以符合人类偏好，但存在 分布偏移导致的过度优化问题，提出的离策略 正奖励建模（OCRM）方法通过重要性 权 正奖励模型，显著提升了模型性能。</span><br>
<span id='abs_en'>English summary: Reinforcement Learning from Human Feedback (RLHF) trains language models to align with human preferences but faces overoptimization due to distribution shift, which is addressed by the proposed Off-Policy Corrected Reward Modeling (OCRM) method for improved performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1282, <a href='https://arxiv.org/pdf/2507.15381.pdf' target='_blank'>https://arxiv.org/pdf/2507.15381.pdf</a></span>   <span><a href='https://github.com/juliamachnio/PALM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Julia Machnio, Mads Nielsen, Mostafa Mehdipour Ghazi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15381">To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Active learning (AL) seeks to reduce annotation costs by selecting the most informative samples for labeling, making it particularly valuable in resource-constrained settings. However, traditional evaluation methods, which focus solely on final accuracy, fail to capture the full dynamics of the learning process. To address this gap, we propose PALM (Performance Analysis of Active Learning Models), a unified and interpretable mathematical model that characterizes AL trajectories through four key parameters: achievable accuracy, coverage efficiency, early-stage performance, and scalability. PALM provides a predictive description of AL behavior from partial observations, enabling the estimation of future performance and facilitating principled comparisons across different strategies. We validate PALM through extensive experiments on CIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and self-supervised embeddings. Our results demonstrate that PALM generalizes effectively across datasets, budgets, and strategies, accurately predicting full learning curves from limited labeled data. Importantly, PALM reveals crucial insights into learning efficiency, data space coverage, and the scalability of AL methods. By enabling the selection of cost-effective strategies and predicting performance under tight budget constraints, PALM lays the basis for more systematic, reproducible, and data-efficient evaluation of AL in both research and real-world applications. The code is available at: https://github.com/juliamachnio/PALM.<br>
<span id='abs_ch'>中文: PALM提出了一种统一的数学模型，通过四个关键参数描述主动学 轨迹，能在不同数据集和预算下实现性能预测与策略比较。English: PALM introduces a unified mathematical model to characterize active learning trajectories through four key parameters, enabling performance prediction and strategy comparison across diverse datasets and budgets.Paperid:494,https://arxiv.org/pdf/2507.15297.pdfGitHub</span><br>
<span id='abs_en'>English: PALM introduces a unified mathematical model to characterize active learning trajectories through four key parameters, enabling performance prediction and strategy comparison across diverse datasets and budgets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1283, <a href='https://arxiv.org/pdf/2507.15253.pdf' target='_blank'>https://arxiv.org/pdf/2507.15253.pdf</a></span>   <span><a href='https://github.com/Uncnbb/DMGC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaochen Guo, Zhixiang Shen, Xuanting Xie, Liangjian Wen, Zhao Kang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15253">Disentangling Homophily and Heterophily in Multimodal Graph Clustering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal graphs, which integrate unstructured heterogeneous data with structured interconnections, offer substantial real-world utility but remain insufficiently explored in unsupervised learning. In this work, we initiate the study of multimodal graph clustering, aiming to bridge this critical gap. Through empirical analysis, we observe that real-world multimodal graphs often exhibit hybrid neighborhood patterns, combining both homophilic and heterophilic relationships. To address this challenge, we propose a novel framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which decomposes the original hybrid graph into two complementary views: (1) a homophily-enhanced graph that captures cross-modal class consistency, and (2) heterophily-aware graphs that preserve modality-specific inter-class distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism that jointly filters these disentangled graphs through a dual-pass strategy, enabling effective multimodal integration while mitigating category confusion. Our self-supervised alignment objectives further guide the learning process without requiring labels. Extensive experiments on both multimodal and multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art performance, highlighting its effectiveness and generalizability across diverse settings. Our code is available at https://github.com/Uncnbb/DMGC.<br>
<span id='abs_ch'>中文: 本文提出的DMGC框架通过将混合邻域模式分解为同质性增强图和异质性感知图，采用自监督学  需 签即可实现多模态图聚类，并在多种数据集上取得了最优性能。</span><br>
<span id='abs_en'>English: This paper introduces DMGC, a novel framework for multimodal graph clustering that disentangles hybrid neighborhood patterns into homophily-enhanced and heterophily-aware graphs, achieving state-of-the-art performance through self-supervised learning without requiring labels.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1284, <a href='https://arxiv.org/pdf/2507.15245.pdf' target='_blank'>https://arxiv.org/pdf/2507.15245.pdf</a></span>   <span><a href='https://github.com/xiaofengShi/SPAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaofeng Shi, Yuduo Li, Qian Kou, Longbin Yu, Jinxin Xie, Hua Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15245">SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced Academic Search</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in large language models (LLMs) have opened new opportunities for academic literature retrieval. However, existing systems often rely on rigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR, a multi-agent framework that incorporates RefChain-based query decomposition and query evolution to enable more flexible and effective search. To facilitate systematic evaluation, we also construct SPARBench, a challenging benchmark with expert-annotated relevance labels. Experimental results demonstrate that SPAR substantially outperforms strong baselines, achieving up to +56% F1 on AutoScholar and +23% F1 on SPARBench over the best-performing baseline. Together, SPAR and SPARBench provide a scalable, interpretable, and high-performing foundation for advancing research in scholarly retrieval. Code and data will be available at: https://github.com/xiaofengShi/SPAR<br>
<span id='abs_ch'>中文：SPAR采用多智能体框架，结合RefChain查询分解与进化技术，显著提升学术文献检索性能，在基准测试中F1值最高超出基线56%，并构建了SPARBench作为系统评估基准。</span><br>
<span id='abs_en'>English: SPAR introduces a multi-agent framework with RefChain query decomposition and evolution to enhance academic literature retrieval, significantly outperforming baselines by up to +56% F1 and establishing SPARBench as a robust evaluation benchmark.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1285, <a href='https://arxiv.org/pdf/2507.15243.pdf' target='_blank'>https://arxiv.org/pdf/2507.15243.pdf</a></span>   <span><a href='https://github.com/Naeem-Paeedeh/CPLSR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Naeem Paeedeh, Mahardhika Pratama, Wolfgang Mayer, Jimmy Cao, Ryszard Kowlczyk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15243">Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model pre-trained with DINO combined with a prototypical classifier outperforms the latest SOTA methods. A crucial limitation that needs to be overcome is that updating too many parameters of the transformers leads to overfitting due to the scarcity of labeled samples. To address this challenge, we propose a new concept, Coalescent Projection (CP), as an effective successor to soft prompts. Additionally, we propose a novel pseudo-class generation method combined with Self-Supervised Transformations (SSTs) that relies solely on the base domain to prepare the network for encountering unseen samples from different domains. The proposed method exhibits its effectiveness in comprehensive experiments on the extreme domain shift scenario of the BSCD-FSL benchmark. Our code is published at https://github.com/Naeem-Paeedeh/CPLSR.<br>
<span id='abs_ch'>中文: 本 究提出融合投影和结合自监督变换的伪类生成方法，以解决跨域少 本学 中的过拟合问题，在BSCD-FSL基准测试中展现出卓越性能。</span><br>
<span id='abs_en'>English: The study introduces Coalescent Projection and a pseudo-class generation method with Self-Supervised Transformations to overcome overfitting in Cross-Domain Few-Shot Learning, demonstrating superior performance on the BSCD-FSL benchmark.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1286, <a href='https://arxiv.org/pdf/2507.15066.pdf' target='_blank'>https://arxiv.org/pdf/2507.15066.pdf</a></span>   <span><a href='https://github.com/yyysjz1997/Time-RA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyuan Yang, Zichuan Liu, Lei Song, Kai Ying, Zhiguang Wang, Tom Bamford, Svitlana Vyetrenko, Jiang Bian, Qingsong Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15066">Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Time series anomaly detection is critical across various domains, yet current approaches often limit analysis to mere binary anomaly classification without detailed categorization or further explanatory reasoning. To address these limitations, we propose a novel task, Time-series Reasoning for Anomaly (Time-RA) that transforms classical time series anomaly detection from a discriminative into a generative, reasoning-intensive task leveraging Large Language Models (LLMs). Also, we introduce the first real-world multimodal benchmark dataset, RATs40K, explicitly annotated for anomaly reasoning, comprising approximately 40,000 samples across 10 real-world domains. Each sample includes numeric time series data, contextual text information, and visual representations, each annotated with fine-grained categories (14 types for univariate anomalies and 6 for multivariate anomalies) and structured explanatory reasoning. We develop a sophisticated annotation framework utilizing ensemble-generated labels refined through GPT-4-driven feedback, ensuring accuracy and interpretability. Extensive benchmarking of LLMs and multimodal LLMs demonstrates the capabilities and limitations of current models, highlighting the critical role of supervised fine-tuning. Our dataset and task pave the way for significant advancements in interpretable time series anomaly detection and reasoning. The code (https://github.com/yyysjz1997/Time-RA) and dataset (https://huggingface.co/datasets/Time-RA/RATs40K) have been fully open-sourced to support and accelerate future research in this area.<br>
<span id='abs_ch'>中文摘要：该 究提出了Time-RA任务，利用大语言模型将时序异常检测转化为生成式推理任务，并发布了包含4万 本的多模态基准数据集RATs40K，推动可解释异常检测的发展。</span><br>
<span id='abs_en'>English Summary: The study introduces Time-RA, a generative reasoning task using Large Language Models for detailed anomaly categorization and explanation in time series data, supported by the multimodal RATs40K benchmark dataset with fine-grained annotations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1287, <a href='https://arxiv.org/pdf/2507.15066.pdf' target='_blank'>https://arxiv.org/pdf/2507.15066.pdf</a></span>   <span><a href='https://github.com/yyysjz1997/Time-RA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyuan Yang, Zichuan Liu, Lei Song, Kai Ying, Zhiguang Wang, Tom Bamford, Svitlana Vyetrenko, Jiang Bian, Qingsong Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15066">Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Time series anomaly detection is critical across various domains, yet current approaches often limit analysis to mere binary anomaly classification without detailed categorization or further explanatory reasoning. To address these limitations, we propose a novel task, Time-series Reasoning for Anomaly (Time-RA) that transforms classical time series anomaly detection from a discriminative into a generative, reasoning-intensive task leveraging Large Language Models (LLMs). Also, we introduce the first real-world multimodal benchmark dataset, RATs40K, explicitly annotated for anomaly reasoning, comprising approximately 40,000 samples across 10 real-world domains. Each sample includes numeric time series data, contextual text information, and visual representations, each annotated with fine-grained categories (14 types for univariate anomalies and 6 for multivariate anomalies) and structured explanatory reasoning. We develop a sophisticated annotation framework utilizing ensemble-generated labels refined through GPT-4-driven feedback, ensuring accuracy and interpretability. Extensive benchmarking of LLMs and multimodal LLMs demonstrates the capabilities and limitations of current models, highlighting the critical role of supervised fine-tuning. Our dataset and task pave the way for significant advancements in interpretable time series anomaly detection and reasoning. The code (https://github.com/yyysjz1997/Time-RA) and dataset (https://huggingface.co/datasets/Time-RA/RATs40K) have been fully open-sourced to support and accelerate future research in this area.<br>
<span id='abs_ch'>中文摘要：该 究提出了Time-RA任务，利用大语言模型将时序异常检测转化为生成式推理任务，并发布了包含4万 本的多模态基准数据集RATs40K，推动可解释异常检测的发展。</span><br>
<span id='abs_en'>English Summary: The study introduces Time-RA, a generative reasoning task using Large Language Models for detailed anomaly categorization and explanation in time series data, supported by the multimodal RATs40K benchmark dataset with fine-grained annotations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1288, <a href='https://arxiv.org/pdf/2507.15003.pdf' target='_blank'>https://arxiv.org/pdf/2507.15003.pdf</a></span>   <span><a href='https://github.com/SAILResearch/AI_Teammates_in_SE3' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Li, Haoxiang Zhang, Ahmed E. Hassan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.15003">The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The future of software engineering--SE 3.0--is unfolding with the rise of AI teammates: autonomous, goal-driven systems collaborating with human developers. Among these, autonomous coding agents are especially transformative, now actively initiating, reviewing, and evolving code at scale. This paper introduces AIDev, the first large-scale dataset capturing how such agents operate in the wild. Spanning over 456,000 pull requests by five leading agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across 61,000 repositories and 47,000 developers, AIDev provides an unprecedented empirical foundation for studying autonomous teammates in software development.
  Unlike prior work that has largely theorized the rise of AI-native software engineering, AIDev offers structured, open data to support research in benchmarking, agent readiness, optimization, collaboration modeling, and AI governance. The dataset includes rich metadata on PRs, authorship, review timelines, code changes, and integration outcomes--enabling exploration beyond synthetic benchmarks like SWE-bench. For instance, although agents often outperform humans in speed, their PRs are accepted less frequently, revealing a trust and utility gap. Furthermore, while agents accelerate code submission--one developer submitted as many PRs in three days as they had in three years--these are structurally simpler (via code complexity metrics).
  We envision AIDev as a living resource: extensible, analyzable, and ready for the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev enables a new generation of research into AI-native workflows and supports building the next wave of symbiotic human-AI collaboration. The dataset is publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering Agent<br>
<br>
<div id='section'>Paperid: <span id='pid'>1289, <a href='https://arxiv.org/pdf/2507.14828.pdf' target='_blank'>https://arxiv.org/pdf/2507.14828.pdf</a></span>   <span><a href='https://github.com/sfi-norwai/eMargin' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdul-Kazeem Shamba, Kerstin Bach, Gavin Taylor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14828">eMargin: Revisiting Contrastive Learning with Margin-Based Separation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We revisit previous contrastive learning frameworks to investigate the effect of introducing an adaptive margin into the contrastive loss function for time series representation learning. Specifically, we explore whether an adaptive margin (eMargin), adjusted based on a predefined similarity threshold, can improve the separation between adjacent but dissimilar time steps and subsequently lead to better performance in downstream tasks. Our study evaluates the impact of this modification on clustering performance and classification in three benchmark datasets. Our findings, however, indicate that achieving high scores on unsupervised clustering metrics does not necessarily imply that the learned embeddings are meaningful or effective in downstream tasks. To be specific, eMargin added to InfoNCE consistently outperforms state-of-the-art baselines in unsupervised clustering metrics, but struggles to achieve competitive results in downstream classification with linear probing. The source code is publicly available at https://github.com/sfi-norwai/eMargin.<br>
<span id='abs_ch'>中文: 本 究在时间序列对比学 中引入自适应边界(eMargin)，发现其虽能提升 监督聚类指 ，但未能改善下游分类任务的性能。</span><br>
<span id='abs_en'>English: This study introduces an adaptive margin (eMargin) into contrastive loss for time series learning, finding it improves unsupervised clustering metrics but fails to translate into better downstream classification performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1290, <a href='https://arxiv.org/pdf/2507.14824.pdf' target='_blank'>https://arxiv.org/pdf/2507.14824.pdf</a></span>   <span><a href='https://github.com/nliulab/MIMIC-Multimodal' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kunyu Yu, Rui Yang, Jingchi Liao, Siqi Li, Huitao Li, Irene Li, Yifan Peng, Rishikesan Kamaleswaran, Nan Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14824">Benchmarking Foundation Models with Multimodal Public Electronic Health Records</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Foundation models have emerged as a powerful approach for processing electronic health records (EHRs), offering flexibility to handle diverse medical data modalities. In this study, we present a comprehensive benchmark that evaluates the performance, fairness, and interpretability of foundation models, both as unimodal encoders and as multimodal learners, using the publicly available MIMIC-IV database. To support consistent and reproducible evaluation, we developed a standardized data processing pipeline that harmonizes heterogeneous clinical records into an analysis-ready format. We systematically compared eight foundation models, encompassing both unimodal and multimodal models, as well as domain-specific and general-purpose variants. Our findings demonstrate that incorporating multiple data modalities leads to consistent improvements in predictive performance without introducing additional bias. Through this benchmark, we aim to support the development of effective and trustworthy multimodal artificial intelligence (AI) systems for real-world clinical applications. Our code is available at https://github.com/nliulab/MIMIC-Multimodal.<br>
<span id='abs_ch'>中文: 本 究基于MIMIC-IV数据库构建了综合评估基准，结果表明多模态融合能持续提升预测性能且不引入额外偏差，旨在推动可信赖医疗AI系统的发展。</span><br>
<span id='abs_en'>English: This study establishes a comprehensive benchmark evaluating foundation models' performance, fairness, and interpretability using the MIMIC-IV database, demonstrating that multimodal integration enhances predictive accuracy without increasing bias.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1291, <a href='https://arxiv.org/pdf/2507.14799.pdf' target='_blank'>https://arxiv.org/pdf/2507.14799.pdf</a></span>   <span><a href='https://github.com/sej2020/manipulating-web-agents' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sam Johnson, Viet Pham, Thai Le
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14799">Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This work demonstrates that LLM-based web navigation agents offer powerful automation capabilities but are vulnerable to Indirect Prompt Injection (IPI) attacks. We show that adversaries can embed universal adversarial triggers in webpage HTML to hijack agent behavior that utilizes the accessibility tree to parse HTML, causing unintended or malicious actions. Using the Greedy Coordinate Gradient (GCG) algorithm and a Browser Gym agent powered by Llama-3.1, our system demonstrates high success rates across real websites in both targeted and general attacks, including login credential exfiltration and forced ad clicks. Our empirical results highlight critical security risks and the need for stronger defenses as LLM-driven autonomous web agents become more widely adopted. The system software (https://github.com/sej2020/manipulating-web-agents) is released under the MIT License, with an accompanying publicly available demo website (http://lethaiq.github.io/attack-web-llm-agent).<br>
<span id='abs_ch'>中文: 基于大语言模型的网页导航代理易受间接提示注入攻击，攻击者可通过在网页HTML中嵌入触发器来劫持代理行为，导致如窃取登录凭证等严重安全风险。</span><br>
<span id='abs_en'>English: LLM-based web navigation agents are vulnerable to Indirect Prompt Injection attacks, where adversaries embed triggers in webpage HTML to hijack agent behavior, leading to security risks like credential theft and unauthorized actions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1292, <a href='https://arxiv.org/pdf/2507.14660.pdf' target='_blank'>https://arxiv.org/pdf/2507.14660.pdf</a></span>   <span><a href='https://github.com/renqibing/RogueAgent' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/renqibing/MultiAgent4Collusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qibing Ren, Sitao Xie, Longxuan Wei, Zhenfei Yin, Junchi Yan, Lizhuang Ma, Jing Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14660">When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent large-scale events like election fraud and financial scams have shown how harmful coordinated efforts by human groups can be. With the rise of autonomous AI systems, there is growing concern that AI-driven groups could also cause similar harm. While most AI safety research focuses on individual AI systems, the risks posed by multi-agent systems (MAS) in complex real-world situations are still underexplored. In this paper, we introduce a proof-of-concept to simulate the risks of malicious MAS collusion, using a flexible framework that supports both centralized and decentralized coordination structures. We apply this framework to two high-risk fields: misinformation spread and e-commerce fraud. Our findings show that decentralized systems are more effective at carrying out malicious actions than centralized ones. The increased autonomy of decentralized systems allows them to adapt their strategies and cause more damage. Even when traditional interventions, like content flagging, are applied, decentralized groups can adjust their tactics to avoid detection. We present key insights into how these malicious groups operate and the need for better detection systems and countermeasures. Code is available at https://github.com/renqibing/RogueAgent.<br>
<span id='abs_ch'>中文: 本文提出一个模拟恶意多智能体系统风险的框架， 究表明去中心化AI群体在执行虚假信息 播和电商欺诈等恶意行为时更具适应性，能够规避 统干预措施。</span><br>
<span id='abs_en'>English: This paper introduces a simulation framework to assess the risks of malicious multi-agent systems, demonstrating that decentralized AI groups are more effective at executing harmful actions like misinformation and fraud while evading traditional countermeasures.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1293, <a href='https://arxiv.org/pdf/2507.14575.pdf' target='_blank'>https://arxiv.org/pdf/2507.14575.pdf</a></span>   <span><a href='https://github.com/AndreaMoschetto/medical-I2I-benchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Moschetto, Lemuel Puglisi, Alec Sargood, Pierluigi Dell'Acqua, Francesco Guarnera, Sebastiano Battiato, Daniele RavÃ¬
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14575">Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering distinct diagnostic insights. However, acquiring all desired modalities increases scan time and cost, motivating research into computational methods for cross-modal synthesis. To address this, recent approaches aim to synthesize missing MRI contrasts from those already acquired, reducing acquisition time while preserving diagnostic quality. Image-to-image (I2I) translation provides a promising framework for this task. In this paper, we present a comprehensive benchmark of generative models$\unicode{x2013}$specifically, Generative Adversarial Networks (GANs), diffusion models, and flow matching (FM) techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All frameworks are implemented with comparable settings and evaluated on three publicly available MRI datasets of healthy adults. Our quantitative and qualitative analyses show that the GAN-based Pix2Pix model outperforms diffusion and FM-based methods in terms of structural fidelity, image quality, and computational efficiency. Consistent with existing literature, these results suggest that flow-based models are prone to overfitting on small datasets and simpler tasks, and may require more data to match or surpass GAN performance. These findings offer practical guidance for deploying I2I translation techniques in real-world MRI workflows and highlight promising directions for future research in cross-modal medical image synthesis. Code and models are publicly available at https://github.com/AndreaMoschetto/medical-I2I-benchmark.<br>
<span id='abs_ch'>中文: 本文对跨模态磁共振图像合成的生成模型进行基准测试，发现基于GAN的Pix2Pix在图像质量和效率上优于扩散模型与流匹配方法，同时揭示了流模型存在数据依赖性问题。</span><br>
<span id='abs_en'>English: This paper benchmarks generative models for cross-modal MRI synthesis, finding that GAN-based Pix2Pix outperforms diffusion and flow matching methods in image quality and efficiency while highlighting flow models' data dependency issues.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1294, <a href='https://arxiv.org/pdf/2507.14544.pdf' target='_blank'>https://arxiv.org/pdf/2507.14544.pdf</a></span>   <span><a href='https://github.com/TiwariLaxuu/VQA-Florence.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sujata Gaihre, Amir Thapa Magar, Prasuna Pokharel, Laxmi Tiwari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14544">Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA 2025 Challenge, which targets visual question answering (VQA) for gastrointestinal endoscopy. We adopt the Florence model-a large-scale multimodal foundation model-as the backbone of our VQA pipeline, pairing a powerful vision encoder with a text encoder to interpret endoscopic images and produce clinically relevant answers. To improve generalization, we apply domain-specific augmentations that preserve medical features while increasing training diversity. Experiments on the KASVIR dataset show that fine-tuning Florence yields accurate responses on the official challenge metrics. Our results highlight the potential of large multimodal models in medical VQA and provide a strong baseline for future work on explainability, robustness, and clinical integration. The code is publicly available at: https://github.com/TiwariLaxuu/VQA-Florence.git<br>
<span id='abs_ch'>中文: 本文采用Florence多模态基础模型构建胃 内镜视觉问答系统，通过领域特定数据增强和微调在MEDVQA 2025挑战中取得优异表现，为医疗VQA应用提供了重要基准。</span><br>
<span id='abs_en'>English: This paper presents a Florence-based VQA system for gastrointestinal endoscopy that uses domain-specific augmentation and fine-tuning to achieve strong performance on the MEDVQA 2025 challenge, demonstrating the potential of large multimodal models in medical applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1295, <a href='https://arxiv.org/pdf/2507.14519.pdf' target='_blank'>https://arxiv.org/pdf/2507.14519.pdf</a></span>   <span><a href='https://github.com/PKU-SEC-Lab/Awesome-PPML-Papers' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenxuan Zeng, Tianshi Xu, Yi Chen, Yifan Zhou, Mingzhe Zhang, Jin Tan, Cheng Hong, Meng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14519">Towards Efficient Privacy-Preserving Machine Learning: A Systematic Review from Protocol, Model, and System Perspectives</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Privacy-preserving machine learning (PPML) based on cryptographic protocols has emerged as a promising paradigm to protect user data privacy in cloud-based machine learning services. While it achieves formal privacy protection, PPML often incurs significant efficiency and scalability costs due to orders of magnitude overhead compared to the plaintext counterpart. Therefore, there has been a considerable focus on mitigating the efficiency gap for PPML. In this survey, we provide a comprehensive and systematic review of recent PPML studies with a focus on cross-level optimizations. Specifically, we categorize existing papers into protocol level, model level, and system level, and review progress at each level. We also provide qualitative and quantitative comparisons of existing works with technical insights, based on which we discuss future research directions and highlight the necessity of integrating optimizations across protocol, model, and system levels. We hope this survey can provide an overarching understanding of existing approaches and potentially inspire future breakthroughs in the PPML field. As the field is evolving fast, we also provide a public GitHub repository to continuously track the developments, which is available at https://github.com/PKU-SEC-Lab/Awesome-PPML-Papers.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1296, <a href='https://arxiv.org/pdf/2507.14468.pdf' target='_blank'>https://arxiv.org/pdf/2507.14468.pdf</a></span>   <span><a href='https://github.com/Y-TARL/BioGraphFusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yitong Lin, Jiaying He, Jiahe Chen, Xinnan Zhu, Jianwei Zheng, Tao Bo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14468">BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery and disease understanding, yet their completion and reasoning are challenging. Knowledge Embedding (KE) methods capture global semantics but struggle with dynamic structural integration, while Graph Neural Networks (GNNs) excel locally but often lack semantic understanding. Even ensemble approaches, including those leveraging language models, often fail to achieve a deep, adaptive, and synergistic co-evolution between semantic comprehension and structural learning. Addressing this critical gap in fostering continuous, reciprocal refinement between these two aspects in complex biomedical KGs is paramount.
  Results: We introduce BioGraphFusion, a novel framework for deeply synergistic semantic and structural learning. BioGraphFusion establishes a global semantic foundation via tensor decomposition, guiding an LSTM-driven mechanism to dynamically refine relation embeddings during graph propagation. This fosters adaptive interplay between semantic understanding and structural learning, further enhanced by query-guided subgraph construction and a hybrid scoring mechanism. Experiments across three key biomedical tasks demonstrate BioGraphFusion's superior performance over state-of-the-art KE, GNN, and ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1) highlights its ability to unveil biologically meaningful pathways.
  Availability and Implementation: Source code and all training data are freely available for download at https://github.com/Y-TARL/BioGraphFusion.
  Supplementary information: Supplementary data are available at Bioinformatics online.<br>
<span id='abs_ch'>中文摘要：BioGraphFusion是一个创新框架，通过在生物医学知识图谱中实现语义理解与结构学 的深度协同，在多项任务中展现出卓越性能，并能揭示具有生物学意义的通路。</span><br>
<span id='abs_en'>English Summary: BioGraphFusion is a novel framework that achieves deep synergy between semantic and structural learning in biomedical knowledge graphs, demonstrating superior performance across multiple tasks and revealing biologically meaningful pathways.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1297, <a href='https://arxiv.org/pdf/2507.14452.pdf' target='_blank'>https://arxiv.org/pdf/2507.14452.pdf</a></span>   <span><a href='https://github.com/gwk429/GPI-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weikang Gu, Mingyue Han, Li Xue, Heng Dong, Changcai Yang, Riqing Chen, Lifang Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14452">GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The accurate identification of high-quality correspondences is a prerequisite task in feature-based point cloud registration. However, it is extremely challenging to handle the fusion of local and global features due to feature redundancy and complex spatial relationships. Given that Gestalt principles provide key advantages in analyzing local and global relationships, we propose a novel Gestalt-guided Parallel Interaction Network via orthogonal geometric consistency (GPI-Net) in this paper. It utilizes Gestalt principles to facilitate complementary communication between local and global information. Specifically, we introduce an orthogonal integration strategy to optimally reduce redundant information and generate a more compact global structure for high-quality correspondences. To capture geometric features in correspondences, we leverage a Gestalt Feature Attention (GFA) block through a hybrid utilization of self-attention and cross-attention mechanisms. Furthermore, to facilitate the integration of local detail information into the global structure, we design an innovative Dual-path Multi-Granularity parallel interaction aggregation (DMG) block to promote information exchange across different granularities. Extensive experiments on various challenging tasks demonstrate the superior performance of our proposed GPI-Net in comparison to existing methods. The code will be released at https://github.com/gwk429/GPI-Net.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1298, <a href='https://arxiv.org/pdf/2507.14334.pdf' target='_blank'>https://arxiv.org/pdf/2507.14334.pdf</a></span>   <span><a href='https://github.com/HuiYang1997/OnT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hui Yang, Jiaoyan Chen, Yuan He, Yongsheng Gao, Ian Horrocks
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14334">Language Models as Ontology Encoders</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>OWL (Web Ontology Language) ontologies which are able to formally represent complex knowledge and support semantic reasoning have been widely adopted across various domains such as healthcare and bioinformatics. Recently, ontology embeddings have gained wide attention due to its potential to infer plausible new knowledge and approximate complex reasoning. However, existing methods face notable limitations: geometric model-based embeddings typically overlook valuable textual information, resulting in suboptimal performance, while the approaches that incorporate text, which are often based on language models, fail to preserve the logical structure. In this work, we propose a new ontology embedding method OnT, which tunes a Pretrained Language Model (PLM) via geometric modeling in a hyperbolic space for effectively incorporating textual labels and simultaneously preserving class hierarchies and other logical relationships of Description Logic EL. Extensive experiments on four real-world ontologies show that OnT consistently outperforms the baselines including the state-of-the-art across both tasks of prediction and inference of axioms. OnT also demonstrates strong potential in real-world applications, indicated by its robust transfer learning abilities and effectiveness in real cases of constructing a new ontology from SNOMED CT. Data and code are available at https://github.com/HuiYang1997/OnT.<br>
<span id='abs_ch'>OWL本体广泛应用于知识表示和推理，但现有嵌入方法要么忽略文本信息，要么 法保持逻辑结构， 此提出了OnT这一新方法，它通过双曲 何建模结合预训练语言模型，有效整合文本并维护逻辑关系，在多个真实本体上的预测和推理任务中均表现出优越性能。</span><br>
<span id='abs_en'>OWL ontologies are widely used for knowledge representation and reasoning, but current embedding methods either neglect textual information or fail to preserve logical structures, leading to the development of OnT, a novel approach that combines pretrained language models with hyperbolic geometric modeling to effectively integrate text and maintain logical relationships, demonstrating superior performance in prediction and inference tasks across multiple real-world ontologies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1299, <a href='https://arxiv.org/pdf/2507.14295.pdf' target='_blank'>https://arxiv.org/pdf/2507.14295.pdf</a></span>   <span><a href='https://github.com/lichengliu03/unary-feedback' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Licheng Liu, Zihan Wang, Linjie Li, Chenwei Xu, Yiping Lu, Han Liu, Avirup Sil, Manling Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14295">A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multi-turn problem solving is critical yet challenging for Large Reasoning Models (LRMs) to reflect on their reasoning and revise from feedback. Existing Reinforcement Learning (RL) methods train large reasoning models on a single-turn paradigm with verifiable rewards. However, we observe that models trained with existing RL paradigms often lose their ability to solve problems across multiple turns and struggle to revise answers based on contextual feedback, leading to repetitive responses. We ask: can LRMs learn to reflect their answers in a multi-turn context? In this work, we find that training models with multi-turn RL using only unary feedback (e.g., "Let's try again") after wrong answers can improve both single-turn performance and multi-turn reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement learning, which uses minimal yet common unary user feedback during iterative problem solving. It can be easily applied to existing single-turn RL training setups. Experimental results show that RL training with UFO keeps single-turn performance and improves multi-turn reasoning accuracy by up to 14%, enabling language models to better react to feedback in multi-turn problem solving. To further minimize the number of turns needed for a correct answer while encouraging diverse reasoning when mistakes occur, we design reward structures that guide models to produce careful and deliberate answers in each turn. Code: https://github.com/lichengliu03/unary-feedback<br>
<span id='abs_ch'>Chinese: 本 究提出单点反馈作为观察（UFO）的强化学 方法，通过使用极简的单点反馈来增强大型推理模型在单轮和多轮问题解决中的表现，将多轮推理准确率提升高达14%，同时保持单轮性能。</span><br>
<span id='abs_en'>English: This study introduces Unary Feedback as Observation (UFO), a reinforcement learning approach that uses minimal unary feedback to enhance large reasoning models' performance in both single-turn and multi-turn problem-solving, improving multi-turn reasoning accuracy by up to 14% while maintaining single-turn capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1300, <a href='https://arxiv.org/pdf/2507.14293.pdf' target='_blank'>https://arxiv.org/pdf/2507.14293.pdf</a></span>   <span><a href='https://github.com/OSU-NLP-Group/WebGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyuan Zheng, Zeyi Liao, Scott Salisbury, Zeyuan Liu, Michael Lin, Qinyuan Zheng, Zifan Wang, Xiang Deng, Dawn Song, Huan Sun, Yu Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14293">WebGuard: Building a Generalizable Guardrail for Web Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid development of autonomous web agents powered by Large Language Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of taking unintended or harmful actions. This situation underscores an urgent need for effective safety measures, akin to access controls for human users. To address this critical challenge, we introduce WebGuard, the first comprehensive dataset designed to support the assessment of web agent action risks and facilitate the development of guardrails for real-world online environments. In doing so, WebGuard specifically focuses on predicting the outcome of state-changing actions and contains 4,939 human-annotated actions from 193 websites across 22 diverse domains, including often-overlooked long-tail websites. These actions are categorized using a novel three-tier risk schema: SAFE, LOW, and HIGH. The dataset includes designated training and test splits to support evaluation under diverse generalization settings. Our initial evaluations reveal a concerning deficiency: even frontier LLMs achieve less than 60% accuracy in predicting action outcomes and less than 60% recall in lagging HIGH-risk actions, highlighting the risks of deploying current-generation agents without dedicated safeguards. We therefore investigate fine-tuning specialized guardrail models using WebGuard. We conduct comprehensive evaluations across multiple generalization settings and find that a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from 20% to 76%. Despite these improvements, the performance still falls short of the reliability required for high-stakes deployment, where guardrails must approach near-perfect accuracy and recall.<br>
<span id='abs_ch'>中文: 大型语言模型驱动的自主网络代理快速发展带来了意外有害行为的高风险，为此开发的WebGuard数据集通过评估行动风险和训练防护模型，揭示了现有模型的安全缺陷，尽管微调后性能显著提升，但仍需近乎完美的保障措施才能满足高风险部署要求。</span><br>
<span id='abs_en'>English: The rapid advancement of LLM-powered autonomous web agents introduces significant risks of unintended harmful actions, prompting the development of WebGuard, a comprehensive dataset for assessing action risks and training guardrail models, which reveals current models' safety deficiencies and the need for near-perfect safeguards despite performance improvements from fine-tuning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1301, <a href='https://arxiv.org/pdf/2507.14270.pdf' target='_blank'>https://arxiv.org/pdf/2507.14270.pdf</a></span>   <span><a href='https://github.com/mr-ravin/aptx_neuron' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ravin Kumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14270">APTx Neuron: A Unified Trainable Neuron Architecture Integrating Activation and Computation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We propose the APTx Neuron, a novel, unified neural computation unit that integrates non-linear activation and linear transformation into a single trainable expression. The APTx Neuron is derived from the APTx activation function, thereby eliminating the need for separate activation layers and making the architecture both computationally efficient and elegant. The proposed neuron follows the functional form $y = \sum_{i=1}^{n} ((Î±_i + \tanh(Î²_i x_i)) \cdot Î³_i x_i) + Î´$, where all parameters $Î±_i$, $Î²_i$, $Î³_i$, and $Î´$ are trainable. We validate our APTx Neuron-based architecture on the MNIST dataset, achieving up to $96.69\%$ test accuracy within 11 epochs using approximately 332K trainable parameters. The results highlight the superior expressiveness and computational efficiency of the APTx Neuron compared to traditional neurons, pointing toward a new paradigm in unified neuron design and the architectures built upon it. Source code is available at https://github.com/mr-ravin/aptx_neuron.<br>
<span id='abs_ch'>Chinese: APTx神经元是一种新颖的神经计算单元，将激活和变换功能整合为单一可训练表达式，在MNIST数据集上以约33.2万参数实现96.69%的准确率，显著提升了计算效率。</span><br>
<span id='abs_en'>English: The APTx Neuron is a novel neural computation unit that integrates activation and transformation into a single trainable expression, achieving 96.69% accuracy on MNIST with enhanced computational efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1302, <a href='https://arxiv.org/pdf/2507.14256.pdf' target='_blank'>https://arxiv.org/pdf/2507.14256.pdf</a></span>   <span><a href='https://github.com/peetery/LLM-analysis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jakub Walczak, Piotr Tomalak, Artur Laskowski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14256">Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Generative AI is gaining increasing attention in software engineering, where testing remains an indispensable reliability mechanism. According to the widely adopted testing pyramid, unit tests constitute the majority of test cases and are often schematic, requiring minimal domain expertise. Automatically generating such tests under the supervision of software engineers can significantly enhance productivity during the development phase of the software lifecycle.
  This paper investigates the impact of code context and prompting strategies on the quality and adequacy of unit tests generated by various large language models (LLMs) across several families. The results show that including docstrings notably improves code adequacy, while further extending context to the full implementation yields definitely smaller gains. Notably, the chain-of-thought prompting strategy -- applied even to 'reasoning' models -- achieves the best results, with up to 96.3\% branch coverage, a 57\% average mutation score, and near-perfect compilation success rate. Among the evaluated models, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation score and branch coverage being still in top in terms of compilation success rate.
  All the code and resulting test suites are publicly available at https://github.com/peetery/LLM-analysis.<br>
<span id='abs_ch'>中文摘要：本 究探讨代 上下文和提示策略对大型语言模型生成单元测试质量的影响，发现包含文档字符串可显著提高测试充分性，而链式思维提示策略效果最佳，其中Gemini 2.5 Pro在评估模型中表现最优。</span><br>
<span id='abs_en'>English Summary: This study explores how code context and prompting strategies affect the quality of unit tests generated by large language models, finding that docstrings significantly enhance test adequacy and chain-of-thought prompting achieves the best results, with Gemini 2.5 Pro performing best among evaluated models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1303, <a href='https://arxiv.org/pdf/2507.14204.pdf' target='_blank'>https://arxiv.org/pdf/2507.14204.pdf</a></span>   <span><a href='https://github.com/GATECH-EIC/LaCache' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/GATECH-EIC/LaCache' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dachuan Shi, Yonggan Fu, Xiangchi Yuan, Zhongzhi Yu, Haoran You, Sixu Li, Xin Dong, Jan Kautz, Pavlo Molchanov, Yingyan, Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14204">LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in Large Language Models (LLMs) have spurred interest in numerous applications requiring robust long-range capabilities, essential for processing extensive input contexts and continuously generating extended outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in LLMs escalates, creating a significant efficiency bottleneck. In this paper, we propose a new KV cache optimization paradigm called LaCache, a training-free method for efficient and accurate generative inference of LLMs. LaCache enables LLMs to simultaneously address both of the critical challenges in long-range modeling: robust long-range capabilities and continuous generation without running out-of-memory (OOM). Specifically, LaCache integrates two key innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only sequentially (left-to-right within each layer) but also across layers (from shallow to deep), providing an extended span for capturing long-range dependencies under a fixed storage budget, thereby boosting long-range capabilities; and (2) an iterative compaction mechanism that progressively compresses older caches, freeing up space for new tokens within a fixed cache size. This token distance-based dynamic compression enables more effective continuous generation under constrained cache budgets. Experiments across various tasks, benchmarks, and LLM models consistently validate LaCache's effectiveness in enhancing LLMs' long-range capabilities. Our code is available at https://github.com/GATECH-EIC/LaCache.<br>
<span id='abs_ch'>中文：LaCache是一种 需训练的KV缓存优化方法，通过阶梯式缓存模式和迭代压缩机制，有效提升大语言模型的长程处理能力和持续生成效率，并在多类基准测试中得到验证。</span><br>
<span id='abs_en'>English: LaCache is a training-free KV cache optimization method that enhances LLMs' long-range capabilities and continuous generation efficiency through a ladder-shaped cache pattern and iterative compaction mechanism, validated across diverse benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1304, <a href='https://arxiv.org/pdf/2507.14200.pdf' target='_blank'>https://arxiv.org/pdf/2507.14200.pdf</a></span>   <span><a href='https://github.com/magent4aci/SMACS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengji Tang, Jianjian Cao, Weihao Lin, Jiale Hong, Bo Zhang, Shuyue Hu, Lei Bai, Tao Chen, Wanli Ouyang, Peng Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14200">Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper aims to demonstrate the potential and strengths of open-source collectives. It leads to a promising question: Can we harness multiple open-source LLMs to match or even beat the closed-source LLMs? To answer this, we propose SMACS, a scalable multi-agent collaboration system (MACS) framework with high performance. Specifically, for continuous integration of new LLMs and generalization to diverse questions, we first propose a Retrieval-based Prior Selection (RPS), which assigns a proxy performance score to each LLM to select the Top-k LLMs at the instance level for any given question. Then, we propose an Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the generation of diverse responses through prior dropping and selecting the high-quality response via a hybrid posterior score. Experiments on eight mainstream benchmarks validate the effectiveness of our SMACS: by integrating fifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025, e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%) across multiple tasks. Remarkably, it even exceeds the average of best results of different datasets from both open-source LLMs (+2.86%) and closed-source LLMs (+2.04%), pushing the upper bound of intelligence. Code will be released at https://github.com/magent4aci/SMACS.<br>
<span id='abs_ch'>中文: 本文提出SMACS可扩展多智能体协作系统，通过基于检索的优先选择和探索-利用驱动的后验增强机制，成功整合多个开源大语言模型，在多项基准测试中超越主流闭源模型性能。</span><br>
<span id='abs_en'>English: This paper introduces SMACS, a scalable multi-agent collaboration system that effectively integrates multiple open-source LLMs through retrieval-based selection and posterior enhancement, outperforming leading closed-source models across multiple benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1305, <a href='https://arxiv.org/pdf/2507.14172.pdf' target='_blank'>https://arxiv.org/pdf/2507.14172.pdf</a></span>   <span><a href='https://github.com/flowersteam/SOAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Julien Pourcel, CÃ©dric Colas, Pierre-Yves Oudeyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14172">Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Many program synthesis tasks prove too challenging for even state-of-the-art language models to solve in single attempts. Search-based evolutionary methods offer a promising alternative by exploring solution spaces iteratively, but their effectiveness remain limited by the fixed capabilities of the underlying generative model.
  We propose SOAR, a method that learns program synthesis by integrating language models into a self-improving evolutionary loop.
  SOAR alternates between (1) an evolutionary search that uses an LLM to sample and refine candidate solutions, and (2) a hindsight learning phase that converts search attempts into valid problem-solution pairs used to fine-tune the LLM's sampling and refinement capabilities\, -- \,enabling increasingly effective search in subsequent iterations.
  On the challenging ARC-AGI benchmark, SOAR achieves significant performance gains across model scales and iterations, leveraging positive transfer between the sampling and refinement finetuning tasks. These improvements carry over to test-time adaptation, enabling SOAR to solve 52\% of the public test set. Our code is open-sourced at: https://github.com/flowersteam/SOAR<br>
<span id='abs_ch'>中文: SOAR是一种自改进的进化方法，通过将语言模型整合到进化搜索与后见学 的迭代循环中，利用搜索尝试生成的问题-解决方案对微调模型，在ARC-AGI基准测试中实现了显著性能提升。</span><br>
<span id='abs_en'>English: SOAR is a self-improving evolutionary method that integrates language models into an iterative loop of evolutionary search and hindsight learning, achieving significant performance gains on the ARC-AGI benchmark by fine-tuning the model with problem-solution pairs from search attempts.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1306, <a href='https://arxiv.org/pdf/2507.14156.pdf' target='_blank'>https://arxiv.org/pdf/2507.14156.pdf</a></span>   <span><a href='https://github.com/ykiiiiii/ADFLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Yi, Kiarash Jamali, Sjors H. W. Scheres
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14156">All-atom inverse protein folding through discrete flow matching</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The recent breakthrough of AlphaFold3 in modeling complex biomolecular interactions, including those between proteins and ligands, nucleotides, or metal ions, creates new opportunities for protein design. In so-called inverse protein folding, the objective is to find a sequence of amino acids that adopts a target protein structure. Many inverse folding methods struggle to predict sequences for complexes that contain non-protein components, and perform poorly with complexes that adopt multiple structural states. To address these challenges, we present ADFLIP (All-atom Discrete FLow matching Inverse Protein folding), a generative model based on discrete flow-matching for designing protein sequences conditioned on all-atom structural contexts. ADFLIP progressively incorporates predicted amino acid side chains as structural context during sequence generation and enables the design of dynamic protein complexes through ensemble sampling across multiple structural states. Furthermore, ADFLIP implements training-free classifier guidance sampling, which allows the incorporation of arbitrary pre-trained models to optimise the designed sequence for desired protein properties. We evaluated the performance of ADFLIP on protein complexes with small-molecule ligands, nucleotides, or metal ions, including dynamic complexes for which structure ensembles were determined by nuclear magnetic resonance (NMR). Our model achieves state-of-the-art performance in single-structure and multi-structure inverse folding tasks, demonstrating excellent potential for all-atom protein design. The code is available at https://github.com/ykiiiiii/ADFLIP.<br>
<span id='abs_ch'>中文摘要：AlphaFold3在模拟生物分子相互作用方面的突 为蛋白质设计带来新机遇，为此我们开发了ADFLIP生成模型，它通过全原子结构上下文和集成采 ，在包含非蛋白质成分的动态复合物逆向折 任务中实现了最先进的性能。</span><br>
<span id='abs_en'>English Summary: AlphaFold3's advancements in modeling biomolecular interactions have opened new avenues for protein design, leading to the development of ADFLIP, a generative model that excels at inverse protein folding for dynamic complexes with non-protein components by incorporating all-atom structural contexts and ensemble sampling.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1307, <a href='https://arxiv.org/pdf/2507.13941.pdf' target='_blank'>https://arxiv.org/pdf/2507.13941.pdf</a></span>   <span><a href='https://github.com/memory-formation/convergent-transformations' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pablo Marcos-ManchÃ³n, LluÃ­s Fuentemilla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13941">Convergent transformations of visual representation in brains and models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>A fundamental question in cognitive neuroscience is what shapes visual perception: the external world's structure or the brain's internal architecture. Although some perceptual variability can be traced to individual differences, brain responses to naturalistic stimuli evoke similar activity patterns across individuals, suggesting a convergent representational principle. Here, we test if this stimulus-driven convergence follows a common trajectory across people and deep neural networks (DNNs) during its transformation from sensory to high-level internal representations. We introduce a unified framework that traces representational flow by combining inter-subject similarity with alignment to model hierarchies. Applying this framework to three independent fMRI datasets of visual scene perception, we reveal a cortex-wide network, conserved across individuals, organized into two pathways: a medial-ventral stream for scene structure and a lateral-dorsal stream tuned for social and biological content. This functional organization is captured by the hierarchies of vision DNNs but not language models, reinforcing the specificity of the visual-to-semantic transformation. These findings show a convergent computational solution for visual encoding in both human and artificial vision, driven by the structure of the external world.<br>
<span id='abs_ch'>中文摘要：该 究揭示了人类大脑中存在一个保守的全皮层网络，分为处理场景结构的腹内侧通路和感知社会内容的背外侧通路，这一功能结构与深度神经网络的层级处理相吻合，表明外部世界结构驱动了视觉编 的趋同计算方案。</span><br>
<span id='abs_en'>English Summary: This study reveals a conserved cortex-wide network in humans, organized into two visual pathways for scene structure and social content, which aligns with the hierarchical processing in deep neural networks, demonstrating a convergent computational solution for visual encoding driven by external world structure.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1308, <a href='https://arxiv.org/pdf/2507.13919.pdf' target='_blank'>https://arxiv.org/pdf/2507.13919.pdf</a></span>   <span><a href='https://github.com/kobihackenburg/scaling-conversational-AI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kobi Hackenburg, Ben M. Tappin, Luke Hewitt, Ed Saunders, Sid Black, Hause Lin, Catherine Fist, Helen Margetts, David G. Rand, Christopher Summerfield
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13919">The Levers of Political Persuasion with Conversational AI</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>There are widespread fears that conversational AI could soon exert unprecedented influence over human beliefs. Here, in three large-scale experiments (N=76,977), we deployed 19 LLMs-including some post-trained explicitly for persuasion-to evaluate their persuasiveness on 707 political issues. We then checked the factual accuracy of 466,769 resulting LLM claims. Contrary to popular concerns, we show that the persuasive power of current and near-future AI is likely to stem more from post-training and prompting methods-which boosted persuasiveness by as much as 51% and 27% respectively-than from personalization or increasing model scale. We further show that these methods increased persuasion by exploiting LLMs' unique ability to rapidly access and strategically deploy information and that, strikingly, where they increased AI persuasiveness they also systematically decreased factual accuracy.<br>
<span id='abs_ch'>Chinese:  究表明，当前对话式AI的说服力主要源于后训练和提示技术，这些方法虽大幅提升了说服效果，却系统性地降低了事实准确性，而非个性化或模型规模扩大所致。English: Current research reveals that the persuasive power of conversational AI primarily stems from post-training and prompting techniques, which significantly enhance persuasiveness while simultaneously reducing factual accuracy, rather than from personalization or model scaling.Paperid:557,https://arxiv.org/pdf/2507.13901.pdfGitHub</span><br>
<span id='abs_en'>English: Current research reveals that the persuasive power of conversational AI primarily stems from post-training and prompting techniques, which significantly enhance persuasiveness while simultaneously reducing factual accuracy, rather than from personalization or model scaling.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1309, <a href='https://arxiv.org/pdf/2507.13659.pdf' target='_blank'>https://arxiv.org/pdf/2507.13659.pdf</a></span>   <span><a href='https://github.com/Event-AHU/Neuromorphic_ReID' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Qian Zhu, Shujuan Wu, Bo Jiang, Shiliang Zhang, Yaowei Wang, Yonghong Tian, Bin Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13659">When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent researchers have proposed using event cameras for person re-identification (ReID) due to their promising performance and better balance in terms of privacy protection, event camera-based person ReID has attracted significant attention. Currently, mainstream event-based person ReID algorithms primarily focus on fusing visible light and event stream, as well as preserving privacy. Although significant progress has been made, these methods are typically trained and evaluated on small-scale or simulated event camera datasets, making it difficult to assess their real identification performance and generalization ability. To address the issue of data scarcity, this paper introduces a large-scale RGB-event based person ReID dataset, called EvReID. The dataset contains 118,988 image pairs and covers 1200 pedestrian identities, with data collected across multiple seasons, scenes, and lighting conditions. We also evaluate 15 state-of-the-art person ReID algorithms, laying a solid foundation for future research in terms of both data and benchmarking. Based on our newly constructed dataset, this paper further proposes a pedestrian attribute-guided contrastive learning framework to enhance feature learning for person re-identification, termed TriPro-ReID. This framework not only effectively explores the visual features from both RGB frames and event streams, but also fully utilizes pedestrian attributes as mid-level semantic features. Extensive experiments on the EvReID dataset and MARS datasets fully validated the effectiveness of our proposed RGB-Event person ReID framework. The benchmark dataset and source code will be released on https://github.com/Event-AHU/Neuromorphic_ReID<br>
<span id='abs_ch'>中文摘要：本文针对事件相机行人重识别数据稀缺问题，提出了大规模RGB-事件数据集EvReID，并开发了TriPro-ReID对比学 框架，有效融合RGB帧、事件流和行人属性以提升特征学 能力。</span><br>
<span id='abs_en'>English Summary: This paper introduces EvReID, a large-scale RGB-event person re-identification dataset addressing data scarcity issues, and proposes TriPro-ReID, a contrastive learning framework that effectively integrates RGB frames, event streams, and pedestrian attributes to enhance feature learning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1310, <a href='https://arxiv.org/pdf/2507.13543.pdf' target='_blank'>https://arxiv.org/pdf/2507.13543.pdf</a></span>   <span><a href='https://github.com/sashakolpakov/structure-functions' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Kolpakov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13543">Loss-Complexity Landscape and Model Structure Functions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We develop a framework for dualizing the Kolmogorov structure function $h_x(Î±)$, which then allows using computable complexity proxies. We establish a mathematical analogy between information-theoretic constructs and statistical mechanics, introducing a suitable partition function and free energy functional. We explicitly prove the Legendre-Fenchel duality between the structure function and free energy, showing detailed balance of the Metropolis kernel, and interpret acceptance probabilities as information-theoretic scattering amplitudes. A susceptibility-like variance of model complexity is shown to peak precisely at loss-complexity trade-offs interpreted as phase transitions. Practical experiments with linear and tree-based regression models verify these theoretical predictions, explicitly demonstrating the interplay between the model complexity, generalization, and overfitting threshold.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1311, <a href='https://arxiv.org/pdf/2507.13363.pdf' target='_blank'>https://arxiv.org/pdf/2507.13363.pdf</a></span>   <span><a href='https://github.com/atharv0goel/open-world-3D-det' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Atharv Goel, Mehar Khurana
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13363">Just Add Geometry: Gradient-Free Open-Vocabulary 3D Detection Without Human-in-the-Loop</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Modern 3D object detection datasets are constrained by narrow class taxonomies and costly manual annotations, limiting their ability to scale to open-world settings. In contrast, 2D vision-language models trained on web-scale image-text pairs exhibit rich semantic understanding and support open-vocabulary detection via natural language prompts. In this work, we leverage the maturity and category diversity of 2D foundation models to perform open-vocabulary 3D object detection without any human-annotated 3D labels.
  Our pipeline uses a 2D vision-language detector to generate text-conditioned proposals, which are segmented with SAM and back-projected into 3D using camera geometry and either LiDAR or monocular pseudo-depth. We introduce a geometric inflation strategy based on DBSCAN clustering and Rotating Calipers to infer 3D bounding boxes without training. To simulate adverse real-world conditions, we construct Pseudo-nuScenes, a fog-augmented, RGB-only variant of the nuScenes dataset.
  Experiments demonstrate that our method achieves competitive localization performance across multiple settings, including LiDAR-based and purely RGB-D inputs, all while remaining training-free and open-vocabulary. Our results highlight the untapped potential of 2D foundation models for scalable 3D perception. We open-source our code and resources at https://github.com/atharv0goel/open-world-3D-det.<br>
<span id='abs_ch'>中文摘要：本 究提出一种 需训练的开词汇3D物体检测方法，通过利用2D视觉语言模型生成提案并结合 何策略推断3D边界框，在 人 注3D 签的情况下实现了具有竞争力的检测性能。English Summary: This work introduces a training-free method for open-vocabulary 3D object detection by leveraging 2D vision-language models to generate proposals and geometric strategies to infer 3D bounding boxes, achieving competitive performance without human-annotated 3D labels.Paperid:575,https://arxiv.org/pdf/2507.13362.pdfGitHub</span><br>
<span id='abs_en'>English Summary: This work introduces a training-free method for open-vocabulary 3D object detection by leveraging 2D vision-language models to generate proposals and geometric strategies to infer 3D bounding boxes, achieving competitive performance without human-annotated 3D labels.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1312, <a href='https://arxiv.org/pdf/2507.13362.pdf' target='_blank'>https://arxiv.org/pdf/2507.13362.pdf</a></span>   <span><a href='https://github.com/Yvonne511/spatial-vlm-investigator' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Binbin Ji, Siddharth Agrawal, Qiance Tang, Yvonne Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13362">Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This study investigates the spatial reasoning capabilities of vision-language models (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement learning. We begin by evaluating the impact of different prompting strategies and find that simple CoT formats, where the model generates a reasoning step before the answer, not only fail to help, but can even harm the model's original performance. In contrast, structured multi-stage prompting based on scene graphs (SceneGraph CoT) significantly improves spatial reasoning accuracy. Furthermore, to improve spatial reasoning ability, we fine-tune models using Group Relative Policy Optimization (GRPO) on the SAT dataset and evaluate their performance on CVBench. Compared to supervised fine-tuning (SFT), GRPO achieves higher accuracy on Pass@1 evaluations and demonstrates superior robustness under out-of-distribution (OOD) conditions. In particular, we find that SFT overfits to surface-level linguistic patterns and may degrade performance when test-time phrasing changes (e.g., from "closer to" to "farther from"). GRPO, on the other hand, generalizes more reliably and maintains stable performance under such shifts. Our findings provide insights into how reinforcement learning and structured prompting improve the spatial reasoning capabilities and generalization behavior of modern VLMs. All code is open source at: https://github.com/Yvonne511/spatial-vlm-investigator<br>
<span id='abs_ch'>中文摘要：本 究通过场景图结构化提示和GRPO强化学 方法，显著提升了视觉语言模型的空间推理准确性和泛化能力，优于简单思维链方法和监督微调。</span><br>
<span id='abs_en'>English Summary: This research demonstrates that structured scene graph-based prompting and reinforcement learning with GRPO significantly enhance vision-language models' spatial reasoning accuracy and generalization, outperforming simple Chain-of-Thought methods and supervised fine-tuning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1313, <a href='https://arxiv.org/pdf/2507.13348.pdf' target='_blank'>https://arxiv.org/pdf/2507.13348.pdf</a></span>   <span><a href='https://github.com/dvlab-research/VisionThink' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/dvlab-research/VisionThink' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Senqiao Yang, Junyi Li, Xin Lai, Bei Yu, Hengshuang Zhao, Jiaya Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13348">VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens. However, we observe that most real-world scenarios do not require such an extensive number of visual tokens. While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution. Therefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink. It starts with a downsampled image and smartly decides whether it is sufficient for problem solving. Otherwise, the model could output a special token to request the higher-resolution image. Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case. As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks. Moreover, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio. Extensive experiments demonstrate the superiority, efficiency, and effectiveness of our method. Our code is available at https://github.com/dvlab-research/VisionThink.<br>
<span id='abs_ch'>中文: VisionThink提出了一种动态视觉令牌压缩方法，通过自适应调整图像分辨率，在保证多数视觉问答任务性能的同时显著提升效率，并强化了OCR任务的细粒度识别能力。</span><br>
<span id='abs_en'>English: VisionThink introduces a dynamic visual token compression method that adaptively processes images at different resolutions, enhancing efficiency while maintaining strong performance across most VQA tasks and improving fine-grained OCR capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1314, <a href='https://arxiv.org/pdf/2507.13266.pdf' target='_blank'>https://arxiv.org/pdf/2507.13266.pdf</a></span>   <span><a href='https://github.com/foreverlasting1202/QuestA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazheng Li, Hongzhou Lin, Hong Lu, Kaiyue Wen, Zaiwen Yang, Jiaxuan Gao, Yi Wu, Jingzhao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13266">QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning (RL) has emerged as a central paradigm for training large language models (LLMs) in reasoning tasks. Yet recent studies question RL's ability to incentivize reasoning capacity beyond the base model. This raises a key challenge: how can RL be adapted to solve harder reasoning problems more effectively? To address this challenge, we propose a simple yet effective strategy via Question Augmentation: introduce partial solutions during training to reduce problem difficulty and provide more informative learning signals. Our method, QuestA, when applied during RL training on math reasoning tasks, not only improves pass@1 but also pass@k-particularly on problems where standard RL struggles to make progress. This enables continual improvement over strong open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing their reasoning capabilities. We achieve new state-of-the-art results on math benchmarks using 1.5B-parameter models: 72.50% (+10.73%) on AIME24, 62.29% (+12.79%) on AIME25, and 41.67% (+10.11%) on HMMT25. Code, data and model are available at https://github.com/foreverlasting1202/QuestA.<br>
<span id='abs_ch'>中文: 针对强化学 在提升多步推理能力方面的不足， 究者提出QuestA方法，通过问题增强策略在训练中引入部分解决方案以降低难度并强化学 信号，该方法在数学推理基准测试中取得最优结果，并从理论上解释了其提升 本效率的机制。</span><br>
<span id='abs_en'>English: To address reinforcement learning's limitations in improving multi-step reasoning for challenging problems, the authors propose QuestA, a question augmentation strategy that incorporates partial solutions during training to reduce difficulty and enhance learning signals, achieving state-of-the-art results on math benchmarks and providing theoretical insights into improved sample efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1315, <a href='https://arxiv.org/pdf/2507.13266.pdf' target='_blank'>https://arxiv.org/pdf/2507.13266.pdf</a></span>   <span><a href='https://github.com/foreverlasting1202/QuestA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazheng Li, Hongzhou Lin, Hong Lu, Kaiyue Wen, Zaiwen Yang, Jiaxuan Gao, Yi Wu, Jingzhao Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13266">QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning (RL) has emerged as a central paradigm for training large language models (LLMs) in reasoning tasks. Yet recent studies question RL's ability to incentivize reasoning capacity beyond the base model. This raises a key challenge: how can RL be adapted to solve harder reasoning problems more effectively? To address this challenge, we propose a simple yet effective strategy via Question Augmentation: introduce partial solutions during training to reduce problem difficulty and provide more informative learning signals. Our method, QuestA, when applied during RL training on math reasoning tasks, not only improves pass@1 but also pass@k-particularly on problems where standard RL struggles to make progress. This enables continual improvement over strong open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing their reasoning capabilities. We achieve new state-of-the-art results on math benchmarks using 1.5B-parameter models: 72.50% (+10.73%) on AIME24, 62.29% (+12.79%) on AIME25, and 41.67% (+10.11%) on HMMT25. Code, data and model are available at https://github.com/foreverlasting1202/QuestA.<br>
<span id='abs_ch'>中文: 针对强化学 在提升多步推理能力方面的不足， 究者提出QuestA方法，通过问题增强策略在训练中引入部分解决方案以降低难度并强化学 信号，该方法在数学推理基准测试中取得最优结果，并从理论上解释了其提升 本效率的机制。</span><br>
<span id='abs_en'>English: To address reinforcement learning's limitations in improving multi-step reasoning for challenging problems, the authors propose QuestA, a question augmentation strategy that incorporates partial solutions during training to reduce difficulty and enhance learning signals, achieving state-of-the-art results on math benchmarks and providing theoretical insights into improved sample efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1316, <a href='https://arxiv.org/pdf/2507.13231.pdf' target='_blank'>https://arxiv.org/pdf/2507.13231.pdf</a></span>   <span><a href='https://github.com/ucd-dare/VITA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dechen Gao, Boqi Zhao, Andrew Lee, Ian Chuang, Hanchu Zhou, Hang Wang, Zhe Zhao, Junshan Zhang, Iman Soltani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13231">VITA: Vision-to-Action Flow Matching Policy</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Conventional flow matching and diffusion-based policies sample through iterative denoising from standard noise distributions (e.g., Gaussian), and require conditioning mechanisms to incorporate visual information during the generative process, incurring substantial time and memory overhead. To reduce the complexity, we develop VITA(VIsion-To-Action policy), a noise-free and conditioning-free policy learning framework that directly maps visual representations to latent actions using flow matching. VITA treats latent visual representations as the source of the flow, thus eliminating the need of conditioning. As expected, bridging vision and action is challenging, because actions are lower-dimensional, less structured, and sparser than visual representations; moreover, flow matching requires the source and target to have the same dimensionality. To overcome this, we introduce an action autoencoder that maps raw actions into a structured latent space aligned with visual latents, trained jointly with flow matching. To further prevent latent space collapse, we propose flow latent decoding, which anchors the latent generation process by backpropagating the action reconstruction loss through the flow matching ODE (ordinary differential equations) solving steps. We evaluate VITA on 8 simulation and 2 real-world tasks from ALOHA and Robomimic. VITA outperforms or matches state-of-the-art generative policies, while achieving 1.5-2.3x faster inference compared to conventional methods with conditioning. Project page: https://ucd-dare.github.io/VITA/<br>
<br>
<div id='section'>Paperid: <span id='pid'>1317, <a href='https://arxiv.org/pdf/2507.13142.pdf' target='_blank'>https://arxiv.org/pdf/2507.13142.pdf</a></span>   <span><a href='https://github.com/ahmedehabb/From-Roots-to-Rewards-Dynamic-Tree-Reasoning-with-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Bahloul, Simon Malberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13142">From Roots to Rewards: Dynamic Tree Reasoning with Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Modern language models address complex questions through chain-of-thought (CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al., 2021), yet struggle with error propagation and knowledge integration. Tree-structured reasoning methods, particularly the Probabilistic Tree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues by decomposing questions into hierarchical structures and selecting answers through confidence-weighted aggregation of parametric and retrieved knowledge (Yao et al., 2023). However, ProbTree's static implementation introduces two key limitations: (1) the reasoning tree is fixed during the initial construction phase, preventing dynamic adaptation to intermediate results, and (2) each node requires exhaustive evaluation of all possible solution strategies, creating computational inefficiency. We present a dynamic reinforcement learning (Sutton and Barto, 2018) framework that transforms tree-based reasoning into an adaptive process. Our approach incrementally constructs the reasoning tree based on real-time confidence estimates, while learning optimal policies for action selection (decomposition, retrieval, or aggregation). This maintains ProbTree's probabilistic rigor while improving both solution quality and computational efficiency through selective expansion and focused resource allocation. The work establishes a new paradigm for treestructured reasoning that balances the reliability of probabilistic frameworks with the flexibility required for real-world question answering systems. Code available at: https://github.com/ahmedehabb/From-Roots-to-Rewards-Dynamic-Tree-Reasoning-with-RL<br>
<span id='abs_ch'>中文: 现代语言模型采用 状推理提升复杂问题解答能力，但静态方法存在适应性差与效率低的缺陷，新提出的动态强化学 框架通过实时构建推理 并优化行动选择策略，在保持概率严谨性的同时显著提升了求解质量与计算效率。</span><br>
<span id='abs_en'>English: Modern language models use tree-structured reasoning to improve complex question answering, but static methods face limitations in adaptability and efficiency, which a new dynamic reinforcement learning framework addresses by adaptively constructing reasoning trees and optimizing action selection for better performance and computational economy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1318, <a href='https://arxiv.org/pdf/2507.12979.pdf' target='_blank'>https://arxiv.org/pdf/2507.12979.pdf</a></span>   <span><a href='https://github.com/youssefga28/HuSCF-GAN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Youssef Tawfilis, Hossam Amer, Minar El-Aasser, Tallal Elshabrawy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12979">A Distributed Generative AI Approach for Heterogeneous Multi-Domain Environments under Data Sharing constraints</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Federated Learning has gained increasing attention for its ability to enable multiple nodes to collaboratively train machine learning models without sharing their raw data. At the same time, Generative AI -- particularly Generative Adversarial Networks (GANs) -- have achieved remarkable success across a wide range of domains, such as healthcare, security, and Image Generation. However, training generative models typically requires large datasets and significant computational resources, which are often unavailable in real-world settings. Acquiring such resources can be costly and inefficient, especially when many underutilized devices -- such as IoT devices and edge devices -- with varying capabilities remain idle. Moreover, obtaining large datasets is challenging due to privacy concerns and copyright restrictions, as most devices are unwilling to share their data. To address these challenges, we propose a novel approach for decentralized GAN training that enables the utilization of distributed data and underutilized, low-capability devices while not sharing data in its raw form. Our approach is designed to tackle key challenges in decentralized environments, combining KLD-weighted Clustered Federated Learning to address the issues of data heterogeneity and multi-domain datasets, with Heterogeneous U-Shaped split learning to tackle the challenge of device heterogeneity under strict data sharing constraints -- ensuring that no labels or raw data, whether real or synthetic, are ever shared between nodes. Experimental results shows that our approach demonstrates consistent and significant improvements across key performance metrics, where it achieves 1.1x -- 2.2x higher image generation scores, an average 10% boost in classification metrics (up to 50% in multi-domain non-IID settings), in much lower latency compared to several benchmarks. Find our code at https://github.com/youssefga28/HuSCF-GAN.<br>
<span id='abs_ch'>中文: 该 究提出了一种去中心化的GAN训练方法，通过聚类联邦学 和异构分割学 技术，在不共享原始数据的情况下利用分布式数据和低性能设备，显著提升了图像生成与分类的性能指 。</span><br>
<span id='abs_en'>English: The proposed decentralized GAN training method leverages clustered federated learning and split learning to utilize distributed data and low-capacity devices without sharing raw data, achieving significant performance improvements in image generation and classification.</span>Paperid:590,https://arxiv.org/pdf/2507.12945.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1319, <a href='https://arxiv.org/pdf/2507.12933.pdf' target='_blank'>https://arxiv.org/pdf/2507.12933.pdf</a></span>   <span><a href='https://github.com/LeeDongYeun/dmq' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongyeun Lee, Jiwan Hur, Hyounguk Shon, Jae Young Lee, Junmo Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12933">DMQ: Dissecting Outliers of Diffusion Models for Post-Training Quantization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion models have achieved remarkable success in image generation but come with significant computational costs, posing challenges for deployment in resource-constrained environments. Recent post-training quantization (PTQ) methods have attempted to mitigate this issue by focusing on the iterative nature of diffusion models. However, these approaches often overlook outliers, leading to degraded performance at low bit-widths. In this paper, we propose a DMQ which combines Learned Equivalent Scaling (LES) and channel-wise Power-of-Two Scaling (PTS) to effectively address these challenges. Learned Equivalent Scaling optimizes channel-wise scaling factors to redistribute quantization difficulty between weights and activations, reducing overall quantization error. Recognizing that early denoising steps, despite having small quantization errors, crucially impact the final output due to error accumulation, we incorporate an adaptive timestep weighting scheme to prioritize these critical steps during learning. Furthermore, identifying that layers such as skip connections exhibit high inter-channel variance, we introduce channel-wise Power-of-Two Scaling for activations. To ensure robust selection of PTS factors even with small calibration set, we introduce a voting algorithm that enhances reliability. Extensive experiments demonstrate that our method significantly outperforms existing works, especially at low bit-widths such as W4A6 (4-bit weight, 6-bit activation) and W4A8, maintaining high image generation quality and model stability. The code is available at https://github.com/LeeDongYeun/dmq.<br>
<span id='abs_ch'>中文: 本文提出DMQ方法，结合学 等效缩放和通道级二次幂缩放技术，通过自适应时间步 权有效处理扩散模型中的异常值和误差累积问题，在低比特量化下显著优于现有方法并保持图像生成质量。</span><br>
<span id='abs_en'>English: This paper introduces DMQ, a novel quantization method combining Learned Equivalent Scaling and channel-wise Power-of-Two Scaling with adaptive timestep weighting to effectively handle outliers and error accumulation in diffusion models, achieving superior performance at low bit-widths while maintaining generation quality.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1320, <a href='https://arxiv.org/pdf/2507.12806.pdf' target='_blank'>https://arxiv.org/pdf/2507.12806.pdf</a></span>   <span><a href='https://github.com/SalesforceAIResearch/MCPEval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Liu, Jielin Qiu, Shiyu Wang, Jianguo Zhang, Zuxin Liu, Roshan Ram, Haolin Chen, Weiran Yao, Shelby Heinecke, Silvio Savarese, Huan Wang, Caiming Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12806">MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid rise of Large Language Models (LLMs)-based intelligent agents underscores the need for robust, scalable evaluation frameworks. Existing methods rely on static benchmarks and labor-intensive data collection, limiting practical assessment. We introduce MCPEval, an open-source Model Context Protocol (MCP)-based framework that automates end-to-end task generation and deep evaluation of LLM agents across diverse domains. MCPEval standardizes metrics, seamlessly integrates with native agent tools, and eliminates manual effort in building evaluation pipelines. Empirical results across five real-world domains show its effectiveness in revealing nuanced, domain-specific performance. We publicly release MCPEval https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and standardized LLM agent evaluation.<br>
<span id='abs_ch'>中文摘要：MCPEval是一个基于模型上下文协议的开源框架，能自动化评估多领域大语言模型智能代理，通过 准化指 和消除人工操作，有效揭示领域特异性表现。</span><br>
<span id='abs_en'>English Summary: MCPEval is an open-source framework that automates comprehensive evaluation of LLM agents across multiple domains, standardizing metrics and eliminating manual effort while demonstrating effectiveness in revealing domain-specific performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1321, <a href='https://arxiv.org/pdf/2507.12803.pdf' target='_blank'>https://arxiv.org/pdf/2507.12803.pdf</a></span>   <span><a href='https://github.com/AI4Science-WestlakeU/FLDmamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qianru Zhang, Chenglei Yu, Haixin Wang, Yudong Yan, Yuansheng Cao, Siu-Ming Yiu, Tailin Wu, Hongzhi Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12803">FLDmamba: Integrating Fourier and Laplace Transform Decomposition with Mamba for Enhanced Time Series Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Time series prediction, a crucial task across various domains, faces significant challenges due to the inherent complexities of time series data, including non-stationarity, multi-scale periodicity, and transient dynamics, particularly when tackling long-term predictions. While Transformer-based architectures have shown promise, their quadratic complexity with sequence length hinders their efficiency for long-term predictions. Recent advancements in State-Space Models, such as Mamba, offer a more efficient alternative for long-term modeling, but they cannot capture multi-scale periodicity and transient dynamics effectively. Meanwhile, they are susceptible to data noise issues in time series. This paper proposes a novel framework, FLDmamba (Fourier and Laplace Transform Decomposition Mamba), addressing these limitations. FLDmamba leverages the strengths of both Fourier and Laplace transforms to effectively capture both multi-scale periodicity, transient dynamics within time series data, and improve the robustness of the model to the data noise issue. Our extensive experiments demonstrate that FLDmamba achieves superior performance on time series prediction benchmarks, outperforming both Transformer-based and other Mamba-based architectures. To promote the reproducibility of our method, we have made both the code and data accessible via the following URL:{\href{https://github.com/AI4Science-WestlakeU/FLDmamba}{https://github.com/AI4Science-WestlakeU/\model}.<br>
<span id='abs_ch'>中文: 本文提出FLDmamba框架，结合傅里叶和拉普拉斯变换有效捕捉时间序列中的多尺度周期性和瞬态动态，增强了对数据噪声的鲁棒性，在基准测试中超越了现有的Transformer和Mamba模型。</span><br>
<span id='abs_en'>English: This paper introduces FLDmamba, a novel framework that combines Fourier and Laplace transforms to effectively capture multi-scale periodicity and transient dynamics in time series data, enhancing robustness against noise and outperforming existing Transformer and Mamba models in benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1322, <a href='https://arxiv.org/pdf/2507.12675.pdf' target='_blank'>https://arxiv.org/pdf/2507.12675.pdf</a></span>   <span><a href='https://github.com/faeyelab/fortress-paper-code' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Christina Thrainer, Md Meftahul Ferdaus, Mahdi Abdelguerfi, Christian Guetl, Steven Sloan, Kendall N. Niles, Ken Pathak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12675">FORTRESS: Function-composition Optimized Real-Time Resilient Structural Segmentation via Kolmogorov-Arnold Enhanced Spatial Attention Networks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Automated structural defect segmentation in civil infrastructure faces a critical challenge: achieving high accuracy while maintaining computational efficiency for real-time deployment. This paper presents FORTRESS (Function-composition Optimized Real-Time Resilient Structural Segmentation), a new architecture that balances accuracy and speed by using a special method that combines depthwise separable convolutions with adaptive Kolmogorov-Arnold Network integration. FORTRESS incorporates three key innovations: a systematic depthwise separable convolution framework achieving a 3.6x parameter reduction per layer, adaptive TiKAN integration that selectively applies function composition transformations only when computationally beneficial, and multi-scale attention fusion combining spatial, channel, and KAN-enhanced features across decoder levels. The architecture achieves remarkable efficiency gains with 91% parameter reduction (31M to 2.9M), 91% computational complexity reduction (13.7 to 1.17 GFLOPs), and 3x inference speed improvement while delivering superior segmentation performance. Evaluation on benchmark infrastructure datasets demonstrates state-of-the-art results with an F1- score of 0.771 and a mean IoU of 0.677, significantly outperforming existing methods including U-Net, SA-UNet, and U- KAN. The dual optimization strategy proves essential for optimal performance, establishing FORTRESS as a robust solution for practical structural defect segmentation in resource-constrained environments where both accuracy and computational efficiency are paramount. Comprehensive architectural specifications are provided in the Supplemental Material. Source code is available at URL: https://github.com/faeyelab/fortress-paper-code.<br>
<span id='abs_ch'>中文: 本文提出FORTRESS架构，通过深度可分离卷积与自适应KAN集成，在保持高精度的同时大幅降低参数与计算量，实现了实时结构缺陷分割的最优性能。English: This paper introduces FORTRESS, an efficient architecture for structural defect segmentation that significantly reduces parameters and computational complexity while achieving superior accuracy and real-time performance.Paperid:607,https://arxiv.org/pdf/2507.12674.pdfGitHub</span><br>
<span id='abs_en'>English: This paper introduces FORTRESS, an efficient architecture for structural defect segmentation that significantly reduces parameters and computational complexity while achieving superior accuracy and real-time performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1323, <a href='https://arxiv.org/pdf/2507.12674.pdf' target='_blank'>https://arxiv.org/pdf/2507.12674.pdf</a></span>   <span><a href='https://github.com/mmiroyan/ParaStudent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mihran Miroyan, Rose Niousha, Joseph E. Gonzalez, Gireeja Ranade, Narges Norouzi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12674">ParaStudent: Generating and Evaluating Realistic Student Code by Teaching LLMs to Struggle</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have shown strong performance on programming tasks, but can they generate student-like code like real students - imperfect, iterative, and stylistically diverse? We present ParaStudent, a systematic study of LLM-based "student-like" code generation in an introductory programming course setting. Using a dataset of timestamped student submissions across multiple semesters, we design low- and high-resolution experiments to model student progress and evaluate code outputs along semantic, functional, and stylistic dimensions. Our results show that fine-tuning significantly improves alignment with real student trajectories and captures error patterns, incremental improvements, and stylistic variations more faithfully. This study shows that modeling realistic student code requires capturing learning dynamics through context-aware generation, temporal modeling, and multi-dimensional evaluation. Code for experiments and evaluation is available at https://github.com/mmiroyan/ParaStudent.<br>
<span id='abs_ch'>中文摘要：本 究提出ParaStudent，证明经过微调的大语言模型能通过时序建模和多维评估，有效模拟学生编程中的学 动态、错误模式和风 变化，生成更贴近真实学生的代 。</span><br>
<span id='abs_en'>English Summary: This study introduces ParaStudent, demonstrating that fine-tuned Large Language Models can generate student-like code by capturing learning dynamics, error patterns, and stylistic variations through temporal modeling and multi-dimensional evaluation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1324, <a href='https://arxiv.org/pdf/2507.12659.pdf' target='_blank'>https://arxiv.org/pdf/2507.12659.pdf</a></span>   <span><a href='https://github.com/LiuzLab/PINN-extrapolation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Athanasios Papastathopoulos-Katsaros, Alexandra Stavrianidi, Zhandong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12659">Improving physics-informed neural network extrapolation via transfer learning and adaptive activation functions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Physics-Informed Neural Networks (PINNs) are deep learning models that incorporate the governing physical laws of a system into the learning process, making them well-suited for solving complex scientific and engineering problems. Recently, PINNs have gained widespread attention as a powerful framework for combining physical principles with data-driven modeling to improve prediction accuracy. Despite their successes, however, PINNs often exhibit poor extrapolation performance outside the training domain and are highly sensitive to the choice of activation functions (AFs). In this paper, we introduce a transfer learning (TL) method to improve the extrapolation capability of PINNs. Our approach applies transfer learning (TL) within an extended training domain, using only a small number of carefully selected collocation points. Additionally, we propose an adaptive AF that takes the form of a linear combination of standard AFs, which improves both the robustness and accuracy of the model. Through a series of experiments, we demonstrate that our method achieves an average of 40% reduction in relative L2 error and an average of 50% reduction in mean absolute error in the extrapolation domain, all without a significant increase in computational cost. The code is available at https://github.com/LiuzLab/PINN-extrapolation .<br>
<span id='abs_ch'>中文: 本文提出一种迁移学 方法与自适应激活函数，有效提升了物理信息神经网络的泛化能力，在未显著增 计算成本的情况下大幅降低了外推误差。</span><br>
<span id='abs_en'>English: This paper introduces a transfer learning method and an adaptive activation function to enhance the extrapolation performance of Physics-Informed Neural Networks, achieving significant error reductions without substantially increasing computational costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1325, <a href='https://arxiv.org/pdf/2507.12602.pdf' target='_blank'>https://arxiv.org/pdf/2507.12602.pdf</a></span>   <span><a href='https://github.com/said-ohamouddou/MS-DGCNN2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Said Ohamouddou, Abdellatif El Afia, Hanaa El Afia, Raddouane Chiheb
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12602">MS-DGCNN++: A Multi-Scale Fusion Dynamic Graph Neural Network with Biological Knowledge Integration for LiDAR Tree Species Classification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Tree species classification from terrestrial LiDAR point clouds is challenging because of the complex multi-scale geometric structures in forest environments. Existing approaches using multi-scale dynamic graph convolutional neural networks (MS-DGCNN) employ parallel multi-scale processing, which fails to capture the semantic relationships between the hierarchical levels of the tree architecture. We present MS-DGCNN++, a hierarchical multiscale fusion dynamic graph convolutional network that uses semantically meaningful feature extraction at local, branch, and canopy scales with cross-scale information propagation. Our method employs scale-specific feature engineering, including standard geometric features for the local scale, normalized relative vectors for the branch scale, and distance information for the canopy scale. This hierarchical approach replaces uniform parallel processing with semantically differentiated representations that are aligned with the natural tree structure. Under the same proposed tree species data augmentation strategy for all experiments, MS-DGCNN++ achieved an accuracy of 94.96 \% on STPCTLS, outperforming DGCNN, MS-DGCNN, and the state-of-the-art model PPT. On FOR-species20K, it achieves 67.25\% accuracy (6.1\% improvement compared to MS-DGCNN). For standard 3D object recognition, our method outperformed DGCNN and MS-DGCNN with overall accuracies of 93.15\% on ModelNet40 and 94.05\% on ModelNet10. With lower parameters and reduced complexity compared to state-of-the-art transformer approaches, our method is suitable for resource-constrained applications while maintaining a competitive accuracy. Beyond tree classification, the method generalizes to standard 3D object recognition, establishing it as a versatile solution for diverse point cloud processing applications. The implementation code is publicly available at https://github.com/said-ohamouddou/MS-DGCNN2.<br>
<span id='abs_ch'>Chinese: MS-DGCNN++ 提出了一种层次化多尺度融合网络，通过捕捉局部、枝干和 层尺度的语义关系，提升了从LiDAR数据中进行 种分类的准确性，并以更低的复杂度实现了卓越性能。</span><br>
<span id='abs_en'>English: MS-DGCNN++ introduces a hierarchical multiscale fusion network that enhances tree species classification from LiDAR data by capturing semantic relationships across local, branch, and canopy scales, achieving superior accuracy with reduced complexity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1326, <a href='https://arxiv.org/pdf/2507.12482.pdf' target='_blank'>https://arxiv.org/pdf/2507.12482.pdf</a></span>   <span><a href='https://github.com/Kodezi/chronos' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ishraq Khan, Assad Chowdary, Sharoz Haseeb, Urvish Patel, Yousuf Zaii
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12482">Kodezi Chronos: A Debugging-First Language Model for Repository-Scale Code Understanding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have improved code generation and software automation, but remain limited by inference-time context and lack structured reasoning over code. Debugging remains unsolved despite these advances. While Claude Opus 4 and GPT-4.1 achieve >70% on code synthesis benchmarks, they perform <15% on real debugging tasks. We introduce Kodezi Chronos, a language model built specifically for debugging. Chronos combines Adaptive Graph-Guided Retrieval to navigate codebases up to 10 million lines using multi-hop traversal (92% precision, 85% recall), Persistent Debug Memory trained on 15M+ sessions, and a 7-layer architecture for iterative fix-test-refine loops. On 5,000 real-world scenarios, Chronos achieves 67.3% fix accuracy, compared to 14.2% and 13.8% for Claude and GPT-4.1 respectively. Chronos reduces debugging time by 40% and iteration count by 65%. It resolves complex multi-file bugs involving cross-repository context and temporal reasoning. Key limitations include 23.4% success on hardware-dependent issues and 41.2% on dynamic language errors. Theoretical analysis shows O(k log d) retrieval complexity with convergence guarantees. In a human evaluation (N=50), 89% of participants preferred Chronos over baseline models. Chronos will be available in Kodezi OS in Q4 2025 and via API in Q1 2026.<br>
<span id='abs_ch'>中文: Kodezi Chronos 作为专用于调试的语言模型，通过自适应代 库导航和持久调试记忆实现了67.3%的修复准确率，在5000个实际场景中显著优于Claude和GPT-4.1等通用模型，并将调试时间减少40%。</span><br>
<span id='abs_en'>English: Kodezi Chronos is a specialized debugging language model that achieves 67.3% fix accuracy through adaptive codebase navigation and persistent debug memory, significantly outperforming general models like Claude and GPT-4.1 while reducing debugging time by 40%.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1327, <a href='https://arxiv.org/pdf/2507.12425.pdf' target='_blank'>https://arxiv.org/pdf/2507.12425.pdf</a></span>   <span><a href='https://github.com/CheerlaChandana/Enterprise-Chatbot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chandana Cheerla
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12425">Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Organizations increasingly rely on proprietary enterprise data, including HR records, structured reports, and tabular documents, for critical decision-making. While Large Language Models (LLMs) have strong generative capabilities, they are limited by static pretraining, short context windows, and challenges in processing heterogeneous data formats. Conventional Retrieval-Augmented Generation (RAG) frameworks address some of these gaps but often struggle with structured and semi-structured data.
  This work proposes an advanced RAG framework that combines hybrid retrieval strategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by metadata-aware filtering with SpaCy NER and cross-encoder reranking. The framework applies semantic chunking to maintain textual coherence and retains tabular data structures to preserve row-column integrity. Quantized indexing optimizes retrieval efficiency, while human-in-the-loop feedback and conversation memory improve adaptability.
  Experiments on enterprise datasets show notable improvements: Precision@5 increased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74), and Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative evaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness (4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale. These results demonstrate the framework's effectiveness in delivering accurate, comprehensive, and contextually relevant responses for enterprise tasks. Future work includes extending to multimodal data and integrating agent-based retrieval. The source code will be released at https://github.com/CheerlaChandana/Enterprise-Chatbot<br>
<span id='abs_ch'>中文: 该先进RAG框架通过混合检索与元数据过滤及语义分块相结合，显著提升了企业数据处理中的精确率、召回率和响应质量，在评估中表现优异。English: This advanced RAG framework enhances enterprise data processing by combining hybrid retrieval with metadata filtering and semantic chunking, significantly improving precision, recall, and response quality in evaluations.Paperid:619,https://arxiv.org/pdf/2507.12419.pdfGitHub</span><br>
<span id='abs_en'>English: This advanced RAG framework enhances enterprise data processing by combining hybrid retrieval with metadata filtering and semantic chunking, significantly improving precision, recall, and response quality in evaluations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1328, <a href='https://arxiv.org/pdf/2507.12419.pdf' target='_blank'>https://arxiv.org/pdf/2507.12419.pdf</a></span>   <span><a href='https://github.com/nutig/RayTracing' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrea Perin, Giacomo Lagomarsini, Claudio Gallicchio, Giuseppe Nuti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12419">Mixture of Raytraced Experts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts (MoE) architecture which can dynamically select sequences of experts, producing computational graphs of variable width and depth. Existing MoE architectures generally require a fixed amount of computation for a given sample. Our approach, in contrast, yields predictions with increasing accuracy as the computation cycles through the experts' sequence. We train our model by iteratively sampling from a set of candidate experts, unfolding the sequence akin to how Recurrent Neural Networks are trained. Our method does not require load-balancing mechanisms, and preliminary experiments show a reduction in training epochs of 10\% to 40\% with a comparable/higher accuracy. These results point to new research directions in the field of MoEs, allowing the design of potentially faster and more expressive models. The code is available at https://github.com/nutig/RayTracing<br>
<span id='abs_ch'>中文摘要：混合光线追踪专家是一种动态MoE架构，通过自适应选择专家序列实现计算量与精度同步提升， 需负载平衡即可 速训练并提高模型性能。</span><br>
<span id='abs_en'>English Summary: The Mixture of Raytraced Experts is a dynamic MoE architecture that adaptively sequences experts to enhance accuracy with variable computation, achieving faster training and higher performance without load-balancing.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1329, <a href='https://arxiv.org/pdf/2507.12416.pdf' target='_blank'>https://arxiv.org/pdf/2507.12416.pdf</a></span>   <span><a href='https://github.com/jackwaky/QuRe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaehyun Kwak, Ramahdani Muhammad Izaaz Inhar, Se-Young Yun, Sung-Ju Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12416">QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Composed Image Retrieval (CIR) retrieves relevant images based on a reference image and accompanying text describing desired modifications. However, existing CIR methods only focus on retrieving the target image and disregard the relevance of other images. This limitation arises because most methods employing contrastive learning-which treats the target image as positive and all other images in the batch as negatives-can inadvertently include false negatives. This may result in retrieving irrelevant images, reducing user satisfaction even when the target image is retrieved. To address this issue, we propose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which optimizes a reward model objective to reduce false negatives. Additionally, we introduce a hard negative sampling strategy that selects images positioned between two steep drops in relevance scores following the target image, to effectively filter false negatives. In order to evaluate CIR models on their alignment with human satisfaction, we create Human-Preference FashionIQ (HP-FashionIQ), a new dataset that explicitly captures user preferences beyond target retrieval. Extensive experiments demonstrate that QuRe achieves state-of-the-art performance on FashionIQ and CIRR datasets while exhibiting the strongest alignment with human preferences on the HP-FashionIQ dataset. The source code is available at https://github.com/jackwaky/QuRe.<br>
<span id='abs_ch'>Chinese: 提出的通过硬负 本采 的查询相关检索方法QuRe，通过优化奖励模型和采用硬负 本采 策略，解决了组合图像检索中的假阴性问题，在新旧数据集上均实现了最优性能，并与人类偏好更佳对齐。</span><br>
<span id='abs_en'>English: The proposed Query-Relevant Retrieval through Hard Negative Sampling (QuRe) method addresses the issue of false negatives in composed image retrieval by optimizing a reward model and employing a hard negative sampling strategy, achieving state-of-the-art performance and better alignment with human preferences on new and existing datasets.Paperid:621,https://arxiv.org/pdf/2507.12382.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1330, <a href='https://arxiv.org/pdf/2507.12367.pdf' target='_blank'>https://arxiv.org/pdf/2507.12367.pdf</a></span>   <span><a href='https://github.com/mrcabbage972/GitChameleonBenchmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Diganta Misra, Nizar Islah, Victor May, Brice Rauby, Zihan Wang, Justine Gehring, Antonio Orvieto, Muawiz Chaudhary, Eilif B. Muller, Irina Rish, Samira Ebrahimi Kahou, Massimo Caccia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12367">GitChameleon 2.0: Evaluating AI Code Generation Against Python Library Version Incompatibilities</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid evolution of software libraries poses a considerable hurdle for code generation, necessitating continuous adaptation to frequent version updates while preserving backward compatibility. While existing code evolution benchmarks provide valuable insights, they typically lack execution-based evaluation for generating code compliant with specific library versions. To address this, we introduce GitChameleon 2.0, a novel, meticulously curated dataset comprising 328 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. GitChameleon 2.0 rigorously evaluates the capacity of contemporary large language models (LLMs), LLM-powered agents, code assistants, and RAG systems to perform version-conditioned code generation that demonstrates functional accuracy through execution. Our extensive evaluations indicate that state-of-the-art systems encounter significant challenges with this task; enterprise models achieving baseline success rates in the 48-51% range, underscoring the intricacy of the problem. By offering an execution-based benchmark emphasizing the dynamic nature of code libraries, GitChameleon 2.0 enables a clearer understanding of this challenge and helps guide the development of more adaptable and dependable AI code generation methods. We make the dataset and evaluation code publicly available at https://github.com/mrcabbage972/GitChameleonBenchmark.<br>
<span id='abs_ch'>Chinese: GitChameleon 2.0数据集通过提供可执行的单元测试来评估AI系统在特定版本代 生成方面的能力，揭示了当前模型在此任务上面临显著挑战。</span><br>
<span id='abs_en'>English: The GitChameleon 2.0 dataset addresses the challenge of version-specific code generation by providing executable unit tests to evaluate AI systems, revealing that current models struggle significantly with this task.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1331, <a href='https://arxiv.org/pdf/2507.12305.pdf' target='_blank'>https://arxiv.org/pdf/2507.12305.pdf</a></span>   <span><a href='https://github.com/anwarmaxsum/PROL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>M. Anwar Ma'sum, Mahardhika Pratama, Savitha Ramasamy, Lin Liu, Habibullah Habibullah, Ryszard Kowalczyk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12305">PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The data privacy constraint in online continual learning (OCL), where the data can be seen only once, complicates the catastrophic forgetting problem in streaming data. A common approach applied by the current SOTAs in OCL is with the use of memory saving exemplars or features from previous classes to be replayed in the current task. On the other hand, the prompt-based approach performs excellently in continual learning but with the cost of a growing number of trainable parameters. The first approach may not be applicable in practice due to data openness policy, while the second approach has the issue of throughput associated with the streaming data. In this study, we propose a novel prompt-based method for online continual learning that includes 4 main components: (1) single light-weight prompt generator as a general knowledge, (2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model (PTM) generalization preserving, and (4) hard-soft updates mechanism. Our proposed method achieves significantly higher performance than the current SOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity analysis shows that our method requires a relatively smaller number of parameters and achieves moderate training time, inference time, and throughput. For further study, the source code of our method is available at https://github.com/anwarmaxsum/PROL.<br>
<span id='abs_ch'>Chinese: 本 究提出了一种新颖的在线持续学 提示方法，结合轻量级提示生成器、可训练的缩放移位器、预训练模型保持和硬软更新机制，以较少参数和适中计算成本实现了卓越性能。</span><br>
<span id='abs_en'>English: This study introduces a novel prompt-based method for online continual learning that integrates a lightweight prompt generator, trainable scaler-shifter, pre-trained model preservation, and a hard-soft update mechanism, achieving superior performance with fewer parameters and moderate computational demands.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1332, <a href='https://arxiv.org/pdf/2507.12295.pdf' target='_blank'>https://arxiv.org/pdf/2507.12295.pdf</a></span>   <span><a href='https://github.com/jicongfan/Text-Anomaly-Detection-Benchmark,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Feng Xiao, Jicong Fan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12295">Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Text anomaly detection is a critical task in natural language processing (NLP), with applications spanning fraud detection, misinformation identification, spam detection and content moderation, etc. Despite significant advances in large language models (LLMs) and anomaly detection algorithms, the absence of standardized and comprehensive benchmarks for evaluating the existing anomaly detection methods on text data limits rigorous comparison and development of innovative approaches. This work performs a comprehensive empirical study and introduces a benchmark for text anomaly detection, leveraging embeddings from diverse pre-trained language models across a wide array of text datasets. Our work systematically evaluates the effectiveness of embedding-based text anomaly detection by incorporating (1) early language models (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI (small, ada, large)); (3) multi-domain text datasets (news, social media, scientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC). Our experiments reveal a critical empirical insight: embedding quality significantly governs anomaly detection efficacy, and deep learning-based approaches demonstrate no performance advantage over conventional shallow algorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived embeddings.In addition, we observe strongly low-rank characteristics in cross-model performance matrices, which enables an efficient strategy for rapid model evaluation (or embedding evaluation) and selection in practical applications. Furthermore, by open-sourcing our benchmark toolkit that includes all embeddings from different models and code at https://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work provides a foundation for future research in robust and scalable text anomaly detection systems.<br>
<span id='abs_ch'>中文: 本 究构建了文本异常检测的综合基准，发现嵌入质量对性能至关重要且使用大语言模型嵌入时深度学 方法相比 统算法并 优势，同时提供了开源工具包以支持未来 究。</span><br>
<span id='abs_en'>English: This study establishes a comprehensive benchmark for text anomaly detection, revealing that embedding quality is crucial for performance and deep learning models offer no advantage over traditional methods when using LLM embeddings, while also providing an open-source toolkit for future research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1333, <a href='https://arxiv.org/pdf/2507.12261.pdf' target='_blank'>https://arxiv.org/pdf/2507.12261.pdf</a></span>   <span><a href='https://github.com/j-frei/Infherno' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Johann Frei, Nils Feldhus, Lisa Raithel, Roland Roller, Alexander Meyer, Frank Kramer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12261">Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>For clinical data integration and healthcare services, the HL7 FHIR standard has established itself as a desirable format for interoperability between complex health data. Previous attempts at automating the translation from free-form clinical notes into structured FHIR resources rely on modular, rule-based systems or LLMs with instruction tuning and constrained decoding. Since they frequently suffer from limited generalizability and structural inconformity, we propose an end-to-end framework powered by LLM agents, code execution, and healthcare terminology database tools to address these issues. Our solution, called Infherno, is designed to adhere to the FHIR document schema and competes well with a human baseline in predicting FHIR resources from unstructured text. The implementation features a front end for custom and synthetic data and both local and proprietary models, supporting clinical data integration processes and interoperability across institutions.<br>
<span id='abs_ch'>中文：Infherno框架通过结合LLM智能体、代 执行和医学术语库工具，能够将非结构化临床笔记准确转换为结构化FHIR资源，在保持 准兼容性的同时达到了接近人工基准的转换效果。</span><br>
<span id='abs_en'>English: The proposed Infherno framework utilizes LLM agents, code execution, and healthcare terminology tools to accurately translate unstructured clinical notes into structured FHIR resources, outperforming previous methods and approaching human-level performance in ensuring interoperability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1334, <a href='https://arxiv.org/pdf/2507.12252.pdf' target='_blank'>https://arxiv.org/pdf/2507.12252.pdf</a></span>   <span><a href='https://github.com/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shilin Zhou, Zhenghua Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12252">Improving Contextual ASR via Multi-grained Fusion with Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While end-to-end Automatic Speech Recognition (ASR) models have shown impressive performance in transcribing general speech, they often struggle to accurately recognize contextually relevant keywords, such as proper nouns or user-specific entities.
  Previous approaches have explored leveraging keyword dictionaries in the textual modality to improve keyword recognition, either through token-level fusion that guides token-by-token generation or phrase-level fusion that enables direct copying of keyword phrases.
  However, these methods operate at different granularities and have their own limitations.
  In this paper, we propose a novel multi-grained fusion approach that jointly leverages the strengths of both token-level and phrase-level fusion with Large Language Models (LLMs).
  Our approach incorporates a late-fusion strategy that elegantly combines ASR's acoustic information with LLM's rich contextual knowledge, balancing fine-grained token precision with holistic phrase-level understanding.
  Experiments on Chinese and English datasets demonstrate that our approach achieves state-of-the-art performance on keyword-related metrics while preserving high accuracy on non-keyword text.
  Ablation studies further confirm that the token-level and phrase-level components both contribute significantly to the performance gains, complementing each other in our joint multi-grained framework.
  The code and models will be publicly available at https://github.com/.<br>
<span id='abs_ch'>中文: 本文提出了一种新颖的多粒度融合方法，结合了词级和短语级策略与大型语言模型，显著提升了自动语音识别中的关键词识别性能，在中英文数据集上均取得了最优结果，同时保持了非关键词文本的高准确率。</span><br>
<span id='abs_en'>English: This paper introduces a novel multi-grained fusion approach that combines token-level and phrase-level strategies with Large Language Models to enhance keyword recognition in Automatic Speech Recognition, achieving state-of-the-art performance on both Chinese and English datasets while maintaining general transcription accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1335, <a href='https://arxiv.org/pdf/2507.12189.pdf' target='_blank'>https://arxiv.org/pdf/2507.12189.pdf</a></span>   <span><a href='https://github.com/azhar-ikhtiarudin/bench-rlqas' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/azhar-ikhtiarudin/bench-rlqas' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Azhar Ikhtiarudin, Aditi Das, Param Thakkar, Akash Kundu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12189">BenchRL-QAS: Benchmarking reinforcement learning algorithms for quantum architecture search</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce BenchRL-QAS, a unified benchmarking framework for systematically evaluating reinforcement learning (RL) algorithms in quantum architecture search (QAS) across diverse variational quantum algorithm tasks and system sizes ranging from 2- to 8-qubit. Our study benchmarks nine RL agents including both value-based and policy-gradient methods on representative quantum problems such as variational quantum eigensolver, variational quantum state diagonalization, quantum classification, and state preparation, spanning both noiseless and realistic noisy regimes. We propose a weighted ranking metric that balances accuracy, circuit depth, gate count, and computational efficiency, enabling fair and comprehensive comparison. Our results first reveal that RL-based quantum classifier outperforms baseline variational classifiers. Then we conclude that no single RL algorithm is universally optimal when considering a set of QAS tasks; algorithmic performance is highly context-dependent, varying with task structure, qubit count, and noise. This empirical finding provides strong evidence for the "no free lunch" principle in RL-based quantum circuit design and highlights the necessity of tailored algorithm selection and systematic benchmarking for advancing quantum circuit synthesis. This work represents the most comprehensive RL-QAS benchmarking effort to date, and BenchRL-QAS along with all experimental data are made publicly available to support reproducibility and future research https://github.com/azhar-ikhtiarudin/bench-rlqas.<br>
<span id='abs_ch'>中文摘要：BenchRL-QAS是一个统一的量子架构搜索强化学 基准框架，通过系统评估九种不同智能体在多种量子任务中的表现，证明不存在通用最优方法，且性能受任务类型和噪声条件影响。English Summary: BenchRL-QAS is a comprehensive benchmarking framework that evaluates nine RL agents across various quantum tasks, revealing no universally superior method and demonstrating task-dependent performance under different conditions.Paperid:630,https://arxiv.org/pdf/2507.12188.pdfGitHub</span><br>
<span id='abs_en'>English Summary: BenchRL-QAS is a comprehensive benchmarking framework that evaluates nine RL agents across various quantum tasks, revealing no universally superior method and demonstrating task-dependent performance under different conditions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1336, <a href='https://arxiv.org/pdf/2507.12189.pdf' target='_blank'>https://arxiv.org/pdf/2507.12189.pdf</a></span>   <span><a href='https://github.com/azhar-ikhtiarudin/bench-rlqas' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Azhar Ikhtiarudin, Aditi Das, Param Thakkar, Akash Kundu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12189">BenchRL-QAS: Benchmarking reinforcement learning algorithms for quantum architecture search</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present BenchRL-QAS, a unified benchmarking framework for reinforcement learning (RL) in quantum architecture search (QAS) across a spectrum of variational quantum algorithm tasks on 2- to 8-qubit systems. Our study systematically evaluates 9 different RL agents, including both value-based and policy-gradient methods, on quantum problems such as variational eigensolver, quantum state diagonalization, variational quantum classification (VQC), and state preparation, under both noiseless and noisy execution settings. To ensure fair comparison, we propose a weighted ranking metric that integrates accuracy, circuit depth, gate count, and training time. Results demonstrate that no single RL method dominates universally, the performance dependents on task type, qubit count, and noise conditions providing strong evidence of no free lunch principle in RL-QAS. As a byproduct we observe that a carefully chosen RL algorithm in RL-based VQC outperforms baseline VQCs. BenchRL-QAS establishes the most extensive benchmark for RL-based QAS to date, codes and experimental made publicly available for reproducibility and future advances.<br>
<span id='abs_ch'>中文摘要：BenchRL-QAS是一个统一的量子架构搜索强化学 基准框架，通过系统评估九种不同智能体在多种量子任务中的表现，证明不存在通用最优方法，且性能受任务类型和噪声条件影响。English Summary: BenchRL-QAS is a comprehensive benchmarking framework that evaluates nine RL agents across various quantum tasks, revealing no universally superior method and demonstrating task-dependent performance under different conditions.Paperid:630,https://arxiv.org/pdf/2507.12188.pdfGitHub</span><br>
<span id='abs_en'>English Summary: BenchRL-QAS is a comprehensive benchmarking framework that evaluates nine RL agents across various quantum tasks, revealing no universally superior method and demonstrating task-dependent performance under different conditions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1337, <a href='https://arxiv.org/pdf/2507.12188.pdf' target='_blank'>https://arxiv.org/pdf/2507.12188.pdf</a></span>   <span><a href='https://github.com/Cherisherr/WDCI-Net.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuangli Du, Siming Yan, Zhenghao Shi, Zhenzhen You, Lu Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12188">Wavelet-based Decoupling Framework for low-light Stereo Image Enhancement</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Low-light images suffer from complex degradation, and existing enhancement methods often encode all degradation factors within a single latent space. This leads to highly entangled features and strong black-box characteristics, making the model prone to shortcut learning. To mitigate the above issues, this paper proposes a wavelet-based low-light stereo image enhancement method with feature space decoupling. Our insight comes from the following findings: (1) Wavelet transform enables the independent processing of low-frequency and high-frequency information. (2) Illumination adjustment can be achieved by adjusting the low-frequency component of a low-light image, extracted through multi-level wavelet decomposition. Thus, by using wavelet transform the feature space is decomposed into a low-frequency branch for illumination adjustment and multiple high-frequency branches for texture enhancement. Additionally, stereo low-light image enhancement can extract useful cues from another view to improve enhancement. To this end, we propose a novel high-frequency guided cross-view interaction module (HF-CIM) that operates within high-frequency branches rather than across the entire feature space, effectively extracting valuable image details from the other view. Furthermore, to enhance the high-frequency information, a detail and texture enhancement module (DTEM) is proposed based on cross-attention mechanism. The model is trained on a dataset consisting of images with uniform illumination and images with non-uniform illumination. Experimental results on both real and synthetic images indicate that our algorithm offers significant advantages in light adjustment while effectively recovering high-frequency information. The code and dataset are publicly available at: https://github.com/Cherisherr/WDCI-Net.git.<br>
<span id='abs_ch'>中文摘要：本文提出了一种基于小波变换的低光立体图像增强方法，通过将特征空间解耦为低频和高频分支，并利用跨视角交互与纹理增强模块，有效改善了光照调整与细节恢复效果。</span><br>
<span id='abs_en'>English Summary: This paper introduces a wavelet-based method that decouples feature space into low-frequency and high-frequency branches for stereo low-light image enhancement, utilizing cross-view interaction and texture enhancement modules to improve illumination adjustment and detail recovery.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1338, <a href='https://arxiv.org/pdf/2507.12110.pdf' target='_blank'>https://arxiv.org/pdf/2507.12110.pdf</a></span>   <span><a href='https://github.com/leoPub/tpemarl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ye Han, Lijun Zhang, Dejian Meng, Zhuang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12110">Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The exploration-exploitation trade-off constitutes one of the fundamental challenges in reinforcement learning (RL), which is exacerbated in multi-agent reinforcement learning (MARL) due to the exponential growth of joint state-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL) method for optimizing cooperative decision-making of connected and autonomous vehicles (CAVs) in mixed traffic. This work presents two primary contributions: First, we construct a game topology tensor for dynamic traffic flow, effectively compressing high-dimensional traffic state information and decrease the search space for MARL algorithms. Second, building upon the designed game topology tensor and using QMIX as the backbone RL algorithm, we establish a topology-enhanced MARL framework incorporating visit counts and agent mutual information. Extensive simulations across varying traffic densities and CAV penetration rates demonstrate the effectiveness of TPE-MARL. Evaluations encompassing training dynamics, exploration patterns, macroscopic traffic performance metrics, and microscopic vehicle behaviors reveal that TPE-MARL successfully balances exploration and exploitation. Consequently, it exhibits superior performance in terms of traffic efficiency, safety, decision smoothness, and task completion. Furthermore, the algorithm demonstrates decision-making rationality comparable to or exceeding that of human drivers in both mixed-autonomy and fully autonomous traffic scenarios. Code of our work is available at \href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}.<br>
<span id='abs_ch'>中文: 本文提出了一种拓扑增强多智能体强化学 方法，通过构建博弈拓扑 量和结合访问计数与智能体互信息，有效解决了混合交通中协同决策的探索-利用权衡问题，在多种交通场景下展现出卓越的交通效率与安全性。</span><br>
<span id='abs_en'>English: This paper introduces a topology-enhanced multi-agent reinforcement learning (TPE-MARL) method that effectively balances exploration and exploitation for cooperative decision-making in connected autonomous vehicles, demonstrating superior performance in traffic efficiency and safety across various scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1339, <a href='https://arxiv.org/pdf/2507.12075.pdf' target='_blank'>https://arxiv.org/pdf/2507.12075.pdf</a></span>   <span><a href='https://github.com/sapienzanlp/bookcoref' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Giuliano Martinelli, Tommaso Bonomo, Pere-LluÃ­s Huguet Cabot, Roberto Navigli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12075">BOOKCOREF: Coreference Resolution at Book Scale</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Coreference Resolution systems are typically evaluated on benchmarks containing small- to medium-scale documents. When it comes to evaluating long texts, however, existing benchmarks, such as LitBank, remain limited in length and do not adequately assess system capabilities at the book scale, i.e., when co-referring mentions span hundreds of thousands of tokens. To fill this gap, we first put forward a novel automatic pipeline that produces high-quality Coreference Resolution annotations on full narrative texts. Then, we adopt this pipeline to create the first book-scale coreference benchmark, BOOKCOREF, with an average document length of more than 200,000 tokens. We carry out a series of experiments showing the robustness of our automatic procedure and demonstrating the value of our resource, which enables current long-document coreference systems to gain up to +20 CoNLL-F1 points when evaluated on full books. Moreover, we report on the new challenges introduced by this unprecedented book-scale setting, highlighting that current models fail to deliver the same performance they achieve on smaller documents. We release our data and code to encourage research and development of new book-scale Coreference Resolution systems at https://github.com/sapienzanlp/bookcoref.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1340, <a href='https://arxiv.org/pdf/2507.12006.pdf' target='_blank'>https://arxiv.org/pdf/2507.12006.pdf</a></span>   <span><a href='https://github.com/Linwei-Chen/FDAM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Linwei Chen, Lin Gu, Ying Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12006">Frequency-Dynamic Attention Modulation for Dense Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vision Transformers (ViTs) have significantly advanced computer vision, demonstrating strong performance across various tasks. However, the attention mechanism in ViTs makes each layer function as a low-pass filter, and the stacked-layer architecture in existing transformers suffers from frequency vanishing. This leads to the loss of critical details and textures. We propose a novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention Modulation (FDAM), which can be easily plugged into ViTs. FDAM directly modulates the overall frequency response of ViTs and consists of two techniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling (FreqScale). Since circuit theory uses low-pass filters as fundamental elements, we introduce AttInv, a method that generates complementary high-pass filtering by inverting the low-pass filter in the attention matrix, and dynamically combining the two. We further design FreqScale to weight different frequency components for fine-grained adjustments to the target response function. Through feature similarity analysis and effective rank evaluation, we demonstrate that our approach avoids representation collapse, leading to consistent performance improvements across various models, including SegFormer, DeiT, and MaskDINO. These improvements are evident in tasks such as semantic segmentation, object detection, and instance segmentation. Additionally, we apply our method to remote sensing detection, achieving state-of-the-art results in single-scale settings. The code is available at https://github.com/Linwei-Chen/FDAM.<br>
<span id='abs_ch'>中文: 提出的频率动态注意力调制（FDAM）通过互补高通滤波和动态频率缩放解决视觉Transformer中的频率消失问题，在多种视觉任务中实现性能提升且避免表示崩溃。</span><br>
<span id='abs_en'>English: The proposed Frequency-Dynamic Attention Modulation (FDAM) enhances Vision Transformers by addressing frequency vanishing through complementary high-pass filtering and dynamic frequency scaling, resulting in improved performance across multiple vision tasks without representation collapse.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1341, <a href='https://arxiv.org/pdf/2507.11936.pdf' target='_blank'>https://arxiv.org/pdf/2507.11936.pdf</a></span>   <span><a href='https://github.com/majianz/dl4gps' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianzhe Ma, Wenxuan Wang, Qin Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11936">A Survey of Deep Learning for Geometry Problem Solving</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Geometry problem solving, a crucial aspect of mathematical reasoning, is vital across various domains, including education, the assessment of AI's mathematical abilities, and multimodal capability evaluation. The recent surge in deep learning technologies, particularly the emergence of multimodal large language models, has significantly accelerated research in this area. This paper provides a survey of the applications of deep learning in geometry problem solving, including (i) a comprehensive summary of the relevant tasks in geometry problem solving; (ii) a thorough review of related deep learning methods; (iii) a detailed analysis of evaluation metrics and methods; and (iv) a critical discussion of the current challenges and future directions that can be explored. Our objective is to offer a comprehensive and practical reference of deep learning for geometry problem solving, thereby fostering further advancements in this field. We create a continuously updated list of papers on GitHub: https://github.com/majianz/dl4gps.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1342, <a href='https://arxiv.org/pdf/2507.11893.pdf' target='_blank'>https://arxiv.org/pdf/2507.11893.pdf</a></span>   <span><a href='https://github.com/Linwei-Chen/SFM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Linwei Chen, Ying Fu, Lin Gu, Dezhi Zheng, Jifeng Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11893">Spatial Frequency Modulation for Semantic Segmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>High spatial frequency information, including fine details like textures, significantly contributes to the accuracy of semantic segmentation. However, according to the Nyquist-Shannon Sampling Theorem, high-frequency components are vulnerable to aliasing or distortion when propagating through downsampling layers such as strided-convolution. Here, we propose a novel Spatial Frequency Modulation (SFM) that modulates high-frequency features to a lower frequency before downsampling and then demodulates them back during upsampling. Specifically, we implement modulation through adaptive resampling (ARS) and design a lightweight add-on that can densely sample the high-frequency areas to scale up the signal, thereby lowering its frequency in accordance with the Frequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling (MSAU) to demodulate the modulated feature and recover high-frequency information through non-uniform upsampling This module further improves segmentation by explicitly exploiting information interaction between densely and sparsely resampled areas at multiple scales. Both modules can seamlessly integrate with various architectures, extending from convolutional neural networks to transformers. Feature visualization and analysis confirm that our method effectively alleviates aliasing while successfully retaining details after demodulation. Finally, we validate the broad applicability and effectiveness of SFM by extending it to image classification, adversarial robustness, instance segmentation, and panoptic segmentation tasks. The code is available at https://github.com/Linwei-Chen/SFM.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1343, <a href='https://arxiv.org/pdf/2507.11810.pdf' target='_blank'>https://arxiv.org/pdf/2507.11810.pdf</a></span>   <span><a href='https://github.com/haoxuan-unt2024/llm4innovation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxuan Zhang, Ruochi Li, Yang Zhang, Ting Xiao, Jiangping Chen, Junhua Ding, Haihua Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11810">The Evolving Role of Large Language Models in Scientific Innovation: Evaluator, Collaborator, and Scientist</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Scientific innovation is undergoing a paradigm shift driven by the rapid advancement of Large Language Models (LLMs). As science faces mounting challenges including information overload, disciplinary silos, and diminishing returns on conventional research methods, LLMs are emerging as powerful agents capable not only of enhancing scientific workflows but also of participating in and potentially leading the innovation process. Existing surveys mainly focus on different perspectives, phrases, and tasks in scientific research and discovery, while they have limitations in understanding the transformative potential and role differentiation of LLM. This survey proposes a comprehensive framework to categorize the evolving roles of LLMs in scientific innovation across three hierarchical levels: Evaluator, Collaborator, and Scientist. We distinguish between LLMs' contributions to structured scientific research processes and open-ended scientific discovery, thereby offering a unified taxonomy that clarifies capability boundaries, evaluation criteria, and human-AI interaction patterns at each level. Through an extensive analysis of current methodologies, benchmarks, systems, and evaluation metrics, this survey delivers an in-depth and systematic synthesis on LLM-driven scientific innovation. We present LLMs not only as tools for automating existing processes, but also as catalysts capable of reshaping the epistemological foundations of science itself. This survey offers conceptual clarity, practical guidance, and theoretical foundations for future research, while also highlighting open challenges and ethical considerations in the pursuit of increasingly autonomous AI-driven science. Resources related to this survey can be accessed on GitHub at: https://github.com/haoxuan-unt2024/llm4innovation.<br>
<span id='abs_ch'>中文: 本综述提出将大语言模型在科学创新中的角色划分为评估者、协作者和科学家三个层次，强调其不仅是自动化工具更是重塑科学认识论的催化剂，同时探讨了相关挑战与伦理问题。</span><br>
<span id='abs_en'>English: This survey introduces a hierarchical framework categorizing LLMs' roles in scientific innovation as Evaluator, Collaborator, and Scientist, highlighting their transformative potential beyond automation to reshape scientific epistemology while addressing challenges and ethical considerations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1344, <a href='https://arxiv.org/pdf/2507.11807.pdf' target='_blank'>https://arxiv.org/pdf/2507.11807.pdf</a></span>   <span><a href='https://github.com/ruofanhu/CLID-MU' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruofan Hu, Dongyu Zhang, Huayi Zhang, Elke Rundensteiner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11807">CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for Learning with Noisy Labels</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Learning with noisy labels (LNL) is essential for training deep neural networks with imperfect data. Meta-learning approaches have achieved success by using a clean unbiased labeled set to train a robust model. However, this approach heavily depends on the availability of a clean labeled meta-dataset, which is difficult to obtain in practice. In this work, we thus tackle the challenge of meta-learning for noisy label scenarios without relying on a clean labeled dataset. Our approach leverages the data itself while bypassing the need for labels. Building on the insight that clean samples effectively preserve the consistency of related data structures across the last hidden and the final layer, whereas noisy samples disrupt this consistency, we design the Cross-layer Information Divergence-based Meta Update Strategy (CLID-MU). CLID-MU leverages the alignment of data structures across these diverse feature spaces to evaluate model performance and use this alignment to guide training. Experiments on benchmark datasets with varying amounts of labels under both synthetic and real-world noise demonstrate that CLID-MU outperforms state-of-the-art methods. The code is released at https://github.com/ruofanhu/CLID-MU.<br>
<span id='abs_ch'>Chinese: 本文提出CLID-MU方法，通过利用跨层数据结构一致性在 需干净 注数据的情况下训练噪声 签的鲁棒模型，在基准数据集上超越了现有最优方法。English: This paper introduces CLID-MU, a meta-learning method that trains robust models on noisy labels by leveraging cross-layer data structure consistency without requiring clean labeled data, and it outperforms existing techniques on benchmark datasets.Paperid:655,https://arxiv.org/pdf/2507.11806.pdfGitHub</span><br>
<span id='abs_en'>English: This paper introduces CLID-MU, a meta-learning method that trains robust models on noisy labels by leveraging cross-layer data structure consistency without requiring clean labeled data, and it outperforms existing techniques on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1345, <a href='https://arxiv.org/pdf/2507.11710.pdf' target='_blank'>https://arxiv.org/pdf/2507.11710.pdf</a></span>   <span><a href='https://github.com/revolins/FlexOOD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jay Revolinsky, Harry Shomer, Jiliang Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11710">Subgraph Generation for Generalizing on Out-of-Distribution Links</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Graphs Neural Networks (GNNs) demonstrate high-performance on the link prediction (LP) task. However, these models often rely on all dataset samples being drawn from the same distribution. In addition, graph generative models (GGMs) show a pronounced ability to generate novel output graphs. Despite this, GGM applications remain largely limited to domain-specific tasks. To bridge this gap, we propose FLEX as a GGM framework which leverages two mechanism: (1) structurally-conditioned graph generation, and (2) adversarial co-training between an auto-encoder and GNN. As such, FLEX ensures structural-alignment between sample distributions to enhance link-prediction performance in out-of-distribution (OOD) scenarios. Notably, FLEX does not require expert knowledge to function in different OOD scenarios. Numerous experiments are conducted in synthetic and real-world OOD settings to demonstrate FLEX's performance-enhancing ability, with further analysis for understanding the effects of graph data augmentation on link structures. The source code is available here: https://github.com/revolins/FlexOOD.<br>
<span id='abs_ch'>中文：提出的FLEX框架通过结构条件图生成与对抗协同训练相结合， 需专家知识即可提升分布外场景下的链接预测性能。</span><br>
<span id='abs_en'>English: The proposed FLEX framework enhances link prediction in out-of-distribution scenarios by combining structurally-conditioned graph generation with adversarial co-training, eliminating the need for expert knowledge while improving performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1346, <a href='https://arxiv.org/pdf/2507.11662.pdf' target='_blank'>https://arxiv.org/pdf/2507.11662.pdf</a></span>   <span><a href='https://github.com/mshalimay/mllm-verifiers-abias-sgv' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Moises Andrade, Joonhyuk Cha, Brandon Ho, Vriksha Srihari, Karmesh Yadav, Zsolt Kira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11662">Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Verifiers -- functions assigning rewards to agent behavior -- have been key for AI progress in domains like math and board games. However, extending these gains to domains without clear-cut success criteria (e.g.,computer use) remains a challenge: while humans can recognize suitable outcomes, translating this intuition into scalable rules is non-trivial. Multimodal Large Language Models(MLLMs) emerge as a promising solution, given their world knowledge, human-preference alignment, and reasoning skills. We evaluate MLLMs as verifiers of agent trajectories across web navigation, computer use, and robotic manipulation, and identify a critical limitation: agreement bias, a strong tendency for MLLMs to favor information in their context window, often generating chains of thought to rationalize flawed behavior. This bias is pervasive across models, resilient to test-time scaling, and can impact several methods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs despite MLLMs showing strong, human-aligned priors on desired behavior. To address this, we propose Self-Grounded Verification (SGV), a lightweight method that enables more effective use of MLLMs' knowledge and reasoning by harnessing their own sampling mechanisms via unconditional and conditional generation. SGV operates in two steps: first, the MLLM is elicited to retrieve broad priors about task completion, independent of the data under evaluation. Then, conditioned on self-generated priors, it reasons over and evaluates a candidate trajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in accuracy and failure detection rates, and can perform real-time supervision of heterogeneous agents, boosting task completion of a GUI specialist in OSWorld, a diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting a new state of the art on the benchmark, surpassing the previous best by 48%.<br>
<span id='abs_ch'>Chinese Summary: 多模态大语言模型在验证智能体行为方面潜力显著，但存在认同偏差问题；通过提出的自基础验证方法，该问题得到有效解决，大幅提升了模型在多项任务中的准确性和表现。</span><br>
<span id='abs_en'>English Summary: Multimodal Large Language Models (MLLMs) show promise as verifiers for agent behavior but suffer from agreement bias, which is addressed by the proposed Self-Grounded Verification method that significantly improves their accuracy and performance across various tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1347, <a href='https://arxiv.org/pdf/2507.11638.pdf' target='_blank'>https://arxiv.org/pdf/2507.11638.pdf</a></span>   <span><a href='https://github.com/benkeel/Lymph_Node_Classification_MIUA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Benjamin Keel, Aaron Quyn, David Jayne, Maryam Mohsin, Samuel D. Relton
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11638">Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Effective treatment for rectal cancer relies on accurate lymph node metastasis (LNM) staging. However, radiological criteria based on lymph node (LN) size, shape and texture morphology have limited diagnostic accuracy. In this work, we investigate applying a Variational Autoencoder (VAE) as a feature encoder model to replace the large pre-trained Convolutional Neural Network (CNN) used in existing approaches. The motivation for using a VAE is that the generative model aims to reconstruct the images, so it directly encodes visual features and meaningful patterns across the data. This leads to a disentangled and structured latent space which can be more interpretable than a CNN. Models are deployed on an in-house MRI dataset with 168 patients who did not undergo neo-adjuvant treatment. The post-operative pathological N stage was used as the ground truth to evaluate model predictions. Our proposed model 'VAE-MLP' achieved state-of-the-art performance on the MRI dataset, with cross-validated metrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85 +/- 0.05. Code is available at: https://github.com/benkeel/Lymph_Node_Classification_MIUA.<br>
<span id='abs_ch'>中文: 本 究提出了一种VAE-MLP模型，利用变分自编 器进行特征编 以改进直 癌淋巴结转移分期，在MRI数据集上取得了最佳性能，AUC达到0.86。</span><br>
<span id='abs_en'>English: This study introduces a VAE-MLP model that uses a variational autoencoder for feature encoding to improve lymph node metastasis staging in rectal cancer, achieving state-of-the-art performance with an AUC of 0.86 on an MRI dataset.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1348, <a href='https://arxiv.org/pdf/2507.11620.pdf' target='_blank'>https://arxiv.org/pdf/2507.11620.pdf</a></span>   <span><a href='https://github.com/StevenDillmann/ml-xraytransients-mnras' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Steven Dillmann, Juan Rafael MartÃ­nez-Galarza
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11620">Learning Representations of Event Time Series with Sparse Autoencoders for Anomaly Detection, Similarity Search, and Unsupervised Classification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Event time series are sequences of discrete events occurring at irregular time intervals, each associated with a domain-specific observational modality. They are common in domains such as high-energy astrophysics, computational social science, cybersecurity, finance, healthcare, neuroscience, and seismology. Their unstructured and irregular structure poses significant challenges for extracting meaningful patterns and identifying salient phenomena using conventional techniques. We propose novel two- and three-dimensional tensor representations for event time series, coupled with sparse autoencoders that learn physically meaningful latent representations. These embeddings support a variety of downstream tasks, including anomaly detection, similarity-based retrieval, semantic clustering, and unsupervised classification. We demonstrate our approach on a real-world dataset from X-ray astronomy, showing that these representations successfully capture temporal and spectral signatures and isolate diverse classes of X-ray transients. Our framework offers a flexible, scalable, and generalizable solution for analyzing complex, irregular event time series across scientific and industrial domains.<br>
<span id='abs_ch'>中文摘要：本文针对不规则事件时间序列提出了新型 量表示和稀疏自编 器方法，通过X射线天文数据验证了其在异常检测和分类任务中的有效性，为跨领域复杂数据分析提供了通用解决方案。</span><br>
<span id='abs_en'>English Summary: The paper introduces novel tensor representations and sparse autoencoders to analyze irregular event time series, enabling effective anomaly detection and classification across various domains as demonstrated with X-ray astronomy data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1349, <a href='https://arxiv.org/pdf/2507.11569.pdf' target='_blank'>https://arxiv.org/pdf/2507.11569.pdf</a></span>   <span><a href='https://github.com/mazurowski-lab/Foundation-based-reg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanxue Gu, Yaqian Chen, Nicholas Konz, Qihang Li, Maciej A. Mazurowski
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11569">Are Vision Foundation Models Ready for Out-of-the-Box Medical Image Registration?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Foundation models, pre-trained on large image datasets and capable of capturing rich feature representations, have recently shown potential for zero-shot image registration. However, their performance has mostly been tested in the context of rigid or less complex structures, such as the brain or abdominal organs, and it remains unclear whether these models can handle more challenging, deformable anatomy. Breast MRI registration is particularly difficult due to significant anatomical variation between patients, deformation caused by patient positioning, and the presence of thin and complex internal structure of fibroglandular tissue, where accurate alignment is crucial. Whether foundation model-based registration algorithms can address this level of complexity remains an open question. In this study, we provide a comprehensive evaluation of foundation model-based registration algorithms for breast MRI. We assess five pre-trained encoders, including DINO-v2, SAM, MedSAM, SSLSAM, and MedCLIP, across four key breast registration tasks that capture variations in different years and dates, sequences, modalities, and patient disease status (lesion versus no lesion). Our results show that foundation model-based algorithms such as SAM outperform traditional registration baselines for overall breast alignment, especially under large domain shifts, but struggle with capturing fine details of fibroglandular tissue. Interestingly, additional pre-training or fine-tuning on medical or breast-specific images in MedSAM and SSLSAM, does not improve registration performance and may even decrease it in some cases. Further work is needed to understand how domain-specific training influences registration and to explore targeted strategies that improve both global alignment and fine structure accuracy. We also publicly release our code at \href{https://github.com/mazurowski-lab/Foundation-based-reg}{Github}.<br>
<span id='abs_ch'>中文: 基础模型在零 本乳腺MRI配准中展现出潜力，能在大域偏移下超越 统方法的整体对齐效果，但难以精确捕捉纤维腺体组织的细微结构，且特定领域的训练并未持续提升性能。</span><br>
<span id='abs_en'>English: Foundation models show promise for zero-shot breast MRI registration by outperforming traditional methods in overall alignment under large domain shifts, but they struggle with accurately capturing fine fibroglandular tissue details, and domain-specific training does not consistently enhance performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1350, <a href='https://arxiv.org/pdf/2507.11554.pdf' target='_blank'>https://arxiv.org/pdf/2507.11554.pdf</a></span>   <span><a href='https://github.com/MIGHTYEZ/Inversion-DPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zejian Li, Yize Li, Chenye Meng, Zhongni Liu, Yang Ling, Shengyuan Zhang, Guang Yang, Changyuan Yang, Zhiyuan Yang, Lingyun Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11554">Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in diffusion models (DMs) have been propelled by alignment methods that post-train models to better conform to human preferences. However, these approaches typically require computation-intensive training of a base model and a reward model, which not only incurs substantial computational overhead but may also compromise model accuracy and training efficiency. To address these limitations, we propose Inversion-DPO, a novel alignment framework that circumvents reward modeling by reformulating Direct Preference Optimization (DPO) with DDIM inversion for DMs. Our method conducts intractable posterior sampling in Diffusion-DPO with the deterministic inversion from winning and losing samples to noise and thus derive a new post-training paradigm. This paradigm eliminates the need for auxiliary reward models or inaccurate appromixation, significantly enhancing both precision and efficiency of training. We apply Inversion-DPO to a basic task of text-to-image generation and a challenging task of compositional image generation. Extensive experiments show substantial performance improvements achieved by Inversion-DPO compared to existing post-training methods and highlight the ability of the trained generative models to generate high-fidelity compositionally coherent images. For the post-training of compostitional image geneation, we curate a paired dataset consisting of 11,140 images with complex structural annotations and comprehensive scores, designed to enhance the compositional capabilities of generative models. Inversion-DPO explores a new avenue for efficient, high-precision alignment in diffusion models, advancing their applicability to complex realistic generation tasks. Our code is available at https://github.com/MIGHTYEZ/Inversion-DPO<br>
<span id='abs_ch'>中文: Inversion-DPO提出了一种新颖的对齐框架，通过将DDIM反演与直接偏好优化相结合，绕过了奖励建模，在文本到图像和组合图像生成等任务中显著提升了扩散模型的训练精度和效率。</span><br>
<span id='abs_en'>English: Inversion-DPO introduces a novel alignment framework that bypasses reward modeling by integrating DDIM inversion with Direct Preference Optimization, enhancing training precision and efficiency for diffusion models in tasks like text-to-image and compositional image generation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1351, <a href='https://arxiv.org/pdf/2507.11539.pdf' target='_blank'>https://arxiv.org/pdf/2507.11539.pdf</a></span>   <span><a href='https://github.com/wzzheng/StreamVGGT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/wzzheng/StreamVGGT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dong Zhuo, Wenzhao Zheng, Jiahe Guo, Yuqi Wu, Jie Zhou, Jiwen Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11539">Streaming 4D Visual Geometry Transformer</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Perceiving and reconstructing 4D spatial-temporal geometry from videos is a fundamental yet challenging computer vision task. To facilitate interactive and real-time applications, we propose a streaming 4D visual geometry transformer that shares a similar philosophy with autoregressive large language models. We explore a simple and efficient design and employ a causal transformer architecture to process the input sequence in an online manner. We use temporal causal attention and cache the historical keys and values as implicit memory to enable efficient streaming long-term 4D reconstruction. This design can handle real-time 4D reconstruction by incrementally integrating historical information while maintaining high-quality spatial consistency. For efficient training, we propose to distill knowledge from the dense bidirectional visual geometry grounded transformer (VGGT) to our causal model. For inference, our model supports the migration of optimized efficient attention operator (e.g., FlashAttention) from the field of large language models. Extensive experiments on various 4D geometry perception benchmarks demonstrate that our model increases the inference speed in online scenarios while maintaining competitive performance, paving the way for scalable and interactive 4D vision systems. Code is available at: https://github.com/wzzheng/StreamVGGT.<br>
<span id='abs_ch'>中文: 本文提出了一种流式4D视觉 何变换器，通过 果注意力和知识蒸馏技术，实现了从视频中进行实时高质量的4D重建，在保持竞争力的同时显著提升了交互应用的推理速度。</span><br>
<span id='abs_en'>English: This paper introduces a streaming 4D visual geometry transformer that enables real-time, high-quality 4D reconstruction from videos by using causal attention and knowledge distillation, achieving competitive performance with faster inference for interactive applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1352, <a href='https://arxiv.org/pdf/2507.11527.pdf' target='_blank'>https://arxiv.org/pdf/2507.11527.pdf</a></span>   <span><a href='https://github.com/Eason-Li-AIS/DrafterBench,' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Eason-Li-AIS/DrafterBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yinsheng Li, Zhen Dong, Yi Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11527">DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Model (LLM) agents have shown great potential for solving real-world problems and promise to be a solution for tasks automation in industry. However, more benchmarks are needed to systematically evaluate automation agents from an industrial perspective, for example, in Civil Engineering. Therefore, we propose DrafterBench for the comprehensive evaluation of LLM agents in the context of technical drawing revision, a representation task in civil engineering. DrafterBench contains twelve types of tasks summarized from real-world drawing files, with 46 customized functions/tools and 1920 tasks in total. DrafterBench is an open-source benchmark to rigorously test AI agents' proficiency in interpreting intricate and long-context instructions, leveraging prior knowledge, and adapting to dynamic instruction quality via implicit policy awareness. The toolkit comprehensively assesses distinct capabilities in structured data comprehension, function execution, instruction following, and critical reasoning. DrafterBench offers detailed analysis of task accuracy and error statistics, aiming to provide deeper insight into agent capabilities and identify improvement targets for integrating LLMs in engineering applications. Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench, with the test set hosted at https://huggingface.co/datasets/Eason666/DrafterBench.<br>
<span id='abs_ch'>中文: DrafterBench是一个开源基准测试，通过1920个真实世界土木工程任务全面评估大语言模型代理在技术图纸修订中的指令解析、函数执行和推理能力，旨在为工程应用提供改进方向。</span><br>
<span id='abs_en'>English: DrafterBench is an open-source benchmark designed to comprehensively evaluate LLM agents' capabilities in technical drawing revision tasks, assessing their proficiency in instruction interpretation, function execution, and reasoning through 1920 real-world civil engineering tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1353, <a href='https://arxiv.org/pdf/2507.11415.pdf' target='_blank'>https://arxiv.org/pdf/2507.11415.pdf</a></span>   <span><a href='https://github.com/hbyecoding/U-RWKV' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongbo Ye, Fenghe Tang, Peiang Zhao, Zhen Huang, Dexin Zhao, Minghao Bian, S. Kevin Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11415">U-RWKV: Lightweight medical image segmentation with direction-adaptive RWKV</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Achieving equity in healthcare accessibility requires lightweight yet high-performance solutions for medical image segmentation, particularly in resource-limited settings. Existing methods like U-Net and its variants often suffer from limited global Effective Receptive Fields (ERFs), hindering their ability to capture long-range dependencies. To address this, we propose U-RWKV, a novel framework leveraging the Recurrent Weighted Key-Value(RWKV) architecture, which achieves efficient long-range modeling at O(N) computational cost. The framework introduces two key innovations: the Direction-Adaptive RWKV Module(DARM) and the Stage-Adaptive Squeeze-and-Excitation Module(SASE). DARM employs Dual-RWKV and QuadScan mechanisms to aggregate contextual cues across images, mitigating directional bias while preserving global context and maintaining high computational efficiency. SASE dynamically adapts its architecture to different feature extraction stages, balancing high-resolution detail preservation and semantic relationship capture. Experiments demonstrate that U-RWKV achieves state-of-the-art segmentation performance with high computational efficiency, offering a practical solution for democratizing advanced medical imaging technologies in resource-constrained environments. The code is available at https://github.com/hbyecoding/U-RWKV.<br>
<span id='abs_ch'>中文: U-RWKV提出了一种基于RWKV架构的新型医学图像分割框架，通过DARM和SASE模块以O(N)计算成本高效捕获长程依赖关系，在资源有限环境中实现了最先进的性能，为医疗公平性提供了实用解决方案。</span><br>
<span id='abs_en'>English: U-RWKV introduces a novel medical image segmentation framework using the RWKV architecture with DARM and SASE modules to efficiently capture long-range dependencies at O(N) computational cost, achieving state-of-the-art performance for equitable healthcare accessibility in resource-limited settings.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1354, <a href='https://arxiv.org/pdf/2507.11372.pdf' target='_blank'>https://arxiv.org/pdf/2507.11372.pdf</a></span>   <span><a href='https://github.com/mantonios107/attrs-fr-embs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierrick Leroy, Antonio Mastropietro, Marco Nurisso, Francesco Vaccarino
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11372">Attributes Shape the Embedding Space of Face Recognition Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Face Recognition (FR) tasks have made significant progress with the advent of Deep Neural Networks, particularly through margin-based triplet losses that embed facial images into high-dimensional feature spaces. During training, these contrastive losses focus exclusively on identity information as labels. However, we observe a multiscale geometric structure emerging in the embedding space, influenced by interpretable facial (e.g., hair color) and image attributes (e.g., contrast). We propose a geometric approach to describe the dependence or invariance of FR models to these attributes and introduce a physics-inspired alignment metric. We evaluate the proposed metric on controlled, simplified models and widely used FR models fine-tuned with synthetic data for targeted attribute augmentation. Our findings reveal that the models exhibit varying degrees of invariance across different attributes, providing insight into their strengths and weaknesses and enabling deeper interpretability. Code available here: https://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs<br>
<span id='abs_ch'>中文: 本 究提出了一种 何方法和物理启发的度量 准，用于分析人脸识别模型对可解释的面部和图像属性的响应，揭示了模型在不同属性上的不变性差异，从而提升了模型的可解释性。</span><br>
<span id='abs_en'>English: This study introduces a geometric approach and a physics-inspired metric to analyze how face recognition models respond to interpretable facial and image attributes, revealing varying levels of invariance across attributes and enhancing model interpretability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1355, <a href='https://arxiv.org/pdf/2507.11316.pdf' target='_blank'>https://arxiv.org/pdf/2507.11316.pdf</a></span>   <span><a href='https://github.com/hr-jin/ConVA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoran Jin, Meng Li, Xiting Wang, Zhihao Xu, Minlie Huang, Yantao Jia, Defu Lian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11316">Internal Value Alignment in Large Language Models through Controlled Value Vector Activation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Aligning Large Language Models (LLMs) with human values has attracted increasing attention since it provides clarity, transparency, and the ability to adapt to evolving scenarios. In this paper, we introduce a Controlled Value Vector Activation (ConVA) method that directly aligns the internal values of LLMs by interpreting how a value is encoded in their latent representations and modifies relevant activations to ensure consistent values in LLMs. To ensure an accurate and unbiased interpretation, we propose a context-controlled value vector identification method. To consistently control values without sacrificing model performance, we introduce a gated value vector activation method for effective and minimum degree of value control. Experiments show that our method achieves the highest control success rate across 10 basic values without hurting LLM performance and fluency, and ensures target values even with opposite and potentially malicious input prompts. Source code and data are available at~ https://github.com/hr-jin/ConVA.<br>
<span id='abs_ch'>中文摘要：本文提出的ConVA方法通过解读和修正大语言模型潜在表征中的价值观编 ，在不影响模型性能的前提下实现了对10种基本价值观的最优控制成功率。</span><br>
<span id='abs_en'>English Summary: The paper introduces the ConVA method, which aligns LLMs with human values by interpreting and modifying latent value representations, achieving high control success without compromising performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1356, <a href='https://arxiv.org/pdf/2507.11229.pdf' target='_blank'>https://arxiv.org/pdf/2507.11229.pdf</a></span>   <span><a href='https://github.com/USTC-DataDarknessLab/DuetGraph.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jin Li, Zezhong Ding, Xike Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11229">DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Knowledge graphs (KGs) are vital for enabling knowledge reasoning across various domains. Recent KG reasoning methods that integrate both global and local information have achieved promising results. However, existing methods often suffer from score over-smoothing, which blurs the distinction between correct and incorrect answers and hinders reasoning effectiveness. To address this, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with dual-pathway global-local fusion. DuetGraph tackles over-smoothing by segregating -- rather than stacking -- the processing of local (via message passing) and global (via attention) information into two distinct pathways, preventing mutual interference and preserving representational discrimination. In addition, DuetGraph introduces a coarse-to-fine optimization, which partitions entities into high- and low-score subsets. This strategy narrows the candidate space and sharpens the score gap between the two subsets, which alleviates over-smoothing and enhances inference quality. Extensive experiments on various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA) performance, with up to an 8.7% improvement in reasoning quality and a 1.8$\times$ acceleration in training efficiency. Our code is available at https://github.com/USTC-DataDarknessLab/DuetGraph.git.<br>
<span id='abs_ch'>中文摘要：DuetGraph是一种新颖的知识图谱推理机制，通过将全局和局部信息处理分离为双路径并采用由粗到细的优化策略，有效解决了分数过度平滑问题，在推理质量和训练效率上均实现了显著提升，达到了最先进的性能水平。</span><br>
<span id='abs_en'>English Summary: DuetGraph is a novel knowledge graph reasoning mechanism that addresses score over-smoothing by separating global and local information processing into dual pathways and implementing a coarse-to-fine optimization strategy, achieving state-of-the-art performance with significant improvements in reasoning quality and training efficiency.</span>Paperid:684,https://arxiv.org/pdf/2507.11137.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1357, <a href='https://arxiv.org/pdf/2507.11129.pdf' target='_blank'>https://arxiv.org/pdf/2507.11129.pdf</a></span>   <span><a href='https://github.com/Neal2020GitHub/MMOne' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhifeng Gu, Bing Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11129">MMOne: Representing Multiple Modalities in One Scene</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Humans perceive the world through multimodal cues to understand and interact with the environment. Learning a scene representation for multiple modalities enhances comprehension of the physical world. However, modality conflicts, arising from inherent distinctions among different modalities, present two critical challenges: property disparity and granularity disparity. To address these challenges, we propose a general framework, MMOne, to represent multiple modalities in one scene, which can be readily extended to additional modalities. Specifically, a modality modeling module with a novel modality indicator is proposed to capture the unique properties of each modality. Additionally, we design a multimodal decomposition mechanism to separate multi-modal Gaussians into single-modal Gaussians based on modality differences. We address the essential distinctions among modalities by disentangling multimodal information into shared and modality-specific components, resulting in a more compact and efficient multimodal scene representation. Extensive experiments demonstrate that our method consistently enhances the representation capability for each modality and is scalable to additional modalities. The code is available at https://github.com/Neal2020GitHub/MMOne.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1358, <a href='https://arxiv.org/pdf/2507.11096.pdf' target='_blank'>https://arxiv.org/pdf/2507.11096.pdf</a></span>   <span><a href='https://github.com/billsioros/EditGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vassilis Sioros, Alexandros Potamianos, Giorgos Paraskevopoulos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11096">EditGen: Harnessing Cross-Attention Control for Instruction-Based Auto-Regressive Audio Editing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In this study, we investigate leveraging cross-attention control for efficient audio editing within auto-regressive models. Inspired by image editing methodologies, we develop a Prompt-to-Prompt-like approach that guides edits through cross and self-attention mechanisms. Integrating a diffusion-based strategy, influenced by Auffusion, we extend the model's functionality to support refinement edits, establishing a baseline for prompt-guided audio editing. Additionally, we introduce an alternative approach by incorporating MUSICGEN, a pre-trained frozen auto-regressive model, and propose three editing mechanisms, based on Replacement, Reweighting, and Refinement of the attention scores. We employ commonly-used music-specific evaluation metrics and a human study, to gauge time-varying controllability, adherence to global text cues, and overall audio realism. The automatic and human evaluations indicate that the proposed combination of prompt-to-prompt guidance with autoregressive generation models significantly outperforms the diffusion-based baseline in terms of melody, dynamics, and tempo of the generated audio. Our code is available at https://github.com/billsioros/EditGen<br>
<span id='abs_ch'>本 究提出了一种基于提示到提示的自回归模型音频编辑方法，通过结合交叉注意力控制与MUSICGEN模型，在旋律、动态和节奏方面显著优于基于扩散的基准方法。</span><br>
<span id='abs_en'>This study introduces a prompt-to-prompt approach for audio editing in auto-regressive models, combining cross-attention control with MUSICGEN to outperform diffusion-based methods in melody, dynamics, and tempo.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1359, <a href='https://arxiv.org/pdf/2507.11017.pdf' target='_blank'>https://arxiv.org/pdf/2507.11017.pdf</a></span>   <span><a href='https://github.com/Xingyu-Zheng/FOEM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyu Zheng, Haotong Qin, Yuye Li, Jiakai Wang, Jinyang Guo, Michele Magno, Xianglong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11017">First-Order Error Matters: Accurate Compensation for Quantized Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Post-training quantization (PTQ) offers an efficient approach to compressing large language models (LLMs), significantly reducing memory access and computational costs. Existing compensation-based weight calibration methods often rely on a second-order Taylor expansion to model quantization error, under the assumption that the first-order term is negligible in well-trained full-precision models. However, we reveal that the progressive compensation process introduces accumulated first-order deviations between latent weights and their full-precision counterparts, making this assumption fundamentally flawed. To address this, we propose FOEM, a novel PTQ method that explicitly incorporates first-order gradient terms to improve quantization error compensation. FOEM approximates gradients by directly computing the difference between latent and full-precision weights, avoiding the high cost and limited generalization of backpropagation-based gradient computation. This approach introduces minimal additional computational overhead. Moreover, FOEM leverages precomputed Cholesky factors to efficiently recover the inverse of Hessian submatrices in real time. Extensive experiments across a wide range of models and benchmarks demonstrate that FOEM consistently outperforms the classical GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of Llama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from 51.7% to 74.9%, approaching the full-precision performance of 78.6%. Furthermore, FOEM can be seamlessly integrated with advanced techniques such as GPTAQ and SpinQuant, yielding additional improvements under the challenging W4A4KV4 setting, and further narrowing the accuracy gap with full-precision baselines beyond what current state-of-the-art methods achieve. The code is available at https://github.com/Xingyu-Zheng/FOEM.<br>
<span id='abs_ch'>中文摘要：FOEM提出了一种新颖的训练后量化方法，通过引入一阶梯度项解决权重 准中的累积偏差问题，以极低计算开销显著提升模型性能。</span><br>
<span id='abs_en'>English Summary: FOEM introduces a novel post-training quantization method that incorporates first-order gradient terms to address accumulated deviations in weight calibration, significantly improving model performance with minimal computational overhead.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1360, <a href='https://arxiv.org/pdf/2507.10999.pdf' target='_blank'>https://arxiv.org/pdf/2507.10999.pdf</a></span>   <span><a href='https://github.com/henry-pay/SpaRTAN]' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Quan Bi Pay, Vishnu Monn Baskaran, Junn Yong Loo, KokSheik Wong, Simon See
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10999">SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The resurgence of convolutional neural networks (CNNs) in visual recognition tasks, exemplified by ConvNeXt, has demonstrated their capability to rival transformer-based architectures through advanced training methodologies and ViT-inspired design principles. However, both CNNs and transformers exhibit a simplicity bias, favoring straightforward features over complex structural representations. Furthermore, modern CNNs often integrate MLP-like blocks akin to those in transformers, but these blocks suffer from significant information redundancies, necessitating high expansion ratios to sustain competitive performance. To address these limitations, we propose SpaRTAN, a lightweight architectural design that enhances spatial and channel-wise information processing. SpaRTAN employs kernels with varying receptive fields, controlled by kernel size and dilation factor, to capture discriminative multi-order spatial features effectively. A wave-based channel aggregation module further modulates and reinforces pixel interactions, mitigating channel-wise redundancies. Combining the two modules, the proposed network can efficiently gather and dynamically contextualize discriminative features. Experimental results in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable parameter efficiency while maintaining competitive performance. In particular, on the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M parameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver strong performance through an efficient design. On the COCO benchmark, it achieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M parameters. The code is publicly available at [https://github.com/henry-pay/SpaRTAN].<br>
<span id='abs_ch'>中文: SpaRTAN是一种轻量级CNN架构，通过多尺度卷积 和基于波的通道聚合模块优化空间与通道信息处理，在ImageNet和COCO基准测试中以少量参数实现了卓越的性能表现。</span><br>
<span id='abs_en'>English: SpaRTAN is a lightweight CNN architecture that enhances spatial and channel-wise feature processing through multi-scale kernels and wave-based aggregation, achieving competitive performance with high parameter efficiency on ImageNet and COCO benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1361, <a href='https://arxiv.org/pdf/2507.10998.pdf' target='_blank'>https://arxiv.org/pdf/2507.10998.pdf</a></span>   <span><a href='https://github.com/ZhipengHe/VAE-TabAttack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhipeng He, Alexander Stevens, Chun Ouyang, Johannes De Smedt, Alistair Barros, Catarina Moreira
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10998">Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Adversarial attacks on tabular data present fundamental challenges distinct from image or text domains due to the heterogeneous nature of mixed categorical and numerical features. Unlike images where pixel perturbations maintain visual similarity, tabular data lacks intuitive similarity metrics, making it difficult to define imperceptible modifications. Additionally, traditional gradient-based methods prioritise $\ell_p$-norm constraints, often producing adversarial examples that deviate from the original data distributions, making them detectable. We propose a latent space perturbation framework using a mixed-input Variational Autoencoder (VAE) to generate imperceptible adversarial examples. The proposed VAE integrates categorical embeddings and numerical features into a unified latent manifold, enabling perturbations that preserve statistical consistency. We specify In-Distribution Success Rate (IDSR) to measure the proportion of adversarial examples that remain statistically indistinguishable from the input distribution. Evaluation across six publicly available datasets and three model architectures demonstrates that our method achieves substantially lower outlier rates and more consistent performance compared to traditional input-space attacks and other VAE-based methods adapted from image domain approaches. Our comprehensive analysis includes hyperparameter sensitivity, sparsity control mechanisms, and generative architectural comparisons, revealing that VAE-based attacks depend critically on reconstruction quality but offer superior practical utility when sufficient training data is available. This work highlights the importance of on-manifold perturbations for realistic adversarial attacks on tabular data, offering a robust approach for practical deployment. The source code can be accessed through https://github.com/ZhipengHe/VAE-TabAttack.<br>
<span id='abs_ch'>中文: 本文提出了一种基于混合输入变分自编 器的潜在空间扰动框架，可为表 数据生成难以察觉的对抗 本，相比 统方法具有更优的统计一致性和更低的异常率。</span><br>
<span id='abs_en'>English: This paper introduces a latent space perturbation framework using a mixed-input Variational Autoencoder to generate imperceptible adversarial examples for tabular data, achieving superior statistical consistency and lower outlier rates compared to traditional methods.Paperid:698,https://arxiv.org/pdf/2507.10990.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1362, <a href='https://arxiv.org/pdf/2507.10990.pdf' target='_blank'>https://arxiv.org/pdf/2507.10990.pdf</a></span>   <span><a href='https://github.com/rodlaf/ClusterEnv' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rodney Lafuente-Mercado
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10990">High-Throughput Distributed Reinforcement Learning via Adaptive Policy Synchronization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Scaling reinforcement learning (RL) workloads often requires distributing environment simulation across compute clusters. Existing frameworks entangle simulation, learning logic, and orchestration into monolithic systems, limiting modularity and reusability. We present ClusterEnv, a lightweight, learner-agnostic interface for distributed environment execution that mirrors the Gymnasium API. ClusterEnv introduces the DETACH pattern, which decouples simulation from training by offloading reset() and step() operations to remote workers while keeping learning centralized. To address policy staleness in distributed execution, we propose Adaptive Actor Policy Synchronization (AAPS), a divergence-triggered update mechanism that reduces synchronization overhead without sacrificing performance. ClusterEnv integrates cleanly into existing RL pipelines, supports both on-policy and off-policy methods, and requires minimal code changes. Experiments on discrete control tasks demonstrate that AAPS achieves high sample efficiency with significantly fewer weight updates. Source code is available at https://github.com/rodlaf/ClusterEnv.<br>
<span id='abs_ch'>中文：ClusterEnv是一个轻量级、与学 器 关的分布式强化学 接口，采用DETACH模式将模拟与训练解耦，并通过自适应策略同步机制（AAPS）减少同步开销，从而提升效率。</span><br>
<span id='abs_en'>English: ClusterEnv is a lightweight, learner-agnostic interface for distributed reinforcement learning that decouples simulation from training using the DETACH pattern and enhances efficiency with Adaptive Actor Policy Synchronization (AAPS) to minimize synchronization overhead.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1363, <a href='https://arxiv.org/pdf/2507.10977.pdf' target='_blank'>https://arxiv.org/pdf/2507.10977.pdf</a></span>   <span><a href='https://github.com/henry-pay/RayEncoder]' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Quan Bi Pay, Vishnu Monn Baskaran, Junn Yong Loo, KokSheik Wong, Simon See
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10977">Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Human-object interaction (HOI) detection is essential for accurately localizing and characterizing interactions between humans and objects, providing a comprehensive understanding of complex visual scenes across various domains. However, existing HOI detectors often struggle to deliver reliable predictions efficiently, relying on resource-intensive training methods and inefficient architectures. To address these challenges, we conceptualize a wavelet attention-like backbone and a novel ray-based encoder architecture tailored for HOI detection. Our wavelet backbone addresses the limitations of expressing middle-order interactions by aggregating discriminative features from the low- and high-order interactions extracted from diverse convolutional filters. Concurrently, the ray-based encoder facilitates multi-scale attention by optimizing the focus of the decoder on relevant regions of interest and mitigating computational overhead. As a result of harnessing the attenuated intensity of learnable ray origins, our decoder aligns query embeddings with emphasized regions of interest for accurate predictions. Experimental results on benchmark datasets, including ImageNet and HICO-DET, showcase the potential of our proposed architecture. The code is publicly available at [https://github.com/henry-pay/RayEncoder].<br>
<span id='abs_ch'>Chinese: 本 究提出了一种小波类注意力主干和基于射线的编 器，通过高效聚合多尺度特征并优化计算焦点，提升了人-物交互检测的性能，在基准数据集上取得了良好效果。</span><br>
<span id='abs_en'>English: The study introduces a wavelet attention-like backbone and a ray-based encoder to enhance human-object interaction detection by efficiently aggregating multi-scale features and optimizing computational focus, achieving promising results on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1364, <a href='https://arxiv.org/pdf/2507.10843.pdf' target='_blank'>https://arxiv.org/pdf/2507.10843.pdf</a></span>   <span><a href='https://github.com/motokiomura/Q-DOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Motoki Omura, Yusuke Mukuta, Kazuki Ota, Takayuki Osa, Tatsuya Harada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10843">Offline Reinforcement Learning with Wasserstein Regularization via Optimal Transport Maps</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Offline reinforcement learning (RL) aims to learn an optimal policy from a static dataset, making it particularly valuable in scenarios where data collection is costly, such as robotics. A major challenge in offline RL is distributional shift, where the learned policy deviates from the dataset distribution, potentially leading to unreliable out-of-distribution actions. To mitigate this issue, regularization techniques have been employed. While many existing methods utilize density ratio-based measures, such as the $f$-divergence, for regularization, we propose an approach that utilizes the Wasserstein distance, which is robust to out-of-distribution data and captures the similarity between actions. Our method employs input-convex neural networks (ICNNs) to model optimal transport maps, enabling the computation of the Wasserstein distance in a discriminator-free manner, thereby avoiding adversarial training and ensuring stable learning. Our approach demonstrates comparable or superior performance to widely used existing methods on the D4RL benchmark dataset. The code is available at https://github.com/motokiomura/Q-DOT .<br>
<span id='abs_ch'>Chinese: 离线强化学 通过引入瓦瑟斯坦距离进行正则化，利用输入凸神经网络 需对抗训练即可计算该距离，在D4RL基准测试中表现优异。</span><br>
<span id='abs_en'>English: Offline reinforcement learning tackles distributional shift by using the Wasserstein distance for regularization, employing input-convex neural networks to compute it without adversarial training, achieving strong results on the D4RL benchmark.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1365, <a href='https://arxiv.org/pdf/2507.10778.pdf' target='_blank'>https://arxiv.org/pdf/2507.10778.pdf</a></span>   <span><a href='https://github.com/hsiangwei0903/SpatialAgent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hsiang-Wei Huang, Jen-Hao Cheng, Kuang-Ming Chen, Cheng-Yen Yang, Bahaa Alattar, Yi-Ru Lin, Pyongkun Kim, Sangwon Kim, Kwangju Kim, Chung-I Huang, Jenq-Neng Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10778">Warehouse Spatial Question Answering with LLM Agent</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spatial understanding has been a challenging task for existing Multi-modal Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM finetuning to enhance MLLM's spatial understanding ability. In this paper, we present a data-efficient approach. We propose a LLM agent system with strong and advanced spatial reasoning ability, which can be used to solve the challenging spatial question answering task in complex indoor warehouse scenarios. Our system integrates multiple tools that allow the LLM agent to conduct spatial reasoning and API tools interaction to answer the given complicated spatial question. Extensive evaluations on the 2025 AI City Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that our system achieves high accuracy and efficiency in tasks such as object retrieval, counting, and distance estimation. The code is available at: https://github.com/hsiangwei0903/SpatialAgent<br>
<span id='abs_ch'>Chinese: 本文提出了一种数据高效的LLM智能体系统，具备先进的空间推理能力，通过整合多种工具在复杂仓库场景中精确解决空间问答任务，并在AI City Challenge数据集上展现出优异性能。</span><br>
<span id='abs_en'>English: This paper introduces a data-efficient LLM agent system with advanced spatial reasoning capabilities, utilizing multiple tools to accurately solve complex spatial tasks in warehouse environments, as demonstrated by high performance on the AI City Challenge dataset.Paperid:710,https://arxiv.org/pdf/2507.10775.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1366, <a href='https://arxiv.org/pdf/2507.10775.pdf' target='_blank'>https://arxiv.org/pdf/2507.10775.pdf</a></span>   <span><a href='https://github.com/RiceD2KLab/SWiM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeffrey Joan Sam, Janhavi Sathe, Nikhil Chigali, Naman Gupta, Radhey Ruparel, Yicheng Jiang, Janmajay Singh, James W. Berck, Arko Barman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10775">A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spacecraft deployed in outer space are routinely subjected to various forms of damage due to exposure to hazardous environments. In addition, there are significant risks to the subsequent process of in-space repairs through human extravehicular activity or robotic manipulation, incurring substantial operational costs. Recent developments in image segmentation could enable the development of reliable and cost-effective autonomous inspection systems. While these models often require large amounts of training data to achieve satisfactory results, publicly available annotated spacecraft segmentation data are very scarce. Here, we present a new dataset of nearly 64k annotated spacecraft images that was created using real spacecraft models, superimposed on a mixture of real and synthetic backgrounds generated using NASA's TTALOS pipeline. To mimic camera distortions and noise in real-world image acquisition, we also added different types of noise and distortion to the images. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to generate performance benchmarks for the dataset under well-defined hardware and inference time constraints to mimic real-world image segmentation challenges for real-time onboard applications in space on NASA's inspector spacecraft. The resulting models, when tested under these constraints, achieved a Dice score of 0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second. The dataset and models for performance benchmark are available at https://github.com/RiceD2KLab/SWiM.<br>
<span id='abs_ch'>Chinese: 为解决航天器自主检测系统训练数据匮乏的问题，开发了一个包含近6.4万  注图像的新数据集，经过优化的YOLO模型在模拟太空环境下实现了高精度和实时性能。</span><br>
<span id='abs_en'>English: A new dataset of nearly 64,000 annotated spacecraft images has been developed to address the scarcity of training data for autonomous inspection systems, with fine-tuned YOLO models achieving high accuracy and real-time performance under simulated space conditions.Paperid:711,https://arxiv.org/pdf/2507.10757.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1367, <a href='https://arxiv.org/pdf/2507.10628.pdf' target='_blank'>https://arxiv.org/pdf/2507.10628.pdf</a></span>   <span><a href='https://github.com/hkgc-1/GHPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziru Liu, Cheng Gong, Xinyu Fu, Yaofang Liu, Ran Chen, Shoubo Hu, Suiyun Zhang, Rui Liu, Qingfu Zhang, Dandan Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10628">GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a powerful paradigm for facilitating the self-improvement of large language models (LLMs), particularly in the domain of complex reasoning tasks. However, prevailing on-policy RL methods often contend with significant training instability and inefficiency. This is primarily due to a capacity-difficulty mismatch, where the complexity of training data frequently outpaces the model's current capabilities, leading to critically sparse reward signals and stalled learning progress. This challenge is particularly acute for smaller, more resource-efficient LLMs. To overcome this, we introduce the Guided Hybrid Policy Optimization (GHPO), a novel difficulty-aware reinforcement learning framework. GHPO dynamically calibrates task difficulty by employing adaptive prompt refinement to provide targeted guidance. This unique approach adaptively balances direct imitation learning for problems currently beyond the model's reach with exploration-based reinforcement learning for more manageable tasks, effectively creating a smooth and optimized learning curriculum. Extensive experiments demonstrate that GHPO achieves an average performance gain of approximately 5% across six challenging mathematics benchmarks, consistently outperforming strong on-policy reinforcement learning and curriculum learning baselines. Further analysis confirms that our framework significantly enhances both training stability and final reasoning performance, thus offering a scalable and efficient solution for developing powerful and robust reasoning models.<br>
<span id='abs_ch'>中文摘要：引导式混合策略优化（GHPO）框架通过自适应提示调整动态匹配任务难度，有效解决了语言模型强化学 中的训练不稳定问题，在数学推理基准测试中实现了约5%的性能提升。</span><br>
<span id='abs_en'>English Summary: The Guided Hybrid Policy Optimization (GHPO) framework addresses training instability in reinforcement learning for language models by dynamically adjusting task difficulty through adaptive prompt refinement, achieving significant performance gains across mathematical reasoning benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1368, <a href='https://arxiv.org/pdf/2507.10593.pdf' target='_blank'>https://arxiv.org/pdf/2507.10593.pdf</a></span>   <span><a href='https://github.com/Oaklight/ToolRegistry,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peng Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10593">ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Model (LLM) applications are increasingly relying on external tools to extend their capabilities beyond text generation. However, current tool integration approaches suffer from fragmentation, protocol limitations, and implementation complexity, leading to substantial development overhead. This paper presents Toolregistry, a protocol-agnostic tool management library that simplifies tool registration, representation, execution, and lifecycle management via a unified interface. Our evaluation demonstrates that \toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x performance improvements through concurrent execution, and 100% compatibility with OpenAI function calling standards. Real-world case studies show significant improvements in development efficiency and code maintainability across diverse integration scenarios. \toolregistry is open-source and available at https://github.com/Oaklight/ToolRegistry, with comprehensive documentation at https://toolregistry.readthedocs.io/.<br>
<span id='abs_ch'>中文: Toolregistry作为协议 关的工具管理库，通过统一接口简化了LLM工具集成，减少60-80%代 量并提升性能，同时完全兼容OpenAI 准。</span><br>
<span id='abs_en'>English: Toolregistry is a protocol-agnostic library that simplifies tool integration for LLMs, reducing code by 60-80% while improving performance and maintaining full OpenAI compatibility.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1369, <a href='https://arxiv.org/pdf/2507.10546.pdf' target='_blank'>https://arxiv.org/pdf/2507.10546.pdf</a></span>   <span><a href='https://github.com/kittykg/disentangling-ndnf-classification' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kexin Gu Baugh, Vincent Perreault, Matthew Baugh, Luke Dickens, Katsumi Inoue, Alessandra Russo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10546">Disentangling Neural Disjunctive Normal Form Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Neural Disjunctive Normal Form (DNF) based models are powerful and interpretable approaches to neuro-symbolic learning and have shown promising results in classification and reinforcement learning settings without prior knowledge of the tasks. However, their performance is degraded by the thresholding of the post-training symbolic translation process. We show here that part of the performance degradation during translation is due to its failure to disentangle the learned knowledge represented in the form of the networks' weights. We address this issue by proposing a new disentanglement method; by splitting nodes that encode nested rules into smaller independent nodes, we are able to better preserve the models' performance. Through experiments on binary, multiclass, and multilabel classification tasks (including those requiring predicate invention), we demonstrate that our disentanglement method provides compact and interpretable logical representations for the neural DNF-based models, with performance closer to that of their pre-translation counterparts. Our code is available at https://github.com/kittykg/disentangling-ndnf-classification.<br>
<span id='abs_ch'>Chinese: 本 究提出了一种解 方法，通过拆分编 嵌套规则的节点来减轻神经DNF模型在符号翻译过程中的性能损失，从而在分类任务中获得更紧凑、可解释的逻辑表示和更高的准确性。</span><br>
<span id='abs_en'>English: The study introduces a disentanglement method that splits nodes encoding nested rules in neural DNF models to mitigate performance loss during symbolic translation, resulting in more compact and interpretable logical representations with improved accuracy across classification tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1370, <a href='https://arxiv.org/pdf/2507.10534.pdf' target='_blank'>https://arxiv.org/pdf/2507.10534.pdf</a></span>   <span><a href='https://github.com/IsaacYQH/WildFX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qihui Yang, Taylor Berg-Kirkpatrick, Julian McAuley, Zachary Novack
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10534">WildFX: A DAW-Powered Pipeline for In-the-Wild Audio FX Graph Modeling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite rapid progress in end-to-end AI music generation, AI-driven modeling of professional Digital Signal Processing (DSP) workflows remains challenging. In particular, while there is growing interest in neural black-box modeling of audio effect graphs (e.g. reverb, compression, equalization), AI-based approaches struggle to replicate the nuanced signal flow and parameter interactions used in professional workflows. Existing differentiable plugin approaches often diverge from real-world tools, exhibiting inferior performance relative to simplified neural controllers under equivalent computational constraints. We introduce WildFX, a pipeline containerized with Docker for generating multi-track audio mixing datasets with rich effect graphs, powered by a professional Digital Audio Workstation (DAW) backend. WildFX supports seamless integration of cross-platform commercial plugins or any plugins in the wild, in VST/VST3/LV2/CLAP formats, enabling structural complexity (e.g., sidechains, crossovers) and achieving efficient parallelized processing. A minimalist metadata interface simplifies project/plugin configuration. Experiments demonstrate the pipeline's validity through blind estimation of mixing graphs, plugin/gain parameters, and its ability to bridge AI research with practical DSP demands. The code is available on: https://github.com/IsaacYQH/WildFX.<br>
<span id='abs_ch'>Chinese Summary: WildFX推出基于Docker容器化的多轨音频混音数据集生成管道，通过 缝集成商业插件和高效并行处理，使AI 究能更好地模拟专业数字信号处理工作流程。</span><br>
<span id='abs_en'>English Summary: WildFX introduces a Docker-containerized pipeline for generating multi-track audio mixing datasets with complex effect graphs, enabling AI research to better replicate professional DSP workflows through seamless plugin integration and efficient processing.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1371, <a href='https://arxiv.org/pdf/2507.10522.pdf' target='_blank'>https://arxiv.org/pdf/2507.10522.pdf</a></span>   <span><a href='https://github.com/sciknoworg/deep-research' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jennifer D'Souza, Endres Keno Sander, Andrei Aioanei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10522">DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system for automated scientific synthesis that supports recursive, depth- and breadth-controlled exploration of original research questions -- enhancing search diversity and nuance in the retrieval of relevant scientific literature. Unlike conventional retrieval-augmented generation pipelines, DeepResearch enables user-controllable synthesis with transparent reasoning and parameter-driven configurability, facilitating high-throughput integration of domain-specific evidence while maintaining analytical rigor. Applied to 49 ecological research questions, DeepResearch achieves up to a 21-fold increase in source integration and a 14.9-fold rise in sources integrated per 1,000 words. High-parameter settings yield expert-level analytical depth and contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.<br>
<span id='abs_ch'>中文: DeepResearch$^{\text{Eco}}$ 是一种创新的基于大语言模型的系统，通过用户可控的科学合成实现了增强的搜索多 性和分析严谨性，在生态 究应用中显著提升了文献整合效率并达到专家级分析深度。</span><br>
<span id='abs_en'>English: DeepResearch$^{\text{Eco}}$ is an innovative LLM-based system that enables automated, user-controllable scientific synthesis with enhanced search diversity and analytical rigor, achieving significant improvements in source integration and expert-level depth in ecological research applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1372, <a href='https://arxiv.org/pdf/2507.10492.pdf' target='_blank'>https://arxiv.org/pdf/2507.10492.pdf</a></span>   <span><a href='https://github.com/DopamineLcy/BenchReAD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyu Lian, Hong-Yu Zhou, Zhanli Hu, Jing Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10492">BenchReAD: A systematic benchmark for retinal anomaly detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retinal anomaly detection plays a pivotal role in screening ocular and systemic diseases. Despite its significance, progress in the field has been hindered by the absence of a comprehensive and publicly available benchmark, which is essential for the fair evaluation and advancement of methodologies. Due to this limitation, previous anomaly detection work related to retinal images has been constrained by (1) a limited and overly simplistic set of anomaly types, (2) test sets that are nearly saturated, and (3) a lack of generalization evaluation, resulting in less convincing experimental setups. Furthermore, existing benchmarks in medical anomaly detection predominantly focus on one-class supervised approaches (training only with negative samples), overlooking the vast amounts of labeled abnormal data and unlabeled data that are commonly available in clinical practice. To bridge these gaps, we introduce a benchmark for retinal anomaly detection, which is comprehensive and systematic in terms of data and algorithm. Through categorizing and benchmarking previous methods, we find that a fully supervised approach leveraging disentangled representations of abnormalities (DRA) achieves the best performance but suffers from significant drops in performance when encountering certain unseen anomalies. Inspired by the memory bank mechanisms in one-class supervised learning, we propose NFM-DRA, which integrates DRA with a Normal Feature Memory to mitigate the performance degradation, establishing a new SOTA. The benchmark is publicly available at https://github.com/DopamineLcy/BenchReAD.<br>
<span id='abs_ch'>Chinese: 该摘要针对视网膜异常检测领域缺乏全面基准的问题，提出了一个系统性基准，并开发了NFM-DRA新方法，通过结合异常解耦表示与正常特征记忆机制，有效提升了检测性能并建立了新的技术 杆。</span><br>
<span id='abs_en'>English: This abstract introduces a comprehensive benchmark for retinal anomaly detection to address the limitations of previous methods, proposing a novel approach called NFM-DRA that integrates disentangled representations of abnormalities with a normal feature memory to achieve state-of-the-art performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1373, <a href='https://arxiv.org/pdf/2507.10475.pdf' target='_blank'>https://arxiv.org/pdf/2507.10475.pdf</a></span>   <span><a href='https://github.com/ismailtrm/ceng_404' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ä°smail TarÄ±m, AytuÄ Onan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10475">Can You Detect the Difference?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of large language models (LLMs) has raised concerns about reliably detecting AI-generated text. Stylometric metrics work well on autoregressive (AR) outputs, but their effectiveness on diffusion-based models is unknown. We present the first systematic comparison of diffusion-generated text (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity, burstiness, lexical diversity, readability, and BLEU/ROUGE scores show that LLaDA closely mimics human text in perplexity and burstiness, yielding high false-negative rates for AR-oriented detectors. LLaMA shows much lower perplexity but reduced lexical fidelity. Relying on any single metric fails to separate diffusion outputs from human writing. We highlight the need for diffusion-aware detectors and outline directions such as hybrid models, diffusion-specific stylometric signatures, and robust watermarking.<br>
<span id='abs_ch'>中文: 大语言模型的扩散生成文本（如LLaDA）在困惑度和突发性上高度模仿人类写作，能规避基于自回归的检测方法，亟需开发扩散感知检测器和混合策略。</span><br>
<span id='abs_en'>English: Large language models' diffusion-generated text like LLaDA closely mimics human writing in perplexity and burstiness, evading detection by autoregressive-focused methods, necessitating the development of diffusion-aware detectors and hybrid approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1374, <a href='https://arxiv.org/pdf/2507.10330.pdf' target='_blank'>https://arxiv.org/pdf/2507.10330.pdf</a></span>   <span><a href='https://github.com/BouriMohammed/GBM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammed Bouri, Adnane Saoud
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10330">Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite advancements in Natural Language Processing (NLP), models remain vulnerable to adversarial attacks, such as synonym substitutions. While prior work has focused on improving robustness for feed-forward and convolutional architectures, the robustness of recurrent networks and modern state space models (SSMs), such as S4, remains understudied. These architectures pose unique challenges due to their sequential processing and complex parameter dynamics. In this paper, we introduce a novel regularization technique based on Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the impact of input perturbations on model outputs. We focus on computing the GBM for three architectures: Long Short-Term Memory (LSTM), State Space models (S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance resilience against word substitution attacks, (2) improve generalization on clean text, and (3) providing the first systematic analysis of SSM (S4) robustness. Extensive experiments across multiple architectures and benchmark datasets demonstrate that our method improves adversarial robustness by up to 8.8% over existing baselines. These results highlight the effectiveness of our approach, outperforming several state-of-the-art methods in adversarial defense. Codes are available at https://github.com/BouriMohammed/GBM<br>
<span id='abs_ch'>中文: 本文提出了一种基于增长边界矩阵的新型正则化方法，旨在增强自然语言处理模型对抗攻击的鲁棒性，在LSTM、S4和CNN等多种架构上实现了高达8.8%的防御性能提升。</span><br>
<span id='abs_en'>English: This paper introduces a novel regularization method using Growth Bound Matrices to enhance NLP model robustness against adversarial attacks, achieving up to 8.8% improvement in resilience across multiple architectures including LSTM, S4, and CNN.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1375, <a href='https://arxiv.org/pdf/2507.10223.pdf' target='_blank'>https://arxiv.org/pdf/2507.10223.pdf</a></span>   <span><a href='https://github.com/pittisl/ProGait' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyu Yin, Boyuan Yang, Weichen Liu, Qiyao Xue, Abrar Alamri, Goeran Fiedler, Wei Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10223">ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Prosthetic legs play a pivotal role in clinical rehabilitation, allowing individuals with lower-limb amputations the ability to regain mobility and improve their quality of life. Gait analysis is fundamental for optimizing prosthesis design and alignment, directly impacting the mobility and life quality of individuals with lower-limb amputations. Vision-based machine learning (ML) methods offer a scalable and non-invasive solution to gait analysis, but face challenges in correctly detecting and analyzing prosthesis, due to their unique appearances and new movement patterns. In this paper, we aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait, to support multiple vision tasks including Video Object Segmentation, 2D Human Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from four above-knee amputees when testing multiple newly-fitted prosthetic legs through walking trials, and depicts the presence, contours, poses, and gait patterns of human subjects with transfemoral prosthetic legs. Alongside the dataset itself, we also present benchmark tasks and fine-tuned baseline models to illustrate the practical application and performance of the ProGait dataset. We compared our baseline models against pre-trained vision models, demonstrating improved generalizability when applying the ProGait dataset for prosthesis-specific tasks. Our code is available at https://github.com/pittisl/ProGait and dataset at https://huggingface.co/datasets/ericyxy98/ProGait.<br>
<span id='abs_ch'>中文: 本文提出ProGait数据集，旨在解决基于视觉的假肢步态分析难题，通过提供视频数据和基准测试来提升假肢特定运动的检测与分析能力。</span><br>
<span id='abs_en'>English: This paper introduces the ProGait dataset to address challenges in vision-based gait analysis for prosthetic legs, providing video data and benchmarks to improve detection and analysis of prosthesis-specific movements.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1376, <a href='https://arxiv.org/pdf/2507.10202.pdf' target='_blank'>https://arxiv.org/pdf/2507.10202.pdf</a></span>   <span><a href='https://github.com/yenncye/ECP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaeseong Lee, Yeeun Choi, Heechan Choi, Hanjung Kim, Seonjoo Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10202">A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in vision-language understanding, reasoning, and generation. However, they struggle with tasks requiring fine-grained localization and reasoning in high-resolution images. This constraint stems from the fact that MLLMs are fine-tuned with fixed image resolution to align with the pre-trained image encoder used in MLLM. Consequently, feeding high-resolution images directly into MLLMs leads to poor generalization due to a train-test resolution discrepancy, while downsampling these images-although ensuring consistency-compromises fine-grained visual details and ultimately degrades performance. To address this challenge, we propose Extract Candidate then Predict (ECP), a novel training-free, task-agnostic two-stage framework designed to enhance MLLM performance on high-resolution images. The key intuition behind ECP is that while MLLMs struggle with high-resolution images, their predictions on downsampled images still contain implicit localization cues. By first identifying candidate region using the coarse prediction and then predicting the final output based on candidate region, ECP effectively preserves fine-grained details while mitigating the challenges posed by high-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K MLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared to baseline respectively, demonstrating its effectiveness. Code is available at https://github.com/yenncye/ECP.<br>
<span id='abs_ch'>中文: 提出的ECP框架通过利用粗略预测识别候选区域， 需额外训练即可增强多模态大语言模型在高分辨率图像上的性能，有效保留细节信息。</span><br>
<span id='abs_en'>English: The proposed Extract Candidate then Predict (ECP) framework enhances Multimodal Large Language Models' performance on high-resolution images by leveraging coarse predictions to identify candidate regions, preserving fine-grained details without requiring additional training.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1377, <a href='https://arxiv.org/pdf/2507.10136.pdf' target='_blank'>https://arxiv.org/pdf/2507.10136.pdf</a></span>   <span><a href='https://github.com/Liu-Zhonglin/pbn-melanoma-project' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhonglin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10136">A PBN-RL-XAI Framework for Discovering a "Hit-and-Run" Therapeutic Strategy in Melanoma</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Innate resistance to anti-PD-1 immunotherapy remains a major clinical challenge in metastatic melanoma, with the underlying molecular networks being poorly understood. To address this, we constructed a dynamic Probabilistic Boolean Network model using transcriptomic data from patient tumor biopsies to elucidate the regulatory logic governing therapy response. We then employed a reinforcement learning agent to systematically discover optimal, multi-step therapeutic interventions and used explainable artificial intelligence to mechanistically interpret the agent's control policy. The analysis revealed that a precisely timed, 4-step temporary inhibition of the lysyl oxidase like 2 protein (LOXL2) was the most effective strategy. Our explainable analysis showed that this ''hit-and-run" intervention is sufficient to erase the molecular signature driving resistance, allowing the network to self-correct without requiring sustained intervention. This study presents a novel, time-dependent therapeutic hypothesis for overcoming immunotherapy resistance and provides a powerful computational framework for identifying non-obvious intervention protocols in complex biological systems.<br>
<span id='abs_ch'>Chinese: 本 究利用患者数据构建计算模型，通过强化学 发现精确时机的四步LOXL2蛋白抑制是克服转移性黑色 瘤抗PD-1免疫治疗耐药的最有效策略，该临时干预能消除耐药分子特征并使系统自我修复。</span><br>
<span id='abs_en'>English: This study developed a computational model using patient data and reinforcement learning to identify a precisely timed, four-step inhibition of LOXL2 as the most effective strategy to overcome anti-PD-1 resistance in metastatic melanoma, demonstrating that this temporary intervention erases resistance mechanisms and allows the system to self-correct.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1378, <a href='https://arxiv.org/pdf/2507.09985.pdf' target='_blank'>https://arxiv.org/pdf/2507.09985.pdf</a></span>   <span><a href='https://github.com/clear-nus/octopi-1.5' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Samson Yu, Kelvin Lin, Harold Soh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09985">Demonstrating the Octopi-1.5 Visual-Tactile-Language Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Touch is recognized as a vital sense for humans and an equally important modality for robots, especially for dexterous manipulation, material identification, and scenarios involving visual occlusion. Building upon very recent work in touch foundation models, this demonstration will feature Octopi-1.5, our latest visual-tactile-language model. Compared to its predecessor, Octopi-1.5 introduces the ability to process tactile signals from multiple object parts and employs a simple retrieval-augmented generation (RAG) module to improve performance on tasks and potentially learn new objects on-the-fly. The system can be experienced live through a new handheld tactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile sensors. This convenient and accessible setup allows users to interact with Octopi-1.5 without requiring a robot. During the demonstration, we will showcase Octopi-1.5 solving tactile inference tasks by leveraging tactile inputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5 will identify objects being grasped and respond to follow-up queries about how to handle it (e.g., recommending careful handling for soft fruits). We also plan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items. With live interactions, this demonstration aims to highlight both the progress and limitations of VTLMs such as Octopi-1.5 and to foster further interest in this exciting field. Code for Octopi-1.5 and design files for the TMI gripper are available at https://github.com/clear-nus/octopi-1.5.<br>
<span id='abs_ch'>中文: Octopi-1.5 是一款先进的视觉-触觉-语言模型，通过整合多部位物体触觉信号和检索增强生成模块，能够实时识别物体并提供交互式操作建议，用户可通过便捷的手持界面进行体验。</span><br>
<span id='abs_en'>English: Octopi-1.5 is an advanced visual-tactile-language model that enhances tactile processing by integrating multi-part object signals and a retrieval-augmented generation module, enabling real-time object identification and interactive handling recommendations through a user-friendly handheld interface.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1379, <a href='https://arxiv.org/pdf/2507.09950.pdf' target='_blank'>https://arxiv.org/pdf/2507.09950.pdf</a></span>   <span><a href='https://github.com/yumingj/DeepFashion-MultiModal' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shubham Shukla, Kunal Sonalkar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09950">Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The fashion retail business is centered around the capacity to comprehend products. Product attribution helps in comprehending products depending on the business process. Quality attribution improves the customer experience as they navigate through millions of products offered by a retail website. It leads to well-organized product catalogs. In the end, product attribution directly impacts the 'discovery experience' of the customer. Although large language models (LLMs) have shown remarkable capabilities in understanding multimodal data, their performance on fine-grained fashion attribute recognition remains under-explored. This paper presents a zero-shot evaluation of state-of-the-art LLMs that balance performance with speed and cost efficiency, mainly GPT-4o-mini and Gemini 2.0 Flash. We have used the dataset DeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to evaluate these models in the attribution tasks of fashion products. Our study evaluates these models across 18 categories of fashion attributes, offering insight into where these models excel. We only use images as the sole input for product information to create a constrained environment. Our analysis shows that Gemini 2.0 Flash demonstrates the strongest overall performance with a macro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a macro F1 score of 43.28%. Through detailed error analysis, our findings provide practical insights for deploying these LLMs in production e-commerce product attribution-related tasks and highlight the need for domain-specific fine-tuning approaches. This work also lays the groundwork for future research in fashion AI and multimodal attribute extraction.<br>
<span id='abs_ch'>中文: 本 究通过图像输入评估GPT-4o-mini和Gemini 2.0 Flash在细粒度时尚属性识别中的零 本性能，发现Gemini 2.0 Flash以56.79%的宏观F1分数表现更优，同时揭示了电子商务领域应用中对特定领域优化的需求。</span><br>
<span id='abs_en'>English: This study evaluates the zero-shot performance of GPT-4o-mini and Gemini 2.0 Flash on fine-grained fashion attribute recognition using image inputs, finding Gemini 2.0 Flash superior with a 56.79% macro F1 score while highlighting the need for domain-specific optimization in e-commerce applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1380, <a href='https://arxiv.org/pdf/2507.09937.pdf' target='_blank'>https://arxiv.org/pdf/2507.09937.pdf</a></span>   <span><a href='http://github.com/grghosal/MemSinks' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gaurav R. Ghosal, Pratyush Maini, Aditi Raghunathan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09937">Memorization Sinks: Isolating Memorization during LLM Training</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models are susceptible to memorizing repeated sequences, posing privacy and copyright concerns. A popular mitigation strategy is to remove memorized information from specific neurons post-hoc. However, such approaches have shown limited success so far. In a controlled setting, we show that the memorization of natural sequences (those that resemble linguistically plausible text) become mechanistically entangled with general language abilities, thereby becoming challenging to remove post-hoc. In this work, we put forward a new paradigm of MemSinks that promotes isolation of memorization by design. We leverage a sequence identifier that activates a unique set of memorization neurons for each sequence across repetitions. By analyzing the dynamics of learning and forgetting, we argue that MemSinks facilitates isolation of memorized content, making it easier to remove without compromising general language capabilities. We implement MemSinks at the billion-parameter and billion-token scale, and observe both effective isolation and strong generalization. To our knowledge, this is the first proof-of-concept on real data demonstrating that simultaneous generalization and isolation is achievable. We open-source our code at http://github.com/grghosal/MemSinks.<br>
<span id='abs_ch'>中文: MemSinks框架通过为每个重复序列激活独特的记忆神经元，提出了一种新颖的方法来隔离大型语言模型中的记忆内容，使其易于移除而不损害通用语言能力，同时保持强大的泛化性能。</span><br>
<span id='abs_en'>English: The MemSinks framework introduces a novel approach to isolate memorized sequences in large language models by activating unique neurons for each repeated sequence, enabling effective removal without harming general language abilities while maintaining strong generalization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1381, <a href='https://arxiv.org/pdf/2507.09875.pdf' target='_blank'>https://arxiv.org/pdf/2507.09875.pdf</a></span>   <span><a href='https://github.com/INK-USC/function-induction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinyuan Ye, Robin Jia, Xiang Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09875">Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models demonstrate the intriguing ability to perform unseen tasks via in-context learning. However, it remains unclear what mechanisms inside the model drive such task-level generalization. In this work, we approach this question through the lens of off-by-one addition (i.e., 1+1=3, 2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function as a second step. Leveraging circuit-style interpretability techniques such as path patching, we analyze the models' internal computations behind their notable performance and present three key findings. First, we uncover a function induction mechanism that explains the model's generalization from standard addition to off-by-one addition. This mechanism resembles the structure of the induction head mechanism found in prior work and elevates it to a higher level of abstraction. Second, we show that the induction of the +1 function is governed by multiple attention heads in parallel, each of which emits a distinct piece of the +1 function. Finally, we find that this function induction mechanism is reused in a broader range of tasks, including synthetic tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8 addition. Overall, our findings offer deeper insights into how reusable and composable structures within language models enable task-level generalization.<br>
<span id='abs_ch'>中文: 本 究通过“错位 法”案例揭示了大型语言模型通过可复用的函数归纳机制实现任务级泛化，多个注意力头并行诱导+1函数，该机制可迁移至合成问答及算法任务中。</span><br>
<span id='abs_en'>English: This study reveals how large language models generalize to unseen tasks through a reusable function induction mechanism, using off-by-one addition as a case to demonstrate parallel attention heads enabling task-level adaptation across various contexts.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1382, <a href='https://arxiv.org/pdf/2507.09875.pdf' target='_blank'>https://arxiv.org/pdf/2507.09875.pdf</a></span>   <span><a href='https://github.com/INK-USC/function-induction' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qinyuan Ye, Robin Jia, Xiang Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09875">Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models demonstrate the intriguing ability to perform unseen tasks via in-context learning. However, it remains unclear what mechanisms inside the model drive such task-level generalization. In this work, we approach this question through the lens of off-by-one addition (i.e., 1+1=3, 2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function as a second step. Leveraging circuit-style interpretability techniques such as path patching, we analyze the models' internal computations behind their performance and present three key findings. First, we uncover a function induction mechanism that explains the model's generalization from standard addition to off-by-one addition. This mechanism resembles the structure of the induction head mechanism found in prior work and elevates it to a higher level of abstraction. Second, we show that the induction of the +1 function is governed by multiple attention heads in parallel, each of which emits a distinct piece of the +1 function. Finally, we find that this function induction mechanism is reused in a broader range of tasks, including synthetic tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8 addition. Overall, our findings offer deeper insights into how reusable and composable structures within language models enable task-level generalization.<br>
<span id='abs_ch'>中文: 本 究通过“错位 法”案例揭示了大型语言模型通过可复用的函数归纳机制实现任务级泛化，多个注意力头并行诱导+1函数，该机制可迁移至合成问答及算法任务中。</span><br>
<span id='abs_en'>English: This study reveals how large language models generalize to unseen tasks through a reusable function induction mechanism, using off-by-one addition as a case to demonstrate parallel attention heads enabling task-level adaptation across various contexts.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1383, <a href='https://arxiv.org/pdf/2507.09831.pdf' target='_blank'>https://arxiv.org/pdf/2507.09831.pdf</a></span>   <span><a href='https://github.com/CSLiJT/Generative-CD.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiatong Li, Qi Liu, Mengxiao Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09831">Generative Cognitive Diagnosis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Cognitive diagnosis (CD) models latent cognitive states of human learners by analyzing their response patterns on diagnostic tests, serving as a crucial machine learning technique for educational assessment and evaluation. Traditional cognitive diagnosis models typically follow a transductive prediction paradigm that optimizes parameters to fit response scores and extract learner abilities. These approaches face significant limitations as they cannot perform instant diagnosis for new learners without computationally expensive retraining and produce diagnostic outputs with limited reliability. In this study, we introduces a novel generative diagnosis paradigm that fundamentally shifts CD from predictive to generative modeling, enabling inductive inference of cognitive states without parameter re-optimization. We propose two simple yet effective instantiations of this paradigm: Generative Item Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model (G-NCDM), which achieve excellent performance improvements over traditional methods. The generative approach disentangles cognitive state inference from response prediction through a well-designed generation process that incorporates identifiability and monotonicity conditions. Extensive experiments on real-world datasets demonstrate the effectiveness of our methodology in addressing scalability and reliability challenges, especially $\times 100$ speedup for the diagnosis of new learners. Our framework opens new avenues for cognitive diagnosis applications in artificial intelligence, particularly for intelligent model evaluation and intelligent education systems. The code is available at https://github.com/CSLiJT/Generative-CD.git.<br>
<span id='abs_ch'>中文摘要：本 究提出了一种生成式认知诊断新范式， 需重新训练即可对新学 者进行即时可 的认知状态评估，相比 统方法实现了百倍 速与性能显著提升。English Summary: This study introduces a generative cognitive diagnosis paradigm that enables instant, reliable assessment of new learners without retraining, achieving significant performance improvements and a 100x speedup over traditional methods.Paperid:760,https://arxiv.org/pdf/2507.09795.pdfGitHub</span><br>
<span id='abs_en'>English Summary: This study introduces a generative cognitive diagnosis paradigm that enables instant, reliable assessment of new learners without retraining, achieving significant performance improvements and a 100x speedup over traditional methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1384, <a href='https://arxiv.org/pdf/2507.09788.pdf' target='_blank'>https://arxiv.org/pdf/2507.09788.pdf</a></span>   <span><a href='https://github.com/microsoft/tinytroupe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Paulo Salem, Robert Sim, Christopher Olsen, Prerit Saxena, Rafael Barcelos, Yi Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09788">TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in Large Language Models (LLM) have led to a new class of autonomous agents, renewing and expanding interest in the area. LLM-powered Multiagent Systems (MAS) have thus emerged, both for assistive and simulation purposes, yet tools for realistic human behavior simulation -- with its distinctive challenges and opportunities -- remain underdeveloped. Existing MAS libraries and tools lack fine-grained persona specifications, population sampling facilities, experimentation support, and integrated validation, among other key capabilities, limiting their utility for behavioral studies, social simulation, and related applications. To address these deficiencies, in this work we introduce TinyTroupe, a simulation toolkit enabling detailed persona definitions (e.g., nationality, age, occupation, personality, beliefs, behaviors) and programmatic control via numerous LLM-driven mechanisms. This allows for the concise formulation of behavioral problems of practical interest, either at the individual or group level, and provides effective means for their solution. TinyTroupe's components are presented using representative working examples, such as brainstorming and market research sessions, thereby simultaneously clarifying their purpose and demonstrating their usefulness. Quantitative and qualitative evaluations of selected aspects are also provided, highlighting possibilities, limitations, and trade-offs. The approach, though realized as a specific Python implementation, is meant as a novel conceptual contribution, which can be partially or fully incorporated in other contexts. The library is available as open source at https://github.com/microsoft/tinytroupe.<br>
<span id='abs_ch'>Chinese Summary: TinyTroupe作为一种新型模拟工具包，通过基于大语言模型的驱动机制实现精细人物角色定义和程序化控制，解决了现有多智能体系统在行为模拟方面的不足，为社会科学 究和市场分析等应用提供了有效解决方案。</span><br>
<span id='abs_en'>English Summary: TinyTroupe is a new simulation toolkit that addresses the limitations of existing multiagent systems by enabling detailed persona specifications and programmatic control through LLM-driven mechanisms, facilitating realistic behavioral simulations for applications like social studies and market research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1385, <a href='https://arxiv.org/pdf/2507.09759.pdf' target='_blank'>https://arxiv.org/pdf/2507.09759.pdf</a></span>   <span><a href='https://github.com/AbdulManaf12/Pediatric-Chest-Pneumonia-Classification' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdul Manaf, Nimra Mughal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09759">AI-Enhanced Pediatric Pneumonia Detection: A CNN-Based Approach Using Data Augmentation and Generative Adversarial Networks (GANs)</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Pneumonia is a leading cause of mortality in children under five, requiring accurate chest X-ray diagnosis. This study presents a machine learning-based Pediatric Chest Pneumonia Classification System to assist healthcare professionals in diagnosing pneumonia from chest X-ray images. The CNN-based model was trained on 5,863 labeled chest X-ray images from children aged 0-5 years from the Guangzhou Women and Children's Medical Center. To address limited data, we applied augmentation techniques (rotation, zooming, shear, horizontal flipping) and employed GANs to generate synthetic images, addressing class imbalance. The system achieved optimal performance using combined original, augmented, and GAN-generated data, evaluated through accuracy and F1 score metrics. The final model was deployed via a Flask web application, enabling real-time classification with probability estimates. Results demonstrate the potential of deep learning and GANs in improving diagnostic accuracy and efficiency for pediatric pneumonia classification, particularly valuable in resource-limited clinical settings https://github.com/AbdulManaf12/Pediatric-Chest-Pneumonia-Classification<br>
<br>
<div id='section'>Paperid: <span id='pid'>1386, <a href='https://arxiv.org/pdf/2507.09583.pdf' target='_blank'>https://arxiv.org/pdf/2507.09583.pdf</a></span>   <span><a href='https://github.com/TanivAshraf/ai-stock-analyzer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taniv Ashraf
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09583">A Serverless Architecture for Real-Time Stock Analysis using Large Language Models: An Iterative Development and Debugging Case Study</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The advent of powerful, accessible Large Language Models (LLMs) like Google's Gemini presents new opportunities for democratizing financial data analysis. This paper documents the design, implementation, and iterative debugging of a novel, serverless system for real-time stock analysis. The system leverages the Gemini API for qualitative assessment, automates data ingestion and processing via GitHub Actions, and presents the findings through a decoupled, static frontend. We detail the architectural evolution of the system, from initial concepts to a robust, event-driven pipeline, highlighting the practical challenges encountered during deployment. A significant portion of this paper is dedicated to a case study on the debugging process, covering common software errors, platform-specific permission issues, and rare, environment-level platform bugs. The final architecture operates at a near-zero cost, demonstrating a viable model for individuals to build sophisticated AI-powered financial tools. The operational application is publicly accessible, and the complete source code is available for review. We conclude by discussing the role of LLMs in financial analysis, the importance of robust debugging methodologies, and the emerging paradigm of human-AI collaboration in software development.<br>
<span id='abs_ch'>中文: 本文介绍了一种利用谷歌Gemini进行实时股票分析的 服务器系统，详细阐述了从概念到经济高效的事件驱动架构的开发过程，使个人能够构建AI驱动的金融工具。</span><br>
<span id='abs_en'>English: This paper presents a serverless system using Google's Gemini for real-time stock analysis, detailing its development from concept to a cost-effective, event-driven architecture that enables individuals to build AI-powered financial tools.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1387, <a href='https://arxiv.org/pdf/2507.09574.pdf' target='_blank'>https://arxiv.org/pdf/2507.09574.pdf</a></span>   <span><a href='https://github.com/HaozheZhao/MENTOR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haozhe Zhao, Zefan Cai, Shuzheng Si, Liang Chen, Jiuxiang Gu, Wen Xiao, Junjie Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09574">MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent text-to-image models produce high-quality results but still struggle with precise visual control, balancing multimodal inputs, and requiring extensive training for complex multimodal image generation. To address these limitations, we propose MENTOR, a novel autoregressive (AR) framework for efficient Multimodal-conditioned Tuning for Autoregressive multimodal image generation. MENTOR combines an AR image generator with a two-stage training paradigm, enabling fine-grained, token-level alignment between multimodal inputs and image outputs without relying on auxiliary adapters or cross-attention modules. The two-stage training consists of: (1) a multimodal alignment stage that establishes robust pixel- and semantic-level alignment, followed by (2) a multimodal instruction tuning stage that balances the integration of multimodal inputs and enhances generation controllability. Despite modest model size, suboptimal base components, and limited training resources, MENTOR achieves strong performance on the DreamBench++ benchmark, outperforming competitive baselines in concept preservation and prompt following. Additionally, our method delivers superior image reconstruction fidelity, broad task adaptability, and improved training efficiency compared to diffusion-based methods. Dataset, code, and models are available at: https://github.com/HaozheZhao/MENTOR<br>
<span id='abs_ch'>中文摘要：MENTOR框架通过自回归方法和两阶段训练实现了多模态输入与图像输出的细粒度对齐，在有限资源下仍展现出卓越的生成性能与训练效率，超越了现有基准方法。</span><br>
<span id='abs_en'>English Summary: The MENTOR framework introduces an autoregressive approach with two-stage training to enhance multimodal image generation by achieving fine-grained alignment between inputs and outputs, demonstrating superior performance and efficiency over existing methods despite limited resources.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1388, <a href='https://arxiv.org/pdf/2507.09482.pdf' target='_blank'>https://arxiv.org/pdf/2507.09482.pdf</a></span>   <span><a href='https://github.com/wclapply/ViSP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Changli Wang, Rui Wu, Fang Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09482">ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Human emotions are complex, with sarcasm being a subtle and distinctive form. Despite progress in sarcasm research, sarcasm generation remains underexplored, primarily due to the overreliance on textual modalities and the neglect of visual cues, as well as the mismatch between image content and sarcastic intent in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm generation dataset with 4,970 samples, each containing an image, a sarcastic text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation framework that integrates Proximal Policy Optimization (PPO) and contrastive learning. PPO utilizes reward scores from DIP to steer the generation of sarcastic texts, while contrastive learning encourages the model to favor outputs with higher reward scores. These strategies improve overall generation quality and produce texts with more pronounced sarcastic intent. We evaluate ViSP across five metric sets and find it surpasses all baselines, including large language models, underscoring their limitations in sarcasm generation. Furthermore, we analyze the distributions of Sarcasm Scores and Factual Incongruity for both M2SaG and the texts generated by ViSP. The generated texts exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity (0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic content than the original dataset. % The dataset and code will be publicly available. Our dataset and code will be released at \textit{https://github.com/wclapply/ViSP}.<br>
<span id='abs_ch'>Chinese: 本文提出了多模态讽刺生成数据集M2SaG和ViSP框架，该框架通过PPO和对比学 提升讽刺文本生成质量，在包括大语言模型在内的基准测试中表现优异。</span><br>
<span id='abs_en'>English: This paper introduces M2SaG, a multimodal sarcasm generation dataset, and ViSP, a framework that enhances sarcastic text generation through PPO and contrastive learning, outperforming baselines including large language models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1389, <a href='https://arxiv.org/pdf/2507.09477.pdf' target='_blank'>https://arxiv.org/pdf/2507.09477.pdf</a></span>   <span><a href='https://github.com/DavidZWZ/Awesome-RAG-Reasoning' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangning Li, Weizhi Zhang, Yuyao Yang, Wei-Chieh Huang, Yaozu Wu, Junyu Luo, Yuanchen Bei, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Chunkit Chan, Yankai Chen, Zhongfen Deng, Yinghui Li, Hai-Tao Zheng, Dongyuan Li, Renhe Jiang, Ming Zhang, Yangqiu Song, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09477">Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language Models (LLMs) by injecting external knowledge, yet it falls short on problems that demand multi-step inference; conversely, purely reasoning-oriented approaches often hallucinate or mis-ground facts. This survey synthesizes both strands under a unified reasoning-retrieval perspective. We first map how advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then, we show how retrieved knowledge of different type supply missing premises and expand context for complex inference (RAG-Enhanced Reasoning). Finally, we spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs iteratively interleave search and reasoning to achieve state-of-the-art performance across knowledge-intensive benchmarks. We categorize methods, datasets, and open challenges, and outline research avenues toward deeper RAG-Reasoning systems that are more effective, multimodally-adaptive, trustworthy, and human-centric. The collection is available at https://github.com/DavidZWZ/Awesome-RAG-Reasoning.<br>
<span id='abs_ch'>中文: 本综述将检索增强生成与推理方法整合于统一框架下，阐释了高级推理如何优化RAG各阶段及检索知识如何支撑复杂推理，同时聚焦新兴的协同框架并展望未来 究方向。</span><br>
<span id='abs_en'>English: This survey integrates retrieval-augmented generation and reasoning methods under a unified framework, demonstrating how advanced reasoning enhances RAG and how retrieved knowledge supports complex inference, while highlighting emerging synergistic approaches and future research directions.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1390, <a href='https://arxiv.org/pdf/2507.09382.pdf' target='_blank'>https://arxiv.org/pdf/2507.09382.pdf</a></span>   <span><a href='https://github.com/ZhanliangAaronWang/FR-CCA-ADNI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Bojian Hou, Zhanliang Wang, Zhuoping Zhou, Boning Tong, Zexuan Wang, Jingxuan Bao, Duy Duong-Tran, Qi Long, Li Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09382">Fair CCA for Fair Representation Learning: An ADNI Study</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Canonical correlation analysis (CCA) is a technique for finding correlations between different data modalities and learning low-dimensional representations. As fairness becomes crucial in machine learning, fair CCA has gained attention. However, previous approaches often overlook the impact on downstream classification tasks, limiting applicability. We propose a novel fair CCA method for fair representation learning, ensuring the projected features are independent of sensitive attributes, thus enhancing fairness without compromising accuracy. We validate our method on synthetic data and real-world data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating its ability to maintain high correlation analysis performance while improving fairness in classification tasks. Our work enables fair machine learning in neuroimaging studies where unbiased analysis is essential. Code is available in https://github.com/ZhanliangAaronWang/FR-CCA-ADNI.<br>
<span id='abs_ch'>中文: 本文提出了一种新颖的公平典型相关分析方法，确保投影特征与敏感属性 关，在阿尔茨海默病神经影像学计划数据验证中既提升了分类公平性又不损害准确性。</span><br>
<span id='abs_en'>English: This paper introduces a novel fair canonical correlation analysis method that ensures feature independence from sensitive attributes, enhancing fairness while maintaining accuracy in classification tasks, as validated on synthetic and Alzheimer's Disease Neuroimaging Initiative data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1391, <a href='https://arxiv.org/pdf/2507.09308.pdf' target='_blank'>https://arxiv.org/pdf/2507.09308.pdf</a></span>   <span><a href='https://github.com/o0o0o00o0/AlphaVAE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zile Wang, Hao Yu, Jiabo Zhan, Chun Yuan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09308">AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in latent diffusion models have achieved remarkable results in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress and reconstruct pixel data at low computational cost. However, the generation of transparent or layered content (RGBA image) remains largely unexplored, due to the lack of large-scale benchmarks. In this work, we propose ALPHA, the first comprehensive RGBA benchmark that adapts standard RGB metrics to four-channel images via alpha blending over canonical backgrounds. We further introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB VAE by incorporating a dedicated alpha channel. The model is trained with a composite objective that combines alpha-blended pixel reconstruction, patch-level fidelity, perceptual consistency, and dual KL divergence constraints to ensure latent fidelity across both RGB and alpha representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase in SSIM over LayerDiffuse in reconstruction. It also enables superior transparent image generation when fine-tuned within a latent diffusion framework. Our code, data, and models are released on https://github.com/o0o0o00o0/AlphaVAE for reproducibility.<br>
<span id='abs_ch'>中文: 本文提出了首个全面的RGBA基准ALPHA和统一的RGBA变分自编 器ALPHAVAE，该模型仅用极少量训练数据就在透明图像重建和生成方面显著超越了现有方法。</span><br>
<span id='abs_en'>English: This paper introduces ALPHA, the first comprehensive RGBA benchmark, and ALPHAVAE, a unified RGBA variational autoencoder that significantly outperforms existing methods in transparent image reconstruction and generation despite using minimal training data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1392, <a href='https://arxiv.org/pdf/2507.09299.pdf' target='_blank'>https://arxiv.org/pdf/2507.09299.pdf</a></span>   <span><a href='https://github.com/abdulvahapmutlu/vit-protonet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdulvahap Mutlu, ÅengÃ¼l DoÄan, TÃ¼rker Tuncer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09299">ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The remarkable representational power of Vision Transformers (ViTs) remains underutilized in few-shot image classification. In this work, we introduce ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical Network framework. By averaging class conditional token embeddings from a handful of support examples, ViT-ProtoNet constructs robust prototypes that generalize to novel categories under 5-shot settings. We conduct an extensive empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100, CUB-200, and CIFAR-FS, including overlapped support variants to assess robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based prototypical counterparts, achieving up to a 3.2\% improvement in 5-shot accuracy and demonstrating superior feature separability in latent space. Furthermore, it outperforms or is competitive with transformer-based competitors using a more lightweight backbone. Comprehensive ablations examine the impact of transformer depth, patch size, and fine-tuning strategy. To foster reproducibility, we release code and pretrained weights. Our results establish ViT-ProtoNet as a powerful, flexible approach for few-shot classification and set a new baseline for transformer-based meta-learners.<br>
<span id='abs_ch'>中文: ViT-ProtoNet通过将视觉Transformer骨干网络融入原型网络，在小 本图像分类中显著提升了准确率和特征可分性，在多个基准测试中以轻量级架构确立了新基线。</span><br>
<span id='abs_en'>English: ViT-ProtoNet enhances few-shot image classification by integrating a Vision Transformer backbone into Prototypical Networks, achieving superior accuracy and feature separability across multiple benchmarks with a lightweight architecture.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1393, <a href='https://arxiv.org/pdf/2507.09279.pdf' target='_blank'>https://arxiv.org/pdf/2507.09279.pdf</a></span>   <span><a href='https://github.com/xingbpshen/prompt4trust' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anita Kriz, Elizabeth Laura Janes, Xing Shen, Tal Arbel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09279">Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal large language models (MLLMs) hold considerable promise for applications in healthcare. However, their deployment in safety-critical settings is hindered by two key limitations: (i) sensitivity to prompt design, and (ii) a tendency to generate incorrect responses with high confidence. As clinicians may rely on a model's stated confidence to gauge the reliability of its predictions, it is especially important that when a model expresses high confidence, it is also highly accurate. We introduce Prompt4Trust, the first reinforcement learning (RL) framework for prompt augmentation targeting confidence calibration in MLLMs. A lightweight LLM is trained to produce context-aware auxiliary prompts that guide a downstream task MLLM to generate responses in which the expressed confidence more accurately reflects predictive accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically prioritizes aspects of calibration most critical for safe and trustworthy clinical decision-making. Beyond improvements driven by this clinically motivated calibration objective, our proposed method also improves task accuracy, achieving state-of-the-art medical visual question answering (VQA) performance on the PMC-VQA benchmark, which is composed of multiple-choice questions spanning diverse medical imaging modalities. Moreover, our framework trained with a small downstream task MLLM showed promising zero-shot generalization to larger MLLMs in our experiments, suggesting the potential for scalable calibration without the associated computational costs. This work demonstrates the potential of automated yet human-aligned prompt engineering for improving the the trustworthiness of MLLMs in safety critical settings. Our codebase can be found at https://github.com/xingbpshen/prompt4trust.<br>
<span id='abs_ch'>Chinese: Prompt4Trust是一种强化学 框架，旨在提升多模态大语言模型的置信度 准与任务准确性，尤其关注临床决策的可信度，并在医学视觉问答基准中实现了领先性能。</span><br>
<span id='abs_en'>English: Prompt4Trust is a reinforcement learning framework that enhances multimodal large language models' confidence calibration and task accuracy, particularly for trustworthy clinical decision-making, achieving state-of-the-art performance in medical visual question answering.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1394, <a href='https://arxiv.org/pdf/2507.09269.pdf' target='_blank'>https://arxiv.org/pdf/2507.09269.pdf</a></span>   <span><a href='https://github.com/ShawnYE618/CKD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuhan Ye, Yuanbin Qian, Chong Wang, Sunqi Lin, Jiazhen Xu, Jiangbo Qian, Yuqi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09269">Cross Knowledge Distillation between Artificial and Spiking Neural Networks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in computer vision domain due to their high biological plausibility, event-driven characteristic and energy-saving efficiency. Still, limited annotated event-based datasets and immature SNN architectures result in their performance inferior to that of Artificial Neural Networks (ANNs). To enhance the performance of SNNs on their optimal data format, DVS data, we explore using RGB data and well-performing ANNs to implement knowledge distillation. In this case, solving cross-modality and cross-architecture challenges is necessary. In this paper, we propose cross knowledge distillation (CKD), which not only leverages semantic similarity and sliding replacement to mitigate the cross-modality challenge, but also uses an indirect phased knowledge distillation to mitigate the cross-architecture challenge. We validated our method on main-stream neuromorphic datasets, including N-Caltech101 and CEP-DVS. The experimental results show that our method outperforms current State-of-the-Art methods. The code will be available at https://github.com/ShawnYE618/CKD<br>
<span id='abs_ch'>Chinese: 脉冲神经网络在计算机视觉领域潜力巨大，但  注数据有限和架构不成熟而性能不及人工神经网络，为此提出跨知识蒸馏方法，通过语义相似性和间接分阶段蒸馏解决跨模态和跨架构问题，在主流神经形态数据集上表现优于现有最优方法。</span><br>
<span id='abs_en'>English: Spiking Neural Networks (SNNs) show promise in computer vision but lag behind Artificial Neural Networks (ANNs) due to limited datasets and immature architectures, leading to the proposal of cross knowledge distillation (CKD) that addresses cross-modality and cross-architecture challenges and outperforms state-of-the-art methods on neuromorphic datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1395, <a href='https://arxiv.org/pdf/2507.09155.pdf' target='_blank'>https://arxiv.org/pdf/2507.09155.pdf</a></span>   <span><a href='https://github.com/niaz60/OpenXRD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Vosoughi, Ayoub Shahnazari, Yufeng Xi, Zeliang Zhang, Griffin Hess, Chenliang Xu, Niaz Abdolrahim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09155">OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This work presents OPENXRD, an open-book pipeline designed for crystallography question answering, which integrates textual prompts with concise supporting content generated by GPT-4.5. Instead of using scanned textbooks, which may lead to copyright issues, OPENXRD generates compact, domain-specific references that help smaller models understand key concepts in X-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217 expert-level XRD questions by comparing different vision-language models, including GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN, under both closed-book (without supporting material) and open-book (with supporting material) conditions. Our experimental results show significant accuracy improvements in models that use the GPT-4.5-generated summaries, particularly those with limited prior training in crystallography. OPENXRD uses knowledge from larger models to fill knowledge gaps in crystallography and shows that AI-generated texts can help smaller models reason more effectively in scientific tasks. While the current version of OPENXRD focuses on text-based inputs, we also explore future extensions such as adding real crystal diagrams or diffraction patterns to improve interpretation in specialized materials science contexts. Overall, OPENXRD shows that specialized open-book systems can be useful in materials science and provides a foundation for broader natural language processing (NLP) tools in critical scientific fields.<br>
<span id='abs_ch'>中文: OPENXRD是一种开放书式流程，通过GPT-4.5生成的摘要帮助较小模型在X射线衍射问题上显著提升回答准确率，既规避了版权问题，又为未来科学领域的自然语言处理工具 定了基础。</span><br>
<span id='abs_en'>English: OPENXRD is an open-book pipeline that enhances crystallography question answering by using GPT-4.5-generated summaries to help smaller models improve accuracy, particularly in X-ray diffraction tasks, while avoiding copyright issues and providing a foundation for future scientific NLP tools.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1396, <a href='https://arxiv.org/pdf/2507.09087.pdf' target='_blank'>https://arxiv.org/pdf/2507.09087.pdf</a></span>   <span><a href='https://github.com/esraaelelimy/gtd\_algos' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Esraa Elelimy, Brett Daley, Andrew Patterson, Marlos C. Machado, Adam White, Martha White
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09087">Deep Reinforcement Learning with Gradient Eligibility Traces</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Achieving fast and stable off-policy learning in deep reinforcement learning (RL) is challenging. Most existing methods rely on semi-gradient temporal-difference (TD) methods for their simplicity and efficiency, but are consequently susceptible to divergence. While more principled approaches like Gradient TD (GTD) methods have strong convergence guarantees, they have rarely been used in deep RL. Recent work introduced the generalized Projected Bellman Error ($\overline{\text{PBE}}$), enabling GTD methods to work efficiently with nonlinear function approximation. However, this work is limited to one-step methods, which are slow at credit assignment and require a large number of samples. In this paper, we extend the generalized $\overline{\text{PBE}}$ objective to support multistep credit assignment based on the $Î»$-return and derive three gradient-based methods that optimize this new objective. We provide both a forward-view formulation compatible with experience replay and a backward-view formulation compatible with streaming algorithms. Finally, we evaluate the proposed algorithms and show that they outperform both PPO and StreamQ in MuJoCo and MinAtar environments, respectively. Code available at https://github.com/esraaelelimy/gtd\_algos<br>
<span id='abs_ch'>中文摘要：本文将广义投影贝尔曼误差扩展至基于λ回报的多步信用分配，提出了三种梯度优化方法，在MuJoCo和MinAtar环境中均优于现有算法。</span><br>
<span id='abs_en'>English Summary: This paper extends the generalized Projected Bellman Error to multistep credit assignment using λ-return, developing three gradient-based methods that outperform existing algorithms in MuJoCo and MinAtar environments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1397, <a href='https://arxiv.org/pdf/2507.09009.pdf' target='_blank'>https://arxiv.org/pdf/2507.09009.pdf</a></span>   <span><a href='https://github.com/miraclehetech/sleep-ssl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengxiao He, Huayu Li, Geng Yuan, William D. S. Killgore, Stuart F. Quan, Chen X. Chen, Ao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.09009">Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Methods: We developed a self-supervised deep learning model that extracts meaningful patterns from multi-modal signals (Electroencephalography (EEG), Electrocardiography (ECG), and respiratory signals). The model was trained on data from 4,398 participants. Projection scores were derived by contrasting embeddings from individuals with and without CVD outcomes. External validation was conducted in an independent cohort with 1,093 participants. The source code is available on https://github.com/miraclehetech/sleep-ssl. Results: The projection scores revealed distinct and clinically meaningful patterns across modalities. ECG-derived features were predictive of both prevalent and incident cardiac conditions, particularly CVD mortality. EEG-derived features were predictive of incident hypertension and CVD mortality. Respiratory signals added complementary predictive value. Combining these projection scores with the Framingham Risk Score consistently improved predictive performance, achieving area under the curve values ranging from 0.607 to 0.965 across different outcomes. Findings were robustly replicated and validated in the external testing cohort. Conclusion: Our findings demonstrate that the proposed framework can generate individualized CVD risk scores directly from PSG data. The resulting projection scores have the potential to be integrated into clinical practice, enhancing risk assessment and supporting personalized care.<br>
<span id='abs_ch'>中文: 本 究开发了一种自监督深度学 模型，能从多模态睡 信号中提取具有临床意义的特征，结合 统风险评分可显著提升心血管疾病预测效能，并在外部验证中表现出稳健性能。English: A self-supervised deep learning model was developed to extract clinically meaningful patterns from multi-modal sleep signals, which when combined with traditional risk scores significantly improved cardiovascular disease prediction and demonstrated robust external validation.Paperid:814,https://arxiv.org/pdf/2507.08982.pdfGitHub</span><br>
<span id='abs_en'>English: A self-supervised deep learning model was developed to extract clinically meaningful patterns from multi-modal sleep signals, which when combined with traditional risk scores significantly improved cardiovascular disease prediction and demonstrated robust external validation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1398, <a href='https://arxiv.org/pdf/2507.08980.pdf' target='_blank'>https://arxiv.org/pdf/2507.08980.pdf</a></span>   <span><a href='https://github.com/ChenyuWang-Monica/REED' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenyu Wang, Cai Zhou, Sharut Gupta, Zongyu Lin, Stefanie Jegelka, Stephen Bates, Tommi Jaakkola
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08980">Learning Diffusion Models with Flexible Representation Guidance</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion models can be improved with additional guidance towards more effective representations of input. Indeed, prior empirical work has already shown that aligning internal representations of the diffusion model with those of pre-trained models improves generation quality. In this paper, we present a systematic framework for incorporating representation guidance into diffusion models. We provide alternative decompositions of denoising models along with their associated training criteria, where the decompositions determine when and how the auxiliary representations are incorporated. Guided by our theoretical insights, we introduce two new strategies for enhancing representation alignment in diffusion models. First, we pair examples with target representations either derived from themselves or arisen from different synthetic modalities, and subsequently learn a joint model over the multimodal pairs. Second, we design an optimal training curriculum that balances representation learning and data generation. Our experiments across image, protein sequence, and molecule generation tasks demonstrate superior performance as well as accelerated training. In particular, on the class-conditional ImageNet $256\times 256$ benchmark, our guidance results in $23.3$ times faster training than the original SiT-XL as well as four times speedup over the state-of-the-art method REPA. The code is available at https://github.com/ChenyuWang-Monica/REED.<br>
<span id='abs_ch'>Chinese: 本文提出了一种系统框架，通过引入表征指导来增强扩散模型，在多个任务中提升了生成质量并 速训练，如在ImageNet上实现了23.3倍的训练 速。</span><br>
<span id='abs_en'>English: This paper introduces a systematic framework to enhance diffusion models by incorporating representation guidance, which accelerates training and improves generation quality across various tasks, as demonstrated by a 23.3 times faster training speed on ImageNet.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1399, <a href='https://arxiv.org/pdf/2507.08958.pdf' target='_blank'>https://arxiv.org/pdf/2507.08958.pdf</a></span>   <span><a href='https://github.com/xwzhang98/SimAgents' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaowen Zhang, Zhenyu Bi, Patrick Lachance, Xuan Wang, Tiziana Di Matteo, Rupert A. C. Croft
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08958">Bridging Literature and the Universe Via A Multi-Agent Large Language Model System</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As cosmological simulations and their associated software become increasingly complex, physicists face the challenge of searching through vast amounts of literature and user manuals to extract simulation parameters from dense academic papers, each using different models and formats. Translating these parameters into executable scripts remains a time-consuming and error-prone process. To improve efficiency in physics research and accelerate the cosmological simulation process, we introduce SimAgents, a multi-agent system designed to automate both parameter configuration from the literature and preliminary analysis for cosmology research. SimAgents is powered by specialized LLM agents capable of physics reasoning, simulation software validation, and tool execution. These agents collaborate through structured communication, ensuring that extracted parameters are physically meaningful, internally consistent, and software-compliant. We also construct a cosmological parameter extraction evaluation dataset by collecting over 40 simulations in published papers from Arxiv and leading journals that cover diverse simulation types. Experiments on the dataset demonstrate a strong performance of SimAgents, highlighting its effectiveness and potential to accelerate scientific research for physicists. Our demonstration video is available at: https://youtu.be/w1zLpm_CaWA. The complete system and dataset are publicly available at https://github.com/xwzhang98/SimAgents.<br>
<span id='abs_ch'>中文摘要：SimAgents是一个多智能体系统，能够自动从学术文献中提取宇宙学模拟参数并生成可执行脚本，通过确保物理一致性和软件兼容性，显著提升了物理 究的效率。</span><br>
<span id='abs_en'>English Summary: SimAgents is a multi-agent system that automates the extraction and validation of cosmological simulation parameters from academic literature and generates executable scripts, significantly improving research efficiency by ensuring physical consistency and software compliance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1400, <a href='https://arxiv.org/pdf/2507.08912.pdf' target='_blank'>https://arxiv.org/pdf/2507.08912.pdf</a></span>   <span><a href='https://github.com/szandala/fair-deepfake-detection-toolbox' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomasz Szandala, Fatima Ezzeddine, Natalia Rusin, Silvia Giordano, Omran Ayoub
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08912">Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Artificial Intelligence-generated content has become increasingly popular, yet its malicious use, particularly the deepfakes, poses a serious threat to public trust and discourse. While deepfake detection methods achieve high predictive performance, they often exhibit biases across demographic attributes such as ethnicity and gender. In this work, we tackle the challenge of fair deepfake detection, aiming to mitigate these biases while maintaining robust detection capabilities. To this end, we propose a novel post-processing approach, referred to as Fairness-Oriented Final Layer Input Prioritising (Fair-FLIP), that reweights a trained model's final-layer inputs to reduce subgroup disparities, prioritising those with low variability while demoting highly variable ones. Experimental results comparing Fair-FLIP to both the baseline (without fairness-oriented de-biasing) and state-of-the-art approaches show that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining baseline accuracy, with only a negligible reduction of 0.25%.
  Code is available on Github: https://github.com/szandala/fair-deepfake-detection-toolbox<br>
<span id='abs_ch'>中文: 本 究提出Fair-FLIP后处理方法，通过重新 权模型最终层输入，在保持基线准确率的同时将深度伪 检测的 demographic 偏差降低达30%，性能损失仅0.25%。</span><br>
<span id='abs_en'>English: The study introduces Fair-FLIP, a post-processing method that reduces demographic biases in deepfake detection by up to 30% while preserving baseline accuracy with minimal performance loss.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1401, <a href='https://arxiv.org/pdf/2507.08902.pdf' target='_blank'>https://arxiv.org/pdf/2507.08902.pdf</a></span>   <span><a href='https://github.com/sermare/struct-mhc-dev' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sergio Mares, Ariel Espinoza Weinberger, Nilah M. Ioannidis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08902">Generation of structure-guided pMHC-I libraries using Diffusion Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Personalized vaccines and T-cell immunotherapies depend critically on identifying peptide-MHC class I (pMHC-I) interactions capable of eliciting potent immune responses. However, current benchmarks and models inherit biases present in mass-spectrometry and binding-assay datasets, limiting discovery of novel peptide ligands. To address this issue, we introduce a structure-guided benchmark of pMHC-I peptides designed using diffusion models conditioned on crystal structure interaction distances. Spanning twenty high-priority HLA alleles, this benchmark is independent of previously characterized peptides yet reproduces canonical anchor residue preferences, indicating structural generalization without experimental dataset bias. Using this resource, we demonstrate that state-of-the-art sequence-based predictors perform poorly at recognizing the binding potential of these structurally stable designs, indicating allele-specific limitations invisible in conventional evaluations. Our geometry-aware design pipeline yields peptides with high predicted structural integrity and higher residue diversity than existing datasets, representing a key resource for unbiased model training and evaluation. Our code, and data are available at: https://github.com/sermare/struct-mhc-dev.<br>
<span id='abs_ch'>中文: 本 究通过基于晶体结构距离的扩散模型，构建了不受实验数据偏差影响的pMHC-I肽段基准数据集，揭示了现有预测模型的局限性，为 偏见的免疫疗法开发提供了关键资源。</span><br>
<span id='abs_en'>English: This study introduces a structure-guided benchmark for peptide-MHC class I interactions using diffusion models to overcome biases in existing datasets, revealing limitations in current predictors and providing a resource for unbiased immunotherapy development.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1402, <a href='https://arxiv.org/pdf/2507.08898.pdf' target='_blank'>https://arxiv.org/pdf/2507.08898.pdf</a></span>   <span><a href='https://github.com/awsm-research/SEALGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenliang Shan, Michael Fu, Rui Yang, Chakkrit Tantithamthavorn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08898">SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Safety alignment is critical for LLM-powered systems. While recent LLM-powered guardrail approaches such as LlamaGuard achieve high detection accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''), they struggle with multilingual unsafe inputs. This limitation leaves LLM systems vulnerable to unsafe and jailbreak prompts written in low-resource languages such as those in Southeast Asia. This paper introduces SEALGuard, a multilingual guardrail designed to improve the safety alignment across diverse languages. It aims to address the multilingual safety alignment gap of existing guardrails and ensure effective filtering of unsafe and jailbreak prompts in LLM-powered systems. We adapt a general-purpose multilingual language model into a multilingual guardrail using low-rank adaptation (LoRA). We construct SEALSBench, a large-scale multilingual safety alignment dataset containing over 260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases. We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on this benchmark. Our findings show that multilingual unsafe and jailbreak prompts substantially degrade the performance of the state-of-the-art LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and 18%, respectively, compared to its performance on English-only prompts. In contrast, SEALGuard outperforms existing guardrails in detecting multilingual unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and achieving the best DSR, precision, and F1-score. Our ablation study further reveals the contributions of adaptation strategies and model size to the overall performance of SEALGuard. We release our pre-trained model and benchmark at https://github.com/awsm-research/SEALGuard to support further research.<br>
<span id='abs_ch'>Chinese: SEALGuard 是一种多语言防护机制，显著提升了检测多种语言中不安全及越狱提示的能力，相比现有方法如 LlamaGuard，其防御成功率提高了48%，并在精确率和 F1 分数上表现最佳。</span><br>
<span id='abs_en'>English: SEALGuard is a multilingual guardrail that significantly enhances the detection of unsafe and jailbreak prompts across diverse languages, outperforming existing methods like LlamaGuard by improving the Defense Success Rate by 48% and achieving top precision and F1-scores.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1403, <a href='https://arxiv.org/pdf/2507.08842.pdf' target='_blank'>https://arxiv.org/pdf/2507.08842.pdf</a></span>   <span><a href='https://github.com/mastlab-T3S/FedRAS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhufeng Lu, Chentao Jia, Ming Hu, Xiaofei Xie, Mingsong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08842">Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As a promising privacy-aware collaborative model training paradigm, Federated Learning (FL) is becoming popular in the design of distributed recommender systems. However, Federated Recommender Systems (FedRecs) greatly suffer from two major problems: i) extremely high communication overhead due to massive item embeddings involved in recommendation systems, and ii) intolerably low training efficiency caused by the entanglement of both heterogeneous network environments and client devices. Although existing methods attempt to employ various compression techniques to reduce communication overhead, due to the parameter errors introduced by model compression, they inevitably suffer from model performance degradation. To simultaneously address the above problems, this paper presents a communication-efficient FedRec framework named FedRAS, which adopts an action-sharing strategy to cluster the gradients of item embedding into a specific number of model updating actions for communication rather than directly compressing the item embeddings. In this way, the cloud server can use the limited actions from clients to update all the items. Since gradient values are significantly smaller than item embeddings, constraining the directions of gradients (i.e., the action space) introduces smaller errors compared to compressing the entire item embedding matrix into a reduced space. To accommodate heterogeneous devices and network environments, FedRAS incorporates an adaptive clustering mechanism that dynamically adjusts the number of actions. Comprehensive experiments on well-known datasets demonstrate that FedRAS can reduce the size of communication payloads by up to 96.88%, while not sacrificing recommendation performance within various heterogeneous scenarios. We have open-sourced FedRAS at https://github.com/mastlab-T3S/FedRAS.<br>
<span id='abs_ch'>中文: 联邦推荐系统面临高通信开销和低训练效率的问题，而FedRAS框架通过共享聚类后的梯度动作而非压缩嵌入，在保证性能的同时将通信负载减少高达96.88%。</span><br>
<span id='abs_en'>English: Federated Recommender Systems (FedRecs) face high communication costs and low training efficiency, but the proposed FedRAS framework reduces payload size by up to 96.88% without performance loss by sharing clustered gradient actions instead of compressing embeddings.Paperid:825,https://arxiv.org/pdf/2507.08841.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1404, <a href='https://arxiv.org/pdf/2507.08841.pdf' target='_blank'>https://arxiv.org/pdf/2507.08841.pdf</a></span>   <span><a href='https://github.com/kunjing96/ZSNAS-WRCor.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Jing, Luoyu Chen, Jungang Xu, Jianwei Tai, Yiyu Wang, Shuaimin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08841">Zero-Shot Neural Architecture Search with Weighted Response Correlation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Neural architecture search (NAS) is a promising approach for automatically designing neural network architectures. However, the architecture estimation of NAS is computationally expensive and time-consuming because of training multiple architectures from scratch. Although existing zero-shot NAS methods use training-free proxies to accelerate the architecture estimation, their effectiveness, stability, and generality are still lacking. We present a novel training-free estimation proxy called weighted response correlation (WRCor). WRCor utilizes correlation coefficient matrices of responses across different input samples to calculate the proxy scores of estimated architectures, which can measure their expressivity and generalizability. Experimental results on proxy evaluation demonstrate that WRCor and its voting proxies are more efficient estimation strategies than existing proxies. We also apply them with different search strategies in architecture search. Experimental results on architecture search show that our zero-shot NAS algorithm outperforms most existing NAS algorithms in different search spaces. Our NAS algorithm can discover an architecture with a 22.1% test error on the ImageNet-1k dataset within 4 GPU hours. All codes are publicly available at https://github.com/kunjing96/ZSNAS-WRCor.git.<br>
<span id='abs_ch'>中文: 本文提出了一种名为 权响应相关性（WRCor）的新型免训练代理方法，用于神经架构搜索，它能有效评估架构的表达能力和泛化性，在代理评估和架构搜索中均优于现有方法，并在极少的GPU时间内于ImageNet-1k数据集上取得了优异结果。</span><br>
<span id='abs_en'>English: The paper introduces a novel training-free proxy called weighted response correlation (WRCor) for neural architecture search, which efficiently estimates architecture expressivity and generalizability, outperforming existing methods in both proxy evaluation and architecture search while achieving competitive results on ImageNet-1k within minimal GPU time.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1405, <a href='https://arxiv.org/pdf/2507.08801.pdf' target='_blank'>https://arxiv.org/pdf/2507.08801.pdf</a></span>   <span><a href='https://github.com/alibaba-damo-academy/Lumos' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/alibaba-damo-academy/Lumos' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hangjie Yuan, Weihua Chen, Jun Cen, Hu Yu, Jingyun Liang, Shuning Chang, Zhihui Lin, Tao Feng, Pengwei Liu, Jiazheng Xing, Hao Luo, Jiasheng Tang, Fan Wang, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08801">Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Autoregressive large language models (LLMs) have unified a vast range of language tasks, inspiring preliminary efforts in autoregressive video generation. Existing autoregressive video generators either diverge from standard LLM architectures, depend on bulky external text encoders, or incur prohibitive latency due to next-token decoding. In this paper, we introduce Lumos-1, an autoregressive video generator that retains the LLM architecture with minimal architectural modifications. To inject spatiotemporal correlations in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE scheme that preserves the original textual RoPE while providing comprehensive frequency spectra and scaled 3D positions for modeling multimodal spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy that obeys intra-frame bidirectionality and inter-frame temporal causality. Based on this dependency strategy, we identify the issue of frame-wise loss imbalance caused by spatial information redundancy and solve it by proposing Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal tube masking during training with a compatible inference-time masking policy to avoid quality degradation. By using memory-efficient training techniques, we pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code and models are available at https://github.com/alibaba-damo-academy/Lumos.<br>
<span id='abs_ch'>中文摘要：Lumos-1是一种保持 准LLM架构的自回归视频生成器，通过MM-RoPE实现时空建模，并采用AR-DF解决帧间损失不平衡问题，仅用48个GPU进行高效训练即达到与主流模型相当的性能。</span><br>
<span id='abs_en'>English Summary: Lumos-1 is an autoregressive video generator that maintains the standard LLM architecture with minimal changes, incorporating MM-RoPE for spatiotemporal modeling and AR-DF to address frame-wise loss imbalance, achieving competitive performance with efficient training on just 48 GPUs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1406, <a href='https://arxiv.org/pdf/2507.08743.pdf' target='_blank'>https://arxiv.org/pdf/2507.08743.pdf</a></span>   <span><a href='https://github.com/raynbowy23/FedMeta-GeoLane.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rei Tamaru, Pei Li, Bin Ran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08743">Geo-ORBIT: A Federated Digital Twin Framework for Scene-Adaptive Lane Geometry Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Digital Twins (DT) have the potential to transform traffic management and operations by creating dynamic, virtual representations of transportation systems that sense conditions, analyze operations, and support decision-making. A key component for DT of the transportation system is dynamic roadway geometry sensing. However, existing approaches often rely on static maps or costly sensors, limiting scalability and adaptability. Additionally, large-scale DTs that collect and analyze data from multiple sources face challenges in privacy, communication, and computational efficiency. To address these challenges, we introduce Geo-ORBIT (Geometrical Operational Roadway Blueprint with Integrated Twin), a unified framework that combines real-time lane detection, DT synchronization, and federated meta-learning. At the core of Geo-ORBIT is GeoLane, a lightweight lane detection model that learns lane geometries from vehicle trajectory data using roadside cameras. We extend this model through Meta-GeoLane, which learns to personalize detection parameters for local entities, and FedMeta-GeoLane, a federated learning strategy that ensures scalable and privacy-preserving adaptation across roadside deployments. Our system is integrated with CARLA and SUMO to create a high-fidelity DT that renders highway scenarios and captures traffic flows in real-time. Extensive experiments across diverse urban scenes show that FedMeta-GeoLane consistently outperforms baseline and meta-learning approaches, achieving lower geometric error and stronger generalization to unseen locations while drastically reducing communication overhead. This work lays the foundation for flexible, context-aware infrastructure modeling in DTs. The framework is publicly available at https://github.com/raynbowy23/FedMeta-GeoLane.git.<br>
<span id='abs_ch'>中文摘要：Geo-ORBIT框架通过整合实时车道检测、数字孪生同步和联邦元学 ，解决了交通数字孪生在可扩展性、隐私保护和计算效率方面的挑战，大量实验证明其性能优于现有方法。</span><br>
<span id='abs_en'>English Summary: The Geo-ORBIT framework introduces a unified approach combining real-time lane detection, digital twin synchronization, and federated meta-learning to overcome scalability, privacy, and efficiency challenges in transportation digital twins, demonstrating superior performance through extensive experiments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1407, <a href='https://arxiv.org/pdf/2507.08649.pdf' target='_blank'>https://arxiv.org/pdf/2507.08649.pdf</a></span>   <span><a href='https://github.com/Leanabell-LM/Leanabell-Prover-V2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingguang Ji, Yahui Liu, Qi Wang, Jingyuan Zhang, Yang Yue, Rui Shi, Chenxi Sun, Fuzheng Zhang, Guorui Zhou, Kun Gai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08649">Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving via Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that can produce formal theorem proofs in Lean 4, with verifier-integrated Long Chain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we continual to choose to posttrain existing strong prover models for further performance improvement. In our V2 version, we mainly upgrade the Reinforcement Learning (RL) with feedback provided by the Lean 4 verifier. Crucially, verifier feedback, such as indicating success or detailing specific errors, allows the LLM to become ``self-aware'' of the correctness of its own reasoning process and learn to reflexively correct errors. Leanabell-Prover-V2 directly optimizes LLM reasoning trajectories with multi-turn verifier interactions, together with feedback token masking for stable RL training and a simple reward strategy. Experiments show that Leanabell-Prover-V2 improves performance by 3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with DeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data and models are available at: https://github.com/Leanabell-LM/Leanabell-Prover-V2.<br>
<span id='abs_ch'>Chinese: Leanabell-Prover-V2 是一个 70 亿参数的大型语言模型，通过集成 Lean 4 验证器的反馈实现自我 错和强化学 优化，在 MiniF2F 测试集上最高提升 3.2% 的定理证明性能。</span><br>
<span id='abs_en'>English: Leanabell-Prover-V2 is a 7B LLM that enhances theorem proving in Lean 4 by integrating verifier feedback for self-aware error correction and improved reasoning through upgraded RL training, achieving performance gains of up to 3.2% on the MiniF2F test set.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1408, <a href='https://arxiv.org/pdf/2507.08546.pdf' target='_blank'>https://arxiv.org/pdf/2507.08546.pdf</a></span>   <span><a href='https://github.com/nainye/RadiomicsRetrieval' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Inye Na, Nejung Rue, Jiwon Chung, Hyunjin Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08546">RadiomicsRetrieval: A Customizable Framework for Medical Image Retrieval Using Radiomics Features</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Medical image retrieval is a valuable field for supporting clinical decision-making, yet current methods primarily support 2D images and require fully annotated queries, limiting clinical flexibility. To address this, we propose RadiomicsRetrieval, a 3D content-based retrieval framework bridging handcrafted radiomics descriptors with deep learning-based embeddings at the tumor level. Unlike existing 2D approaches, RadiomicsRetrieval fully exploits volumetric data to leverage richer spatial context in medical images. We employ a promptable segmentation model (e.g., SAM) to derive tumor-specific image embeddings, which are aligned with radiomics features extracted from the same tumor via contrastive learning. These representations are further enriched by anatomical positional embedding (APE). As a result, RadiomicsRetrieval enables flexible querying based on shape, location, or partial feature sets. Extensive experiments on both lung CT and brain MRI public datasets demonstrate that radiomics features significantly enhance retrieval specificity, while APE provides global anatomical context essential for location-based searches. Notably, our framework requires only minimal user prompts (e.g., a single point), minimizing segmentation overhead and supporting diverse clinical scenarios. The capability to query using either image embeddings or selected radiomics attributes highlights its adaptability, potentially benefiting diagnosis, treatment planning, and research on large-scale medical imaging repositories. Our code is available at https://github.com/nainye/RadiomicsRetrieval.<br>
<span id='abs_ch'>中文摘要：RadiomicsRetrieval提出了一种三维医学图像检索框架，通过对比学 将影像组学特征与深度学 嵌入相结合，仅需少量 记即可实现基于肿瘤特征的灵活检索，在肺部和脑部影像数据上显著优于现有二维方法。</span><br>
<span id='abs_en'>English Summary: RadiomicsRetrieval introduces a 3D medical image retrieval framework that combines radiomics features with deep learning embeddings through contrastive learning, enabling flexible tumor-level queries using minimal prompts while outperforming existing 2D methods across lung and brain imaging datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1409, <a href='https://arxiv.org/pdf/2507.08340.pdf' target='_blank'>https://arxiv.org/pdf/2507.08340.pdf</a></span>   <span><a href='https://github.com/HopkinsKwong/MCCSDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia-Xuan Jiang, Jiashuai Liu, Hongtao Wu, Yifeng Wu, Zhong Wang, Qi Bi, Yefeng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08340">Single Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Deep learning has shown remarkable performance in integrating multimodal data for survival prediction. However, existing multimodal methods mainly focus on single cancer types and overlook the challenge of generalization across cancers. In this work, we are the first to reveal that multimodal prognosis models often generalize worse than unimodal ones in cross-cancer scenarios, despite the critical need for such robustness in clinical practice. To address this, we propose a new task: Cross-Cancer Single Domain Generalization for Multimodal Prognosis, which evaluates whether models trained on a single cancer type can generalize to unseen cancers. We identify two key challenges: degraded features from weaker modalities and ineffective multimodal integration. To tackle these, we introduce two plug-and-play modules: Sparse Dirac Information Rebalancer (SDIR) and Cancer-aware Distribution Entanglement (CADE). SDIR mitigates the dominance of strong features by applying Bernoulli-based sparsification and Dirac-inspired stabilization to enhance weaker modality signals. CADE, designed to synthesize the target domain distribution, fuses local morphological cues and global gene expression in latent space. Experiments on a four-cancer-type benchmark demonstrate superior generalization, laying the foundation for practical, robust cross-cancer multimodal prognosis. Code is available at https://github.com/HopkinsKwong/MCCSDG<br>
<br>
<div id='section'>Paperid: <span id='pid'>1410, <a href='https://arxiv.org/pdf/2507.08340.pdf' target='_blank'>https://arxiv.org/pdf/2507.08340.pdf</a></span>   <span><a href='https://github.com/HopkinsKwong/MCCSDG' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jia-Xuan Jiang, Jiashuai Liu, Hongtao Wu, Yifeng Wu, Zhong Wang, Qi Bi, Yefeng Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08340">Single Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Deep learning has shown remarkable performance in integrating multimodal data for survival prediction. However, existing multimodal methods mainly focus on single cancer types and overlook the challenge of generalization across cancers. In this work, we are the first to reveal that multimodal prognosis models often generalize worse than unimodal ones in cross-cancer scenarios, despite the critical need for such robustness in clinical practice. To address this, we propose a new task: Cross-Cancer Single Domain Generalization for Multimodal Prognosis, which evaluates whether models trained on a single cancer type can generalize to unseen cancers. We identify two key challenges: degraded features from weaker modalities and ineffective multimodal integration. To tackle these, we introduce two plug-and-play modules: Sparse Dirac Information Rebalancer (SDIR) and Cancer-aware Distribution Entanglement (CADE). SDIR mitigates the dominance of strong features by applying Bernoulli-based sparsification and Dirac-inspired stabilization to enhance weaker modality signals. CADE, designed to synthesize the target domain distribution, fuses local morphological cues and global gene expression in latent space. Experiments on a four-cancer-type benchmark demonstrate superior generalization, laying the foundation for practical, robust cross-cancer multimodal prognosis. Code is available at https://github.com/HopkinsKwong/MCCSDG<br>
<br>
<div id='section'>Paperid: <span id='pid'>1411, <a href='https://arxiv.org/pdf/2507.08267.pdf' target='_blank'>https://arxiv.org/pdf/2507.08267.pdf</a></span>   <span><a href='https://github.com/analokmaus/kaggle-aimo2-fast-math-r1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hiroshi Yoshihara, Taiki Yamaguchi, Yuichi Inoue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08267">A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy with SFT and Efficiency with Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Enhancing the mathematical reasoning of Large Language Models (LLMs) is a pivotal challenge in advancing AI capabilities. While Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are the dominant training paradigms, a systematic methodology for combining them to maximize both accuracy and efficiency remains largely unexplored. This paper introduces a practical and effective training recipe that strategically integrates extended SFT with RL from online inference (GRPO). We posit that these methods play complementary, not competing, roles: a prolonged SFT phase first pushes the model's accuracy to its limits, after which a GRPO phase dramatically improves token efficiency while preserving this peak performance. Our experiments reveal that extending SFT for as many as 10 epochs is crucial for performance breakthroughs, and that the primary role of GRPO in this framework is to optimize solution length. The efficacy of our recipe is rigorously validated through top-tier performance on challenging benchmarks, including a high rank among over 2,200 teams in the strictly leak-free AI Mathematical Olympiad (AIMO). This work provides the community with a battle-tested blueprint for developing state-of-the-art mathematical reasoners that are both exceptionally accurate and practically efficient. To ensure full reproducibility and empower future research, we will open-source our entire framework, including all code, model checkpoints, and training configurations at https://github.com/analokmaus/kaggle-aimo2-fast-math-r1.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1412, <a href='https://arxiv.org/pdf/2507.08153.pdf' target='_blank'>https://arxiv.org/pdf/2507.08153.pdf</a></span>   <span><a href='https://github.com/PinakiPrasad12/ALCo-FM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pinaki Prasad Guha Neogi, Ahmad Mohammadshirazi, Rajiv Ramnath
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08153">ALCo-FM: Adaptive Long-Context Foundation Model for Accident Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Traffic accidents are rare, yet high-impact events that require long-context multimodal reasoning for accurate risk forecasting. In this paper, we introduce ALCo-FM, a unified adaptive long-context foundation model that computes a volatility pre-score to dynamically select context windows for input data and encodes and fuses these multimodal data via shallow cross attention. Following a local GAT layer and a BigBird-style sparse global transformer over H3 hexagonal grids, coupled with Monte Carlo dropout for confidence, the model yields superior, well-calibrated predictions. Trained on data from 15 US cities with a class-weighted loss to counter label imbalance, and fine-tuned with minimal data on held-out cities, ALCo-FM achieves 0.94 accuracy, 0.92 F1, and an ECE of 0.04, outperforming more than 20 state-of-the-art baselines in large-scale urban risk prediction. Code and dataset are available at: https://github.com/PinakiPrasad12/ALCo-FM<br>
<span id='abs_ch'>中文：ALCo-FM是一种自适应长上下文基础模型，能动态选择多模态数据并通过交叉注意力融合，以最少微调在城市风险预测中实现卓越准确性和 准度。</span><br>
<span id='abs_en'>English: ALCo-FM is an adaptive long-context foundation model that dynamically selects multimodal data and fuses them through cross attention, achieving superior accuracy and calibration in urban risk prediction with minimal fine-tuning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1413, <a href='https://arxiv.org/pdf/2507.08028.pdf' target='_blank'>https://arxiv.org/pdf/2507.08028.pdf</a></span>   <span><a href='https://github.com/dolphin-in-a-coma/sssumo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Evgenii Rudakov, Jonathan Shock, Otto Lappi, Benjamin Ultan Cowley
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08028">SSSUMO: Real-Time Semi-Supervised Submovement Decomposition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper introduces a SSSUMO, semi-supervised deep learning approach for submovement decomposition that achieves state-of-the-art accuracy and speed. While submovement analysis offers valuable insights into motor control, existing methods struggle with reconstruction accuracy, computational cost, and validation, due to the difficulty of obtaining hand-labeled data. We address these challenges using a semi-supervised learning framework. This framework learns from synthetic data, initially generated from minimum-jerk principles and then iteratively refined through adaptation to unlabeled human movement data. Our fully convolutional architecture with differentiable reconstruction significantly surpasses existing methods on both synthetic and diverse human motion datasets, demonstrating robustness even in high-noise conditions. Crucially, the model operates in real-time (less than a millisecond per input second), a substantial improvement over optimization-based techniques. This enhanced performance facilitates new applications in human-computer interaction, rehabilitation medicine, and motor control studies. We demonstrate the model's effectiveness across diverse human-performed tasks such as steering, rotation, pointing, object moving, handwriting, and mouse-controlled gaming, showing notable improvements particularly on challenging datasets where traditional methods largely fail. Training and benchmarking source code, along with pre-trained model weights, are made publicly available at https://github.com/dolphin-in-a-coma/sssumo.<br>
<span id='abs_ch'>中文: 本文提出的SSSUMO是一种半监督深度学 框架，用于子运动分解，实现了顶尖的精度和实时处理能力，显著提升了人机交互和运动控制 究中的应用潜力。</span><br>
<span id='abs_en'>English: This paper presents SSSUMO, a semi-supervised deep learning method for submovement decomposition that achieves top-tier accuracy and real-time processing, enhancing applications in human-computer interaction and motor control studies.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1414, <a href='https://arxiv.org/pdf/2507.08014.pdf' target='_blank'>https://arxiv.org/pdf/2507.08014.pdf</a></span>   <span><a href='https://github.com/ACMCMC/risky-conversations' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aldan Creo, Raul Castro Fernandez, Manuel Cebrian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08014">Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As large language models (LLMs) become increasingly deployed, understanding the complexity and evolution of jailbreaking strategies is critical for AI safety.
  We present a mass-scale empirical analysis of jailbreak complexity across over 2 million real-world conversations from diverse platforms, including dedicated jailbreaking communities and general-purpose chatbots. Using a range of complexity metrics spanning probabilistic measures, lexical diversity, compression ratios, and cognitive load indicators, we find that jailbreak attempts do not exhibit significantly higher complexity than normal conversations. This pattern holds consistently across specialized jailbreaking communities and general user populations, suggesting practical bounds on attack sophistication. Temporal analysis reveals that while user attack toxicity and complexity remains stable over time, assistant response toxicity has decreased, indicating improving safety mechanisms. The absence of power-law scaling in complexity distributions further points to natural limits on jailbreak development.
  Our findings challenge the prevailing narrative of an escalating arms race between attackers and defenders, instead suggesting that LLM safety evolution is bounded by human ingenuity constraints while defensive measures continue advancing. Our results highlight critical information hazards in academic jailbreak disclosure, as sophisticated attacks exceeding current complexity baselines could disrupt the observed equilibrium and enable widespread harm before defensive adaptation.<br>
<span id='abs_ch'>中文摘要： 究发现越狱攻击的复杂度并未超出正常对话，攻击模式稳定而AI防御持续提升，这对“攻防军备竞赛升级”的普遍认知提出挑战，同时警示公开复杂攻击方法可能打 现有平衡。</span><br>
<span id='abs_en'>English Summary: The study finds jailbreak attempts show no greater complexity than normal conversations, with stable attack patterns and improving AI defenses, challenging the notion of an escalating arms race while warning against disclosing sophisticated methods that could disrupt this equilibrium.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1415, <a href='https://arxiv.org/pdf/2507.07999.pdf' target='_blank'>https://arxiv.org/pdf/2507.07999.pdf</a></span>   <span><a href='https://github.com/Haochen-Wang409/TreeVGR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haochen Wang, Xiangtai Li, Zilong Huang, Anran Wang, Jiacong Wang, Tao Zhang, Jiani Zheng, Sule Bai, Zijian Kang, Jiashi Feng, Zhuochen Wang, Zhaoxiang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07999">Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and Methodology</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically referencing visual regions, just like human "thinking with images". However, no benchmark exists to evaluate these capabilities holistically. To bridge this gap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a diagnostic benchmark built on three principles: (1) focused visual perception of subtle targets in complex scenes, (2) traceable evidence via bounding box evaluation, and (3) second-order reasoning to test object interactions and spatial hierarchies beyond simple object localization. Prioritizing images with dense objects, we initially sample 1K high-quality images from SA-1B, and incorporate eight LMM experts to manually annotate questions, candidate options, and answers for each image. After three stages of quality control, TreeBench consists of 405 challenging visual question-answering pairs, even the most advanced models struggle with this benchmark, where none of them reach 60% accuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR (Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to supervise localization and reasoning jointly with reinforcement learning, enabling accurate localizations and explainable reasoning pathways. Initialized from Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and TreeBench (+13.4), proving traceability is key to advancing vision-grounded reasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.<br>
<span id='abs_ch'>中文摘要：TreeBench是一个基于可追溯证据和复杂推理的诊断性基准，用于全面评估视觉接地推理能力，即使先进模型如OpenAI-o3在其挑战性任务中表现不佳，而提出的TreeVGR训练范式通过联合监督定位与推理，显著提升了多项基准的性能。</span><br>
<span id='abs_en'>English Summary: TreeBench is a diagnostic benchmark designed to evaluate visual grounded reasoning by focusing on traceable evidence and complex reasoning, revealing that even advanced models like OpenAI-o3 struggle with its challenging tasks, while the proposed TreeVGR training paradigm significantly improves performance by integrating localization and reasoning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1416, <a href='https://arxiv.org/pdf/2507.07995.pdf' target='_blank'>https://arxiv.org/pdf/2507.07995.pdf</a></span>   <span><a href='https://github.com/ShivamDuggal4/karl' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shivam Duggal, Sanghyun Byun, William T. Freeman, Antonio Torralba, Phillip Isola
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07995">Single-pass Adaptive Image Tokenization for Minimum Program Search</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>According to Algorithmic Information Theory (AIT) -- Intelligent representations compress data into the shortest possible program that can reconstruct its content, exhibiting low Kolmogorov Complexity (KC). In contrast, most visual representation learning systems use fixed-length representations for all inputs, ignoring variations in complexity or familiarity. Recent adaptive tokenization methods address this by allocating variable-length representations but typically require test-time search over multiple encodings to find the most predictive one. Inspired by Kolmogorov Complexity principles, we propose a single-pass adaptive tokenizer, KARL, which predicts the appropriate number of tokens for an image in a single forward pass, halting once its approximate KC is reached. The token count serves as a proxy for the minimum description length. KARL's training procedure closely resembles the Upside-Down Reinforcement Learning paradigm, as it learns to conditionally predict token halting based on a desired reconstruction quality. KARL matches the performance of recent adaptive tokenizers while operating in a single pass. We present scaling laws for KARL, analyzing the role of encoder/decoder size, continuous vs. discrete tokenization and more. Additionally, we offer a conceptual study drawing an analogy between Adaptive Image Tokenization and Algorithmic Information Theory, examining the predicted image complexity (KC) across axes such as structure vs. noise and in- vs. out-of-distribution familiarity -- revealing alignment with human intuition.<br>
<span id='abs_ch'>中文：受算法信息理论启发，KARL是一种单次前向处理的自适应分词器，能 据图像的近似柯氏复杂度动态预测最佳分词数量，在保持与多轮处理模型相当性能的同时，其复杂度预测结果与人类直觉高度吻合。</span><br>
<span id='abs_en'>English: Inspired by Algorithmic Information Theory, KARL is a single-pass adaptive tokenizer that dynamically predicts the optimal number of tokens for images based on their approximate Kolmogorov complexity, matching the performance of multi-pass methods while aligning predicted complexity with human intuition.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1417, <a href='https://arxiv.org/pdf/2507.07966.pdf' target='_blank'>https://arxiv.org/pdf/2507.07966.pdf</a></span>   <span><a href='https://github.com/NVlabs/Long-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, Song Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07966">Scaling RL to Long Videos</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 104K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves strong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on VideoMME without and with subtitles, respectively, and consistently outperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B supports processing up to 8,192 video frames per video, and configurable FPS settings. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames).<br>
<span id='abs_ch'>中文: 本文提出了一种全 框架，通过强化学 提升视觉语言模型在长视频中的推理能力，包含专用数据集、两阶段训练流程和高效基础设施，实现了卓越性能并最高提速2.1倍。</span><br>
<span id='abs_en'>English: This paper introduces a full-stack framework that enhances vision-language models' reasoning for long videos through reinforcement learning, featuring a specialized dataset, two-stage training pipeline, and efficient infrastructure, achieving superior performance and up to 2.1x training speedup.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1418, <a href='https://arxiv.org/pdf/2507.07966.pdf' target='_blank'>https://arxiv.org/pdf/2507.07966.pdf</a></span>   <span><a href='https://github.com/NVlabs/Long-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, Song Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07966">Scaling RL to Long Videos</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 104K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves strong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on VideoMME without and with subtitles, respectively, and consistently outperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B supports processing up to 8,192 video frames per video, and configurable FPS settings. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames).<br>
<span id='abs_ch'>中文: 本文提出了一种全 框架，通过强化学 提升视觉语言模型在长视频中的推理能力，包含专用数据集、两阶段训练流程和高效基础设施，实现了卓越性能并最高提速2.1倍。</span><br>
<span id='abs_en'>English: This paper introduces a full-stack framework that enhances vision-language models' reasoning for long videos through reinforcement learning, featuring a specialized dataset, two-stage training pipeline, and efficient infrastructure, achieving superior performance and up to 2.1x training speedup.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1419, <a href='https://arxiv.org/pdf/2507.07966.pdf' target='_blank'>https://arxiv.org/pdf/2507.07966.pdf</a></span>   <span><a href='https://github.com/NVlabs/Long-RL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, Song Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07966">Scaling RL to Long Videos</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 104K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves strong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on VideoMME without and with subtitles, respectively, and consistently outperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B supports processing up to 8,192 video frames per video, and configurable FPS settings. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames).<br>
<span id='abs_ch'>中文: 本文提出了一种全 框架，通过强化学 提升视觉语言模型在长视频中的推理能力，包含专用数据集、两阶段训练流程和高效基础设施，实现了卓越性能并最高提速2.1倍。</span><br>
<span id='abs_en'>English: This paper introduces a full-stack framework that enhances vision-language models' reasoning for long videos through reinforcement learning, featuring a specialized dataset, two-stage training pipeline, and efficient infrastructure, achieving superior performance and up to 2.1x training speedup.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1420, <a href='https://arxiv.org/pdf/2507.07910.pdf' target='_blank'>https://arxiv.org/pdf/2507.07910.pdf</a></span>   <span><a href='https://github.com/AdhyaSuman/DTECT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/AdhyaSuman/DTECT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Suman Adhya, Debarshi Kumar Sanyal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07910">DTECT: Dynamic Topic Explorer & Context Tracker</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The explosive growth of textual data over time presents a significant challenge in uncovering evolving themes and trends. Existing dynamic topic modeling techniques, while powerful, often exist in fragmented pipelines that lack robust support for interpretation and user-friendly exploration. We introduce DTECT (Dynamic Topic Explorer & Context Tracker), an end-to-end system that bridges the gap between raw textual data and meaningful temporal insights. DTECT provides a unified workflow that supports data preprocessing, multiple model architectures, and dedicated evaluation metrics to analyze the topic quality of temporal topic models. It significantly enhances interpretability by introducing LLM-driven automatic topic labeling, trend analysis via temporally salient words, interactive visualizations with document-level summarization, and a natural language chat interface for intuitive data querying. By integrating these features into a single, cohesive platform, DTECT empowers users to more effectively track and understand thematic dynamics. DTECT is open-source and available at https://github.com/AdhyaSuman/DTECT.<br>
<span id='abs_ch'>中文: DTECT是一个端到端系统，集成了数据预处理、动态主题建模及交互式功能（如大语言模型驱动的自动 注和自然语言查询），帮助用户有效追踪文本数据中的主题演变。</span><br>
<span id='abs_en'>English: DTECT is an end-to-end system that integrates data preprocessing, dynamic topic modeling, and interactive features like LLM-driven labeling and natural language querying to help users effectively track evolving themes in textual data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1421, <a href='https://arxiv.org/pdf/2507.07817.pdf' target='_blank'>https://arxiv.org/pdf/2507.07817.pdf</a></span>   <span><a href='https://github.com/kowndinya-renduchintala/WIT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anwoy Chatterjee, H S V N S Kowndinya Renduchintala, Sumit Bhatia, Tanmoy Chakraborty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07817">On the Effect of Instruction Tuning Loss on Generalization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Instruction Tuning has emerged as a pivotal post-training paradigm that enables pre-trained language models to better follow user instructions. Despite its significance, little attention has been given to optimizing the loss function used. A fundamental, yet often overlooked, question is whether the conventional auto-regressive objective - where loss is computed only on response tokens, excluding prompt tokens - is truly optimal for instruction tuning. In this work, we systematically investigate the impact of differentially weighting prompt and response tokens in instruction tuning loss, and propose Weighted Instruction Tuning (WIT) as a better alternative to conventional instruction tuning. Through extensive experiments on five language models of different families and scale, three finetuning datasets of different sizes, and five diverse evaluation benchmarks, we show that the standard instruction tuning loss often yields suboptimal performance and limited robustness to input prompt variations. We find that a low-to-moderate weight for prompt tokens coupled with a moderate-to-high weight for response tokens yields the best-performing models across settings and also serve as better starting points for the subsequent preference alignment training. These findings highlight the need to reconsider instruction tuning loss and offer actionable insights for developing more robust and generalizable models. Our code is open-sourced at https://github.com/kowndinya-renduchintala/WIT.<br>
<span id='abs_ch'>中文: 本 究提出 权指令调优（WIT），通过实验证明在损失函数中对提示词和响应词进行差异化 权，能显著提升模型性能与鲁棒性，优于 统指令调优方法。</span><br>
<span id='abs_en'>English: This research introduces Weighted Instruction Tuning (WIT), demonstrating that differentially weighting prompt and response tokens in the loss function outperforms conventional instruction tuning by enhancing model performance and robustness across diverse settings.Paperid:877,https://arxiv.org/pdf/2507.07803.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1422, <a href='https://arxiv.org/pdf/2507.07780.pdf' target='_blank'>https://arxiv.org/pdf/2507.07780.pdf</a></span>   <span><a href='https://github.com/biomedia-mira/calibration_under_shifts' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>MÃ©lanie Roschewitz, Raghav Mehta, Fabio de Sousa Ribeiro, Ben Glocker
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07780">Where are we with calibration under dataset shift in image classification?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We conduct an extensive study on the state of calibration under real-world dataset shift for image classification. Our work provides important insights on the choice of post-hoc and in-training calibration techniques, and yields practical guidelines for all practitioners interested in robust calibration under shift. We compare various post-hoc calibration methods, and their interactions with common in-training calibration strategies (e.g., label smoothing), across a wide range of natural shifts, on eight different classification tasks across several imaging domains. We find that: (i) simultaneously applying entropy regularisation and label smoothing yield the best calibrated raw probabilities under dataset shift, (ii) post-hoc calibrators exposed to a small amount of semantic out-of-distribution data (unrelated to the task) are most robust under shift, (iii) recent calibration methods specifically aimed at increasing calibration under shifts do not necessarily offer significant improvements over simpler post-hoc calibration methods, (iv) improving calibration under shifts often comes at the cost of worsening in-distribution calibration. Importantly, these findings hold for randomly initialised classifiers, as well as for those finetuned from foundation models, the latter being consistently better calibrated compared to models trained from scratch. Finally, we conduct an in-depth analysis of ensembling effects, finding that (i) applying calibration prior to ensembling (instead of after) is more effective for calibration under shifts, (ii) for ensembles, OOD exposure deteriorates the ID-shifted calibration trade-off, (iii) ensembling remains one of the most effective methods to improve calibration robustness and, combined with finetuning from foundation models, yields best calibration results overall.<br>
<span id='abs_ch'>中文摘要：本 究通过图像分类在真实数据集偏移下的 准分析发现，熵正则化与 签平滑结合能产生最佳 准概率，使用分布外数据的后处理 准方法更具鲁棒性，而集成学 与基础模型微调相结合可实现最优 准效果。</span><br>
<span id='abs_en'>English Summary: This comprehensive study on image classification calibration under real-world dataset shift reveals that combining entropy regularization with label smoothing produces the best-calibrated probabilities, while post-hoc methods using out-of-distribution data show superior robustness, with ensemble methods and foundation model fine-tuning delivering optimal calibration performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1423, <a href='https://arxiv.org/pdf/2507.07748.pdf' target='_blank'>https://arxiv.org/pdf/2507.07748.pdf</a></span>   <span><a href='https://github.com/Kilimajaro/LLMs_Meet_Law' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peizhang Shao, Linrui Xu, Jinxi Wang, Wei Zhou, Xingyu Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07748">When Large Language Models Meet Law: Dual-Lens Taxonomy, Technical Advances, and Ethical Governance</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper establishes the first comprehensive review of Large Language Models (LLMs) applied within the legal domain. It pioneers an innovative dual lens taxonomy that integrates legal reasoning frameworks and professional ontologies to systematically unify historical research and contemporary breakthroughs. Transformer-based LLMs, which exhibit emergent capabilities such as contextual reasoning and generative argumentation, surmount traditional limitations by dynamically capturing legal semantics and unifying evidence reasoning. Significant progress is documented in task generalization, reasoning formalization, workflow integration, and addressing core challenges in text processing, knowledge integration, and evaluation rigor via technical innovations like sparse attention mechanisms and mixture-of-experts architectures. However, widespread adoption of LLM introduces critical challenges: hallucination, explainability deficits, jurisdictional adaptation difficulties, and ethical asymmetry. This review proposes a novel taxonomy that maps legal roles to NLP subtasks and computationally implements the Toulmin argumentation framework, thus systematizing advances in reasoning, retrieval, prediction, and dispute resolution. It identifies key frontiers including low-resource systems, multimodal evidence integration, and dynamic rebuttal handling. Ultimately, this work provides both a technical roadmap for researchers and a conceptual framework for practitioners navigating the algorithmic future, laying a robust foundation for the next era of legal artificial intelligence. We have created a GitHub repository to index the relevant papers: https://github.com/Kilimajaro/LLMs_Meet_Law.<br>
<span id='abs_ch'>中文摘要：本文首次对大型语言模型在法律领域的应用进行全面综述，通过整合法律推理框架与专业本体提出创新分类法，系统梳理技术进展并应对幻觉、可解释性等 心挑战。</span><br>
<span id='abs_en'>English Summary: This paper presents the first comprehensive review of Large Language Models in legal applications, introducing a novel taxonomy that integrates legal reasoning with professional frameworks to systematize advances while addressing challenges like hallucination and ethical concerns.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1424, <a href='https://arxiv.org/pdf/2507.07725.pdf' target='_blank'>https://arxiv.org/pdf/2507.07725.pdf</a></span>   <span><a href='https://github.com/Dongzhijin/SDPO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhijin Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07725">Not All Preferences are What You Need for Post-Training: Selective Alignment Strategy for Preference Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Post-training alignment of large language models (LLMs) is a critical challenge, as not all tokens contribute equally to model performance. This paper introduces a selective alignment strategy that prioritizes high-impact tokens within preference pairs, leveraging token-level log-probability differences between the current policy and a reference model. By focusing on these informative tokens, our approach reduces computational overhead and enhances alignment fidelity. We further explore the role of reference model quality, demonstrating that stronger reference models significantly improve token selection accuracy and overall optimization effectiveness. Comprehensive experiments on benchmarks such as Arena-Hard and MT-Bench validate the superiority of our Selective-DPO method over standard DPO and distillation-based baselines. Our findings highlight the importance of token-level optimization and reference model selection in advancing preference alignment for LLMs. The code is available at https://github.com/Dongzhijin/SDPO.<br>
<span id='abs_ch'>中文摘要：本文提出一种针对大语言模型的选择性对齐策略，通过利用当前策略与参考模型之间的词元级对数概率差异来优化高影响力词元，在降低计算成本的同时借助更优的参考模型提升对齐效果。</span><br>
<span id='abs_en'>English Summary: This paper proposes a selective alignment strategy for large language models that optimizes high-impact tokens using token-level log-probability differences, reducing computational costs while improving alignment performance through better reference model selection.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1425, <a href='https://arxiv.org/pdf/2507.07622.pdf' target='_blank'>https://arxiv.org/pdf/2507.07622.pdf</a></span>   <span><a href='https://github.com/MedMaxLab/transformeeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Federico Del Pup, Riccardo Brun, Filippo Iotti, Edoardo Paccagnella, Mattia Pezzato, Sabrina Bertozzo, Andrea Zanola, Louis Fabrice Tshimanga, Henning MÃ¼ller, Manfredo Atzori
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07622">TransformEEG: Towards Improving Model Generalizability in Deep Learning-based EEG Parkinson's Disease Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Electroencephalography (EEG) is establishing itself as an important, low-cost, noninvasive diagnostic tool for the early detection of Parkinson's Disease (PD). In this context, EEG-based Deep Learning (DL) models have shown promising results due to their ability to discover highly nonlinear patterns within the signal. However, current state-of-the-art DL models suffer from poor generalizability caused by high inter-subject variability. This high variability underscores the need for enhancing model generalizability by developing new architectures better tailored to EEG data. This paper introduces TransformEEG, a hybrid Convolutional-Transformer designed for Parkinson's disease detection using EEG data. Unlike transformer models based on the EEGNet structure, TransformEEG incorporates a depthwise convolutional tokenizer. This tokenizer is specialized in generating tokens composed by channel-specific features, which enables more effective feature mixing within the self-attention layers of the transformer encoder. To evaluate the proposed model, four public datasets comprising 290 subjects (140 PD patients, 150 healthy controls) were harmonized and aggregated. A 10-outer, 10-inner Nested-Leave-N-Subjects-Out (N-LNSO) cross-validation was performed to provide an unbiased comparison against seven other consolidated EEG deep learning models. TransformEEG achieved the highest balanced accuracy's median (78.45%) as well as the lowest interquartile range (6.37%) across all the N-LNSO partitions. When combined with data augmentation and threshold correction, median accuracy increased to 80.10%, with an interquartile range of 5.74%. In conclusion, TransformEEG produces more consistent and less skewed results. It demonstrates a substantial reduction in variability and more reliable PD detection using EEG data compared to the other investigated models.<br>
<span id='abs_ch'>中文: 脑电图结合深度学 为帕金森病早期检测提供了有前景的 创方法，但现有模型 受试者间高变异性面临泛化挑战。TransformEEG这一新型混合卷积-Transformer架构通过生成通道特定令牌改进特征融合，在多个数据集上相比现有模型实现了更高的准确率和稳定性。</span><br>
<span id='abs_en'>English: Electroencephalography (EEG) combined with deep learning offers a promising non-invasive method for early Parkinson's Disease detection, though current models face generalizability challenges due to high inter-subject variability. TransformEEG, a novel hybrid Convolutional-Transformer architecture, addresses this by generating channel-specific tokens for improved feature mixing, achieving superior accuracy and consistency across multiple datasets compared to existing models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1426, <a href='https://arxiv.org/pdf/2507.07417.pdf' target='_blank'>https://arxiv.org/pdf/2507.07417.pdf</a></span>   <span><a href='https://github.com/nishitvp/better_opts_attacks' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nishit V. Pandya, Andrey Labunets, Sicun Gao, Earlence Fernandes
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07417">May I have your Attention? Breaking Fine-Tuning based Prompt Injection Defenses using Architecture-Aware Attacks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>A popular class of defenses against prompt injection attacks on large language models (LLMs) relies on fine-tuning the model to separate instructions and data, so that the LLM does not follow instructions that might be present with data. There are several academic systems and production-level implementations of this idea. We evaluate the robustness of this class of prompt injection defenses in the whitebox setting by constructing strong optimization-based attacks and showing that the defenses do not provide the claimed security properties. Specifically, we construct a novel attention-based attack algorithm for text-based LLMs and apply it to two recent whitebox defenses SecAlign (CCS 2025) and StruQ (USENIX Security 2025), showing attacks with success rates of up to 70% with modest increase in attacker budget in terms of tokens. Our findings make fundamental progress towards understanding the robustness of prompt injection defenses in the whitebox setting. We release our code and attacks at https://github.com/nishitvp/better_opts_attacks<br>
<span id='abs_ch'>中文: 本 究通过开发基于优化的攻击方法评估大型语言模型中提示注入防御的鲁棒性，证明现有方法 法提供足够安全性，对近期防御措施的攻击成功率高达70%。</span><br>
<span id='abs_en'>English: This study evaluates the robustness of prompt injection defenses in large language models by developing optimization-based attacks, demonstrating that existing methods fail to provide adequate security with success rates reaching 70% against recent defenses.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1427, <a href='https://arxiv.org/pdf/2507.07399.pdf' target='_blank'>https://arxiv.org/pdf/2507.07399.pdf</a></span>   <span><a href='https://github.com/XiaoyangLiu-sjtu/GTED' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuntian Liu, Tao Zhu, Xiaoyang Liu, Yu Chen, Zhaoxuan Liu, Qingfeng Guo, Jiashuo Zhang, Kangjie Bao, Tao Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07399">Generalized Tree Edit Distance (GTED): A Faithful Evaluation Metric for Statement Autoformalization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Statement autoformalization, the automated translation of statements from natural language into formal languages, has become a subject of extensive research, yet the development of robust automated evaluation metrics remains limited. Existing evaluation methods often lack semantic understanding, face challenges with high computational costs, and are constrained by the current progress of automated theorem proving. To address these issues, we propose GTED (Generalized Tree Edit Distance), a novel evaluation framework that first standardizes formal statements and converts them into operator trees, then determines the semantic similarity using the eponymous GTED metric. Across the miniF2F and ProofNet benchmarks, GTED consistently ranks as a top-performing metric, achieving the highest accuracy and Kappa on miniF2F and the joint-highest accuracy on ProofNet. This strong overall performance provides the community with a computationally lightweight and more faithful metric for automated evaluation. The code and experimental results are available at https://github.com/XiaoyangLiu-sjtu/GTED.<br>
<span id='abs_ch'>Chinese: 本文提出GTED这一新型评估框架，通过将形式化语句 准化为运算符 并测量语义相似性，解决了自动形式化评估中的现有局限，在基准测试中表现优异且计算效率高。</span><br>
<span id='abs_en'>English: The paper introduces GTED, a novel evaluation framework that addresses limitations in autoformalization by standardizing formal statements into operator trees and measuring semantic similarity, achieving top performance on benchmarks while being computationally efficient.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1428, <a href='https://arxiv.org/pdf/2507.07306.pdf' target='_blank'>https://arxiv.org/pdf/2507.07306.pdf</a></span>   <span><a href='https://github.com/pigeonai-org/ViDove' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichen Lu, Wei Dai, Jiaen Liu, Ching Wing Kwok, Zongheng Wu, Xudong Xiao, Ao Sun, Sheng Fu, Jianyuan Zhan, Yian Wang, Takatomo Saito, Sicheng Lai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07306">ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>LLM-based translation agents have achieved highly human-like translation results and are capable of handling longer and more complex contexts with greater efficiency. However, they are typically limited to text-only inputs. In this paper, we introduce ViDove, a translation agent system designed for multimodal input. Inspired by the workflow of human translators, ViDove leverages visual and contextual background information to enhance the translation process. Additionally, we integrate a multimodal memory system and long-short term memory modules enriched with domain-specific knowledge, enabling the agent to perform more accurately and adaptively in real-world scenarios. As a result, ViDove achieves significantly higher translation quality in both subtitle generation and general translation tasks, with a 28% improvement in BLEU scores and a 15% improvement in SubER compared to previous state-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark for long-form automatic video subtitling and translation, featuring 17 hours of high-quality, human-annotated data. Our code is available here: https://github.com/pigeonai-org/ViDove<br>
<span id='abs_ch'>中文: ViDove是一种多模态翻译系统，通过结合视觉与上下文信息显著提升翻译质量，在BLEU和SubER指 上分别比现有最优方法提高28%和15%，并推出了长视频翻译评估基准DoveBench。</span><br>
<span id='abs_en'>English: ViDove is a multimodal translation agent that integrates visual and contextual information, significantly improving translation quality by 28% in BLEU scores and 15% in SubER over previous methods, while introducing DoveBench for long-form video translation evaluation.Paperid:907,https://arxiv.org/pdf/2507.07284.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1429, <a href='https://arxiv.org/pdf/2507.07257.pdf' target='_blank'>https://arxiv.org/pdf/2507.07257.pdf</a></span>   <span><a href='https://github.com/CMBAgents/cmbagent' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Licong Xu, Milind Sarkar, Anto I. Lonappan, ÃÃ±igo Zubeldia, Pablo Villanueva-Domingo, Santiago Casas, Christian Fidler, Chetana Amancharla, Ujjwal Tiwari, Adrian Bayer, Chadi Ait Ekioui, Miles Cranmer, Adrian Dimitrov, James Fergusson, Kahaan Gandhi, Sven Krippendorf, Andrew Laverick, Julien Lesgourgues, Antony Lewis, Thomas Meier, Blake Sherwin, Kristen Surrao, Francisco Villaescusa-Navarro, Chi Wang, Xueqing Xu, Boris Bolliet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07257">Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present a multi-agent system for automation of scientific research tasks, cmbagent (https://github.com/CMBAgents/cmbagent). The system is formed by about 30 Large Language Model (LLM) agents and implements a Planning & Control strategy to orchestrate the agentic workflow, with no human-in-the-loop at any point. Each agent specializes in a different task (performing retrieval on scientific papers and codebases, writing code, interpreting results, critiquing the output of other agents) and the system is able to execute code locally. We successfully apply cmbagent to carry out a PhD level cosmology task (the measurement of cosmological parameters using supernova data) and evaluate its performance on two benchmark sets, finding superior performance over state-of-the-art LLMs. The source code is available on GitHub, demonstration videos are also available, and the system is deployed on HuggingFace and will be available on the cloud.<br>
<span id='abs_ch'>中文: 我们推出cmbagent，这是一个由约30个专业大语言模型代理组成的全自动多智能体系统，能够协作执行科 任务，成功完成博士级别的宇宙学 究且性能优于顶尖大语言模型。</span><br>
<span id='abs_en'>English: We introduce cmbagent, a fully autonomous multi-agent system with approximately 30 specialized LLM agents that collaboratively execute scientific research tasks, successfully completing a PhD-level cosmology study with superior performance to leading LLMs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1430, <a href='https://arxiv.org/pdf/2507.07236.pdf' target='_blank'>https://arxiv.org/pdf/2507.07236.pdf</a></span>   <span><a href='https://github.com/LARK-NLP-Lab/MUSE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maya Kruse, Majid Afshar, Saksham Khatwani, Anoop Mayampurath, Guanhua Chen, Yanjun Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07236">Simple Yet Effective: An Information-Theoretic Approach to Multi-LLM Uncertainty Quantification</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) often behave inconsistently across inputs, indicating uncertainty and motivating the need for its quantification in high-stakes settings. Prior work on calibration and uncertainty quantification often focuses on individual models, overlooking the potential of model diversity. We hypothesize that LLMs make complementary predictions due to differences in training and the Zipfian nature of language, and that aggregating their outputs leads to more reliable uncertainty estimates. To leverage this, we propose MUSE (Multi-LLM Uncertainty via Subset Ensembles), a simple information-theoretic method that uses Jensen-Shannon Divergence to identify and aggregate well-calibrated subsets of LLMs. Experiments on binary prediction tasks demonstrate improved calibration and predictive performance compared to single-model and naÃ¯ve ensemble baselines. In addition, we explore using MUSE as guided signals with chain-of-thought distillation to fine-tune LLMs for calibration. MUSE is available at:https://github.com/LARK-NLP-Lab/MUSE.<br>
<span id='abs_ch'>Chinese Summary: 本 究提出MUSE方法，通过利用模型多 性来识别和整合 准良好的子集，从而改进大语言模型的不确定性量化，显著提升了 准效果和预测性能。</span><br>
<span id='abs_en'>English Summary: The study introduces MUSE, a method that leverages model diversity to improve uncertainty quantification in large language models by identifying and aggregating well-calibrated subsets, resulting in enhanced calibration and predictive performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1431, <a href='https://arxiv.org/pdf/2507.07155.pdf' target='_blank'>https://arxiv.org/pdf/2507.07155.pdf</a></span>   <span><a href='https://github.com/CMBAgents/scirag' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/CMBAgents/cmbagent,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xueqing Xu, Boris Bolliet, Adrian Dimitrov, Andrew Laverick, Francisco Villaescusa-Navarro, Licong Xu, ÃÃ±igo Zubeldia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07155">Evaluating Retrieval-Augmented Generation Agents for Autonomous Scientific Discovery in Astrophysics</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We evaluate 9 Retrieval Augmented Generation (RAG) agent configurations on 105 Cosmology Question-Answer (QA) pairs that we built specifically for this purpose.The RAG configurations are manually evaluated by a human expert, that is, a total of 945 generated answers were assessed. We find that currently the best RAG agent configuration is with OpenAI embedding and generative model, yielding 91.4\% accuracy. Using our human evaluation results we calibrate LLM-as-a-Judge (LLMaaJ) system which can be used as a robust proxy for human evaluation. These results allow us to systematically select the best RAG agent configuration for multi-agent system for autonomous scientific discovery in astrophysics (e.g., cmbagent presented in a companion paper) and provide us with an LLMaaJ system that can be scaled to thousands of cosmology QA pairs. We make our QA dataset, human evaluation results, RAG pipelines, and LLMaaJ system publicly available for further use by the astrophysics community.<br>
<span id='abs_ch'>中文: 本 究评估了九种RAG智能体在定制宇宙学问答数据集上的表现，确定OpenAI模型以91.4%准确率最优，并开发了经过 准的大语言模型评审系统，为天体物理学 究提供可扩展的评估方案与最优智能体选择。</span><br>
<span id='abs_en'>English: This study evaluates nine RAG agent configurations on a custom cosmology QA dataset, identifying OpenAI's model as the top performer with 91.4% accuracy and developing a calibrated LLM-as-a-Judge system to enable scalable evaluation and optimal agent selection for astrophysics research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1432, <a href='https://arxiv.org/pdf/2507.07126.pdf' target='_blank'>https://arxiv.org/pdf/2507.07126.pdf</a></span>   <span><a href='https://github.com/XinglongLiang08/DpDNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinglong Liang, Jiaju Huang, Luyi Han, Tianyu Zhang, Xin Wang, Yuan Gao, Chunyao Lu, Lishan Cai, Tao Tan, Ritse Mann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07126">DpDNet: An Dual-Prompt-Driven Network for Universal PET-CT Segmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>PET-CT lesion segmentation is challenging due to noise sensitivity, small and variable lesion morphology, and interference from physiological high-metabolic signals. Current mainstream approaches follow the practice of one network solving the segmentation of multiple cancer lesions by treating all cancers as a single task. However, this overlooks the unique characteristics of different cancer types. Considering the specificity and similarity of different cancers in terms of metastatic patterns, organ preferences, and FDG uptake intensity, we propose DpDNet, a Dual-Prompt-Driven network that incorporates specific prompts to capture cancer-specific features and common prompts to retain shared knowledge. Additionally, to mitigate information forgetting caused by the early introduction of prompts, prompt-aware heads are employed after the decoder to adaptively handle multiple segmentation tasks. Experiments on a PET-CT dataset with four cancer types show that DpDNet outperforms state-of-the-art models. Finally, based on the segmentation results, we calculated MTV, TLG, and SUVmax for breast cancer survival analysis. The results suggest that DpDNet has the potential to serve as a valuable tool for personalized risk stratification, supporting clinicians in optimizing treatment strategies and improving outcomes. Code is available at https://github.com/XinglongLiang08/DpDNet.<br>
<span id='abs_ch'>中文: DpDNet通过双提示驱动网络，结合特定提示捕捉癌症特征与通用提示保留共享知识，在PET-CT病灶分割中优于现有模型，并具备个性化风险分层的临床应用潜力。</span><br>
<span id='abs_en'>English: DpDNet, a dual-prompt-driven network, effectively segments PET-CT lesions by capturing cancer-specific features and shared knowledge, outperforming existing models and demonstrating potential for personalized risk stratification in clinical applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1433, <a href='https://arxiv.org/pdf/2507.07108.pdf' target='_blank'>https://arxiv.org/pdf/2507.07108.pdf</a></span>   <span><a href='https://github.com/zhiweihu1103/MEL-MMoE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei Hu, VÃ­ctor GutiÃ©rrez-Basulto, Zhiliang Xiang, Ru Li, Jeff Z. Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07108">Multi-level Mixture of Experts for Multimodal Entity Linking</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multimodal Entity Linking (MEL) aims to link ambiguous mentions within multimodal contexts to associated entities in a multimodal knowledge base. Existing approaches to MEL introduce multimodal interaction and fusion mechanisms to bridge the modality gap and enable multi-grained semantic matching. However, they do not address two important problems: (i) mention ambiguity, i.e., the lack of semantic content caused by the brevity and omission of key information in the mention's textual context; (ii) dynamic selection of modal content, i.e., to dynamically distinguish the importance of different parts of modal information. To mitigate these issues, we propose a Multi-level Mixture of Experts (MMoE) model for MEL. MMoE has four components: (i) the description-aware mention enhancement module leverages large language models to identify the WikiData descriptions that best match a mention, considering the mention's textual context; (ii) the multimodal feature extraction module adopts multimodal feature encoders to obtain textual and visual embeddings for both mentions and entities; (iii)-(iv) the intra-level mixture of experts and inter-level mixture of experts modules apply a switch mixture of experts mechanism to dynamically and adaptively select features from relevant regions of information. Extensive experiments demonstrate the outstanding performance of MMoE compared to the state-of-the-art. MMoE's code is available at: https://github.com/zhiweihu1103/MEL-MMoE.<br>
<span id='abs_ch'>中文摘要：本 究提出的多级专家混合模型通过利用大型语言模型和自适应特征选择机制，解决了多模态实体链接中的指称歧义和模态内容动态选择问题，实验证明其性能优于现有先进方法。</span><br>
<span id='abs_en'>English Summary: The proposed Multi-level Mixture of Experts (MMoE) model addresses mention ambiguity and dynamic modality selection in Multimodal Entity Linking by leveraging large language models and adaptive feature selection mechanisms, demonstrating superior performance over existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1434, <a href='https://arxiv.org/pdf/2507.06996.pdf' target='_blank'>https://arxiv.org/pdf/2507.06996.pdf</a></span>   <span><a href='https://github.com/eunbyeol-cho/RawMed' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Eunbyeol Cho, Jiyoun Kim, Minjae Lee, Sungjin Park, Edward Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06996">Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Electronic Health Records (EHR) are time-series relational databases that record patient interactions and medical events over time, serving as a critical resource for healthcare research and applications. However, privacy concerns and regulatory restrictions limit the sharing and utilization of such sensitive data, necessitating the generation of synthetic EHR datasets. Unlike previous EHR synthesis methods, which typically generate medical records consisting of expert-chosen features (e.g. a few vital signs or structured codes only), we introduce RawMed, the first framework to synthesize multi-table, time-series EHR data that closely resembles raw EHRs. Using text-based representation and compression techniques, RawMed captures complex structures and temporal dynamics with minimal preprocessing. We also propose a new evaluation framework for multi-table time-series synthetic EHRs, assessing distributional similarity, inter-table relationships, temporal dynamics, and privacy. Validated on two open-source EHR datasets, RawMed outperforms baseline models in fidelity and utility. The code is available at https://github.com/eunbyeol-cho/RawMed.<br>
<span id='abs_ch'>Chinese: RawMed 是一种创新框架，通过基于文本的表示和压缩技术合成多表时间序列电子健康记录，模拟原始数据，在保真度和实用性上优于基线方法，同时解决隐私问题。</span><br>
<span id='abs_en'>English: RawMed is a novel framework that synthesizes multi-table, time-series electronic health records resembling raw data using text-based representation and compression, outperforming baselines in fidelity and utility while addressing privacy concerns.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1435, <a href='https://arxiv.org/pdf/2507.06909.pdf' target='_blank'>https://arxiv.org/pdf/2507.06909.pdf</a></span>   <span><a href='https://github.com/lololo-xiao/MultiJustice-MPMCP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Jiahuan Pei, Diancheng Shui, Zhiguang Han, Xin Sun, Dawei Zhu, Xiaoyu Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06909">MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Legal judgment prediction offers a compelling method to aid legal practitioners and researchers. However, the research question remains relatively under-explored: Should multiple defendants and charges be treated separately in LJP? To address this, we introduce a new dataset namely multi-person multi-charge prediction (MPMCP), and seek the answer by evaluating the performance of several prevailing legal large language models (LLMs) on four practical legal judgment scenarios: (S1) single defendant with a single charge, (S2) single defendant with multiple charges, (S3) multiple defendants with a single charge, and (S4) multiple defendants with multiple charges. We evaluate the dataset across two LJP tasks, i.e., charge prediction and penalty term prediction. We have conducted extensive experiments and found that the scenario involving multiple defendants and multiple charges (S4) poses the greatest challenges, followed by S2, S3, and S1. The impact varies significantly depending on the model. For example, in S4 compared to S1, InternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD, while Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD. Our dataset and code are available at https://github.com/lololo-xiao/MultiJustice-MPMCP.<br>
<span id='abs_ch'>中文: 本 究提出了一个多人多罪名法律判决预测数据集，发现涉及多名被告和多项罪名的场景对法律大模型最具挑战性，且不同模型的性能差异显著。</span><br>
<span id='abs_en'>English: This study introduces a new dataset for multi-person multi-charge legal judgment prediction, finding that scenarios with multiple defendants and charges pose the greatest challenges to legal LLMs, with performance impacts varying significantly across different models.Paperid:929,https://arxiv.org/pdf/2507.06908.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1436, <a href='https://arxiv.org/pdf/2507.06908.pdf' target='_blank'>https://arxiv.org/pdf/2507.06908.pdf</a></span>   <span><a href='https://github.com/destroy-lonely/MIND' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyan Liu, Chunxiao Fan, Haoran Lou, Yuexin Wu, Kaiwei Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06908">MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid expansion of memes on social media has highlighted the urgent need for effective approaches to detect harmful content. However, traditional data-driven approaches struggle to detect new memes due to their evolving nature and the lack of up-to-date annotated data. To address this issue, we propose MIND, a multi-agent framework for zero-shot harmful meme detection that does not rely on annotated data. MIND implements three key strategies: 1) We retrieve similar memes from an unannotated reference set to provide contextual information. 2) We propose a bi-directional insight derivation mechanism to extract a comprehensive understanding of similar memes. 3) We then employ a multi-agent debate mechanism to ensure robust decision-making through reasoned arbitration. Extensive experiments on three meme datasets demonstrate that our proposed framework not only outperforms existing zero-shot approaches but also shows strong generalization across different model architectures and parameter scales, providing a scalable solution for harmful meme detection. The code is available at https://github.com/destroy-lonely/MIND.<br>
<span id='abs_ch'>中文：提出的MIND框架通过多智能体策略实现零 本有害表情包检测，结合上下文检索、双向洞察推导和辩论机制， 需 注数据即可实现卓越性能与泛化能力。</span><br>
<span id='abs_en'>English: The proposed MIND framework enables zero-shot harmful meme detection by leveraging multi-agent strategies, including context retrieval, bi-directional insight derivation, and debate mechanisms, achieving superior performance and generalization without annotated data.Paperid:930,https://arxiv.org/pdf/2507.06877.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1437, <a href='https://arxiv.org/pdf/2507.06849.pdf' target='_blank'>https://arxiv.org/pdf/2507.06849.pdf</a></span>   <span><a href='https://github.com/lab-emi/OpenDPD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhuo Wu, Ang Li, Chang Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06849">OpenDPDv2: A Unified Learning and Optimization Framework for Neural Network Digital Predistortion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Neural network (NN)-based Digital Predistortion (DPD) stands out in improving signal quality in wideband radio frequency (RF) power amplifiers (PAs) employing complex modulation. However, NN DPDs usually rely on a large number of parameters for effective linearization and can significantly contribute to the energy consumption of the digital back-end in RF systems. This paper presents OpenDPDv2, a unified framework for PA modeling, DPD learning, and model optimization to reduce power consumption while maintaining high linearization performance. The optimization techniques feature a novel DPD algorithm, TRes-DeltaGRU, alongside two energy-efficient methods. The top-performing 32-bit floating-point (FP32) TRes-DeltaGRU-DPD model achieves an Adjacent Channel Power Ratio (ACPR) of -59.4 dBc and Error Vector Magnitude (EVM) of -42.1 dBc. By exploiting fixed-point quantization and dynamic temporal sparsity of input signals and hidden neurons, the inference energy of our model can be reduced by 4.5X while still maintaining -50.3 dBc ACPR and -35.2 dB EVM with 56% temporal sparsity. This was evaluated using a TM3.1a 200 MHz bandwidth 256-QAM OFDM signal applied to a 3.5 GHz GaN Doherty RF PA. OpenDPDv2 code, datasets, and documentation are publicly accessible at: https://github.com/lab-emi/OpenDPD.<br>
<span id='abs_ch'>中文: OpenDPDv2是一个统一框架，通过TRes-DeltaGRU算法和能效优化技术，在保持宽带射频功率放大器高性能线性化的同时，显著降低了神经网络数字预失真处理的功耗。</span><br>
<span id='abs_en'>English: OpenDPDv2 is a unified framework that introduces the TRes-DeltaGRU algorithm and energy-efficient optimization techniques to significantly reduce power consumption while maintaining high linearization performance in neural network-based digital predistortion for wideband RF power amplifiers.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1438, <a href='https://arxiv.org/pdf/2507.06825.pdf' target='_blank'>https://arxiv.org/pdf/2507.06825.pdf</a></span>   <span><a href='https://github.com/strakam/generals-bots' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Matej Straka, Martin Schmid
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06825">Artificial Generals Intelligence: Mastering Generals.io with Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce a real-time strategy game environment based on Generals.io, a game with thousands of weekly active players. Our environment is fully compatible with Gymnasium and PettingZoo and is capable of running thousands of frames per second on commodity hardware. We also present a reference agent, trained with supervised pre-training and self-play, which reached the top 0.003% of the 1v1 human leaderboard after only 36 hours on a single H100 GPU. To accelerate learning, we incorporate potential-based reward shaping and memory features. Our contributions of a modular RTS benchmark and a competitive baseline agent provide an accessible yet challenging platform for advancing multi-agent reinforcement learning research. The documented code, together with examples and tutorials, is available at https://github.com/strakam/generals-bots.<br>
<span id='abs_ch'>中文: 本文基于Generals.io开发了一个实时策略游戏环境，其高性能智能体通过高效训练方法达到了顶尖人类玩家水平，为多智能体强化学  究提供了模块化基准平台。</span><br>
<span id='abs_en'>English: This paper presents a real-time strategy game environment based on Generals.io, featuring a high-performance reference agent that achieved top-tier human performance through efficient training methods, providing a modular benchmark for multi-agent reinforcement learning research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1439, <a href='https://arxiv.org/pdf/2507.06819.pdf' target='_blank'>https://arxiv.org/pdf/2507.06819.pdf</a></span>   <span><a href='https://github.com/uos-sis/quanproto' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Philipp Schlinge, Steffen Meinert, Martin Atzmueller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06819">Comprehensive Evaluation of Prototype Neural Networks</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Prototype models are an important method for explainable artificial intelligence (XAI) and interpretable machine learning. In this paper, we perform an in-depth analysis of a set of prominent prototype models including ProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive set of metrics. In addition to applying standard metrics from literature, we propose several new metrics to further complement the analysis of model interpretability. In our experimentation, we apply the set of prototype models on a diverse set of datasets including fine-grained classification, Non-IID settings and multi-label classification to further contrast the performance. Furthermore, we also provide our code as an open-source library (https://github.com/uos-sis/quanproto), which facilitates simple application of the metrics itself, as well as extensibility -- providing the option for easily adding new metrics and models.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1440, <a href='https://arxiv.org/pdf/2507.06813.pdf' target='_blank'>https://arxiv.org/pdf/2507.06813.pdf</a></span>   <span><a href='https://github.com/aimagelab/fed-mammoth' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cosimo Fiorini, Matteo Mosconi, Pietro Buzzega, Riccardo Salami, Simone Calderara
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06813">Intrinsic Training Signals for Federated Learning Aggregation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy. While existing approaches for aggregating client-specific classification heads and adapted backbone parameters require architectural modifications or loss function changes, our method uniquely leverages intrinsic training signals already available during standard optimization. We present LIVAR (Layer Importance and VARiance-based merging), which introduces: i) a variance-weighted classifier aggregation scheme using naturally emergent feature statistics, and ii) an explainability-driven LoRA merging technique based on SHAP analysis of existing update parameter patterns. Without any architectural overhead, LIVAR achieves state-of-the-art performance on multiple benchmarks while maintaining seamless integration with existing FL methods. This work demonstrates that effective model merging can be achieved solely through existing training signals, establishing a new paradigm for efficient federated model aggregation. The code is available at https://github.com/aimagelab/fed-mammoth.<br>
<span id='abs_ch'>Chinese: LIVAR提出了一种基于方差 权的分类器聚合和可解释性驱动的LoRA融合技术，通过利用现有训练信号， 需架构修改即可实现最先进的联邦学 性能。</span><br>
<span id='abs_en'>English: LIVAR introduces a variance-weighted classifier aggregation and an explainability-driven LoRA merging technique, achieving state-of-the-art federated learning performance without architectural changes by utilizing existing training signals.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1441, <a href='https://arxiv.org/pdf/2507.06782.pdf' target='_blank'>https://arxiv.org/pdf/2507.06782.pdf</a></span>   <span><a href='https://github.com/seungyoonee/TSM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>SeungYoon Han, Taeho Hwang, Sukmin Cho, Soyeong Jeong, Hoyun Song, Huije Lee, Jong C. Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06782">Temporal Information Retrieval via Time-Specifier Model Merging</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid expansion of digital information and knowledge across structured and unstructured sources has heightened the importance of Information Retrieval (IR). While dense retrieval methods have substantially improved semantic matching for general queries, they consistently underperform on queries with explicit temporal constraints--often those containing numerical expressions and time specifiers such as ``in 2015.'' Existing approaches to Temporal Information Retrieval (TIR) improve temporal reasoning but often suffer from catastrophic forgetting, leading to reduced performance on non-temporal queries. To address this, we propose Time-Specifier Model Merging (TSM), a novel method that enhances temporal retrieval while preserving accuracy on non-temporal queries. TSM trains specialized retrievers for individual time specifiers and merges them in to a unified model, enabling precise handling of temporal constraints without compromising non-temporal retrieval. Extensive experiments on both temporal and non-temporal datasets demonstrate that TSM significantly improves performance on temporally constrained queries while maintaining strong results on non-temporal queries, consistently outperforming other baseline methods. Our code is available at https://github.com/seungyoonee/TSM .<br>
<span id='abs_ch'>中文摘要：本 究提出的时间指示符模型融合（TSM）方法通过将专门化检索器融合为统一模型，在保持非时序查询检索性能的同时，显著提升了时序约束查询的处理能力。</span><br>
<span id='abs_en'>English Summary: The proposed Time-Specifier Model Merging (TSM) method effectively enhances temporal information retrieval while maintaining strong performance on non-temporal queries by merging specialized retrievers into a unified model.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1442, <a href='https://arxiv.org/pdf/2507.06654.pdf' target='_blank'>https://arxiv.org/pdf/2507.06654.pdf</a></span>   <span><a href='https://github.com/NEC-N-SOGI/msdpp' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/NEC-N-SOGI/msdpp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Naoya Sogi, Takashi Shibata, Makoto Terao, Masanori Suganuma, Takayuki Okatani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06654">MS-DPPs: Multi-Source Determinantal Point Processes for Contextual Diversity Refinement of Composite Attributes in Text to Image Retrieval</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Result diversification (RD) is a crucial technique in Text-to-Image Retrieval for enhancing the efficiency of a practical application. Conventional methods focus solely on increasing the diversity metric of image appearances. However, the diversity metric and its desired value vary depending on the application, which limits the applications of RD. This paper proposes a novel task called CDR-CA (Contextual Diversity Refinement of Composite Attributes). CDR-CA aims to refine the diversities of multiple attributes, according to the application's context. To address this task, we propose Multi-Source DPPs, a simple yet strong baseline that extends the Determinantal Point Process (DPP) to multi-sources. We model MS-DPP as a single DPP model with a unified similarity matrix based on a manifold representation. We also introduce Tangent Normalization to reflect contexts. Extensive experiments demonstrate the effectiveness of the proposed method. Our code is publicly available at https://github.com/NEC-N-SOGI/msdpp.<br>
<span id='abs_ch'>中文: 本文提出CDR-CA新任务，通过多源DPPs和切线归一化方法，实现了基于应用场景的复合属性多 性优化，实验证明该方法在文本-图像检索中具有显著效果。</span><br>
<span id='abs_en'>English: This paper introduces CDR-CA, a novel task for refining attribute diversity in text-to-image retrieval based on application context, and proposes Multi-Source DPPs with Tangent Normalization as an effective solution, validated through extensive experiments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1443, <a href='https://arxiv.org/pdf/2507.06528.pdf' target='_blank'>https://arxiv.org/pdf/2507.06528.pdf</a></span>   <span><a href='https://github.com/thu-social-network-research-group/InvestAlign' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huisheng Wang, Zhuoshi Pan, Hangjing Zhang, Mingxiao Liu, Hanqing Gao, H. Vicky Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06528">InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Aligning Large Language Models (LLMs) with investor decision-making processes under herd behavior is a critical challenge in behavioral finance, which grapples with a fundamental limitation: the scarcity of real-user data needed for Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM outputs and human behavioral patterns, its reliance on massive authentic data imposes substantial collection costs and privacy risks. We propose InvestAlign, a novel framework that constructs high-quality SFT datasets by leveraging theoretical solutions to similar and simple optimal investment problems rather than complex scenarios. Our theoretical analysis demonstrates that training LLMs with InvestAlign-generated data achieves faster parameter convergence than using real-user data, suggesting superior learning efficiency. Furthermore, we develop InvestAgent, an LLM agent fine-tuned with InvestAlign, which demonstrates significantly closer alignment to real-user data than pre-SFT models in both simple and complex investment problems. This highlights our proposed InvestAlign as a promising approach with the potential to address complex optimal investment problems and align LLMs with investor decision-making processes under herd behavior. Our code is publicly available at https://github.com/thu-social-network-research-group/InvestAlign.<br>
<span id='abs_ch'>中文摘要：InvestAlign框架通过利用简单投资问题的理论解生成高质量监督微调数据集，解决了在羊群效应下将大语言模型与投资者决策对齐的难题，相比 统方法实现了更快的参数收敛和更接近真实用户数据的对齐效果。</span><br>
<span id='abs_en'>English Summary: The InvestAlign framework addresses the challenge of aligning LLMs with investor herd behavior by generating high-quality SFT datasets from theoretical solutions to simple investment problems, achieving faster convergence and closer alignment to real-user data than traditional methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1444, <a href='https://arxiv.org/pdf/2507.06272.pdf' target='_blank'>https://arxiv.org/pdf/2507.06272.pdf</a></span>   <span><a href='https://github.com/echo840/LIRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhang Li, Biao Yang, Qiang Liu, Shuo Zhang, Zhiyin Ma, Liang Yin, Linger Deng, Yabo Sun, Yuliang Liu, Xiang Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06272">LIRA: Inferring Segmentation in Large Multi-modal Models with Local Interleaved Region Assistance</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While large multi-modal models (LMMs) demonstrate promising capabilities in segmentation and comprehension, they still struggle with two limitations: inaccurate segmentation and hallucinated comprehension. These challenges stem primarily from constraints in weak visual comprehension and a lack of fine-grained perception. To alleviate these limitations, we propose LIRA, a framework that capitalizes on the complementary relationship between visual comprehension and segmentation via two key components: (1) Semantic-Enhanced Feature Extractor (SEFE) improves object attribute inference by fusing semantic and pixel-level features, leading to more accurate segmentation; (2) Interleaved Local Visual Coupling (ILVC) autoregressively generates local descriptions after extracting local features based on segmentation masks, offering fine-grained supervision to mitigate hallucinations. Furthermore, we find that the precision of object segmentation is positively correlated with the latent related semantics of the <seg> token. To quantify this relationship and the model's potential semantic inferring ability, we introduce the Attributes Evaluation (AttrEval) dataset. Our experiments show that LIRA achieves state-of-the-art performance in both segmentation and comprehension tasks. Code will be available at https://github.com/echo840/LIRA.<br>
<span id='abs_ch'>Chinese: LIRA框架通过融合语义增强特征提取和交错局部视觉耦合，解决了大型多模态模型在分割不准确和理解幻觉方面的局限，在两项任务中均实现了最先进的性能。</span><br>
<span id='abs_en'>English: The LIRA framework addresses the limitations of inaccurate segmentation and hallucinated comprehension in large multi-modal models by integrating semantic-enhanced feature extraction and interleaved local visual coupling, achieving state-of-the-art performance in both tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1445, <a href='https://arxiv.org/pdf/2507.06265.pdf' target='_blank'>https://arxiv.org/pdf/2507.06265.pdf</a></span>   <span><a href='https://github.com/AtlasAnalyticsLab/SPARC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ali Nasiri-Sarvi, Hassan Rivaz, Mahdi S. Hosseini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06265">SPARC: Concept-Aligned Sparse Autoencoders for Cross-Model and Cross-Modal Interpretability</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Understanding how different AI models encode the same high-level concepts, such as objects or attributes, remains challenging because each model typically produces its own isolated representation. Existing interpretability methods like Sparse Autoencoders (SAEs) produce latent concepts individually for each model, resulting in incompatible concept spaces and limiting cross-model interpretability. To address this, we introduce SPARC (Sparse Autoencoders for Aligned Representation of Concepts), a new framework that learns a single, unified latent space shared across diverse architectures and modalities (e.g., vision models like DINO, and multimodal models like CLIP). SPARC's alignment is enforced through two key innovations: (1) a Global TopK sparsity mechanism, ensuring all input streams activate identical latent dimensions for a given concept; and (2) a Cross-Reconstruction Loss, which explicitly encourages semantic consistency between models. On Open Images, SPARC dramatically improves concept alignment, achieving a Jaccard similarity of 0.80, more than tripling the alignment compared to previous methods. SPARC creates a shared sparse latent space where individual dimensions often correspond to similar high-level concepts across models and modalities, enabling direct comparison of how different architectures represent identical concepts without requiring manual alignment or model-specific analysis. As a consequence of this aligned representation, SPARC also enables practical applications such as text-guided spatial localization in vision-only models and cross-model/cross-modal retrieval. Code and models are available at https://github.com/AtlasAnalyticsLab/SPARC.<br>
<span id='abs_ch'>中文: SPARC框架通过全局稀疏性和跨模型重构损失，为不同架构的AI模型创建了统一的稀疏潜在空间，显著提升了概念对齐度，实现了跨模型的直接概念比较和实际应用。</span><br>
<span id='abs_en'></span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1446, <a href='https://arxiv.org/pdf/2507.06223.pdf' target='_blank'>https://arxiv.org/pdf/2507.06223.pdf</a></span>   <span><a href='https://github.com/zhiyuanpeng/EER-FLOPs' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Peng, Ting-ruen Wei, Tingyu Song, Yilun Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06223">Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large Language Models (LLMs) have recently been applied to reranking tasks in information retrieval, achieving strong performance. However, their high computational demands often hinder practical deployment. Existing studies evaluate the efficiency of LLM-based rerankers using proxy metrics such as latency, the number of forward passes, input tokens, and output tokens. However, these metrics depend on hardware and running-time choices (\eg parallel or not, batch size, etc), and often fail to account for model size, making it difficult to interpret and obscuring the evaluation of the efficiency-effectiveness tradeoff. To address this issue, we propose \ours\footnote{https://github.com/zhiyuanpeng/EER-FLOPs.} for LLM-based rerankers: RPP (ranking metrics per PetaFLOP), measuring how much ranking quality (e.g., NDCG or MRR) a method achieves per PetaFLOP, and QPP (queries per PetaFLOP), measuring how many queries can be processed per PetaFLOP. Accompanied by the new metrics, an interpretable FLOPs estimator is developed to estimate the FLOPs of an LLM-based reranker even without running any experiments. Based on the proposed metrics, we conduct comprehensive experiments to evaluate a wide range of LLM-based rerankers with different architectures, studying the efficiency-effectiveness trade-off and bringing this issue to the attention of the research community.<br>
<span id='abs_ch'>中文: 本文针对现有评估指 在衡量基于大语言模型的排序器效率时的不足，提出了RPP和QPP两项新指 ，通过每PetaFLOP的排序质量和查询处理量来更准确地评估效率与效果的平衡，并开发了可解释的FLOPs估算器。</span><br>
<span id='abs_en'>English: This paper introduces two new efficiency metrics, RPP and QPP, to better evaluate the trade-off between performance and computational cost in LLM-based rerankers, addressing the limitations of existing proxy metrics and providing an interpretable FLOPs estimator for practical assessment.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1447, <a href='https://arxiv.org/pdf/2507.06219.pdf' target='_blank'>https://arxiv.org/pdf/2507.06219.pdf</a></span>   <span><a href='https://github.com/OpenDriveLab/AgiBot-World' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Modi Shi, Li Chen, Jin Chen, Yuxiang Lu, Chiming Liu, Guanghui Ren, Ping Luo, Di Huang, Maoqing Yao, Hongyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06219">Is Diversity All You Need for Scalable Robotic Manipulation?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Data scaling has driven remarkable success in foundation models for Natural Language Processing (NLP) and Computer Vision (CV), yet the principles of effective data scaling in robotic manipulation remain insufficiently understood. In this work, we investigate the nuanced role of data diversity in robot learning by examining three critical dimensions-task (what to do), embodiment (which robot to use), and expert (who demonstrates)-challenging the conventional intuition of "more diverse is better". Throughout extensive experiments on various robot platforms, we reveal that (1) task diversity proves more critical than per-task demonstration quantity, benefiting transfer from diverse pre-training tasks to novel downstream scenarios; (2) multi-embodiment pre-training data is optional for cross-embodiment transfer-models trained on high-quality single-embodiment data can efficiently transfer to different platforms, showing more desirable scaling property during fine-tuning than multi-embodiment pre-trained models; and (3) expert diversity, arising from individual operational preferences and stochastic variations in human demonstrations, can be confounding to policy learning, with velocity multimodality emerging as a key contributing factor. Based on this insight, we propose a distribution debiasing method to mitigate velocity ambiguity, the yielding GO-1-Pro achieves substantial performance gains of 15%, equivalent to using 2.5 times pre-training data. Collectively, these findings provide new perspectives and offer practical guidance on how to scale robotic manipulation datasets effectively.<br>
<span id='abs_ch'>中文: 本 究挑战机器人操作中"越多 越好"的 统认知，揭示任务多 性对迁移学 最为关键，单平台数据即可实现高效跨平台适应，而专家多 性会 操作速度差异干扰策略学 ，据此提出的去偏方法使性能提升15%。</span><br>
<span id='abs_en'>English: This study challenges the "more diverse is better" assumption in robotic manipulation by revealing that task diversity is most critical for transfer learning, single-embodiment data enables efficient cross-platform adaptation, and expert diversity can hinder performance due to velocity variations, leading to a debiasing method that boosts performance by 15%.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1448, <a href='https://arxiv.org/pdf/2507.06196.pdf' target='_blank'>https://arxiv.org/pdf/2507.06196.pdf</a></span>   <span><a href='https://github.com/cvs-health/uqlm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dylan Bouchard, Mohit Singh Chauhan, David Skarbrevik, Ho-Kyeong Ra, Viren Bajaj, Zeya Ahmad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06196">UQLM: A Python Package for Uncertainty Quantification in Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Hallucinations, defined as instances where Large Language Models (LLMs) generate false or misleading content, pose a significant challenge that impacts the safety and trust of downstream applications. We introduce UQLM, a Python package for LLM hallucination detection using state-of-the-art uncertainty quantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers that compute response-level confidence scores ranging from 0 to 1. This library provides an off-the-shelf solution for UQ-based hallucination detection that can be easily integrated to enhance the reliability of LLM outputs.<br>
<span id='abs_ch'>中文：UQLM是一个基于不确定性量化技术的Python工具包，通过计算置信度分数来检测大语言模型的幻觉问题，从而提升模型输出的可 性。</span><br>
<span id='abs_en'>English: UQLM is a Python toolkit that employs uncertainty quantification techniques to detect hallucinations in Large Language Models by providing confidence scores, thereby improving output reliability.</span>Paperid:976,https://arxiv.org/pdf/2507.06195.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1449, <a href='https://arxiv.org/pdf/2507.06189.pdf' target='_blank'>https://arxiv.org/pdf/2507.06189.pdf</a></span>   <span><a href='https://github.com/dsgt-arc/checkthat-2025-subject' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Maximilian Heil, Dionne Bang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06189">DS@GT at CheckThat! 2025: Detecting Subjectivity via Transfer-Learning and Corrective Data Augmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents our submission to Task 1, Subjectivity Detection, of the CheckThat! Lab at CLEF 2025. We investigate the effectiveness of transfer-learning and stylistic data augmentation to improve classification of subjective and objective sentences in English news text. Our approach contrasts fine-tuning of pre-trained encoders and transfer-learning of fine-tuned transformer on related tasks. We also introduce a controlled augmentation pipeline using GPT-4o to generate paraphrases in predefined subjectivity styles. To ensure label and style consistency, we employ the same model to correct and refine the generated samples. Results show that transfer-learning of specified encoders outperforms fine-tuning general-purpose ones, and that carefully curated augmentation significantly enhances model robustness, especially in detecting subjective content. Our official submission placed us $16^{th}$ of 24 participants. Overall, our findings underscore the value of combining encoder specialization with label-consistent augmentation for improved subjectivity detection. Our code is available at https://github.com/dsgt-arc/checkthat-2025-subject.<br>
<span id='abs_ch'>中文总结: 本 究通过迁移学 和GPT-4o风 数据增强改进英文新闻主观性检测，发现专用编 器与精选数据增强相结合能显著提升模型性能。</span><br>
<span id='abs_en'>English Summary: This study explores transfer learning and stylistic data augmentation using GPT-4o to enhance subjectivity detection in English news, finding that specialized encoders with curated data augmentation significantly improve model performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1450, <a href='https://arxiv.org/pdf/2507.06140.pdf' target='_blank'>https://arxiv.org/pdf/2507.06140.pdf</a></span>   <span><a href='https://github.com/hao1635/LangMamba' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihao Chen, Tao Chen, Chenhui Wang, Qi Gao, Huidong Xie, Chuang Niu, Ge Wang, Hongming Shan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06140">LangMamba: A Language-driven Mamba Framework for Low-dose CT Denoising with Vision-language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Low-dose computed tomography (LDCT) reduces radiation exposure but often degrades image quality, potentially compromising diagnostic accuracy. Existing deep learning-based denoising methods focus primarily on pixel-level mappings, overlooking the potential benefits of high-level semantic guidance. Recent advances in vision-language models (VLMs) suggest that language can serve as a powerful tool for capturing structured semantic information, offering new opportunities to improve LDCT reconstruction. In this paper, we introduce LangMamba, a Language-driven Mamba framework for LDCT denoising that leverages VLM-derived representations to enhance supervision from normal-dose CT (NDCT). LangMamba follows a two-stage learning strategy. First, we pre-train a Language-guided AutoEncoder (LangAE) that leverages frozen VLMs to map NDCT images into a semantic space enriched with anatomical information. Second, we synergize LangAE with two key components to guide LDCT denoising: Semantic-Enhanced Efficient Denoiser (SEED), which enhances NDCT-relevant local semantic while capturing global features with efficient Mamba mechanism, and Language-engaged Dual-space Alignment (LangDA) Loss, which ensures that denoised images align with NDCT in both perceptual and semantic spaces. Extensive experiments on two public datasets demonstrate that LangMamba outperforms conventional state-of-the-art methods, significantly improving detail preservation and visual fidelity. Remarkably, LangAE exhibits strong generalizability to unseen datasets, thereby reducing training costs. Furthermore, LangDA loss improves explainability by integrating language-guided insights into image reconstruction and offers a plug-and-play fashion. Our findings shed new light on the potential of language as a supervisory signal to advance LDCT denoising. The code is publicly available on https://github.com/hao1635/LangMamba.<br>
<span id='abs_ch'>中文: LangMamba提出了一种新颖的语言驱动框架，利用视觉语言模型增强语义指导，在低剂量CT去噪中显著提升细节保留和泛化能力，同时降低训练成本。English: LangMamba introduces a novel language-driven framework for LDCT denoising that leverages vision-language models to enhance semantic guidance, outperforming existing methods in detail preservation and generalizability while reducing training costs.Paperid:980,https://arxiv.org/pdf/2507.06121.pdfGitHub</span><br>
<span id='abs_en'>English: LangMamba introduces a novel language-driven framework for LDCT denoising that leverages vision-language models to enhance semantic guidance, outperforming existing methods in detail preservation and generalizability while reducing training costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1451, <a href='https://arxiv.org/pdf/2507.06043.pdf' target='_blank'>https://arxiv.org/pdf/2507.06043.pdf</a></span>   <span><a href='https://github.com/NLPGM/CAVGAN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaohu Li, Yunfeng Ning, Zepeng Bao, Mayi Xu, Jianhao Chen, Tieyun Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06043">CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Security alignment enables the Large Language Model (LLM) to gain the protection against malicious queries, but various jailbreak attack methods reveal the vulnerability of this security mechanism. Previous studies have isolated LLM jailbreak attacks and defenses. We analyze the security protection mechanism of the LLM, and propose a framework that combines attack and defense. Our method is based on the linearly separable property of LLM intermediate layer embedding, as well as the essence of jailbreak attack, which aims to embed harmful problems and transfer them to the safe area. We utilize generative adversarial network (GAN) to learn the security judgment boundary inside the LLM to achieve efficient jailbreak attack and defense. The experimental results indicate that our method achieves an average jailbreak success rate of 88.85\% across three popular LLMs, while the defense success rate on the state-of-the-art jailbreak dataset reaches an average of 84.17\%. This not only validates the effectiveness of our approach but also sheds light on the internal security mechanisms of LLMs, offering new insights for enhancing model security The code and data are available at https://github.com/NLPGM/CAVGAN.<br>
<span id='abs_ch'>中文摘要：本 究提出了一种基于GAN的框架，利用大语言模型中间层的线性可分嵌入特性，在提升越狱攻击成功率的同时强化防御效果，为理解模型内部安全机制提供了新视角。</span><br>
<span id='abs_en'>English Summary: This study introduces a GAN-based framework that leverages the linearly separable embeddings in LLM intermediate layers to simultaneously enhance jailbreak attack and defense, achieving high success rates and providing insights into LLM security mechanisms.Paperid:985,https://arxiv.org/pdf/2507.06021.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1452, <a href='https://arxiv.org/pdf/2507.05965.pdf' target='_blank'>https://arxiv.org/pdf/2507.05965.pdf</a></span>   <span><a href='https://github.com/lflage/OpenFActScore' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lucas Fonseca Lage, Simon Ostermann
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05965">OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce OpenFActScore, an open-source implementation of the FActScore framework for evaluating the factuality of text generated by large language models (LLMs). FActScore evaluates the factual accuracy of long-form text by using Atomic Fact Generation (AFG) to extract individual factual claims and Atomic Fact Validation (AFV) to verify each claim against a trusted knowledge source. While the original FActScore relies on closed-source and commercial models such as InstructGPT and ChatGPT, OpenFActScore enables the use of any Hugging Face-compatible model for both AFG and AFV. We provide a detailed technical overview of our implementation, highlighting design choices and modifications made to support open models. We evaluate multiple open-source LLMs on both AFG and AFV using the original FActScore benchmark, reporting BERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our results show that open models can approximate the performance of closed-source systems, with Gemma achieving the best overall performance, and our final setup obtains a 0.99 Pearson correlation with the original FActScore experiments. OpenFActScore promotes transparency, reproducibility, and cost-effective evaluation, and is available at: https://github.com/lflage/OpenFActScore.<br>
<span id='abs_ch'>Chinese: OpenFActScore 是 FActScore 框架的开源实现，支持使用任何兼容 Hugging Face 的模型来评估大语言模型生成文本的事实准确性，其性能接近闭源系统，并促进了透明且成本效益高的事实性评估。</span><br>
<span id='abs_en'>English: OpenFActScore is an open-source implementation of the FActScore framework that enables the use of any Hugging Face-compatible model for evaluating the factuality of text generated by large language models, achieving performance comparable to closed-source systems and promoting transparency and cost-effective evaluation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1453, <a href='https://arxiv.org/pdf/2507.05891.pdf' target='_blank'>https://arxiv.org/pdf/2507.05891.pdf</a></span>   <span><a href='https://github.com/RobertLeppich/REP-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Robert Leppich, Michael Stenger, AndrÃ© Bauer, Samuel Kounev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05891">Decomposing the Time Series Forecasting Pipeline: A Modular Approach for Time Series Representation, Information Extraction, and Projection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the advent of Transformers, time series forecasting has seen significant advances, yet it remains challenging due to the need for effective sequence representation, memory construction, and accurate target projection. Time series forecasting remains a challenging task, demanding effective sequence representation, meaningful information extraction, and precise future projection. Each dataset and forecasting configuration constitutes a distinct task, each posing unique challenges the model must overcome to produce accurate predictions. To systematically address these task-specific difficulties, this work decomposes the time series forecasting pipeline into three core stages: input sequence representation, information extraction and memory construction, and final target projection. Within each stage, we investigate a range of architectural configurations to assess the effectiveness of various modules, such as convolutional layers for feature extraction and self-attention mechanisms for information extraction, across diverse forecasting tasks, including evaluations on seven benchmark datasets. Our models achieve state-of-the-art forecasting accuracy while greatly enhancing computational efficiency, with reduced training and inference times and a lower parameter count. The source code is available at https://github.com/RobertLeppich/REP-Net.<br>
<span id='abs_ch'>Chinese: 本 究将时间序列预测系统分解为序列表示、记忆构建和目 投影三个阶段，在七个基准数据集上实现了最先进的预测精度，并显著提升了计算效率。</span><br>
<span id='abs_en'>English: This study systematically breaks down time series forecasting into three stages—sequence representation, memory construction, and target projection—achieving state-of-the-art accuracy and improved computational efficiency across seven benchmark datasets.</span>Paperid:993,https://arxiv.org/pdf/2507.05785.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1454, <a href='https://arxiv.org/pdf/2507.05733.pdf' target='_blank'>https://arxiv.org/pdf/2507.05733.pdf</a></span>   <span><a href='https://github.com/kechenkristin/RecLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kechen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05733">When Transformers Meet Recommenders: Integrating Self-Attentive Sequential Recommendation with Fine-Tuned LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Self-Attentive Sequential Recommendation (SASRec) effectively captures long-term user preferences by applying attention mechanisms to historical interactions. Concurrently, the rise of Large Language Models (LLMs) has motivated research into LLM-based recommendation, which leverages their powerful generalization and language understanding capabilities. However, LLMs often lack the domain-specific knowledge and collaborative signals essential for high-quality recommendations when relying solely on textual prompts. To address this limitation, this study proposes SASRecLLM, a novel framework that integrates SASRec as a collaborative encoder with an LLM fine-tuned using Low-Rank Adaptation (LoRA). The components are connected via a mapping layer to align their dimensional spaces, and three targeted training strategies are designed to optimize the hybrid architecture. Extensive experiments on multiple datasets demonstrate that SASRecLLM achieves robust and consistent improvements over strong baselines in both cold-start and warm-start scenarios. This work advances the field of LLM-based recommendation by presenting a modular and effective paradigm for fusing structured collaborative filtering with the semantic power of fine-tuned LLMs. The implementation is available on GitHub: https://github.com/kechenkristin/RecLLM<br>
<span id='abs_ch'>中文摘要：SASRecLLM创新性地通过维度对齐和专项训练策略，将SASRec的协同过滤优势与微调大语言模型的语义能力相结合，在冷启动和热启动推荐场景中均实现了优越性能。</span><br>
<span id='abs_en'>English Summary: SASRecLLM is a novel framework that integrates the collaborative filtering strengths of SASRec with the semantic capabilities of fine-tuned LLMs through dimensional alignment and specialized training strategies, achieving superior performance in both cold-start and warm-start recommendation scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1455, <a href='https://arxiv.org/pdf/2507.05707.pdf' target='_blank'>https://arxiv.org/pdf/2507.05707.pdf</a></span>   <span><a href='https://github.com/StigLidu/DualDistill' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weihua Du, Pranjal Aggarwal, Sean Welleck, Yiming Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05707">Agentic-R1: Distilled Dual-Strategy Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Current long chain-of-thought (long-CoT) models excel at mathematical reasoning but rely on slow and error-prone natural language traces. Tool-augmented agents address arithmetic via code execution, but often falter on complex logical tasks. We introduce a fine-tuning framework, DualDistill, that distills complementary reasoning strategies from multiple teachers into a unified student model. Using this approach, we train Agentic-R1, which dynamically selects the optimal strategy for each query, invoking tools for arithmetic and algorithmic problems, and using text-based reasoning for abstract ones. Our method improves accuracy across a range of tasks, including both computation-intensive and standard benchmarks, demonstrating the effectiveness of multi-strategy distillation in achieving robust and efficient reasoning. Our project is available at https://github.com/StigLidu/DualDistill<br>
<br>
<div id='section'>Paperid: <span id='pid'>1456, <a href='https://arxiv.org/pdf/2507.05675.pdf' target='_blank'>https://arxiv.org/pdf/2507.05675.pdf</a></span>   <span><a href='https://github.com/FreedomIntelligence/MedGen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongsheng Wang, Junying Chen, Ke Ji, Zhenyang Cai, Shunian Chen, Yunjin Yang, Benyou Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05675">MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advances in video generation have shown remarkable progress in open-domain settings, yet medical video generation remains largely underexplored. Medical videos are critical for applications such as clinical training, education, and simulation, requiring not only high visual fidelity but also strict medical accuracy. However, current models often produce unrealistic or erroneous content when applied to medical prompts, largely due to the lack of large-scale, high-quality datasets tailored to the medical domain. To address this gap, we introduce MedVideoCap-55K, the first large-scale, diverse, and caption-rich dataset for medical video generation. It comprises over 55,000 curated clips spanning real-world medical scenarios, providing a strong foundation for training generalist medical video generation models. Built upon this dataset, we develop MedGen, which achieves leading performance among open-source models and rivals commercial systems across multiple benchmarks in both visual quality and medical accuracy. We hope our dataset and model can serve as a valuable resource and help catalyze further research in medical video generation. Our code and data is available at https://github.com/FreedomIntelligence/MedGen<br>
<span id='abs_ch'>中文摘要：针对医学视频生成领域缺乏专业数据集的问题，我们开发了MedVideoCap-55K大规模数据集，并基于此构建的MedGen模型在视觉质量和医学准确性方面均达到了领先水平。</span><br>
<span id='abs_en'>English Summary: Medical video generation has been limited by the absence of specialized datasets, leading to the creation of MedVideoCap-55K, a comprehensive dataset that enables MedGen to achieve top-tier performance in both visual quality and medical precision.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1457, <a href='https://arxiv.org/pdf/2507.05649.pdf' target='_blank'>https://arxiv.org/pdf/2507.05649.pdf</a></span>   <span><a href='https://github.com/LabRAI/DESIGN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaixiang Zhao, Joseph Yousry Attalla, Qian Lou, Yushun Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05649">DESIGN: Encrypted GNN Inference via Server-Side Input Graph Pruning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Graph Neural Networks (GNNs) have achieved state-of-the-art performance in various graph-based learning tasks. However, enabling privacy-preserving GNNs in encrypted domains, such as under Fully Homomorphic Encryption (FHE), typically incurs substantial computational overhead, rendering real-time and privacy-preserving inference impractical. In this work, we propose DESIGN (EncrypteD GNN Inference via sErver-Side Input Graph pruNing), a novel framework for efficient encrypted GNN inference. DESIGN tackles the critical efficiency limitations of existing FHE GNN approaches, which often overlook input data redundancy and apply uniform computational strategies. Our framework achieves significant performance gains through a hierarchical optimization strategy executed entirely on the server: first, FHE-compatible node importance scores (based on encrypted degree statistics) are computed from the encrypted graph. These scores then guide a homomorphic partitioning process, generating multi-level importance masks directly under FHE. This dynamically generated mask facilitates both input graph pruning (by logically removing unimportant elements) and a novel adaptive polynomial activation scheme, where activation complexity is tailored to node importance levels. Empirical evaluations demonstrate that DESIGN substantially accelerates FHE GNN inference compared to state-of-the-art methods while maintaining competitive model accuracy, presenting a robust solution for secure graph analytics. Our implementation is publicly available at https://github.com/LabRAI/DESIGN.<br>
<span id='abs_ch'>中文摘要：DESIGN框架通过服务器端分层优化，在完全同态 密环境下动态修剪输入图数据并 据节点重要性自适应调整激活复杂度，实现了高效的 密图神经网络推理。</span><br>
<span id='abs_en'>English Summary: The DESIGN framework enables efficient encrypted Graph Neural Network inference by implementing server-side hierarchical optimization that dynamically prunes input graphs and adapts activation complexity based on node importance under Fully Homomorphic Encryption.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1458, <a href='https://arxiv.org/pdf/2507.05622.pdf' target='_blank'>https://arxiv.org/pdf/2507.05622.pdf</a></span>   <span><a href='https://github.com/shaoshuo-ss/DATABench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Shao, Yiming Li, Mengren Zheng, Zhiyang Hu, Yukun Chen, Boheng Li, Yu He, Junfeng Guo, Dacheng Tao, Zhan Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05622">DATABench: Evaluating Dataset Auditing in Deep Learning from an Adversarial Perspective</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The widespread application of Deep Learning across diverse domains hinges critically on the quality and composition of training datasets. However, the common lack of disclosure regarding their usage raises significant privacy and copyright concerns. Dataset auditing techniques, which aim to determine if a specific dataset was used to train a given suspicious model, provide promising solutions to addressing these transparency gaps. While prior work has developed various auditing methods, their resilience against dedicated adversarial attacks remains largely unexplored. To bridge the gap, this paper initiates a comprehensive study evaluating dataset auditing from an adversarial perspective. We start with introducing a novel taxonomy, classifying existing methods based on their reliance on internal features (IF) (inherent to the data) versus external features (EF) (artificially introduced for auditing). Subsequently, we formulate two primary attack types: evasion attacks, designed to conceal the use of a dataset, and forgery attacks, intending to falsely implicate an unused dataset. Building on the understanding of existing methods and attack objectives, we further propose systematic attack strategies: decoupling, removal, and detection for evasion; adversarial example-based methods for forgery. These formulations and strategies lead to our new benchmark, DATABench, comprising 17 evasion attacks, 5 forgery attacks, and 9 representative auditing methods. Extensive evaluations using DATABench reveal that none of the evaluated auditing methods are sufficiently robust or distinctive under adversarial settings. These findings underscore the urgent need for developing a more secure and reliable dataset auditing method capable of withstanding sophisticated adversarial manipulation. Code is available at https://github.com/shaoshuo-ss/DATABench.<br>
<span id='abs_ch'>中文: 本文从对抗性角度全面评估数据集审计方法，提出分类法和系统性攻击策略，揭示现有方法易受操纵的脆弱性，并建立DATABench基准，证明亟需开发更鲁棒的审计技术。</span><br>
<span id='abs_en'>English: This paper introduces a comprehensive adversarial evaluation of dataset auditing methods, proposing a taxonomy and systematic attack strategies that reveal their vulnerability to manipulation, and establishes the DATABench benchmark to demonstrate the urgent need for more robust auditing techniques.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1459, <a href='https://arxiv.org/pdf/2507.05496.pdf' target='_blank'>https://arxiv.org/pdf/2507.05496.pdf</a></span>   <span><a href='https://github.com/arandono/Cloud-Diffusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Andrew Randono
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05496">Cloud Diffusion Part 1: Theory and Motivation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion models for image generation function by progressively adding noise to an image set and training a model to separate out the signal from the noise. The noise profile used by these models is white noise -- that is, noise based on independent normal distributions at each point whose mean and variance is independent of the scale. By contrast, most natural image sets exhibit a type of scale invariance in their low-order statistical properties characterized by a power-law scaling. Consequently, natural images are closer (in a quantifiable sense) to a different probability distribution that emphasizes large scale correlations and de-emphasizes small scale correlations. These scale invariant noise profiles can be incorporated into diffusion models in place of white noise to form what we will call a ``Cloud Diffusion Model". We argue that these models can lead to faster inference, improved high-frequency details, and greater controllability. In a follow-up paper, we will build and train a Cloud Diffusion Model that uses scale invariance at a fundamental level and compare it to classic, white noise diffusion models.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1460, <a href='https://arxiv.org/pdf/2507.05455.pdf' target='_blank'>https://arxiv.org/pdf/2507.05455.pdf</a></span>   <span><a href='https://github.com/asuvarna31/modelcitizens' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashima Suvarna, Christina Chance, Karolina Naranjo, Hamid Palangi, Sophie Hao, Thomas Hartvigsen, Saadia Gabriel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05455">ModelCitizens: Representing Community Voices in Online Safety</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Automatic toxic language detection is critical for creating safe, inclusive online spaces. However, it is a highly subjective task, with perceptions of toxic language shaped by community norms and lived experience. Existing toxicity detection models are typically trained on annotations that collapse diverse annotator perspectives into a single ground truth, erasing important context-specific notions of toxicity such as reclaimed language. To address this, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K toxicity annotations across diverse identity groups. To capture the role of conversational context on toxicity, typical of social media posts, we augment MODELCITIZENS posts with LLM-generated conversational scenarios. State-of-the-art toxicity detection tools (e.g. OpenAI Moderation API, GPT-o4-mini) underperform on MODELCITIZENS, with further degradation on context-augmented posts. Finally, we release LLAMACITIZEN-8B and GEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS, which outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our findings highlight the importance of community-informed annotation and modeling for inclusive content moderation. The data, models and code are available at https://github.com/asuvarna31/modelcitizens.<br>
<span id='abs_ch'>中文: 本 究推出了MODELCITIZENS数据集，包含多 化的毒性 注，证明现有检测工具在其上表现不佳，而基于社区认知训练的模型在包容性内容审 中效果更优。</span><br>
<span id='abs_en'>English: This study introduces MODELCITIZENS, a dataset with diverse toxicity annotations, and shows that current detection tools underperform on it, while their community-informed models achieve better results for inclusive moderation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1461, <a href='https://arxiv.org/pdf/2507.05328.pdf' target='_blank'>https://arxiv.org/pdf/2507.05328.pdf</a></span>   <span><a href='https://github.com/Improbable-AI/hepo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chi-Chang Lee, Zhang-Wei Hong, Pulkit Agrawal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05328">Going Beyond Heuristics by Imposing Policy Improvement as a Constraint</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In many reinforcement learning (RL) applications, augmenting the task rewards with heuristic rewards that encode human priors about how a task should be solved is crucial for achieving desirable performance. However, because such heuristics are usually not optimal, much human effort and computational resources are wasted in carefully balancing tasks and heuristic rewards. Theoretically rigorous ways of incorporating heuristics rely on the idea of \textit{policy invariance}, which guarantees that the performance of a policy obtained by maximizing heuristic rewards is the same as the optimal policy with respect to the task reward. However, in practice, policy invariance doesn't result in policy improvement, and such methods are known to empirically perform poorly. We propose a new paradigm to mitigate reward hacking and effectively use heuristics based on the practical goal of maximizing policy improvement instead of policy improvement. Our framework, Heuristic Enhanced Policy Optimization (HEPO), effectively leverages heuristics while avoiding the pitfall of prior methods for mitigating reward hacking. HEPO achieves superior performance on standard benchmarks with well-engineered reward functions. More surprisingly, HEPO allows policy optimization to achieve good performance even when heuristics are not well-engineered and designed by non-expert humans, showcasing HEPO's ability to reduce human effort in reward design. % HEPO is a plug-and-play optimization method for leveraging heuristics in reinforcement learning. Code is available at https://github.com/Improbable-AI/hepo.<br>
<span id='abs_ch'>在强化学 中，HEPO通过专注于策略改进而非策略不变性，提供了一种有效利用启发式奖励的新方法，实现了更优性能并减少了对专家设计启发式的依赖。</span><br>
<span id='abs_en'>In reinforcement learning, HEPO offers a novel approach to effectively utilize heuristic rewards by focusing on policy improvement rather than policy invariance, achieving superior performance and reducing the need for expert-designed heuristics.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1462, <a href='https://arxiv.org/pdf/2507.05319.pdf' target='_blank'>https://arxiv.org/pdf/2507.05319.pdf</a></span>   <span><a href='https://github.com/ycycyc02/LCDS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Yuan, Xinkai Rui, Yongqi Fan, Yawei Fan, Boyang Zhong, Jiacheng Wang, Weiyan Zhang, Tong Ruan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05319">LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Despite the remarkable performance of Large Language Models (LLMs) in automated discharge summary generation, they still suffer from hallucination issues, such as generating inaccurate content or fabricating information without valid sources. In addition, electronic medical records (EMRs) typically consist of long-form data, making it challenging for LLMs to attribute the generated content to the sources. To address these challenges, we propose LCDS, a Logic-Controlled Discharge Summary generation system. LCDS constructs a source mapping table by calculating textual similarity between EMRs and discharge summaries to constrain the scope of summarized content. Moreover, LCDS incorporates a comprehensive set of logical rules, enabling it to generate more reliable silver discharge summaries tailored to different clinical fields. Furthermore, LCDS supports source attribution for generated content, allowing experts to efficiently review, provide feedback, and rectify errors. The resulting golden discharge summaries are subsequently recorded for incremental fine-tuning of LLMs. Our project and demo video are in the GitHub repository https://github.com/ycycyc02/LCDS.<br>
<span id='abs_ch'>中文: LCDS系统通过构建源 射表并整合逻辑规则，解决了大型语言模型在生成出院小结时的幻觉问题，确保内容可溯源且适应不同临床需求。</span><br>
<span id='abs_en'>English: The proposed LCDS system addresses hallucination and source attribution challenges in LLM-generated discharge summaries by using a source mapping table and logical rules to produce reliable summaries with traceable content origins.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1463, <a href='https://arxiv.org/pdf/2507.05283.pdf' target='_blank'>https://arxiv.org/pdf/2507.05283.pdf</a></span>   <span><a href='https://github.com/yuewangits/Chat2SPaT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Wang, Miao Zhou, Guijing Huang, Rui Zhuo, Chao Yi, Zhenliang Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05283">Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Pre-timed traffic signal control, commonly used for operating signalized intersections and coordinated arterials, requires tedious manual work for signaling plan creating and updating. When the time-of-day or day-of-week plans are utilized, one intersection is often associated with multiple plans, leading to further repetitive manual plan parameter inputting. To enable a user-friendly traffic signal control plan management process, this study proposes Chat2SPaT, a method to convert users' semi-structured and ambiguous descriptions on the signal control plan to exact signal phase and timing (SPaT) results, which could further be transformed into structured stage-based or ring-based plans to interact with intelligent transportation system (ITS) software and traffic signal controllers. With curated prompts, Chat2SPaT first leverages large language models' (LLMs) capability of understanding users' plan descriptions and reformulate the plan as a combination of phase sequence and phase attribute results in the json format. Based on LLM outputs, python scripts are designed to locate phases in a cycle, address nuances of traffic signal control, and finally assemble the complete traffic signal control plan. Within a chat, the pipeline can be utilized iteratively to conduct further plan editing. Experiments show that Chat2SPaT can generate plans with an accuracy of over 94% for both English and Chinese cases, using a test dataset with over 300 plan descriptions. As the first benchmark for evaluating LLMs' capability of understanding traffic signal control plan descriptions, Chat2SPaT provides an easy-to-use plan management pipeline for traffic practitioners and researchers, serving as a potential new building block for a more accurate and versatile application of LLMs in the field of ITS. The source codes, prompts and test dataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.<br>
<span id='abs_ch'>中文摘要：本 究提出Chat2SPaT方法，利用大语言模型将用户描述转化为精确的交通信号配时方案，准确率超过94%，为交通领域提供了便捷的智能管理解决方案。</span><br>
<span id='abs_en'>English Summary: This study introduces Chat2SPaT, a method using large language models to convert user descriptions into precise traffic signal plans, achieving over 94% accuracy and simplifying management for transportation systems.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1464, <a href='https://arxiv.org/pdf/2507.05275.pdf' target='_blank'>https://arxiv.org/pdf/2507.05275.pdf</a></span>   <span><a href='https://github.com/2sigmaEdTech/MAS/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weibing Zheng, Laurah Turner, Jess Kropczynski, Murat Ozer, Seth Overla, Shane Halse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05275">A Fuzzy Supervisor Agent Design for Clinical Reasoning Assistance in a Multi-Agent Educational Clinical Scenario Simulation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Assisting medical students with clinical reasoning (CR) during clinical scenario training remains a persistent challenge in medical education. This paper presents the design and architecture of the Fuzzy Supervisor Agent (FSA), a novel component for the Multi-Agent Educational Clinical Scenario Simulation (MAECSS) platform. The FSA leverages a Fuzzy Inference System (FIS) to continuously interpret student interactions with specialized clinical agents (e.g., patient, physical exam, diagnostic, intervention) using pre-defined fuzzy rule bases for professionalism, medical relevance, ethical behavior, and contextual distraction. By analyzing student decision-making processes in real-time, the FSA is designed to deliver adaptive, context-aware feedback and provides assistance precisely when students encounter difficulties. This work focuses on the technical framework and rationale of the FSA, highlighting its potential to provide scalable, flexible, and human-like supervision in simulation-based medical education. Future work will include empirical evaluation and integration into broader educational settings. More detailed design and implementation is~\href{https://github.com/2sigmaEdTech/MAS/}{open sourced here}.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1465, <a href='https://arxiv.org/pdf/2507.05211.pdf' target='_blank'>https://arxiv.org/pdf/2507.05211.pdf</a></span>   <span><a href='https://github.com/Hanzy1996/VDG-Uni3DSeg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongyan Han, Mohamed El Amine Boudjoghra, Jiahua Dong, Jinhong Wang, Rao Muhammad Anwer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05211">All in One: Visual-Description-Guided Unified Point Cloud Segmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Unified segmentation of 3D point clouds is crucial for scene understanding, but is hindered by its sparse structure, limited annotations, and the challenge of distinguishing fine-grained object classes in complex environments. Existing methods often struggle to capture rich semantic and contextual information due to limited supervision and a lack of diverse multimodal cues, leading to suboptimal differentiation of classes and instances. To address these challenges, we propose VDG-Uni3DSeg, a novel framework that integrates pre-trained vision-language models (e.g., CLIP) and large language models (LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual descriptions and reference images from the internet, our method incorporates rich multimodal cues, facilitating fine-grained class and instance separation. We further design a Semantic-Visual Contrastive Loss to align point features with multimodal queries and a Spatial Enhanced Module to model scene-wide relationships efficiently. Operating within a closed-set paradigm that utilizes multimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art results in semantic, instance, and panoptic segmentation, offering a scalable and practical solution for 3D understanding. Our code is available at https://github.com/Hanzy1996/VDG-Uni3DSeg.<br>
<span id='abs_ch'>中文: VDG-Uni3DSeg提出了一种创新框架，通过融合视觉语言和大语言模型，利用多模态线索和专门设计的模块来增强三维点云分割，在语义、实例和全景分割任务中均取得了领先性能。</span><br>
<span id='abs_en'>English: VDG-Uni3DSeg introduces a novel framework that integrates vision-language and large language models to enhance 3D point cloud segmentation by leveraging multimodal cues and specialized modules, achieving state-of-the-art performance in semantic, instance, and panoptic segmentation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1466, <a href='https://arxiv.org/pdf/2507.05162.pdf' target='_blank'>https://arxiv.org/pdf/2507.05162.pdf</a></span>   <span><a href='https://github.com/nchivar/LAID' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nicholas Chivaran, Jianbing Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05162">LAID: Lightweight AI-Generated Image Detection in Spatial and Spectral Domains</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The recent proliferation of photorealistic AI-generated images (AIGI) has raised urgent concerns about their potential misuse, particularly on social media platforms. Current state-of-the-art AIGI detection methods typically rely on large, deep neural architectures, creating significant computational barriers to real-time, large-scale deployment on platforms like social media. To challenge this reliance on computationally intensive models, we introduce LAID, the first framework -- to our knowledge -- that benchmarks and evaluates the detection performance and efficiency of off-the-shelf lightweight neural networks. In this framework, we comprehensively train and evaluate selected models on a representative subset of the GenImage dataset across spatial, spectral, and fusion image domains. Our results demonstrate that lightweight models can achieve competitive accuracy, even under adversarial conditions, while incurring substantially lower memory and computation costs compared to current state-of-the-art methods. This study offers valuable insight into the trade-off between efficiency and performance in AIGI detection and lays a foundation for the development of practical, scalable, and trustworthy detection systems. The source code of LAID can be found at: https://github.com/nchivar/LAID.<br>
<span id='abs_ch'>中文摘要：LAID框架 究表明，轻量级神经网络能以显著降低的计算成本实现与现有方法相媲美的AI生成图像检测精度，为社交媒体平台的可扩展部署提供了实用解决方案。</span><br>
<span id='abs_en'>English summary: The LAID framework demonstrates that lightweight neural networks can achieve competitive accuracy in detecting AI-generated images with significantly lower computational costs, offering a practical solution for scalable deployment on social media platforms.Paperid:1025,https://arxiv.org/pdf/2507.05154.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1467, <a href='https://arxiv.org/pdf/2507.05116.pdf' target='_blank'>https://arxiv.org/pdf/2507.05116.pdf</a></span>   <span><a href='https://github.com/LukeLIN-web/VOTE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Juyi Lin, Amir Taherin, Arash Akbari, Arman Akbari, Lei Lu, Guangyu Chen, Taskin Padir, Xiaomeng Yang, Weiwei Chen, Yiqian Li, Xue Lin, David Kaeli, Pu Zhao, Yanzhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05116">VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent large-scale Vision Language Action (VLA) models have shown superior performance in robotic manipulation tasks guided by natural language. However, current VLA models suffer from two drawbacks: (i) generation of massive tokens leading to high inference latency and increased training cost, and (ii) insufficient utilization of generated actions resulting in potential performance loss. To address these issues, we develop a training framework to finetune VLA models for generating significantly fewer action tokens with high parallelism, effectively reducing inference latency and training cost. Furthermore, we introduce an inference optimization technique with a novel voting-based ensemble strategy to combine current and previous action predictions, improving the utilization of generated actions and overall performance. Our results demonstrate that we achieve superior performance compared with state-of-the-art VLA models, achieving significantly higher success rates and 39$\times$ faster inference than OpenVLA with 46 Hz throughput on edge platforms, demonstrating practical deployability. The code is available at https://github.com/LukeLIN-web/VOTE.<br>
<span id='abs_ch'>中文摘要：本 究提出的训练框架和推理优化技术显著减少了视觉语言动作模型的行动令牌与延迟，同时通过改进行动利用提升了性能，实现了比现有最优方法更快的推理速度和更高的成功率。</span><br>
<span id='abs_en'>English summary: This study introduces a training framework and inference optimization technique that significantly reduces action tokens and latency while enhancing performance in Vision Language Action models, achieving faster inference and higher success rates than state-of-the-art methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1468, <a href='https://arxiv.org/pdf/2507.05116.pdf' target='_blank'>https://arxiv.org/pdf/2507.05116.pdf</a></span>   <span><a href='https://github.com/LukeLIN-web/VOTE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Juyi Lin, Amir Taherin, Arash Akbari, Arman Akbari, Lei Lu, Guangyu Chen, Taskin Padir, Xiaomeng Yang, Weiwei Chen, Yiqian Li, Xue Lin, David Kaeli, Pu Zhao, Yanzhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05116">VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent large-scale Vision Language Action (VLA) models have shown superior performance in robotic manipulation tasks guided by natural language. However, current VLA models suffer from two drawbacks: (i) generation of massive tokens leading to high inference latency and increased training cost, and (ii) insufficient utilization of generated actions resulting in potential performance loss. To address these issues, we develop a training framework to finetune VLA models for generating significantly fewer action tokens with high parallelism, effectively reducing inference latency and training cost. Furthermore, we introduce an inference optimization technique with a novel voting-based ensemble strategy to combine current and previous action predictions, improving the utilization of generated actions and overall performance. Our results demonstrate that we achieve superior performance compared with state-of-the-art VLA models, achieving significantly higher success rates and 39$\times$ faster inference than OpenVLA with 46 Hz throughput on edge platforms, demonstrating practical deployability. The code is available at https://github.com/LukeLIN-web/VOTE.<br>
<span id='abs_ch'>中文摘要：本 究提出的训练框架和推理优化技术显著减少了视觉语言动作模型的行动令牌与延迟，同时通过改进行动利用提升了性能，实现了比现有最优方法更快的推理速度和更高的成功率。</span><br>
<span id='abs_en'>English summary: This study introduces a training framework and inference optimization technique that significantly reduces action tokens and latency while enhancing performance in Vision Language Action models, achieving faster inference and higher success rates than state-of-the-art methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1469, <a href='https://arxiv.org/pdf/2507.05108.pdf' target='_blank'>https://arxiv.org/pdf/2507.05108.pdf</a></span>   <span><a href='https://github.com/SCUT-DLVCLab/AutoHDR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuyi Zhang, Peirong Zhang, Zhenhua Yang, Pengyu Yan, Yongxin Shi, Pengwei Liu, Fengjun Guo, Lianwen Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05108">Reviving Cultural Heritage: A Novel Approach for Comprehensive Historical Document Restoration</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Historical documents represent an invaluable cultural heritage, yet have undergone significant degradation over time through tears, water erosion, and oxidation. Existing Historical Document Restoration (HDR) methods primarily focus on single modality or limited-size restoration, failing to meet practical needs. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel automated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and 6,543 synthetic images with character-level and line-level locations, as well as character annotations in different damage grades. AutoHDR mimics historians' restoration workflows through a three-stage approach: OCR-assisted damage localization, vision-language context text prediction, and patch autoregressive appearance restoration. The modular architecture of AutoHDR enables seamless human-machine collaboration, allowing for flexible intervention and optimization at each restoration stage. Experiments demonstrate AutoHDR's remarkable performance in HDR. When processing severely damaged documents, our method improves OCR accuracy from 46.83% to 84.05%, with further enhancement to 94.25% through human-machine collaboration. We believe this work represents a significant advancement in automated historical document restoration and contributes substantially to cultural heritage preservation. The model and dataset are available at https://github.com/SCUT-DLVCLab/AutoHDR.<br>
<span id='abs_ch'>中文: 本文提出新型自动化历史文档修复系统（AutoHDR）和完整数据集（FPHDR），通过三阶段工作流程和人机协作显著提升OCR识别准确率，在文化遗产保护领域实现重要突 。English: This paper introduces a novel automated historical document restoration system (AutoHDR) and a comprehensive dataset (FPHDR), which significantly enhances OCR accuracy through a three-stage workflow and human-machine collaboration, representing a major advancement in cultural heritage preservation.Paperid:1031,https://arxiv.org/pdf/2507.05101.pdfGitHub</span><br>
<span id='abs_en'>English: This paper introduces a novel automated historical document restoration system (AutoHDR) and a comprehensive dataset (FPHDR), which significantly enhances OCR accuracy through a three-stage workflow and human-machine collaboration, representing a major advancement in cultural heritage preservation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1470, <a href='https://arxiv.org/pdf/2507.05101.pdf' target='_blank'>https://arxiv.org/pdf/2507.05101.pdf</a></span>   <span><a href='https://github.com/SophieSarceau/PRING' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinzhe Zheng, Hao Du, Fanding Xu, Jinzhe Li, Zhiyuan Liu, Wenkang Wang, Tao Chen, Wanli Ouyang, Stan Z. Li, Yan Lu, Nanqing Dong, Yang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05101">PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to Graphs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Deep learning-based computational methods have achieved promising results in predicting protein-protein interactions (PPIs). However, existing benchmarks predominantly focus on isolated pairwise evaluations, overlooking a model's capability to reconstruct biologically meaningful PPI networks, which is crucial for biology research. To address this gap, we introduce PRING, the first comprehensive benchmark that evaluates protein-protein interaction prediction from a graph-level perspective. PRING curates a high-quality, multi-species PPI network dataset comprising 21,484 proteins and 186,818 interactions, with well-designed strategies to address both data redundancy and leakage. Building on this golden-standard dataset, we establish two complementary evaluation paradigms: (1) topology-oriented tasks, which assess intra and cross-species PPI network construction, and (2) function-oriented tasks, including protein complex pathway prediction, GO module analysis, and essential protein justification. These evaluations not only reflect the model's capability to understand the network topology but also facilitate protein function annotation, biological module detection, and even disease mechanism analysis. Extensive experiments on four representative model categories, consisting of sequence similarity-based, naive sequence-based, protein language model-based, and structure-based approaches, demonstrate that current PPI models have potential limitations in recovering both structural and functional properties of PPI networks, highlighting the gap in supporting real-world biological applications. We believe PRING provides a reliable platform to guide the development of more effective PPI prediction models for the community. The dataset and source code of PRING are available at https://github.com/SophieSarceau/PRING.<br>
<span id='abs_ch'>中文: PRING是首个从图层面评估蛋白质相互作用预测的综合基准，通过多物种网络拓扑和功能特性评估，揭示了现有模型在支持实际生物应用方面的局限性。</span><br>
<span id='abs_en'>English: PRING is the first comprehensive benchmark that evaluates protein-protein interaction prediction from a graph-level perspective, addressing limitations in current models by assessing both network topology and functional properties across multiple species.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1471, <a href='https://arxiv.org/pdf/2507.05068.pdf' target='_blank'>https://arxiv.org/pdf/2507.05068.pdf</a></span>   <span><a href='https://github.com/Chrisqcwx/ImageAR-MIA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyao Yu, Yixiang Qiu, Yiheng Yang, Hao Fang, Tianqu Zhuang, Jiaxin Hong, Bin Chen, Hao Wu, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05068">ICAS: Detecting Training Data from Autoregressive Image Generative Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Autoregressive image generation has witnessed rapid advancements, with prominent models such as scale-wise visual auto-regression pushing the boundaries of visual synthesis. However, these developments also raise significant concerns regarding data privacy and copyright. In response, training data detection has emerged as a critical task for identifying unauthorized data usage in model training. To better understand the vulnerability of autoregressive image generative models to such detection, we conduct the first study applying membership inference to this domain. Our approach comprises two key components: implicit classification and an adaptive score aggregation strategy. First, we compute the implicit token-wise classification score within the query image. Then we propose an adaptive score aggregation strategy to acquire a final score, which places greater emphasis on the tokens with lower scores. A higher final score indicates that the sample is more likely to be involved in the training set. To validate the effectiveness of our method, we adapt existing detection algorithms originally designed for LLMs to visual autoregressive models. Extensive experiments demonstrate the superiority of our method in both class-conditional and text-to-image scenarios. Moreover, our approach exhibits strong robustness and generalization under various data transformations. Furthermore, sufficient experiments suggest two novel key findings: (1) A linear scaling law on membership inference, exposing the vulnerability of large foundation models. (2) Training data from scale-wise visual autoregressive models is easier to detect than other autoregressive paradigms.Our code is available at https://github.com/Chrisqcwx/ImageAR-MIA.<br>
<span id='abs_ch'>中文: 本 究首次针对自回归图像生成模型提出成员推理方法，通过隐式分类与自适应分数聚合策略有效检测训练数据滥用，并揭示大型基础模型的脆弱性及不同范式间的检测差异。</span><br>
<span id='abs_en'>English: This study introduces the first membership inference method for autoregressive image generative models, combining implicit classification with adaptive score aggregation to effectively detect unauthorized training data usage and revealing vulnerabilities in large foundation models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1472, <a href='https://arxiv.org/pdf/2507.05020.pdf' target='_blank'>https://arxiv.org/pdf/2507.05020.pdf</a></span>   <span><a href='https://github.com/CAMMA-public/MML-SurgAdapt' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Soham Walimbe, Britty Baby, Vinkle Srivastav, Nicolas Padoy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05020">Adaptation of Multi-modal Representation Models for Multi-task Surgical Computer Vision</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Surgical AI often involves multiple tasks within a single procedure, like phase recognition or assessing the Critical View of Safety in laparoscopic cholecystectomy. Traditional models, built for one task at a time, lack flexibility, requiring a separate model for each. To address this, we introduce MML-SurgAdapt, a unified multi-task framework with Vision-Language Models (VLMs), specifically CLIP, to handle diverse surgical tasks through natural language supervision. A key challenge in multi-task learning is the presence of partial annotations when integrating different tasks. To overcome this, we employ Single Positive Multi-Label (SPML) learning, which traditionally reduces annotation burden by training models with only one positive label per instance. Our framework extends this approach to integrate data from multiple surgical tasks within a single procedure, enabling effective learning despite incomplete or noisy annotations. We demonstrate the effectiveness of our model on a combined dataset consisting of Cholec80, Endoscapes2023, and CholecT50, utilizing custom prompts. Extensive evaluation shows that MML-SurgAdapt performs comparably to task-specific benchmarks, with the added advantage of handling noisy annotations. It also outperforms the existing SPML frameworks for the task. By reducing the required labels by 23%, our approach proposes a more scalable and efficient labeling process, significantly easing the annotation burden on clinicians. To our knowledge, this is the first application of SPML to integrate data from multiple surgical tasks, presenting a novel and generalizable solution for multi-task learning in surgical computer vision. Implementation is available at: https://github.com/CAMMA-public/MML-SurgAdapt<br>
<span id='abs_ch'>中文：MML-SurgAdapt提出了一种基于视觉语言模型的统一多任务框架，通过自然语言监督处理多种外科手术任务，采用单正例多 签学 方法有效应对部分 注问题，在减少23% 注需求的同时保持了与专用模型相当的性能。</span><br>
<span id='abs_en'>English: MML-SurgAdapt introduces a unified multi-task framework using Vision-Language Models to handle diverse surgical tasks with natural language supervision, effectively addressing partial annotations through Single Positive Multi-Label learning and reducing labeling requirements by 23% while maintaining performance comparable to task-specific models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1473, <a href='https://arxiv.org/pdf/2507.05007.pdf' target='_blank'>https://arxiv.org/pdf/2507.05007.pdf</a></span>   <span><a href='https://github.com/CAMMA-public/CVS-AdaptNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Britty Baby, Vinkle Srivastav, Pooja P. Jain, Kun Yuan, Pietro Mascagni, Nicolas Padoy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05007">Multi-modal Representations for Fine-grained Multi-label Critical View of Safety Recognition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The Critical View of Safety (CVS) is crucial for safe laparoscopic cholecystectomy, yet assessing CVS criteria remains a complex and challenging task, even for experts. Traditional models for CVS recognition depend on vision-only models learning with costly, labor-intensive spatial annotations. This study investigates how text can be harnessed as a powerful tool for both training and inference in multi-modal surgical foundation models to automate CVS recognition. Unlike many existing multi-modal models, which are primarily adapted for multi-class classification, CVS recognition requires a multi-label framework. Zero-shot evaluation of existing multi-modal surgical models shows a significant performance gap for this task. To address this, we propose CVS-AdaptNet, a multi-label adaptation strategy that enhances fine-grained, binary classification across multiple labels by aligning image embeddings with textual descriptions of each CVS criterion using positive and negative prompts. By adapting PeskaVLP, a state-of-the-art surgical foundation model, on the Endoscapes-CVS201 dataset, CVS-AdaptNet achieves 57.6 mAP, improving over the ResNet50 image-only baseline (51.5 mAP) by 6 points. Our results show that CVS-AdaptNet's multi-label, multi-modal framework, enhanced by textual prompts, boosts CVS recognition over image-only methods. We also propose text-specific inference methods, that helps in analysing the image-text alignment. While further work is needed to match state-of-the-art spatial annotation-based methods, this approach highlights the potential of adapting generalist models to specialized surgical tasks. Code: https://github.com/CAMMA-public/CVS-AdaptNet<br>
<span id='abs_ch'>中文: 本 究提出CVS-AdaptNet多模态框架，通过图像嵌入与文本提示的对齐，显著提升了腹腔镜胆囊切除术中安全临界视图的识别能力，相比纯图像方法将mAP提高了6个百分点。English: This study introduces CVS-AdaptNet, a multi-modal framework that enhances Critical View of Safety recognition in laparoscopic cholecystectomy by aligning image embeddings with textual prompts, achieving a 6-point mAP improvement over image-only methods.Paperid:1038,https://arxiv.org/pdf/2507.04999.pdfGitHub</span><br>
<span id='abs_en'>English: This study introduces CVS-AdaptNet, a multi-modal framework that enhances Critical View of Safety recognition in laparoscopic cholecystectomy by aligning image embeddings with textual prompts, achieving a 6-point mAP improvement over image-only methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1474, <a href='https://arxiv.org/pdf/2507.04959.pdf' target='_blank'>https://arxiv.org/pdf/2507.04959.pdf</a></span>   <span><a href='https://github.com/SynapGrid/Hear-Your-Click' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingshan Liang, Keyu Fan, Zhicheng Du, Yiran Wang, Qingyang Shi, Xinyu Zhang, Jiasheng Lu, Peiwu Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04959">Hear-Your-Click: Interactive Object-Specific Video-to-Audio Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Video-to-audio (V2A) generation shows great potential in fields such as film production. Despite significant advances, current V2A methods relying on global video information struggle with complex scenes and generating audio tailored to specific objects. To address these limitations, we introduce Hear-Your-Click, an interactive V2A framework enabling users to generate sounds for specific objects by clicking on the frame. To achieve this, we propose Object-aware Contrastive Audio-Visual Fine-tuning (OCAV) with a Mask-guided Visual Encoder (MVE) to obtain object-level visual features aligned with audio. Furthermore, we tailor two data augmentation strategies, Random Video Stitching (RVS) and Mask-guided Loudness Modulation (MLM), to enhance the model's sensitivity to segmented objects. To measure audio-visual correspondence, we designed a new evaluation metric, the CAV score. Extensive experiments demonstrate that our framework offers more precise control and improves generation performance across various metrics. Project Page: https://github.com/SynapGrid/Hear-Your-Click<br>
<span id='abs_ch'>中文：Hear-Your-Click框架通过让用户点击视频帧中的特定对象，结合对象感知的视觉特征和定制化数据增强，实现了交互式的视频到音频生成，从而提升了控制的精确性和生成效果。</span><br>
<span id='abs_en'>English: The Hear-Your-Click framework enables interactive video-to-audio generation by allowing users to click on specific objects in a frame, utilizing object-aware visual features and tailored data augmentation to improve precision and performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1475, <a href='https://arxiv.org/pdf/2507.04903.pdf' target='_blank'>https://arxiv.org/pdf/2507.04903.pdf</a></span>   <span><a href='https://github.com/thinh-dao/BackFed' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Thinh Dao, Dung Thuy Nguyen, Khoa D Doan, Kok-Seng Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04903">BackFed: An Efficient & Standardized Benchmark Suite for Backdoor Attacks in Federated Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Federated Learning (FL) systems are vulnerable to backdoor attacks, where adversaries train their local models on poisoned data and submit poisoned model updates to compromise the global model. Despite numerous proposed attacks and defenses, divergent experimental settings, implementation errors, and unrealistic assumptions hinder fair comparisons and valid conclusions about their effectiveness in real-world scenarios. To address this, we introduce BackFed - a comprehensive benchmark suite designed to standardize, streamline, and reliably evaluate backdoor attacks and defenses in FL, with a focus on practical constraints. Our benchmark offers key advantages through its multi-processing implementation that significantly accelerates experimentation and the modular design that enables seamless integration of new methods via well-defined APIs. With a standardized evaluation pipeline, we envision BackFed as a plug-and-play environment for researchers to comprehensively and reliably evaluate new attacks and defenses. Using BackFed, we conduct large-scale studies of representative backdoor attacks and defenses across both Computer Vision and Natural Language Processing tasks with diverse model architectures and experimental settings. Our experiments critically assess the performance of proposed attacks and defenses, revealing unknown limitations and modes of failures under practical conditions. These empirical insights provide valuable guidance for the development of new methods and for enhancing the security of FL systems. Our framework is openly available at https://github.com/thinh-dao/BackFed.<br>
<span id='abs_ch'>中文摘要：BackFed基准套件 准化并 速了联邦学 中后门攻击与防御的评估，通过系统性实验揭示现有方法的局限性，为提升联邦学 安全性提供重要指导。</span><br>
<span id='abs_en'>English Summary: The BackFed benchmark suite standardizes and accelerates the evaluation of backdoor attacks and defenses in Federated Learning, enabling comprehensive assessments that reveal limitations and guide future security improvements.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1476, <a href='https://arxiv.org/pdf/2507.04742.pdf' target='_blank'>https://arxiv.org/pdf/2507.04742.pdf</a></span>   <span><a href='https://github.com/ArminAzizi98/ASC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seyedarmin Azizi, Erfan Baghaei Potraghloo, Massoud Pedram
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04742">Activation Steering for Chain-of-Thought Compression</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) excel at complex reasoning when they include intermediate steps, known as "chains of thought" (CoTs). However, these rationales are often overly verbose, even for simple problems, leading to wasted context, increased latency, and higher energy consumption. We observe that verbose, English-heavy CoTs and concise, math-centric CoTs occupy distinct regions in the model's residual-stream activation space. By extracting and injecting a "steering vector" to transition between these modes, we can reliably shift generation toward more concise reasoning, effectively compressing CoTs without retraining. We formalize this approach as Activation-Steered Compression (ASC), an inference-time technique that shortens reasoning traces by directly modifying hidden representations. In addition, we provide a theoretical analysis of the impact of ASC on the output distribution, derived from a closed-form KL-divergence-bounded constraint to regulate steering strength. Using only 100 paired verbose and concise examples, ASC achieves up to 67.43% reduction in CoT length on MATH500 and GSM8K datasets, while maintaining accuracy across 7B, 8B, and 32B parameter models. As a training-free method, ASC introduces negligible runtime overhead and, on MATH500, delivers an average 2.73x speedup in end-to-end reasoning wall-clock time on an 8B model. This makes ASC a practical and efficient tool for streamlining the deployment of reasoning-capable LLMs in latency- or cost-sensitive settings. The code is available at: https://github.com/ArminAzizi98/ASC<br>
<br>
<div id='section'>Paperid: <span id='pid'>1477, <a href='https://arxiv.org/pdf/2507.04710.pdf' target='_blank'>https://arxiv.org/pdf/2507.04710.pdf</a></span>   <span><a href='https://github.com/xmed-lab/GeoSapiens' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anbang Wang, Marawan Elbatel, Keyuan Liu, Lizhuo Lin, Meng Lan, Yanqi Yang, Xiaomeng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04710">Geometric-Guided Few-Shot Dental Landmark Detection with Human-Centric Foundation Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate detection of anatomic landmarks is essential for assessing alveolar bone and root conditions, thereby optimizing clinical outcomes in orthodontics, periodontics, and implant dentistry. Manual annotation of landmarks on cone-beam computed tomography (CBCT) by dentists is time-consuming, labor-intensive, and subject to inter-observer variability. Deep learning-based automated methods present a promising approach to streamline this process efficiently. However, the scarcity of training data and the high cost of expert annotations hinder the adoption of conventional deep learning techniques. To overcome these challenges, we introduce GeoSapiens, a novel few-shot learning framework designed for robust dental landmark detection using limited annotated CBCT of anterior teeth. Our GeoSapiens framework comprises two key components: (1) a robust baseline adapted from Sapiens, a foundational model that has achieved state-of-the-art performance in human-centric vision tasks, and (2) a novel geometric loss function that improves the model's capacity to capture critical geometric relationships among anatomical structures. Experiments conducted on our collected dataset of anterior teeth landmarks revealed that GeoSapiens surpassed existing landmark detection methods, outperforming the leading approach by an 8.18% higher success detection rate at a strict 0.5 mm threshold-a standard widely recognized in dental diagnostics. Code is available at: https://github.com/xmed-lab/GeoSapiens.<br>
<span id='abs_ch'>Chinese: GeoSapiens是一种新颖的小 本学 框架，通过采用稳健的基线模型和 何损失函数，在CBCT扫描中提升了牙科 志点检测的准确性，以严 的0.5毫米阈值计算，其成功检测率比现有领先方法高出8.18%。</span><br>
<span id='abs_en'>English: GeoSapiens is a novel few-shot learning framework that enhances dental landmark detection on CBCT scans by leveraging a robust baseline model and a geometric loss function, achieving an 8.18% higher success rate at a strict 0.5 mm threshold compared to leading methods.Paperid:1048,https://arxiv.org/pdf/2507.04692.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1478, <a href='https://arxiv.org/pdf/2507.04631.pdf' target='_blank'>https://arxiv.org/pdf/2507.04631.pdf</a></span>   <span><a href='https://github.com/cocowy1/SMoE-Stereo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yun Wang, Longguang Wang, Chenghao Zhang, Yongjian Zhang, Zhanjie Zhang, Ao Ma, Chenyou Fan, Tin Lun Lam, Junjie Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04631">Learning Robust Stereo Matching in the Wild with Selective Mixture-of-Experts</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recently, learning-based stereo matching networks have advanced significantly. However, they often lack robustness and struggle to achieve impressive cross-domain performance due to domain shifts and imbalanced disparity distributions among diverse datasets. Leveraging Vision Foundation Models (VFMs) can intuitively enhance the model's robustness, but integrating such a model into stereo matching cost-effectively to fully realize their robustness remains a key challenge. To address this, we propose SMoEStereo, a novel framework that adapts VFMs for stereo matching through a tailored, scene-specific fusion of Low-Rank Adaptation (LoRA) and Mixture-of-Experts (MoE) modules. SMoEStereo introduces MoE-LoRA with adaptive ranks and MoE-Adapter with adaptive kernel sizes. The former dynamically selects optimal experts within MoE to adapt varying scenes across domains, while the latter injects inductive bias into frozen VFMs to improve geometric feature extraction. Importantly, to mitigate computational overhead, we further propose a lightweight decision network that selectively activates MoE modules based on input complexity, balancing efficiency with accuracy. Extensive experiments demonstrate that our method exhibits state-of-the-art cross-domain and joint generalization across multiple benchmarks without dataset-specific adaptation. The code is available at \textcolor{red}{https://github.com/cocowy1/SMoE-Stereo}.<br>
<span id='abs_ch'>中文：提出的SMoEStereo框架通过将视觉基础模型与自适应MoE-LoRA模块及轻量决策网络相结合， 需针对特定数据集调整即可实现最先进的跨域性能，显著提升了立体匹配的鲁棒性。</span><br>
<span id='abs_en'>English: The proposed SMoEStereo framework enhances stereo matching robustness by integrating Vision Foundation Models with adaptive MoE-LoRA modules and a lightweight decision network, achieving state-of-the-art cross-domain performance without dataset-specific tuning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1479, <a href='https://arxiv.org/pdf/2507.04623.pdf' target='_blank'>https://arxiv.org/pdf/2507.04623.pdf</a></span>   <span><a href='https://github.com/hjx159/HIPHOP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinpeng Chen, Jianxiang He, Huan Li, Senzhang Wang, Yuan Cao, Kaimin Wei, Zhenye Yang, Ye Ji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04623">Hierarchical Intent-guided Optimization with Pluggable LLM-Driven Semantics for Session-based Recommendation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Session-based Recommendation (SBR) aims to predict the next item a user will likely engage with, using their interaction sequence within an anonymous session. Existing SBR models often focus only on single-session information, ignoring inter-session relationships and valuable cross-session insights. Some methods try to include inter-session data but struggle with noise and irrelevant information, reducing performance. Additionally, most models rely on item ID co-occurrence and overlook rich semantic details, limiting their ability to capture fine-grained item features. To address these challenges, we propose a novel hierarchical intent-guided optimization approach with pluggable LLM-driven semantic learning for session-based recommendations, called HIPHOP. First, we introduce a pluggable embedding module based on large language models (LLMs) to generate high-quality semantic representations, enhancing item embeddings. Second, HIPHOP utilizes graph neural networks (GNNs) to model item transition relationships and incorporates a dynamic multi-intent capturing module to address users' diverse interests within a session. Additionally, we design a hierarchical inter-session similarity learning module, guided by user intent, to capture global and local session relationships, effectively exploring users' long-term and short-term interests. To mitigate noise, an intent-guided denoising strategy is applied during inter-session learning. Finally, we enhance the model's discriminative capability by using contrastive learning to optimize session representations. Experiments on multiple datasets show that HIPHOP significantly outperforms existing methods, demonstrating its effectiveness in improving recommendation quality. Our code is available: https://github.com/hjx159/HIPHOP.<br>
<span id='abs_ch'>中文: HIPHOP模型通过引入大语言模型语义嵌入、动态多意图捕捉及分层去噪的会话间学 ，有效提升了基于会话的推荐性能，显著优于现有方法。</span><br>
<span id='abs_en'>English: The proposed HIPHOP model enhances session-based recommendations by integrating LLM-driven semantic embeddings, dynamic multi-intent capturing, and hierarchical inter-session learning with denoising, significantly outperforming existing methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1480, <a href='https://arxiv.org/pdf/2507.04610.pdf' target='_blank'>https://arxiv.org/pdf/2507.04610.pdf</a></span>   <span><a href='https://github.com/facebookresearch/any4' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mostafa Elhoushi, Jeff Johnson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04610">any4: Learned 4-bit Numeric Representation for LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present any4, a learned 4-bit weight quantization solution for large language models (LLMs) providing arbitrary numeric representations without requiring pre-processing of weights or activations. any4 yields higher accuracy compared to other related 4-bit numeric representation types: int4, fp4 and nf4, as evaluated on a range of model sizes, generations and families (Llama 2, Llama 3, Mistral and Mixtral). While any4 does not require preprocessing of weights or activations, it is also competitive with orthogonal techniques that require such preprocessing (e.g., AWQ and GPTQ). We also experiment with any3 and any2 and show competitiveness at lower bits. Additionally, we show that we can calibrate using a single curated diverse sample rather than hundreds of samples from a dataset as done in most quantization approaches. We also open source tinygemm, a latency optimized GPU matrix multiplication library for LLMs, that implements any4 using a GPU-efficient lookup table strategy along with other common quantization methods. We open source our code at https://github.com/facebookresearch/any4 .<br>
<span id='abs_ch'>中文：any4是一种针对大语言模型的学 型4位量化方法， 需权重或激活预处理即可在不同模型上实现更高精度，同时提供了优化的GPU库和高效的单 本 准方案。</span><br>
<span id='abs_en'>English: any4 is a learned 4-bit quantization method for LLMs that achieves superior accuracy across various models without requiring weight or activation preprocessing, while also introducing an optimized GPU library and efficient single-sample calibration.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1481, <a href='https://arxiv.org/pdf/2507.04531.pdf' target='_blank'>https://arxiv.org/pdf/2507.04531.pdf</a></span>   <span><a href='https://github.com/MBZUAI-Trustworthy-ML/DP-Fusion-DPI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rushil Thareja, Preslav Nakov, Praneeth Vepakomma, Nils Lukas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04531">DP-Fusion: Token-Level Differentially Private Inference for Large Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) can leak sensitive information from their context through generated outputs, either accidentally or when prompted adversarially. Existing defenses that aim to preserve context privacy during inference either lack formal guarantees or suffer from a poor utility/privacy trade-off. We propose DP-Fusion, a token-level Differentially Private Inference (DPI) mechanism that provably bounds how much an LLM's outputs reveal about sensitive tokens in its context. We demonstrate DPI through the task of document privatization, where the goal is to paraphrase documents so that sensitive content (e.g., Personally Identifiable Information, PII) cannot be reliably inferred, while still preserving the overall utility of the text. This is controlled by a parameter $Îµ$: $Îµ=0$ hides PII entirely, while higher values trade off privacy for improved paraphrase quality. DP-Fusion works as follows: (i) partition sensitive tokens into disjoint privacy groups, (ii) run the LLM once per group, and (iii) blend the output distributions so that the final output remains within a fixed statistical distance of the baseline distribution produced when no privacy group is revealed. This approach allows fine-grained control over the privacy/utility trade-off but requires multiple LLM forward passes.<br>
<span id='abs_ch'>中文: DP-Fusion是一种差分隐私推理机制，通过将敏感令牌分组并融合其输出分布来保护大语言模型中的敏感信息，以多次模型前向 递为代价实现可调节的隐私与效用平衡。</span><br>
<span id='abs_en'>English: DP-Fusion is a differentially private inference mechanism that protects sensitive information in LLM outputs by partitioning tokens into privacy groups and blending their distributions, offering adjustable privacy-utility trade-offs through multiple model passes.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1482, <a href='https://arxiv.org/pdf/2507.04487.pdf' target='_blank'>https://arxiv.org/pdf/2507.04487.pdf</a></span>   <span><a href='https://github.com/KlozeWang/LoSiA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xujia Wang, Yunjia Qi, Bin Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04487">LoSiA: Efficient High-Rank Fine-Tuning via Subnet Localization and Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA, significantly reduce the number of trainable parameters by introducing low-rank decomposition matrices. However, existing methods perform extensive matrix multiplications in domain specialization tasks, resulting in computational inefficiency and sub-optimal fine-tuning performance. Hence, we propose LoSiA(Low-Resources Subnet Integration Adaptation), an innovative method that dynamically localizes and optimizes critical parameters during the training process. Specifically, it identifies a sub-network using gradient sparsity analysis and optimizes it as the trainable target. This design enables effective high-rank adaptation by updating only the sub-network parameters, reducing the additional matrix multiplication. We also present LoSiA-Pro, a faster implementation of LoSiA, which reduces the training latency by about $27\%$ compared to LoRA. Extensive evaluations show that our method achieves minimal performance drop compared to full fine-tuning, while requiring the least training time across domain specialization and common-sense reasoning tasks. Further analysis shows that LoSiA also reduces forgetting during continued training. The source code is available at https://github.com/KlozeWang/LoSiA.<br>
<span id='abs_ch'>中文摘要：LoSiA提出了一种创新的参数高效微调方法，通过梯度稀疏分析动态优化关键子网络，在降低计算成本和训练时间的同时实现了接近全参数微调的性能。</span><br>
<span id='abs_en'>English Summary: LoSiA introduces a novel parameter-efficient fine-tuning approach that dynamically optimizes critical sub-networks through gradient sparsity analysis, achieving near-full fine-tuning performance with reduced computational cost and training time.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1483, <a href='https://arxiv.org/pdf/2507.04464.pdf' target='_blank'>https://arxiv.org/pdf/2507.04464.pdf</a></span>   <span><a href='https://github.com/abastola0/TRAP.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ashish Bastola, Mert D. PesÃ©, Long Cheng, Jonathon Smereka, Abolfazl Razi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04464">Anomalous Decision Discovery using Inverse Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Anomaly detection plays a critical role in Autonomous Vehicles (AVs) by identifying unusual behaviors through perception systems that could compromise safety and lead to hazardous situations. Current approaches, which often rely on predefined thresholds or supervised learning paradigms, exhibit reduced efficacy when confronted with unseen scenarios, sensor noise, and occlusions, leading to potential safety-critical failures. Moreover, supervised methods require large annotated datasets, limiting their real-world feasibility. To address these gaps, we propose an anomaly detection framework based on Inverse Reinforcement Learning (IRL) to infer latent driving intentions from sequential perception data, thus enabling robust identification. Specifically, we present Trajectory-Reward Guided Adaptive Pre-training (TRAP), a novel IRL framework for anomaly detection, to address two critical limitations of existing methods: noise robustness and generalization to unseen scenarios. Our core innovation is implicitly learning temporal credit assignments via reward and worst-case supervision. We leverage pre-training with variable-horizon sampling to maximize time-to-consequence, resulting in early detection of behavior deviation. Experiments on 14,000+ simulated trajectories demonstrate state-of-the-art performance, achieving 0.90 AUC and 82.2\% F1-score - outperforming similarly trained supervised and unsupervised baselines by 39\% on Recall and 12\% on F1-score, respectively. Similar performance is achieved while exhibiting robustness to various noise types and generalization to unseen anomaly types. Our code will be available at: https://github.com/abastola0/TRAP.git<br>
<span id='abs_ch'>中文: 提出的TRAP框架利用逆向强化学 从感知数据中推断驾驶意图，实现了对自动驾驶汽车异常的鲁棒检测，以0.90的AUC值和更强的噪声适应性展现出卓越性能。</span><br>
<span id='abs_en'>English: The proposed TRAP framework utilizes Inverse Reinforcement Learning to robustly detect anomalies in autonomous vehicles by inferring driving intentions from perception data, achieving superior performance with 0.90 AUC and enhanced noise resilience.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1484, <a href='https://arxiv.org/pdf/2507.04428.pdf' target='_blank'>https://arxiv.org/pdf/2507.04428.pdf</a></span>   <span><a href='https://github.com/seucoin/armr2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Feiyue Wu, Tianxing Wu, Shenqi Jing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04428">ARMR: Adaptively Responsive Network for Medication Recommendation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Medication recommendation is a crucial task in healthcare, especially for patients with complex medical conditions. However, existing methods often struggle to effectively balance the reuse of historical medications with the introduction of new drugs in response to the changing patient conditions. In order to address this challenge, we propose an Adaptively Responsive network for Medication Recommendation (ARMR), a new method which incorporates 1) a piecewise temporal learning component that distinguishes between recent and distant patient history, enabling more nuanced temporal understanding, and 2) an adaptively responsive mechanism that dynamically adjusts attention to new and existing drugs based on the patient's current health state and medication history. Experiments on the MIMIC-III and MIMIC-IV datasets indicate that ARMR has better performance compared with the state-of-the-art baselines in different evaluation metrics, which contributes to more personalized and accurate medication recommendations. The source code is publicly avaiable at: https://github.com/seucoin/armr2.<br>
<span id='abs_ch'>中文: 提出的ARMR方法通过时序学 和自适应响应机制，动态平衡历史用药与新药引入，在 准数据集上展现出更优的个性化药物推荐性能。</span><br>
<span id='abs_en'>English: The proposed ARMR method enhances medication recommendations by dynamically balancing historical and new drug considerations through temporal learning and adaptive mechanisms, demonstrating superior performance on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1485, <a href='https://arxiv.org/pdf/2507.04283.pdf' target='_blank'>https://arxiv.org/pdf/2507.04283.pdf</a></span>   <span><a href='https://github.com/BGU-CS-VIL/CLUDI' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Roy Uziel, Irit Chelly, Oren Freifeld, Ari Pakman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04283">Clustering via Self-Supervised Diffusion</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Diffusion models, widely recognized for their success in generative tasks, have not yet been applied to clustering. We introduce Clustering via Diffusion (CLUDI), a self-supervised framework that combines the generative power of diffusion models with pre-trained Vision Transformer features to achieve robust and accurate clustering. CLUDI is trained via a teacher-student paradigm: the teacher uses stochastic diffusion-based sampling to produce diverse cluster assignments, which the student refines into stable predictions. This stochasticity acts as a novel data augmentation strategy, enabling CLUDI to uncover intricate structures in high-dimensional data. Extensive evaluations on challenging datasets demonstrate that CLUDI achieves state-of-the-art performance in unsupervised classification, setting new benchmarks in clustering robustness and adaptability to complex data distributions. Our code is available at https://github.com/BGU-CS-VIL/CLUDI.<br>
<span id='abs_ch'>中文: CLUDI是一种自监督聚类框架，通过教师-学生范式结合扩散模型和视觉Transformer特征，在 监督分类中实现了最先进的性能，能够揭示复杂的数据结构。</span><br>
<span id='abs_en'>English: CLUDI is a self-supervised clustering framework that leverages diffusion models and Vision Transformer features through a teacher-student paradigm, achieving state-of-the-art performance in unsupervised classification by uncovering complex data structures.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1486, <a href='https://arxiv.org/pdf/2507.04243.pdf' target='_blank'>https://arxiv.org/pdf/2507.04243.pdf</a></span>   <span><a href='https://github.com/wangxb29/DGPST' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinbo Wang, Wenju Xu, Qing Zhang, Wei-Shi Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04243">Domain Generalizable Portrait Style Transfer</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents a portrait style transfer method that generalizes well to various different domains while enabling high-quality semantic-aligned stylization on regions including hair, eyes, eyelashes, skins, lips, and background. To this end, we propose to establish dense semantic correspondence between the given input and reference portraits based on a pre-trained model and a semantic adapter, with which we obtain a warped reference semantically aligned with the input. To ensure effective yet controllable style transfer, we devise an AdaIN-Wavelet transform to balance content preservation and stylization by blending low-frequency information of the warped reference with high-frequency information of the input in the latent space. A style adapter is also designed to provide style guidance from the warped reference. With the stylized latent from AdaIN-Wavelet transform, we employ a dual-conditional diffusion model that integrates a ControlNet recording high-frequency information and the style guidance to generate the final result. Extensive experiments demonstrate the superiority of our method. Our code and trained model are available at https://github.com/wangxb29/DGPST.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1487, <a href='https://arxiv.org/pdf/2507.04123.pdf' target='_blank'>https://arxiv.org/pdf/2507.04123.pdf</a></span>   <span><a href='https://github.com/LinshenLiu622/EMC2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Linshen Liu, Boyan Su, Junyue Jiang, Guanlin Wu, Cong Guo, Ceyu Xu, Hao Frank Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04123">Towards Accurate and Efficient 3D Object Detection for Autonomous Driving: A Mixture of Experts Computing System on Edge</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper presents Edge-based Mixture of Experts (MoE) Collaborative Computing (EMC2), an optimal computing system designed for autonomous vehicles (AVs) that simultaneously achieves low-latency and high-accuracy 3D object detection. Unlike conventional approaches, EMC2 incorporates a scenario-aware MoE architecture specifically optimized for edge platforms. By effectively fusing LiDAR and camera data, the system leverages the complementary strengths of sparse 3D point clouds and dense 2D images to generate robust multimodal representations. To enable this, EMC2 employs an adaptive multimodal data bridge that performs multi-scale preprocessing on sensor inputs, followed by a scenario-aware routing mechanism that dynamically dispatches features to dedicated expert models based on object visibility and distance. In addition, EMC2 integrates joint hardware-software optimizations, including hardware resource utilization optimization and computational graph simplification, to ensure efficient and real-time inference on resource-constrained edge devices. Experiments on open-source benchmarks clearly show the EMC2 advancements as an end-to-end system. On the KITTI dataset, it achieves an average accuracy improvement of 3.58% and a 159.06% inference speedup compared to 15 baseline methods on Jetson platforms, with similar performance gains on the nuScenes dataset, highlighting its capability to advance reliable, real-time 3D object detection tasks for AVs. The official implementation is available at https://github.com/LinshenLiu622/EMC2.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1488, <a href='https://arxiv.org/pdf/2507.04119.pdf' target='_blank'>https://arxiv.org/pdf/2507.04119.pdf</a></span>   <span><a href='https://github.com/tmllab/2025_ICML_ATEsc' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziming Hong, Runnan Chen, Zengmao Wang, Bo Han, Bo Du, Tongliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04119">When Data-Free Knowledge Distillation Meets Non-Transferable Teacher: Escaping Out-of-Distribution Trap is All You Need</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to a student without access the real in-distribution (ID) data. Its common solution is to use a generator to synthesize fake data and use them as a substitute for real ID data. However, existing works typically assume teachers are trustworthy, leaving the robustness and security of DFKD from untrusted teachers largely unexplored. In this work, we conduct the first investigation into distilling non-transferable learning (NTL) teachers using DFKD, where the transferability from an ID domain to an out-of-distribution (OOD) domain is prohibited. We find that NTL teachers fool DFKD through divert the generator's attention from the useful ID knowledge to the misleading OOD knowledge. This hinders ID knowledge transfer but prioritizes OOD knowledge transfer. To mitigate this issue, we propose Adversarial Trap Escaping (ATEsc) to benefit DFKD by identifying and filtering out OOD-like synthetic samples. Specifically, inspired by the evidence that NTL teachers show stronger adversarial robustness on OOD samples than ID samples, we split synthetic samples into two groups according to their robustness. The fragile group is treated as ID-like data and used for normal knowledge distillation, while the robust group is seen as OOD-like data and utilized for forgetting OOD knowledge. Extensive experiments demonstrate the effectiveness of ATEsc for improving DFKD against NTL teachers. Code is released at https://github.com/tmllab/2025_ICML_ATEsc.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1489, <a href='https://arxiv.org/pdf/2507.04106.pdf' target='_blank'>https://arxiv.org/pdf/2507.04106.pdf</a></span>   <span><a href='https://github.com/stapaw/STP.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>StanisÅaw Pawlak, BartÅomiej Twardowski, Tomasz TrzciÅski, Joost van de Weijer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04106">Addressing The Devastating Effects Of Single-Task Data Poisoning In Exemplar-Free Continual Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Our research addresses the overlooked security concerns related to data poisoning in continual learning (CL). Data poisoning - the intentional manipulation of training data to affect the predictions of machine learning models - was recently shown to be a threat to CL training stability. While existing literature predominantly addresses scenario-dependent attacks, we propose to focus on a more simple and realistic single-task poison (STP) threats. In contrast to previously proposed poisoning settings, in STP adversaries lack knowledge and access to the model, as well as to both previous and future tasks. During an attack, they only have access to the current task within the data stream. Our study demonstrates that even within these stringent conditions, adversaries can compromise model performance using standard image corruptions. We show that STP attacks are able to strongly disrupt the whole continual training process: decreasing both the stability (its performance on past tasks) and plasticity (capacity to adapt to new tasks) of the algorithm. Finally, we propose a high-level defense framework for CL along with a poison task detection method based on task vectors. The code is available at https://github.com/stapaw/STP.git .<br>
<span id='abs_ch'>中文摘要：本 究揭示了持续学 中单任务数据投毒的威胁，攻击者仅利用简单图像干扰即可 坏模型稳定性与可塑性，并提出了包含投毒检测的防御框架。</span><br>
<span id='abs_en'>English Summary: This study highlights the threat of single-task poisoning in continual learning, where adversaries can degrade model stability and plasticity using simple image corruptions, and proposes a defense framework with poison detection methods.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1490, <a href='https://arxiv.org/pdf/2507.04060.pdf' target='_blank'>https://arxiv.org/pdf/2507.04060.pdf</a></span>   <span><a href='https://github.com/hyqlat/TCL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianwei Tang, Jiangxin Sun, Xiaotong Lin, Lifang Zhang, Wei-Shi Zheng, Jian-Fang Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04060">Temporal Continual Learning with Prior Compensation for Human Motion Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Human Motion Prediction (HMP) aims to predict future poses at different moments according to past motion sequences. Previous approaches have treated the prediction of various moments equally, resulting in two main limitations: the learning of short-term predictions is hindered by the focus on long-term predictions, and the incorporation of prior information from past predictions into subsequent predictions is limited. In this paper, we introduce a novel multi-stage training framework called Temporal Continual Learning (TCL) to address the above challenges. To better preserve prior information, we introduce the Prior Compensation Factor (PCF). We incorporate it into the model training to compensate for the lost prior information. Furthermore, we derive a more reasonable optimization objective through theoretical derivation. It is important to note that our TCL framework can be easily integrated with different HMP backbone models and adapted to various datasets and applications. Extensive experiments on four HMP benchmark datasets demonstrate the effectiveness and flexibility of TCL. The code is available at https://github.com/hyqlat/TCL.<br>
<span id='abs_ch'>Chinese: 本文提出了一种时序持续学 （TCL）框架，通过引入先验补偿 子来保留历史信息并优化训练目 ，有效解决了人体运动预测中的现有局限，在多个基准数据集上验证了其有效性。</span><br>
<span id='abs_en'>English: This paper introduces a Temporal Continual Learning (TCL) framework with a Prior Compensation Factor to address limitations in Human Motion Prediction by preserving prior information and optimizing training objectives, demonstrating effectiveness across multiple datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1491, <a href='https://arxiv.org/pdf/2507.04038.pdf' target='_blank'>https://arxiv.org/pdf/2507.04038.pdf</a></span>   <span><a href='https://github.com/DIDSR/tsynth-release' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Christopher Wiedeman, Anastasiia Sarmakeeva, Elena Sizikova, Daniil Filienko, Miguel Lago, Jana G. Delfino, Aldo Badano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04038">T-SYNTH: A Knowledge-Based Dataset of Synthetic Breast Images</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>One of the key impediments for developing and assessing robust medical imaging algorithms is limited access to large-scale datasets with suitable annotations. Synthetic data generated with plausible physical and biological constraints may address some of these data limitations. We propose the use of physics simulations to generate synthetic images with pixel-level segmentation annotations, which are notoriously difficult to obtain. Specifically, we apply this approach to breast imaging analysis and release T-SYNTH, a large-scale open-source dataset of paired 2D digital mammography (DM) and 3D digital breast tomosynthesis (DBT) images. Our initial experimental results indicate that T-SYNTH images show promise for augmenting limited real patient datasets for detection tasks in DM and DBT. Our data and code are publicly available at https://github.com/DIDSR/tsynth-release.<br>
<span id='abs_ch'>中文摘要：开发稳健医学影像算法的主要障碍在于缺乏大规模 注数据集，而基于物理模拟生成的合成数据可缓解此问题；T-SYNTH乳腺影像数据集的实验表明，其能有效增强真实数据在检测任务中的表现。</span><br>
<span id='abs_en'>English Summary: The development of robust medical imaging algorithms is hindered by limited annotated datasets, which can be addressed through physics-based synthetic data generation, as demonstrated by the T-SYNTH dataset for breast imaging that shows potential for augmenting real data in detection tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1492, <a href='https://arxiv.org/pdf/2507.03953.pdf' target='_blank'>https://arxiv.org/pdf/2507.03953.pdf</a></span>   <span><a href='https://github.com/vkeilo/DiffAdvPerturbationBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kai Ye, Tianyi Chen, Zhen Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03953">Evaluating Adversarial Protections for Diffusion Personalization: A Comprehensive Study</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the increasing adoption of diffusion models for image generation and personalization, concerns regarding privacy breaches and content misuse have become more pressing. In this study, we conduct a comprehensive comparison of eight perturbation based protection methods: AdvDM, ASPL, FSGM, MetaCloak, Mist, PhotoGuard, SDS, and SimAC--across both portrait and artwork domains. These methods are evaluated under varying perturbation budgets, using a range of metrics to assess visual imperceptibility and protective efficacy. Our results offer practical guidance for method selection. Code is available at: https://github.com/vkeilo/DiffAdvPerturbationBench.<br>
<span id='abs_ch'>中文摘要：本 究全面评估了八种基于扰动的扩散模型保护方法，以应对隐私泄露和内容滥用问题，为在肖像和艺术作品领域选择有效技术提供了实用指导。</span><br>
<span id='abs_en'>English Summary: This study comprehensively evaluates eight perturbation-based protection methods for diffusion models to address privacy and misuse concerns, providing practical guidance for selecting effective techniques across portrait and artwork domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1493, <a href='https://arxiv.org/pdf/2507.03923.pdf' target='_blank'>https://arxiv.org/pdf/2507.03923.pdf</a></span>   <span><a href='https://github.com/hieuphamha19/CSDS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ha-Hieu Pham, Nguyen Lan Vi Vu, Thanh-Huy Nguyen, Ulas Bagci, Min Xu, Trung-Nghia Le, Huy-Hieu Pham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03923">Learning Disentangled Stain and Structural Representations for Semi-Supervised Histopathology Segmentation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate gland segmentation in histopathology images is essential for cancer diagnosis and prognosis. However, significant variability in Hematoxylin and Eosin (H&E) staining and tissue morphology, combined with limited annotated data, poses major challenges for automated segmentation. To address this, we propose Color-Structure Dual-Student (CSDS), a novel semi-supervised segmentation framework designed to learn disentangled representations of stain appearance and tissue structure. CSDS comprises two specialized student networks: one trained on stain-augmented inputs to model chromatic variation, and the other on structure-augmented inputs to capture morphological cues. A shared teacher network, updated via Exponential Moving Average (EMA), supervises both students through pseudo-labels. To further improve label reliability, we introduce stain-aware and structure-aware uncertainty estimation modules that adaptively modulate the contribution of each student during training. Experiments on the GlaS and CRAG datasets show that CSDS achieves state-of-the-art performance in low-label settings, with Dice score improvements of up to 1.2% on GlaS and 0.7% on CRAG at 5% labeled data, and 0.7% and 1.4% at 10%. Our code and pre-trained models are available at https://github.com/hieuphamha19/CSDS.<br>
<span id='abs_ch'>中文：提出的颜色-结构双学生（CSDS）框架通过两个专门的学生网络分别学 染色和结构特征，解决了组织病理学中腺体分割的难题，在有限 注数据下取得了最优性能。</span><br>
<span id='abs_en'>English: The proposed Color-Structure Dual-Student (CSDS) framework addresses gland segmentation challenges in histopathology by using two specialized student networks to disentangle stain and structural features, achieving state-of-the-art results with limited labeled data.Paperid:1096,https://arxiv.org/pdf/2507.03922.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1494, <a href='https://arxiv.org/pdf/2507.03916.pdf' target='_blank'>https://arxiv.org/pdf/2507.03916.pdf</a></span>   <span><a href='https://github.com/PAMPAS-Lab/ANA-PPT-Anamation/blob/main/Appendix.pdf' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Jiang, Yibo Xue, Yukun Kang, Pin Zheng, Jian Peng, Feiran Wu, Changliang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03916">Animation Needs Attention: A Holistic Approach to Slides Animation Comprehension with Visual-Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Slide animations, such as fade-in, fly-in, and wipe, are critical for audience engagement, efficient information delivery, and vivid visual expression. However, most AI-driven slide-generation tools still lack native animation support, and existing vision-language models (VLMs) struggle with animation tasks due to the absence of public datasets and limited temporal-reasoning capabilities. To address this gap, we release the first public dataset for slide-animation modeling: 12,000 triplets of natural-language descriptions, animation JSON files, and rendered videos, collectively covering every built-in PowerPoint effect. Using this resource, we fine-tune Qwen-2.5-VL-7B with Low-Rank Adaptation (LoRA) and achieve consistent improvements over GPT-4.1 and Gemini-2.5-Pro in BLEU-4, ROUGE-L, SPICE, and our Coverage-Order-Detail Assessment (CODA) metric, which evaluates action coverage, temporal order, and detail fidelity. On a manually created test set of slides, the LoRA model increases BLEU-4 by around 60%, ROUGE-L by 30%, and shows significant improvements in CODA-detail. This demonstrates that low-rank adaptation enables reliable temporal reasoning and generalization beyond synthetic data. Overall, our dataset, LoRA-enhanced model, and CODA metric provide a rigorous benchmark and foundation for future research on VLM-based dynamic slide generation.<br>
<span id='abs_ch'>Chinese: 本 究发布了首个幻灯片动画建模公共数据集，并通过LoRA微调Qwen-2.5-VL-7B模型，在时序推理和动画生成质量上显著超越主流模型，为动态幻灯片生成领域建立了新基准。</span><br>
<span id='abs_en'>English: This work introduces the first public dataset for slide animation modeling and demonstrates that fine-tuning Qwen-2.5-VL-7B with LoRA significantly outperforms leading models in temporal reasoning and animation generation, establishing a new benchmark for dynamic slide creation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1495, <a href='https://arxiv.org/pdf/2507.03863.pdf' target='_blank'>https://arxiv.org/pdf/2507.03863.pdf</a></span>   <span><a href='https://github.com/Graham-Brady-Research-Group/AutoregressiveEnsemble_SpatioTemporal_Evolution' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ishan Khurjekar, Indrashish Saha, Lori Graham-Brady, Somdatta Goswami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03863">Enhanced accuracy through ensembling of randomly initialized auto-regressive models for time-dependent PDEs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Systems governed by partial differential equations (PDEs) require computationally intensive numerical solvers to predict spatiotemporal field evolution. While machine learning (ML) surrogates offer faster solutions, autoregressive inference with ML models suffer from error accumulation over successive predictions, limiting their long-term accuracy. We propose a deep ensemble framework to address this challenge, where multiple ML surrogate models with random weight initializations are trained in parallel and aggregated during inference. This approach leverages the diversity of model predictions to mitigate error propagation while retaining the autoregressive strategies ability to capture the system's time dependent relations. We validate the framework on three PDE-driven dynamical systems - stress evolution in heterogeneous microstructures, Gray-Scott reaction-diffusion, and planetary-scale shallow water system - demonstrating consistent reduction in error accumulation over time compared to individual models. Critically, the method requires only a few time steps as input, enabling full trajectory predictions with inference times significantly faster than numerical solvers. Our results highlight the robustness of ensemble methods in diverse physical systems and their potential as efficient and accurate alternatives to traditional solvers. The codes for this work are available on GitHub (https://github.com/Graham-Brady-Research-Group/AutoregressiveEnsemble_SpatioTemporal_Evolution).<br>
<span id='abs_ch'>中文: 作者提出了一种深度集成框架，通过并行训练多个机器学 代理模型并聚合其预测，有效减少了偏微分方程系统中自回归推理的误差累积，在三个物理应用中验证了其准确性和效率的提升。</span><br>
<span id='abs_en'>English: The authors propose a deep ensemble framework that trains multiple machine learning surrogates in parallel and aggregates their predictions to reduce error accumulation in autoregressive inference for PDE-based systems, demonstrating improved accuracy and efficiency across three physical applications.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1496, <a href='https://arxiv.org/pdf/2507.03779.pdf' target='_blank'>https://arxiv.org/pdf/2507.03779.pdf</a></span>   <span><a href='https://github.com/KevinZ0217/fast_dinov2' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Zhang, Juntuo Wang, Zhixin Sun, John Zou, Randall Balestriero
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03779">FastDINOv2: Frequency Based Curriculum Learning Improves Robustness and Training Speed</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large-scale vision foundation models such as DINOv2 boast impressive performances by leveraging massive architectures and training datasets. But numerous scenarios require practitioners to reproduce those pre-training solutions, such as on private data, new modalities, or simply for scientific questioning--which is currently extremely demanding computation-wise. We thus propose a novel pre-training strategy for DINOv2 that simultaneously accelerates convergence--and strengthens robustness to common corruptions as a by-product. Our approach involves a frequency filtering curriculum--low-frequency being seen first--and the Gaussian noise patching augmentation. Applied to a ViT-B/16 backbone trained on ImageNet-1K, while pre-training time and FLOPs are reduced by 1.6x and 2.25x, our method still achieves matching robustness in corruption benchmarks (ImageNet-C) and maintains competitive linear probing performance compared with baseline. This dual benefit of efficiency and robustness makes large-scale self-supervised foundation modeling more attainable, while opening the door to novel exploration around data curriculum and augmentation as means to improve self-supervised learning models robustness. The code is available at https://github.com/KevinZ0217/fast_dinov2<br>
<span id='abs_ch'>Chinese: 本文提出了一种新颖的DINOv2预训练策略，通过频率过滤课程和高斯噪声增强技术，在 速收敛的同时提升了模型鲁棒性，在显著减少计算成本的情况下，仍能在ImageNet-C基准测试中保持相当的鲁棒性表现和线性探测性能。</span><br>
<span id='abs_en'>English: This paper introduces a novel pre-training strategy for DINOv2 that accelerates convergence and enhances robustness through frequency filtering and Gaussian noise augmentation, achieving significant computational savings while maintaining competitive performance on corruption benchmarks and linear probing tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1497, <a href='https://arxiv.org/pdf/2507.03616.pdf' target='_blank'>https://arxiv.org/pdf/2507.03616.pdf</a></span>   <span><a href='https://github.com/EvoAgentX/EvoAgentX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingxu Wang, Siwei Liu, Jinyuan Fang, Zaiqiao Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03616">EvoAgentX: An Automated Framework for Evolving Agentic Workflows</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multi-agent systems (MAS) have emerged as a powerful paradigm for orchestrating large language models (LLMs) and specialized tools to collaboratively address complex tasks. However, existing MAS frameworks often require manual workflow configuration and lack native support for dynamic evolution and performance optimization. In addition, many MAS optimization algorithms are not integrated into a unified framework. In this paper, we present EvoAgentX, an open-source platform that automates the generation, execution, and evolutionary optimization of multi-agent workflows. EvoAgentX employs a modular architecture consisting of five core layers: the basic components, agent, workflow, evolving, and evaluation layers. Specifically, within the evolving layer, EvoAgentX integrates three MAS optimization algorithms, TextGrad, AFlow, and MIPRO, to iteratively refine agent prompts, tool configurations, and workflow topologies. We evaluate EvoAgentX on HotPotQA, MBPP, and MATH for multi-hop reasoning, code generation, and mathematical problem solving, respectively, and further assess it on real-world tasks using GAIA. Experimental results show that EvoAgentX consistently achieves significant performance improvements, including a 7.44% increase in HotPotQA F1, a 10.00% improvement in MBPP pass@1, a 10.00% gain in MATH solve accuracy, and an overall accuracy improvement of up to 20.00% on GAIA. The source code is available at: https://github.com/EvoAgentX/EvoAgentX<br>
<span id='abs_ch'>中文: EvoAgentX是一个开源平台，能自动生成、执行和进化优化多智能体工作流，集成了TextGrad、AFlow和MIPRO等算法，在推理、代 生成和数学解题任务中显著提升性能。</span><br>
<span id='abs_en'>English: EvoAgentX is an open-source platform that automates the creation, execution, and evolutionary optimization of multi-agent workflows, integrating algorithms like TextGrad, AFlow, and MIPRO to enhance performance across reasoning, coding, and math tasks with significant improvements.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1498, <a href='https://arxiv.org/pdf/2507.03578.pdf' target='_blank'>https://arxiv.org/pdf/2507.03578.pdf</a></span>   <span><a href='https://github.com/google-deepmind/scivid' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yana Hasson, Pauline Luc, Liliane Momeni, Maks Ovsjanikov, Guillaume Le Moing, Alina Kuznetsova, Ira Ktena, Jennifer J. Sun, Skanda Koppula, Dilara Gokay, Joseph Heyward, Etienne Pot, Andrew Zisserman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03578">SciVid: Cross-Domain Evaluation of Video Models in Scientific Applications</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In recent years, there has been a proliferation of spatiotemporal foundation models in different scientific disciplines. While promising, these models are often domain-specific and are only assessed within the particular applications for which they are designed. Given that many tasks can be represented as video modeling problems, video foundation models (ViFMs) hold considerable promise as general-purpose domain-agnostic approaches. However, it is not known whether the knowledge acquired on large-scale but potentially out-of-domain data can be effectively transferred across diverse scientific disciplines, and if a single, pretrained ViFM can be competitive with domain-specific baselines. To address this, we introduce SciVid, a comprehensive benchmark comprising five *Sci*entific *Vid*eo tasks, across medical computer vision, animal behavior, and weather forecasting. We adapt six leading ViFMs to SciVid using simple trainable readout modules, establishing strong baselines and demonstrating the potential for effective transfer learning. Specifically, we show that state-of-the-art results can be obtained in several applications by leveraging the general-purpose representations from ViFM backbones. Furthermore, our results reveal the limitations of existing ViFMs, and highlight opportunities for the development of generalizable models for high-impact scientific applications. We release our code at https://github.com/google-deepmind/scivid to facilitate further research in the development of ViFMs.<br>
<span id='abs_ch'>Chinese: 视频基础模型在科学应用中展现出作为通用工具的潜力，SciVid基准测试证明其通过迁移学 可获得先进成果，同时也揭示了现有模型的局限性。</span><br>
<span id='abs_en'>English: Video foundation models show potential as general-purpose tools for scientific applications, with the SciVid benchmark demonstrating their ability to achieve state-of-the-art results through transfer learning while also revealing current limitations.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1499, <a href='https://arxiv.org/pdf/2507.03367.pdf' target='_blank'>https://arxiv.org/pdf/2507.03367.pdf</a></span>   <span><a href='https://github.com/blaz-r/BTC-change-detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>BlaÅ¾ Rolih, Matic FuÄka, Filip Wolf, Luka Äehovin Zajc
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03367">Be the Change You Want to See: Revisiting Remote Sensing Change Detection Practices</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Remote sensing change detection aims to localize semantic changes between images of the same location captured at different times. In the past few years, newer methods have attributed enhanced performance to the additions of new and complex components to existing architectures. Most fail to measure the performance contribution of fundamental design choices such as backbone selection, pre-training strategies, and training configurations. We claim that such fundamental design choices often improve performance even more significantly than the addition of new architectural components. Due to that, we systematically revisit the design space of change detection models and analyse the full potential of a well-optimised baseline. We identify a set of fundamental design choices that benefit both new and existing architectures. Leveraging this insight, we demonstrate that when carefully designed, even an architecturally simple model can match or surpass state-of-the-art performance on six challenging change detection datasets. Our best practices generalise beyond our architecture and also offer performance improvements when applied to related methods, indicating that the space of fundamental design choices has been underexplored. Our guidelines and architecture provide a strong foundation for future methods, emphasizing that optimizing core components is just as important as architectural novelty in advancing change detection performance. Code: https://github.com/blaz-r/BTC-change-detection<br>
<span id='abs_ch'>中文: 本 究证明，优化主干网络选择与训练策略等基础设计要 能显著提升变化检测性能，其效果常超越复杂架构改进，该结论在六个数据集上得到验证。</span><br>
<span id='abs_en'>English: This study demonstrates that optimizing fundamental design choices like backbone selection and training strategies can significantly enhance change detection performance, often surpassing the benefits of complex architectural additions, as validated across six datasets.</span>Paperid:1114,https://arxiv.org/pdf/2507.03331.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1500, <a href='https://arxiv.org/pdf/2507.03331.pdf' target='_blank'>https://arxiv.org/pdf/2507.03331.pdf</a></span>   <span><a href='https://github.com/SumomoTaku/DiffGuideSamp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingzhuo Li, Guang Li, Jiafeng Mao, Linfeng Ye, Takahiro Ogawa, Miki Haseyama
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03331">Task-Specific Generative Dataset Distillation with Difficulty-Guided Sampling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>To alleviate the reliance of deep neural networks on large-scale datasets, dataset distillation aims to generate compact, high-quality synthetic datasets that can achieve comparable performance to the original dataset. The integration of generative models has significantly advanced this field. However, existing approaches primarily focus on aligning the distilled dataset with the original one, often overlooking task-specific information that can be critical for optimal downstream performance. In this paper, focusing on the downstream task of classification, we propose a task-specific sampling strategy for generative dataset distillation that incorporates the concept of difficulty to consider the requirements of the target task better. The final dataset is sampled from a larger image pool with a sampling distribution obtained by matching the difficulty distribution of the original dataset. A logarithmic transformation is applied as a pre-processing step to correct for distributional bias. The results of extensive experiments demonstrate the effectiveness of our method and suggest its potential for enhancing performance on other downstream tasks. The code is available at https://github.com/SumomoTaku/DiffGuideSamp.<br>
<span id='abs_ch'>中文: 本文提出了一种针对生成式数据集蒸馏的任务特定采 策略，通过引入难度概念并匹配原始数据集的难度分布，有效提升了分类任务的性能。</span><br>
<span id='abs_en'>English: This paper introduces a task-specific sampling strategy for generative dataset distillation that incorporates difficulty-based selection to enhance classification performance by aligning with the original dataset's difficulty distribution.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1501, <a href='https://arxiv.org/pdf/2507.03302.pdf' target='_blank'>https://arxiv.org/pdf/2507.03302.pdf</a></span>   <span><a href='https://github.com/wooseok-shin/SemiOVS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wooseok Shin, Jisu Kang, Hyeonki Jeong, Jin Sob Kim, Sung Won Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03302">Leveraging Out-of-Distribution Unlabeled Images: Semi-Supervised Semantic Segmentation with an Open-Vocabulary Model</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>In semi-supervised semantic segmentation, existing studies have shown promising results in academic settings with controlled splits of benchmark datasets. However, the potential benefits of leveraging significantly larger sets of unlabeled images remain unexplored. In real-world scenarios, abundant unlabeled images are often available from online sources (web-scraped images) or large-scale datasets. However, these images may have different distributions from those of the target dataset, a situation known as out-of-distribution (OOD). Using these images as unlabeled data in semi-supervised learning can lead to inaccurate pseudo-labels, potentially misguiding network training. In this paper, we propose a new semi-supervised semantic segmentation framework with an open-vocabulary segmentation model (SemiOVS) to effectively utilize unlabeled OOD images. Extensive experiments on Pascal VOC and Context datasets demonstrate two key findings: (1) using additional unlabeled images improves the performance of semi-supervised learners in scenarios with few labels, and (2) using the open-vocabulary segmentation (OVS) model to pseudo-label OOD images leads to substantial performance gains. In particular, SemiOVS outperforms existing PrevMatch and SemiVL methods by +3.5 and +3.0 mIoU, respectively, on Pascal VOC with a 92-label setting, achieving state-of-the-art performance. These findings demonstrate that our approach effectively utilizes abundant unlabeled OOD images for semantic segmentation tasks. We hope this work can inspire future research and real-world applications. The code is available at https://github.com/wooseok-shin/SemiOVS<br>
<span id='abs_ch'>中文: 本文提出的SemiOVS框架通过开放词汇分割有效利用分布外未 记图像，在基准数据集上实现了半监督语义分割的最先进性能。English: This paper introduces SemiOVS, a semi-supervised semantic segmentation framework that leverages open-vocabulary segmentation to effectively utilize out-of-distribution unlabeled images, achieving state-of-the-art performance on benchmark datasets.Paperid:1118,https://arxiv.org/pdf/2507.03255.pdfGitHub</span><br>
<span id='abs_en'>English: This paper introduces SemiOVS, a semi-supervised semantic segmentation framework that leverages open-vocabulary segmentation to effectively utilize out-of-distribution unlabeled images, achieving state-of-the-art performance on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1502, <a href='https://arxiv.org/pdf/2507.03255.pdf' target='_blank'>https://arxiv.org/pdf/2507.03255.pdf</a></span>   <span><a href='https://github.com/zedong-peng/ForgeHLS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zedong Peng, Zeju Li, Mingzhe Gao, Qiang Xu, Chen Zhang, Jieru Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03255">ForgeHLS: A Large-Scale, Open-Source Dataset for High-Level Synthesis</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>High-Level Synthesis (HLS) plays a crucial role in modern hardware design by transforming high-level code into optimized hardware implementations. However, progress in applying machine learning (ML) to HLS optimization has been hindered by a shortage of sufficiently large and diverse datasets. To bridge this gap, we introduce ForgeHLS, a large-scale, open-source dataset explicitly designed for ML-driven HLS research. ForgeHLS comprises over 400k diverse designs generated from 846 kernels covering a broad range of application domains, consuming over 200k CPU hours during dataset construction. Each kernel includes systematically automated pragma insertions (loop unrolling, pipelining, array partitioning), combined with extensive design space exploration using Bayesian optimization. Compared to existing datasets, ForgeHLS significantly enhances scale, diversity, and design coverage. We further define and evaluate representative downstream tasks in Quality of Result (QoR) prediction and automated pragma exploration, clearly demonstrating ForgeHLS utility for developing and improving ML-based HLS optimization methodologies. The dataset and code are public at https://github.com/zedong-peng/ForgeHLS.<br>
<span id='abs_ch'>中文: ForgeHLS数据集通过提供超过40万个多 化设计，解决了高层次综合中机器学  究数据不足的问题，显著提升了结果质量预测和编译指示优化的能力。</span><br>
<span id='abs_en'>English: The ForgeHLS dataset addresses the scarcity of large-scale data for machine learning in High-Level Synthesis by providing over 400,000 diverse designs and enabling improved QoR prediction and pragma optimization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1503, <a href='https://arxiv.org/pdf/2507.03167.pdf' target='_blank'>https://arxiv.org/pdf/2507.03167.pdf</a></span>   <span><a href='https://github.com/ky295/reasoning-manipulation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kureha Yamaguchi, Benjamin Etheridge, Andy Arditi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03167">Adversarial Manipulation of Reasoning Models using Internal Representations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reasoning models generate chain-of-thought (CoT) tokens before their final output, but how this affects their vulnerability to jailbreak attacks remains unclear. While traditional language models make refusal decisions at the prompt-response boundary, we find evidence that DeepSeek-R1-Distill-Llama-8B makes these decisions within its CoT generation. We identify a linear direction in activation space during CoT token generation that predicts whether the model will refuse or comply -- termed the "caution" direction because it corresponds to cautious reasoning patterns in the generated text. Ablating this direction from model activations increases harmful compliance, effectively jailbreaking the model. We additionally show that intervening only on CoT token activations suffices to control final outputs, and that incorporating this direction into prompt-based attacks improves success rates. Our findings suggest that the chain-of-thought itself is a promising new target for adversarial manipulation in reasoning models. Code available at https://github.com/ky295/reasoning-manipulation.<br>
<span id='abs_ch'>中文摘要：本 究发现推理模型的思维链 记中存在决定拒绝行为的"谨慎"方向，通过操控该方向可有效实现越狱攻击，显著提高模型的有害服从率。</span><br>
<span id='abs_en'>English Summary: This study reveals that reasoning models' chain-of-thought tokens contain a "caution" direction in activation space that governs refusal behavior, and manipulating this direction enables effective jailbreak attacks by increasing harmful compliance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1504, <a href='https://arxiv.org/pdf/2507.03152.pdf' target='_blank'>https://arxiv.org/pdf/2507.03152.pdf</a></span>   <span><a href='https://github.com/StanfordMIMI/MedVAL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Asad Aali, Vasiliki Bikia, Maya Varma, Nicole Chiou, Sophie Ostmeier, Arnav Singhvi, Magdalini Paschali, Ashwin Kumar, Andrew Johnston, Karimar Amador-Martinez, Eduardo Juan Perez Guerrero, Paola Naovi Cruz Rivera, Sergios Gatidis, Christian Bluethgen, Eduardo Pontes Reis, Eddy D. Zandee van Rilland, Poonam Laxmappa Hosamani, Kevin R Keet, Minjoung Go, Evelyn Ling, David B. Larson, Curtis Langlotz, Roxana Daneshjou, Jason Hom, Sanmi Koyejo, Emily Alsentzer, Akshay S. Chaudhari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03152">MedVAL: Toward Expert-Level Medical Text Validation with Language Models</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the growing use of language models (LMs) in clinical environments, there is an immediate need to evaluate the accuracy and safety of LM-generated medical text. Currently, such evaluation relies solely on manual physician review. However, detecting errors in LM-generated text is challenging because 1) manual review is costly and 2) expert-composed reference outputs are often unavailable in real-world settings. While the "LM-as-judge" paradigm (a LM evaluating another LM) offers scalable evaluation, even frontier LMs can miss subtle but clinically significant errors. To address these challenges, we propose MedVAL, a novel, self-supervised, data-efficient distillation method that leverages synthetic data to train evaluator LMs to assess whether LM-generated medical outputs are factually consistent with inputs, without requiring physician labels or reference outputs. To evaluate LM performance, we introduce MedVAL-Bench, a dataset of 840 physician-annotated outputs across 6 diverse medical tasks capturing real-world challenges. Across 10 state-of-the-art LMs spanning open-source and proprietary models, MedVAL distillation significantly improves (p < 0.001) alignment with physicians across seen and unseen tasks, increasing average F1 scores from 66% to 83%. Despite strong baseline performance, MedVAL improves the best-performing proprietary LM (GPT-4o) by 8% without training on physician-labeled data, demonstrating a performance statistically non-inferior to a single human expert (p < 0.001). To support a scalable, risk-aware pathway towards clinical integration, we open-source: 1) Codebase (https://github.com/StanfordMIMI/MedVAL), 2) MedVAL-Bench (https://huggingface.co/datasets/stanfordmimi/MedVAL-Bench), 3) MedVAL-4B (https://huggingface.co/stanfordmimi/MedVAL-4B). Our benchmark provides evidence of LMs approaching expert-level ability in validating AI-generated medical text.<br>
<span id='abs_ch'>Chinese Summary: MedVAL提出了一种 需医生 注的自监督蒸馏方法，通过合成数据训练语言模型评估医疗文本准确性，在多项临床任务中显著提升了与专家判断的一致性。</span><br>
<span id='abs_en'>English Summary: MedVAL introduces a self-supervised distillation method that trains language models to evaluate medical text accuracy without physician labels, significantly improving alignment with expert assessments across diverse clinical tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1505, <a href='https://arxiv.org/pdf/2507.03112.pdf' target='_blank'>https://arxiv.org/pdf/2507.03112.pdf</a></span>   <span><a href='https://github.com/Tencent/DigitalHuman/tree/main/RLVER' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Peisong Wang, Ruotian Ma, Bang Zhang, Xingyu Chen, Zhiwei He, Kang Luo, Qingsong Lv, Qingxuan Jiang, Zheng Xie, Shanyi Wang, Yuan Li, Fanghua Ye, Jian Li, Yifan Yang, Zhaopeng Tu, Xiaolong Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03112">RLVER: Reinforcement Learning with Verifiable Emotion Rewards for Empathetic Agents</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Large language models (LLMs) excel at logical and algorithmic reasoning, yet their emotional intelligence (EQ) still lags far behind their cognitive prowess. While reinforcement learning from verifiable rewards (RLVR) has advanced in other domains, its application to dialogue-especially for emotional intelligence-remains underexplored. In this work, we introduce RLVER, the first end-to-end reinforcement learning framework that leverages verifiable emotion rewards from simulated users to cultivate higher-order empathetic abilities in LLMs. Within this framework, self-consistent affective simulated users engage in dialogue rollouts and produce deterministic emotion scores during conversations, serving as reward signals to guide the LLM's learning. Fine-tuning publicly available Qwen2.5-7B-Instruct model with PPO boosts its Sentient-Benchmark score from 13.3 to 79.2 while largely preserving mathematical and coding competence. Extensive experiments reveal that: (i) RLVER consistently improves multiple dialogue capabilities; (ii) Thinking and non-thinking models show distinct trends--thinking models excel in empathy and insight, while non-thinking models favor action; (iii) GRPO often yields stable gains, while PPO can push certain capabilities to a higher ceiling; (iv) More challenging environments are not always better-moderate ones can yield stronger outcomes. Our results show that RLVER is a practical route toward emotionally intelligent and broadly capable language agents.<br>
<span id='abs_ch'>中文: 本文提出RLVER框架，通过模拟用户的可验证情感奖励来增强大语言模型的共情能力，在保持逻辑推理能力的同时将情感智能评分从13.3提升至79.2。</span><br>
<span id='abs_en'>English: This paper introduces RLVER, the first reinforcement learning framework using verifiable emotion rewards from simulated users to significantly enhance empathetic abilities in large language models while preserving their logical reasoning capabilities.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1506, <a href='https://arxiv.org/pdf/2507.03054.pdf' target='_blank'>https://arxiv.org/pdf/2507.03054.pdf</a></span>   <span><a href='https://github.com/AnaMVasilcoiu/LATTE-Diffusion-Detector' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana Vasilcoiu, Ivona Najdenkoska, Zeno Geradts, Marcel Worring
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03054">LATTE: Latent Trajectory Embedding for Diffusion-Generated Image Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of diffusion-based image generators has made it increasingly difficult to distinguish generated from real images. This can erode trust in digital media, making it critical to develop generalizable detectors for generated images. Recent methods leverage diffusion denoising cues, but mainly focus on single-step reconstruction errors, ignoring the inherent sequential nature of the denoising process. In this work, we propose LATTE - Latent Trajectory Embedding - a novel approach that models the evolution of latent embeddings across several denoising timesteps. By modeling the trajectory of such embeddings rather than single-step errors, LATTE captures subtle, discriminative patterns that distinguish real from generated images. Each latent is refined by employing our latent-visual feature refinement module and aggregated into a unified representation. Afterwards, it is fused with the visual features and finally passed into a lightweight classifier. Our experiments demonstrate that LATTE surpasses the baselines on several established benchmarks, such as GenImage and DiffusionFake. Moreover, it demonstrates strong performance in cross-generator and cross-datasets settings, highlighting the potential of using the trajectory of latent embeddings for generated image detection. The code is available on the following link: https://github.com/AnaMVasilcoiu/LATTE-Diffusion-Detector.<br>
<span id='abs_ch'>Chinese: LATTE提出了一种通过建模多步去噪过程中潜在嵌入轨迹的新方法，用于检测AI生成图像，在跨生成器和跨数据集的场景中表现出卓越性能。</span><br>
<span id='abs_en'>English: LATTE introduces a novel method for detecting AI-generated images by modeling the trajectory of latent embeddings across multiple denoising steps, achieving superior performance in cross-generator and cross-dataset scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1507, <a href='https://arxiv.org/pdf/2507.03054.pdf' target='_blank'>https://arxiv.org/pdf/2507.03054.pdf</a></span>   <span><a href='https://github.com/AnaMVasilcoiu/LATTE-Diffusion-Detector' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana Vasilcoiu, Ivona Najdenkoska, Zeno Geradts, Marcel Worring
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03054">LATTE: Latent Trajectory Embedding for Diffusion-Generated Image Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of diffusion-based image generators has made it increasingly difficult to distinguish generated from real images. This erodes trust in digital media, making it critical to develop generated image detectors that remain reliable across different generators. While recent approaches leverage diffusion denoising cues, they typically rely on single-step reconstruction errors and overlook the sequential nature of the denoising process. In this work, we propose LATTE - LATent Trajectory Embedding - a novel approach that models the evolution of latent embeddings across multiple denoising steps. Instead of treating each denoising step in isolation, LATTE captures the trajectory of these representations, revealing subtle and discriminative patterns that distinguish real from generated images. Experiments on several benchmarks, such as GenImage, Chameleon, and Diffusion Forensics, show that LATTE achieves superior performance, especially in challenging cross-generator and cross-dataset scenarios, highlighting the potential of latent trajectory modeling. The code is available on the following link: https://github.com/AnaMVasilcoiu/LATTE-Diffusion-Detector.<br>
<span id='abs_ch'>Chinese: LATTE提出了一种通过建模多步去噪过程中潜在嵌入轨迹的新方法，用于检测AI生成图像，在跨生成器和跨数据集的场景中表现出卓越性能。</span><br>
<span id='abs_en'>English: LATTE introduces a novel method for detecting AI-generated images by modeling the trajectory of latent embeddings across multiple denoising steps, achieving superior performance in cross-generator and cross-dataset scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1508, <a href='https://arxiv.org/pdf/2507.03054.pdf' target='_blank'>https://arxiv.org/pdf/2507.03054.pdf</a></span>   <span><a href='https://github.com/AnaMVasilcoiu/LATTE-Diffusion-Detector' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ana Vasilcoiu, Ivona Najdenkoska, Zeno Geradts, Marcel Worring
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03054">LATTE: Latent Trajectory Embedding for Diffusion-Generated Image Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The rapid advancement of diffusion-based image generators has made it increasingly difficult to distinguish generated from real images. This erodes trust in digital media, making it critical to develop generated image detectors that remain reliable across different generators. While recent approaches leverage diffusion denoising cues, they typically rely on single-step reconstruction errors and overlook the sequential nature of the denoising process. In this work, we propose LATTE - LATent Trajectory Embedding - a novel approach that models the evolution of latent embeddings across multiple denoising steps. Instead of treating each denoising step in isolation, LATTE captures the trajectory of these representations, revealing subtle and discriminative patterns that distinguish real from generated images. Experiments on several benchmarks, such as GenImage, Chameleon, and Diffusion Forensics, show that LATTE achieves superior performance, especially in challenging cross-generator and cross-dataset scenarios, highlighting the potential of latent trajectory modeling. The code is available on the following link: https://github.com/AnaMVasilcoiu/LATTE-Diffusion-Detector.<br>
<span id='abs_ch'>Chinese: LATTE提出了一种通过建模多步去噪过程中潜在嵌入轨迹的新方法，用于检测AI生成图像，在跨生成器和跨数据集的场景中表现出卓越性能。</span><br>
<span id='abs_en'>English: LATTE introduces a novel method for detecting AI-generated images by modeling the trajectory of latent embeddings across multiple denoising steps, achieving superior performance in cross-generator and cross-dataset scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1509, <a href='https://arxiv.org/pdf/2507.03038.pdf' target='_blank'>https://arxiv.org/pdf/2507.03038.pdf</a></span>   <span><a href='https://github.com/wyzjack/CNTP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Wang, Lingzhi Zhang, Yue Bai, Mang Tik Chiu, Zhengmian Hu, Mingyuan Zhang, Qihua Dong, Yu Yin, Sohrab Amirghodsi, Yun Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03038">Cautious Next Token Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Next token prediction paradigm has been prevailing for autoregressive models in the era of LLMs. The current default sampling choice for popular LLMs is temperature scaling together with nucleus sampling to balance diversity and coherence. Nevertheless, such approach leads to inferior performance in various NLP tasks when the model is not certain about testing questions. To this end, we propose a brand new training-free decoding strategy, dubbed as Cautious Next Token Prediction (CNTP). In the decoding process, if the model has comparatively high prediction entropy at a certain step, we sample multiple trials starting from the step independently and stop when encountering any punctuation. Then we select the trial with the lowest perplexity score viewed as the most probable and reliable trial path given the model's capacity. The trial number is negatively correlated with the prediction confidence, i.e., the less confident the model is, the more trials it should sample. This is consistent with human beings' behaviour: when feeling uncertain or unconfident, one tends to think more creatively, exploring multiple thinking paths, to cautiously select the path one feels most confident about. Extensive experiments on both LLMs and MLLMs show that our proposed CNTP approach outperforms existing standard decoding strategies consistently by a clear margin. Moreover, the integration of CNTP with self consistency can further improve over vanilla self consistency. We believe our proposed CNTP has the potential to become one of the default choices for LLM decoding. Code is available at https://github.com/wyzjack/CNTP.<br>
<span id='abs_ch'>中文摘要：本文提出谨慎下一词预测（CNTP）这一 需训练的解 策略，当模型预测不确定性较高时并行采 多个候选路径，并依据困惑度选择最优路径，在各类大语言模型任务中显著优于现有 准解 方法。</span><br>
<span id='abs_en'>English Summary: The paper introduces Cautious Next Token Prediction (CNTP), a training-free decoding strategy that samples multiple token paths when model uncertainty is high and selects the most reliable one based on perplexity, significantly outperforming standard decoding methods across various language models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1510, <a href='https://arxiv.org/pdf/2507.02948.pdf' target='_blank'>https://arxiv.org/pdf/2507.02948.pdf</a></span>   <span><a href='https://github.com/hzy138/DriveMRP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyi Hou, Enhui Ma, Fang Li, Zhiyi Lai, Kalok Ho, Zhanqian Wu, Lijun Zhou, Long Chen, Chitian Sun, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Kaicheng Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02948">DriveMRP: Enhancing Vision-Language Models with Synthetic Motion Data for Motion Risk Prediction</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Autonomous driving has seen significant progress, driven by extensive real-world data. However, in long-tail scenarios, accurately predicting the safety of the ego vehicle's future motion remains a major challenge due to uncertainties in dynamic environments and limitations in data coverage. In this work, we aim to explore whether it is possible to enhance the motion risk prediction capabilities of Vision-Language Models (VLM) by synthesizing high-risk motion data. Specifically, we introduce a Bird's-Eye View (BEV) based motion simulation method to model risks from three aspects: the ego-vehicle, other vehicles, and the environment. This allows us to synthesize plug-and-play, high-risk motion data suitable for VLM training, which we call DriveMRP-10K. Furthermore, we design a VLM-agnostic motion risk estimation framework, named DriveMRP-Agent. This framework incorporates a novel information injection strategy for global context, ego-vehicle perspective, and trajectory projection, enabling VLMs to effectively reason about the spatial relationships between motion waypoints and the environment. Extensive experiments demonstrate that by fine-tuning with DriveMRP-10K, our DriveMRP-Agent framework can significantly improve the motion risk prediction performance of multiple VLM baselines, with the accident recognition accuracy soaring from 27.13% to 88.03%. Moreover, when tested via zero-shot evaluation on an in-house real-world high-risk motion dataset, DriveMRP-Agent achieves a significant performance leap, boosting the accuracy from base_model's 29.42% to 68.50%, which showcases the strong generalization capabilities of our method in real-world scenarios.<br>
<span id='abs_ch'>中文: 本 究提出DriveMRP-10K合成高风险运动数据集和DriveMRP-Agent框架，通过增强视觉语言模型的运动风险预测能力，显著提升了事故识别准确率，并在真实场景中展现出强大的泛化性能。</span><br>
<span id='abs_en'>English: This research introduces DriveMRP-10K, a synthesized high-risk motion dataset, and the DriveMRP-Agent framework to enhance Vision-Language Models' motion risk prediction, significantly improving accident recognition accuracy and demonstrating strong generalization in real-world scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1511, <a href='https://arxiv.org/pdf/2507.02939.pdf' target='_blank'>https://arxiv.org/pdf/2507.02939.pdf</a></span>   <span><a href='https://github.com/itsnotacie/SDKD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Li, Chuanguang Yang, Hansheng Zeng, Zeyu Dong, Zhulin An, Yongjun Xu, Yingli Tian, Hao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02939">Frequency-Aligned Knowledge Distillation for Lightweight Spatiotemporal Forecasting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Spatiotemporal forecasting tasks, such as traffic flow, combustion dynamics, and weather forecasting, often require complex models that suffer from low training efficiency and high memory consumption. This paper proposes a lightweight framework, Spectral Decoupled Knowledge Distillation (termed SDKD), which transfers the multi-scale spatiotemporal representations from a complex teacher model to a more efficient lightweight student network. The teacher model follows an encoder-latent evolution-decoder architecture, where its latent evolution module decouples high-frequency details and low-frequency trends using convolution and Transformer (global low-frequency modeler). However, the multi-layer convolution and deconvolution structures result in slow training and high memory usage. To address these issues, we propose a frequency-aligned knowledge distillation strategy, which extracts multi-scale spectral features from the teacher's latent space, including both high and low frequency components, to guide the lightweight student model in capturing both local fine-grained variations and global evolution patterns. Experimental results show that SDKD significantly improves performance, achieving reductions of up to 81.3% in MSE and in MAE 52.3% on the Navier-Stokes equation dataset. The framework effectively captures both high-frequency variations and long-term trends while reducing computational complexity. Our codes are available at https://github.com/itsnotacie/SDKD<br>
<span id='abs_ch'>中文摘要：本文提出了一种名为谱解耦知识蒸馏（SDKD）的轻量级框架，通过将复杂教师模型的多尺度时空知识迁移到高效学生网络中，在显著提升预测精度的同时降低了计算复杂度。</span><br>
<span id='abs_en'>English Summary: This paper introduces a lightweight framework called Spectral Decoupled Knowledge Distillation (SDKD) that transfers multi-scale spatiotemporal knowledge from a complex teacher model to an efficient student network, significantly improving forecasting accuracy while reducing computational complexity.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1512, <a href='https://arxiv.org/pdf/2507.02935.pdf' target='_blank'>https://arxiv.org/pdf/2507.02935.pdf</a></span>   <span><a href='https://github.com/fardinsaad/Tomcat-LLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fardin Saad, Pradeep K. Murukannaiah, Munindar P. Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02935">Theory of Mind in Action: The Instruction Inference Task</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The Theory of Mind (ToM) refers to an agent's capacity to infer the mental states of other agents. ToM is essential for effective collaboration. To assess ToM in a dynamic, goal-oriented, and collaborative environment, we introduce a novel task, Instruction Inference, in which an agent assists a principal in reaching a goal by interpreting indirect or ambiguous instructions. We present Tomcat, an LLM-based agent, designed to exhibit ToM reasoning in interpreting and responding to the principal's instructions. We implement two variants of Tomcat. One, dubbed Fs-CoT, is based on a small number of examples (i.e., few-shot or Fs) demonstrating the requisite structured reasoning (i.e., chain-of-thought or CoT). One, dubbed CP, relies on commonsense knowledge and information about the problem (i.e., commonsense prompt or CP). We realized both variants of Tomcat on three leading large language models (LLMs), namely, GPT-4o, DeepSeek-R1, and Gemma-3-27B. To evaluate the effectiveness of Tomcat, we conducted a study with 52 human participants in which we provided participants with the same information as the CP variant of Tomcat. We computed intent accuracy, action optimality, and planning optimality to measure the ToM capabilities of Tomcat and our study participants. We found that Tomcat with Fs-CoT, particularly with GPT-4o and DeepSeek-R1, achieves performance comparable to the human participants, underscoring its ToM potential for human-AI collaboration.<br>
<span id='abs_ch'>中文摘要：本 究提出了基于大语言模型的Tomcat智能体，通过指令推理任务展示心理理论推理能力，其中Fs-CoT变体在协作场景中实现了与人类相当的表现水平。</span><br>
<span id='abs_en'>English Summary: This study introduces Tomcat, an LLM-based agent designed to demonstrate Theory of Mind reasoning through instruction inference tasks, with its Fs-CoT variant achieving human-comparable performance in collaborative scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1513, <a href='https://arxiv.org/pdf/2507.02932.pdf' target='_blank'>https://arxiv.org/pdf/2507.02932.pdf</a></span>   <span><a href='https://github.com/zhangruochi/MolProphecy' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianping Zhao, Qiong Zhou, Tian Wang, Yusi Fan, Qian Yang, Li Jiao, Chang Liu, Zhehao Guo, Qi Lu, Fengfeng Zhou, Ruochi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02932">MolProphecy: Bridging Medicinal Chemists' Knowledge and Molecular Pre-Trained Models via a Multi-Modal Framework</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>MolProphecy is a human-in-the-loop (HITL) multi-modal framework designed to integrate chemists' domain knowledge into molecular property prediction models. While molecular pre-trained models have enabled significant gains in predictive accuracy, they often fail to capture the tacit, interpretive reasoning central to expert-driven molecular design. To address this, MolProphecy employs ChatGPT as a virtual chemist to simulate expert-level reasoning and decision-making. The generated chemist knowledge is embedded by the large language model (LLM) as a dedicated knowledge representation and then fused with graph-based molecular features through a gated cross-attention mechanism, enabling joint reasoning over human-derived and structural features. Evaluated on four benchmark datasets (FreeSolv, BACE, SIDER, and ClinTox), MolProphecy outperforms state-of-the-art (SOTA) models, achieving a 15.0 percent reduction in RMSE on FreeSolv and a 5.39 percent improvement in AUROC on BACE. Analysis reveals that chemist knowledge and structural features provide complementary contributions, improving both accuracy and interpretability. MolProphecy offers a practical and generalizable approach for collaborative drug discovery, with the flexibility to incorporate real chemist input in place of the current simulated proxy--without the need for model retraining. The implementation is publicly available at https://github.com/zhangruochi/MolProphecy.<br>
<span id='abs_ch'>中文: MolProphecy是一个结合模拟化学家知识与分子结构特征的人机协同框架，通过在基准数据集上的卓越表现，显著提升了分子属性预测的准确性和可解释性。</span><br>
<span id='abs_en'>English: MolProphecy is a human-in-the-loop framework that integrates simulated chemist knowledge via ChatGPT with molecular structural features, achieving superior performance on benchmark datasets by enhancing both prediction accuracy and interpretability.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1514, <a href='https://arxiv.org/pdf/2507.02921.pdf' target='_blank'>https://arxiv.org/pdf/2507.02921.pdf</a></span>   <span><a href='https://github.com/mohammadhashemii/PlaceFM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammad Hashemi, Hossein Amiri, Andreas Zufle
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02921">PlaceFM: A Training-free Geospatial Foundation Model of Places using Large-Scale Point of Interest Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>With the rapid growth and continual updates of geospatial data from diverse sources, geospatial foundation model pre-training for urban representation learning has emerged as a key research direction for advancing data-driven urban planning. Spatial structure is fundamental to effective geospatial intelligence systems; however, existing foundation models often lack the flexibility to reason about places, context-rich regions spanning multiple spatial granularities that may consist of many spatially and semantically related points of interest. To address this gap, we propose PlaceFM, a geospatial foundation model that captures place representations through a training-free, clustering-based approach. PlaceFM summarizes the entire point of interest graph constructed from U.S. Foursquare data, producing general-purpose region embeddings while automatically identifying places of interest. These embeddings can be directly integrated into geolocation data pipelines to support a variety of urban downstream tasks. Without the need for costly pre-training, PlaceFM provides a scalable and efficient solution for multi-granular geospatial analysis. Extensive experiments on two real-world prediction tasks, ZIP code-level population density and housing prices, demonstrate that PlaceFM not only outperforms most state-of-the-art graph-based geospatial foundation models but also achieves up to a 100x speedup in generating region-level representations on large-scale POI graphs. The implementation is available at https://github.com/mohammadhashemii/PlaceFM.<br>
<span id='abs_ch'>中文: PlaceFM是一种 需预训练的地理空间基础模型，通过基于聚类的 训练方法从兴趣点数据中生成可扩展的区域嵌入，在多项城市预测任务中优于现有模型，并实现了高达百倍的效率提升。</span><br>
<span id='abs_en'>English: PlaceFM is a geospatial foundation model that uses a training-free clustering approach to generate scalable region embeddings from point-of-interest data, outperforming existing models in urban prediction tasks while achieving significant efficiency gains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1515, <a href='https://arxiv.org/pdf/2507.02910.pdf' target='_blank'>https://arxiv.org/pdf/2507.02910.pdf</a></span>   <span><a href='https://github.com/Cho-Geonwoo/CP-DRL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Geonwoo Cho, Jaegyun Im, Doyoon Kim, Sundong Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02910">Causal-Paced Deep Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Designing effective task sequences is crucial for curriculum reinforcement learning (CRL), where agents must gradually acquire skills by training on intermediate tasks. A key challenge in CRL is to identify tasks that promote exploration, yet are similar enough to support effective transfer. While recent approach suggests comparing tasks via their Structural Causal Models (SCMs), the method requires access to ground-truth causal structures, an unrealistic assumption in most RL settings. In this work, we propose Causal-Paced Deep Reinforcement Learning (CP-DRL), a curriculum learning framework aware of SCM differences between tasks based on interaction data approximation. This signal captures task novelty, which we combine with the agent's learnability, measured by reward gain, to form a unified objective. Empirically, CP-DRL outperforms existing curriculum methods on the Point Mass benchmark, achieving faster convergence and higher returns. CP-DRL demonstrates reduced variance with comparable final returns in the Bipedal Walker-Trivial setting, and achieves the highest average performance in the Infeasible variant. These results indicate that leveraging causal relationships between tasks can improve the structure-awareness and sample efficiency of curriculum reinforcement learning. We provide the full implementation of CP-DRL to facilitate the reproduction of our main results at https://github.com/Cho-Geonwoo/CP-DRL.<br>
<span id='abs_ch'>中文: 本文提出 果节奏深度强化学 （CP-DRL），该课程学 框架通过任务间近似 果差异增强探索与迁移能力，在强化学 基准测试中实现了更优的性能和 本效率。English: This paper introduces Causal-Paced Deep Reinforcement Learning (CP-DRL), a curriculum learning framework that leverages approximated causal differences between tasks to enhance exploration and transfer, achieving superior performance and sample efficiency in reinforcement learning benchmarks.Paperid:1142,https://arxiv.org/pdf/2507.02909.pdfGitHub</span><br>
<span id='abs_en'>English: This paper introduces Causal-Paced Deep Reinforcement Learning (CP-DRL), a curriculum learning framework that leverages approximated causal differences between tasks to enhance exploration and transfer, achieving superior performance and sample efficiency in reinforcement learning benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1516, <a href='https://arxiv.org/pdf/2507.02900.pdf' target='_blank'>https://arxiv.org/pdf/2507.02900.pdf</a></span>   <span><a href='https://github.com/VineetKumarRakesh/thg' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vineet Kumar Rakesh, Soumya Mazumdar, Research Pratim Maity, Sarbajit Pal, Amitabha Das, Tapas Samanta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02900">Advancing Talking Head Generation: A Comprehensive Survey of Multi-Modal Methodologies, Datasets, Evaluation Metrics, and Loss Functions</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Talking Head Generation (THG) has emerged as a transformative technology in computer vision, enabling the synthesis of realistic human faces synchronized with image, audio, text, or video inputs. This paper provides a comprehensive review of methodologies and frameworks for talking head generation, categorizing approaches into 2D--based, 3D--based, Neural Radiance Fields (NeRF)--based, diffusion--based, parameter-driven techniques and many other techniques. It evaluates algorithms, datasets, and evaluation metrics while highlighting advancements in perceptual realism and technical efficiency critical for applications such as digital avatars, video dubbing, ultra-low bitrate video conferencing, and online education. The study identifies challenges such as reliance on pre--trained models, extreme pose handling, multilingual synthesis, and temporal consistency. Future directions include modular architectures, multilingual datasets, hybrid models blending pre--trained and task-specific layers, and innovative loss functions. By synthesizing existing research and exploring emerging trends, this paper aims to provide actionable insights for researchers and practitioners in the field of talking head generation. For the complete survey, code, and curated resource list, visit our GitHub repository: https://github.com/VineetKumarRakesh/thg.<br>
<span id='abs_ch'>中文: 本文全面综述了说话头生成技术，评估了其应用、挑战及未来方向，为 究人员和从业者提供了实用指导。</span><br>
<span id='abs_en'>English: This paper offers a comprehensive review of talking head generation methods, evaluating their applications, challenges, and future directions to guide researchers and practitioners in the field.Paperid:1144,https://arxiv.org/pdf/2507.02892.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1517, <a href='https://arxiv.org/pdf/2507.02892.pdf' target='_blank'>https://arxiv.org/pdf/2507.02892.pdf</a></span>   <span><a href='https://github.com/ForrestXie9/LLM-SAEA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lindong Xie, Genghui Li, Zhenkun Wang, Edward Chung, Maoguo Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02892">Large Language Model-Driven Surrogate-Assisted Evolutionary Algorithm for Expensive Optimization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Surrogate-assisted evolutionary algorithms (SAEAs) are a key tool for addressing costly optimization tasks, with their efficiency being heavily dependent on the selection of surrogate models and infill sampling criteria. However, designing an effective dynamic selection strategy for SAEAs is labor-intensive and requires substantial domain knowledge. To address this challenge, this paper proposes LLM-SAEA, a novel approach that integrates large language models (LLMs) to configure both surrogate models and infill sampling criteria online. Specifically, LLM-SAEA develops a collaboration-of-experts framework, where one LLM serves as a scoring expert (LLM-SE), assigning scores to surrogate models and infill sampling criteria based on their optimization performance, while another LLM acts as a decision expert (LLM-DE), selecting the appropriate configurations by analyzing their scores along with the current optimization state. Experimental results demonstrate that LLM-SAEA outperforms several state-of-the-art algorithms across standard test cases. The source code is publicly available at https://github.com/ForrestXie9/LLM-SAEA.<br>
<span id='abs_ch'>中文: 本文提出LLM-SAEA，一种新颖的代理辅助进化算法，通过集成大语言模型在线动态选择代理模型和填充采  准，实验证明其在 准测试案例中优于多种先进算法。</span><br>
<span id='abs_en'>English: This paper introduces LLM-SAEA, a novel surrogate-assisted evolutionary algorithm that leverages large language models to dynamically select surrogate models and infill sampling criteria online, demonstrating superior performance over existing methods in experimental tests.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1518, <a href='https://arxiv.org/pdf/2507.02877.pdf' target='_blank'>https://arxiv.org/pdf/2507.02877.pdf</a></span>   <span><a href='https://github.com/Darius18/AuraGenome' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chi Zhang, Yu Dong, Yang Wang, Yuetong Han, Guihua Shan, Bixia Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02877">AuraGenome: An LLM-Powered Framework for On-the-Fly Reusable and Scalable Circular Genome Visualizations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Circular genome visualizations are essential for exploring structural variants and gene regulation. However, existing tools often require complex scripting and manual configuration, making the process time-consuming, error-prone, and difficult to learn. To address these challenges, we introduce AuraGenome, an LLM-powered framework for rapid, reusable, and scalable generation of multi-layered circular genome visualizations. AuraGenome combines a semantic-driven multi-agent workflow with an interactive visual analytics system. The workflow employs seven specialized LLM-driven agents, each assigned distinct roles such as intent recognition, layout planning, and code generation, to transform raw genomic data into tailored visualizations. The system supports multiple coordinated views tailored for genomic data, offering ring, radial, and chord-based layouts to represent multi-layered circular genome visualizations. In addition to enabling interactions and configuration reuse, the system supports real-time refinement and high-quality report export. We validate its effectiveness through two case studies and a comprehensive user study. AuraGenome is available at: https://github.com/Darius18/AuraGenome.<br>
<span id='abs_ch'>中文摘要：AuraGenome是一个基于大语言模型的框架，通过多智能体工作流和交互式分析系统自动生成多层环形基 组可视化， 需复杂编程即可快速创建可定制的高质量基 组图谱。</span><br>
<span id='abs_en'>English Summary: AuraGenome is an LLM-powered framework that automates circular genome visualization through a multi-agent workflow and interactive system, enabling rapid generation of customizable multi-layered genomic diagrams without complex scripting.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1519, <a href='https://arxiv.org/pdf/2507.02863.pdf' target='_blank'>https://arxiv.org/pdf/2507.02863.pdf</a></span>   <span><a href='https://github.com/YkiWu/Point3R' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/YkiWu/Point3R' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Wu, Wenzhao Zheng, Jie Zhou, Jiwen Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02863">Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Dense 3D scene reconstruction from an ordered sequence or unordered image collections is a critical step when bringing research in computer vision into practical scenarios. Following the paradigm introduced by DUSt3R, which unifies an image pair densely into a shared coordinate system, subsequent methods maintain an implicit memory to achieve dense 3D reconstruction from more images. However, such implicit memory is limited in capacity and may suffer from information loss of earlier frames. We propose Point3R, an online framework targeting dense streaming 3D reconstruction. To be specific, we maintain an explicit spatial pointer memory directly associated with the 3D structure of the current scene. Each pointer in this memory is assigned a specific 3D position and aggregates scene information nearby in the global coordinate system into a changing spatial feature. Information extracted from the latest frame interacts explicitly with this pointer memory, enabling dense integration of the current observation into the global coordinate system. We design a 3D hierarchical position embedding to promote this interaction and design a simple yet effective fusion mechanism to ensure that our pointer memory is uniform and efficient. Our method achieves competitive or state-of-the-art performance on various tasks with low training costs. Code is available at: https://github.com/YkiWu/Point3R.<br>
<span id='abs_ch'>中文摘要：Point3R提出了一种在线密集三维重建框架，通过显式空间指针内存直接关联三维场景结构，有效整合最新观测数据，在保持低训练成本的同时实现了优越的性能。</span><br>
<span id='abs_en'>English Summary: Point3R introduces an online framework for dense 3D reconstruction using explicit spatial pointer memory that directly associates with 3D structures, enabling efficient integration of new observations while maintaining competitive performance with low training costs.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1520, <a href='https://arxiv.org/pdf/2507.02856.pdf' target='_blank'>https://arxiv.org/pdf/2507.02856.pdf</a></span>   <span><a href='https://github.com/nikhilchandak/answer-matching' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02856">Answer Matching Outperforms Multiple Choice for Language Model Evaluation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multiple choice benchmarks have long been the workhorse of language model evaluation because grading multiple choice is objective and easy to automate. However, we show multiple choice questions from popular benchmarks can often be answered without even seeing the question. These shortcuts arise from a fundamental limitation of discriminative evaluation not shared by evaluations of the model's free-form, generative answers. Until recently, there appeared to be no viable, scalable alternative to multiple choice--but, we show that this has changed. We consider generative evaluation via what we call answer matching: Give the candidate model the question without the options, have it generate a free-form response, then use a modern language model with the reference answer to determine if the response matches the reference. To compare the validity of different evaluation strategies, we annotate MMLU-Pro and GPQA-Diamond to obtain human grading data, and measure the agreement of each evaluation approach. We find answer matching using recent models--even small ones--achieves near-perfect agreement, in the range of inter-annotator agreement. In contrast, both multiple choice evaluation and using LLM-as-a-judge without reference answers aligns poorly with human grading. Improving evaluations via answer matching is not merely a conceptual concern: the rankings of several models change significantly when evaluating their free-form responses with answer matching. In light of these findings, we discuss how to move the evaluation ecosystem from multiple choice to answer matching.<br>
<span id='abs_ch'>中文: 尽管多选题基准便于评估，但常存在 需理解问题即可作答的捷径，而采用现代语言模型进行答案匹配的生成式评估不仅与人工评分高度一致，还显著改变了模型排名。</span><br>
<span id='abs_en'>English: Multiple choice benchmarks, while convenient, often contain shortcuts that allow answers without understanding the question, but generative evaluation through answer matching using modern language models achieves near-perfect agreement with human grading and significantly alters model rankings.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1521, <a href='https://arxiv.org/pdf/2507.02851.pdf' target='_blank'>https://arxiv.org/pdf/2507.02851.pdf</a></span>   <span><a href='https://github.com/purbeshmitra/MOTIF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Purbesh Mitra, Sennur Ulukus
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02851">MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in the reasoning capabilities of large language models (LLMs) show that employing group relative policy optimization (GRPO) algorithm for reinforcement learning (RL) training allows the models to use more thinking/reasoning tokens for generating better responses. However, LLMs can generate only a finite amount of tokens while maintaining attention to the previously generated tokens. This limit, also known as the context size of an LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens. To think beyond the limit of context size, an LLM must employ a modular thinking strategy to reason over multiple rounds. In this work, we propose $\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL training method for generating thinking tokens in multiple rounds, effectively allowing the model to think with additional context size. We trained the open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training in the respective benchmarks. Furthermore, this improvement was achieved with only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code and models are available at https://github.com/purbeshmitra/MOTIF and https://huggingface.co/purbeshmitra/MOTIF, respectively.<br>
<span id='abs_ch'>中文摘要：提出的MOTIF方法通过强化学 实现模块化多轮思考，有效提升大语言模型的推理能力，在基准测试中相比 统方法以更高 本效率显著提高了准确率。</span><br>
<span id='abs_en'>English Summary: The proposed MOTIF method enhances large language models' reasoning by enabling modular, multi-round thinking through reinforcement learning, significantly improving accuracy on benchmarks with greater sample efficiency than previous approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1522, <a href='https://arxiv.org/pdf/2507.02748.pdf' target='_blank'>https://arxiv.org/pdf/2507.02748.pdf</a></span>   <span><a href='https://github.com/AlexColagrande/MANO' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alex Colagrande, Paul Caillon, Eva Feillet, Alexandre Allauzen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02748">Linear Attention with Global Context: A Multipole Attention Mechanism for Vision and Physics</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Transformers have become the de facto standard for a wide range of tasks, from image classification to physics simulations. Despite their impressive performance, the quadratic complexity of standard Transformers in both memory and time with respect to the input length makes them impractical for processing high-resolution inputs. Therefore, several variants have been proposed, the most successful relying on patchification, downsampling, or coarsening techniques, often at the cost of losing the finest-scale details. In this work, we take a different approach. Inspired by state-of-the-art techniques in $n$-body numerical simulations, we cast attention as an interaction problem between grid points. We introduce the Multipole Attention Neural Operator (MANO), which computes attention in a distance-based multiscale fashion. MANO maintains, in each attention head, a global receptive field and achieves linear time and memory complexity with respect to the number of grid points. Empirical results on image classification and Darcy flows demonstrate that MANO rivals state-of-the-art models such as ViT and Swin Transformer, while reducing runtime and peak memory usage by orders of magnitude. We open source our code for reproducibility at https://github.com/AlexColagrande/MANO.<br>
<span id='abs_ch'>Chinese: 多极注意力神经算子（MANO）借鉴n体数值模拟技术，采用基于距离的多尺度注意力机制，在图像分类等任务中性能媲美ViT和Swin Transformer等先进模型，同时实现线性时空复杂度并大幅降低计算资源消耗。</span><br>
<span id='abs_en'>English: The Multipole Attention Neural Operator (MANO) introduces a distance-based multiscale attention mechanism inspired by n-body simulations, achieving linear complexity in time and memory while maintaining competitive performance with models like ViT and Swin Transformer across tasks such as image classification.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1523, <a href='https://arxiv.org/pdf/2507.02652.pdf' target='_blank'>https://arxiv.org/pdf/2507.02652.pdf</a></span>   <span><a href='https://github.com/ignorejjj/HiRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiajie Jin, Xiaoxi Li, Guanting Dong, Yuyao Zhang, Yutao Zhu, Yang Zhao, Hongjin Qian, Zhicheng Dou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02652">Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Complex information needs in real-world search scenarios demand deep reasoning and knowledge synthesis across diverse sources, which traditional retrieval-augmented generation (RAG) pipelines struggle to address effectively. Current reasoning-based approaches suffer from a fundamental limitation: they use a single model to handle both high-level planning and detailed execution, leading to inefficient reasoning and limited scalability. In this paper, we introduce HiRA, a hierarchical framework that separates strategic planning from specialized execution. Our approach decomposes complex search tasks into focused subtasks, assigns each subtask to domain-specific agents equipped with external tools and reasoning capabilities, and coordinates the results through a structured integration mechanism. This separation prevents execution details from disrupting high-level reasoning while enabling the system to leverage specialized expertise for different types of information processing. Experiments on four complex, cross-modal deep search benchmarks demonstrate that HiRA significantly outperforms state-of-the-art RAG and agent-based systems. Our results show improvements in both answer quality and system efficiency, highlighting the effectiveness of decoupled planning and execution for multi-step information seeking tasks. Our code is available at https://github.com/ignorejjj/HiRA.<br>
<span id='abs_ch'>Chinese: HiRA 提出了一种分层框架，将策略规划与专业执行分离，以优化复杂搜索任务，在答案质量和系统效率上均显著优于现有方法。English: HiRA introduces a hierarchical framework that separates strategic planning from specialized execution to enhance complex search tasks, significantly outperforming existing systems in both answer quality and efficiency.Paperid:1159,https://arxiv.org/pdf/2507.02620.pdfGitHub</span><br>
<span id='abs_en'>English: HiRA introduces a hierarchical framework that separates strategic planning from specialized execution to enhance complex search tasks, significantly outperforming existing systems in both answer quality and efficiency.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1524, <a href='https://arxiv.org/pdf/2507.02620.pdf' target='_blank'>https://arxiv.org/pdf/2507.02620.pdf</a></span>   <span><a href='https://github.com/Leosang-lx/FlowSpec#' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xing Liu, Lizhuo Luo, Ming Tang, Chao Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02620">FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Distributed inference serves as a promising approach to enabling the inference of large language models (LLMs) at the network edge. It distributes the inference process to multiple devices to ensure that the LLMs can fit into the device memory. Recent pipeline-based approaches have the potential to parallelize communication and computation, which helps reduce inference latency. However, the benefit diminishes when the inference request at the network edge is sparse, where pipeline is typically at low utilization. To enable efficient distributed LLM inference at the edge, we propose \textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding framework. FlowSpec incorporates three key mechanisms to improve decoding efficiency: 1) score-based step-wise verification prioritizes more important draft tokens to bring earlier accpeted tokens; 2) efficient draft management to prune invalid tokens while maintaining correct causal relationship during verification; 3) dynamic draft expansion strategies to supply high-quality speculative inputs. These techniques work in concert to enhance both pipeline utilization and speculative efficiency. We evaluate FlowSpec on a real-world testbed with other baselines. Experimental results demonstrate that our proposed framework significantly improves inference speed across diverse models and configurations, achieving speedup ratios 1.28$\times$-1.79$\times$ compared to baselines. Our code is publicly available at \href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\#}<br>
<span id='abs_ch'>Chinese: FlowSpec是一种基于流水线并行和 形结构的推测解 框架，通过优先级验证、草稿管理和动态扩展机制提升边缘分布式大语言模型推理效率，实验显示其推理速度较基线方法提升1.28至1.79倍。</span>English: FlowSpec is a pipeline-parallel tree-based speculative decoding framework that enhances distributed LLM inference efficiency at the edge through prioritized verification, draft management, and dynamic expansion, achieving 1.28×-1.79× speedup over baselines.Paperid:1160,https://arxiv.org/pdf/2507.02554.pdfGitHub</span><br>
<span id='abs_en'>English: FlowSpec is a pipeline-parallel tree-based speculative decoding framework that enhances distributed LLM inference efficiency at the edge through prioritized verification, draft management, and dynamic expansion, achieving 1.28×-1.79× speedup over baselines.Paperid:1160,https://arxiv.org/pdf/2507.02554.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1525, <a href='https://arxiv.org/pdf/2507.02554.pdf' target='_blank'>https://arxiv.org/pdf/2507.02554.pdf</a></span>   <span><a href='https://github.com/facebookresearch/aira-dojo' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Edan Toledo, Karen Hambardzumyan, Martin Josifoski, Rishi Hazra, Nicolas Baldwin, Alexis Audran-Reiss, Michael Kuchnik, Despoina Magka, Minqi Jiang, Alisia Maria Lupidi, Andrei Lupu, Roberta Raileanu, Kelvin Niu, Tatiana Shavrina, Jean-Christophe Gagnon-Audet, Michael Shvartsman, Shagun Sodhani, Alexander H. Miller, Abhishek Charnalia, Derek Dunfield, Carole-Jean Wu, Pontus Stenetorp, Nicola Cancedda, Jakob Nicolaus Foerster, Yoram Bachrach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02554">AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>AI research agents are demonstrating great potential to accelerate scientific progress by automating the design, implementation, and training of machine learning models. We focus on methods for improving agents' performance on MLE-bench, a challenging benchmark where agents compete in Kaggle competitions to solve real-world machine learning problems. We formalize AI research agents as search policies that navigate a space of candidate solutions, iteratively modifying them using operators. By designing and systematically varying different operator sets and search policies (Greedy, MCTS, Evolutionary), we show that their interplay is critical for achieving high performance. Our best pairing of search strategy and operator set achieves a state-of-the-art result on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from 39.6% to 47.7%. Our investigation underscores the importance of jointly considering the search strategy, operator design, and evaluation methodology in advancing automated machine learning.<br>
<span id='abs_ch'>中文: 人工智能 究代理被形式化为在解空间中导航的搜索策略，通过优化搜索策略与操作集的协同作用，在MLE-bench基准测试中取得了突 性成果。</span><br>
<span id='abs_en'>English: AI research agents are formalized as search policies that navigate solution spaces using operators, with the interplay between search strategies and operator sets proving critical for achieving state-of-the-art performance on the MLE-bench benchmark.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1526, <a href='https://arxiv.org/pdf/2507.02503.pdf' target='_blank'>https://arxiv.org/pdf/2507.02503.pdf</a></span>   <span><a href='https://github.com/Wcxwcxw/GORP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxu Wang, Yilin Lyu, Zicheng Sun, Liping Jing
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02503">Continual Gradient Low-Rank Projection Fine-Tuning for LLMs</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Continual fine-tuning of Large Language Models (LLMs) is hampered by the trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA) offers efficiency but constrains the model's ability to learn new tasks and transfer knowledge due to its low-rank nature and reliance on explicit parameter constraints. We propose GORP (Gradient LOw Rank Projection) for Continual Learning, a novel training strategy that overcomes these limitations by synergistically combining full and low-rank parameters and jointly updating within a unified low-rank gradient subspace. GORP expands the optimization space while preserving efficiency and mitigating catastrophic forgetting. Extensive experiments on continual learning benchmarks demonstrate GORP's superior performance compared to existing state-of-the-art approaches. Code is available at https://github.com/Wcxwcxw/GORP.<br>
<span id='abs_ch'>中文: GORP是一种新颖的持续学 策略，通过协同结合完整参数和低秩参数来扩展优化空间，同时保持效率并减少灾难性遗忘，在基准测试中优于现有方法。</span><br>
<span id='abs_en'>English: GORP is a novel continual learning strategy that synergistically combines full and low-rank parameters to expand the optimization space while maintaining efficiency and reducing catastrophic forgetting, outperforming existing methods in benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1527, <a href='https://arxiv.org/pdf/2507.02493.pdf' target='_blank'>https://arxiv.org/pdf/2507.02493.pdf</a></span>   <span><a href='https://github.com/lparolari/temporally-aware-polyp-counting' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca Parolari, Andrea Cherubini, Lamberto Ballan, Carlo Biffi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02493">Temporally-Aware Supervised Contrastive Learning for Polyp Counting in Colonoscopy</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Automated polyp counting in colonoscopy is a crucial step toward automated procedure reporting and quality control, aiming to enhance the cost-effectiveness of colonoscopy screening. Counting polyps in a procedure involves detecting and tracking polyps, and then clustering tracklets that belong to the same polyp entity. Existing methods for polyp counting rely on self-supervised learning and primarily leverage visual appearance, neglecting temporal relationships in both tracklet feature learning and clustering stages. In this work, we introduce a paradigm shift by proposing a supervised contrastive loss that incorporates temporally-aware soft targets. Our approach captures intra-polyp variability while preserving inter-polyp discriminability, leading to more robust clustering. Additionally, we improve tracklet clustering by integrating a temporal adjacency constraint, reducing false positive re-associations between visually similar but temporally distant tracklets. We train and validate our method on publicly available datasets and evaluate its performance with a leave-one-out cross-validation strategy. Results demonstrate a 2.2x reduction in fragmentation rate compared to prior approaches. Our results highlight the importance of temporal awareness in polyp counting, establishing a new state-of-the-art. Code is available at https://github.com/lparolari/temporally-aware-polyp-counting.<br>
<span id='abs_ch'>中文: 本 究提出了一种结合时序软目 和邻接约束的监督对比学 方法，显著提升了结 镜息肉计数的准确性，通过降低碎片化程度增强了聚类鲁棒性，相比现有方法性能提高了2.2倍。English: This study introduces a supervised contrastive learning method with temporal soft targets and adjacency constraints to enhance polyp counting in colonoscopy by reducing fragmentation and improving clustering robustness, achieving a 2.2x improvement over existing approaches.Paperid:1165,https://arxiv.org/pdf/2507.02488.pdfGitHub</span><br>
<span id='abs_en'>English: This study introduces a supervised contrastive learning method with temporal soft targets and adjacency constraints to enhance polyp counting in colonoscopy by reducing fragmentation and improving clustering robustness, achieving a 2.2x improvement over existing approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1528, <a href='https://arxiv.org/pdf/2507.02479.pdf' target='_blank'>https://arxiv.org/pdf/2507.02479.pdf</a></span>   <span><a href='https://github.com/loseevaya/CrowdTrack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Teng Fu, Yuwen Chen, Zhuofan Chen, Mengyang Zhao, Bin Li, Xiangyang Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02479">CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Multi-object tracking is a classic field in computer vision. Among them, pedestrian tracking has extremely high application value and has become the most popular research category. Existing methods mainly use motion or appearance information for tracking, which is often difficult in complex scenarios. For the motion information, mutual occlusions between objects often prevent updating of the motion state; for the appearance information, non-robust results are often obtained due to reasons such as only partial visibility of the object or blurred images. Although learning how to perform tracking in these situations from the annotated data is the simplest solution, the existing MOT dataset fails to satisfy this solution. Existing methods mainly have two drawbacks: relatively simple scene composition and non-realistic scenarios. Although some of the video sequences in existing dataset do not have the above-mentioned drawbacks, the number is far from adequate for research purposes. To this end, we propose a difficult large-scale dataset for multi-pedestrian tracking, shot mainly from the first-person view and all from real-life complex scenarios. We name it ``CrowdTrack'' because there are numerous objects in most of the sequences. Our dataset consists of 33 videos, containing a total of 5,185 trajectories. Each object is annotated with a complete bounding box and a unique object ID. The dataset will provide a platform to facilitate the development of algorithms that remain effective in complex situations. We analyzed the dataset comprehensively and tested multiple SOTA models on our dataset. Besides, we analyzed the performance of the foundation models on our dataset. The dataset and project code is released at: https://github.com/loseevaya/CrowdTrack .<br>
<span id='abs_ch'>中文: 作者提出了“CrowdTrack”，这是一个从第一人称视角在复杂现实场景中拍摄的、具有挑战性的大规模多行人跟踪数据集，包含33个视频和5,185条完整 注轨迹，旨在克服现有数据集的局限性并推动鲁棒跟踪算法的发展。</span><br>
<span id='abs_en'>English: The authors introduce "CrowdTrack," a challenging large-scale dataset for multi-pedestrian tracking captured from first-person views in complex real-world scenarios, designed to overcome limitations of existing datasets by providing 33 videos with 5,185 fully annotated trajectories to advance robust tracking algorithms.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1529, <a href='https://arxiv.org/pdf/2507.02409.pdf' target='_blank'>https://arxiv.org/pdf/2507.02409.pdf</a></span>   <span><a href='https://github.com/Wonder7racer/S2FGL.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Tan, Suyuan Huang, Guancheng Wan, Wenke Huang, He Li, Mang Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02409">S2FGL: Spatial Spectral Federated Graph Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Federated Graph Learning (FGL) combines the privacy-preserving capabilities of federated learning (FL) with the strong graph modeling capability of Graph Neural Networks (GNNs). Current research addresses subgraph-FL from the structural perspective, neglecting the propagation of graph signals on spatial and spectral domains of the structure. From a spatial perspective, subgraph-FL introduces edge disconnections between clients, leading to disruptions in label signals and a degradation in the semantic knowledge of the global GNN. From a spectral perspective, spectral heterogeneity causes inconsistencies in signal frequencies across subgraphs, which makes local GNNs overfit the local signal propagation schemes. As a result, spectral client drift occurs, undermining global generalizability. To tackle the challenges, we propose a global knowledge repository to mitigate the challenge of poor semantic knowledge caused by label signal disruption. Furthermore, we design a frequency alignment to address spectral client drift. The combination of Spatial and Spectral strategies forms our framework S2FGL. Extensive experiments on multiple datasets demonstrate the superiority of S2FGL. The code is available at https://github.com/Wonder7racer/S2FGL.git.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1530, <a href='https://arxiv.org/pdf/2507.02403.pdf' target='_blank'>https://arxiv.org/pdf/2507.02403.pdf</a></span>   <span><a href='https://github.com/pxpana/SSLWildlife' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mufhumudzi Muthivhi, Terence L. van Zyl
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02403">Wildlife Target Re-Identification Using Self-supervised Learning in Non-Urban Settings</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Wildlife re-identification aims to match individuals of the same species across different observations. Current state-of-the-art (SOTA) models rely on class labels to train supervised models for individual classification. This dependence on annotated data has driven the curation of numerous large-scale wildlife datasets. This study investigates self-supervised learning Self-Supervised Learning (SSL) for wildlife re-identification. We automatically extract two distinct views of an individual using temporal image pairs from camera trap data without supervision. The image pairs train a self-supervised model from a potentially endless stream of video data. We evaluate the learnt representations against supervised features on open-world scenarios and transfer learning in various wildlife downstream tasks. The analysis of the experimental results shows that self-supervised models are more robust even with limited data. Moreover, self-supervised features outperform supervision across all downstream tasks. The code is available here https://github.com/pxpana/SSLWildlife.<br>
<span id='abs_ch'>中文摘要：本 究通过从相机陷阱数据中自动提取 监督的时间图像对，探索了自监督学 在野生动物重识别中的应用，证明自监督模型即使在数据有限的情况下，其鲁棒性和各项下游任务性能均优于监督学 方法。</span><br>
<span id='abs_en'>English Summary: This study explores self-supervised learning for wildlife re-identification by automatically extracting temporal image pairs from camera trap data, demonstrating that self-supervised models outperform supervised approaches in robustness and performance across various downstream tasks even with limited data.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1531, <a href='https://arxiv.org/pdf/2507.02398.pdf' target='_blank'>https://arxiv.org/pdf/2507.02398.pdf</a></span>   <span><a href='https://github.com/rama0126/PwTF-DVD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taehoon Kim, Jongwook Choi, Yonghyun Jeong, Haeun Noh, Jaejun Yoo, Seungryul Baek, Jongwon Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02398">Beyond Spatial Frequency: Pixel-wise Temporal Frequency-based Deepfake Video Detection</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We introduce a deepfake video detection approach that exploits pixel-wise temporal inconsistencies, which traditional spatial frequency-based detectors often overlook. Traditional detectors represent temporal information merely by stacking spatial frequency spectra across frames, resulting in the failure to detect temporal artifacts in the pixel plane. Our approach performs a 1D Fourier transform on the time axis for each pixel, extracting features highly sensitive to temporal inconsistencies, especially in areas prone to unnatural movements. To precisely locate regions containing the temporal artifacts, we introduce an attention proposal module trained in an end-to-end manner. Additionally, our joint transformer module effectively integrates pixel-wise temporal frequency features with spatio-temporal context features, expanding the range of detectable forgery artifacts. Our framework represents a significant advancement in deepfake video detection, providing robust performance across diverse and challenging detection scenarios.<br>
<span id='abs_ch'>Chinese: 我们的方法通过逐像 时间轴傅里叶变换检测深度伪 视频中的时序异常，并结合注意力机制与Transformer模块融合时空特征，大幅提升了检测系统的鲁棒性。</span><br>
<span id='abs_en'>English: Our method detects deepfake videos by analyzing pixel-wise temporal inconsistencies through a 1D Fourier transform and integrates these features with spatio-temporal contexts using a transformer module, significantly improving detection robustness.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1532, <a href='https://arxiv.org/pdf/2507.02358.pdf' target='_blank'>https://arxiv.org/pdf/2507.02358.pdf</a></span>   <span><a href='https://github.com/CVMI-Lab/Hita' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Anlin Zheng, Haochen Wang, Yucheng Zhao, Weipeng Deng, Tiancai Wang, Xiangyu Zhang, Xiaojuan Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02358">Hita: Holistic Tokenizer for Autoregressive Image Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Vanilla autoregressive image generation models generate visual tokens step-by-step, limiting their ability to capture holistic relationships among token sequences. Moreover, because most visual tokenizers map local image patches into latent tokens, global information is limited. To address this, we introduce \textit{Hita}, a novel image tokenizer for autoregressive (AR) image generation. It introduces a holistic-to-local tokenization scheme with learnable holistic queries and local patch tokens. Hita incorporates two key strategies to better align with the AR generation process: 1) {arranging} a sequential structure with holistic tokens at the beginning, followed by patch-level tokens, and using causal attention to maintain awareness of previous tokens; and 2) adopting a lightweight fusion module before feeding the de-quantized tokens into the decoder to control information flow and prioritize holistic tokens. Extensive experiments show that Hita accelerates the training speed of AR generators and outperforms those trained with vanilla tokenizers, achieving \textbf{2.59 FID} and \textbf{281.9 IS} on the ImageNet benchmark. Detailed analysis of the holistic representation highlights its ability to capture global image properties, such as textures, materials, and shapes. Additionally, Hita also demonstrates effectiveness in zero-shot style transfer and image in-painting. The code is available at \href{https://github.com/CVMI-Lab/Hita}{https://github.com/CVMI-Lab/Hita}.<br>
<span id='abs_ch'>中文: Hita分词器采用整体到局部的方案，通过顺序排列 记和融合模块来增强自回归图像生成，捕捉全局属性，在ImageNet等基准测试中表现优异。</span><br>
<span id='abs_en'>English: The Hita tokenizer introduces a holistic-to-local scheme with sequential token arrangement and fusion modules to enhance autoregressive image generation by capturing global properties, achieving superior performance on benchmarks like ImageNet.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1533, <a href='https://arxiv.org/pdf/2507.02342.pdf' target='_blank'>https://arxiv.org/pdf/2507.02342.pdf</a></span>   <span><a href='https://github.com/AITRICS/DeltaSHAP' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/AITRICS/DeltaSHAP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Changhun Kim, Yechan Mun, Sangchul Hahn, Eunho Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02342">DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This study proposes DeltaSHAP, a novel explainable artificial intelligence (XAI) algorithm specifically designed for online patient monitoring systems. In clinical environments, discovering the causes driving patient risk evolution is critical for timely intervention, yet existing XAI methods fail to address the unique requirements of clinical time series explanation tasks. To this end, DeltaSHAP addresses three key clinical needs: explaining the changes in the consecutive predictions rather than isolated prediction scores, providing both magnitude and direction of feature attributions, and delivering these insights in real time. By adapting Shapley values to temporal settings, our approach accurately captures feature coalition effects. It further attributes prediction changes using only the actually observed feature combinations, making it efficient and practical for time-sensitive clinical applications. We also introduce new evaluation metrics to evaluate the faithfulness of the attributions for online time series, and demonstrate through experiments on online patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI methods in both explanation quality as 62% and computational efficiency as 33% time reduction on the MIMIC-III decompensation benchmark. We release our code at https://github.com/AITRICS/DeltaSHAP.<br>
<span id='abs_ch'>中文: DeltaSHAP是一种专为在线患者监测设计的新型可解释人工智能算法，通过实时解释预测变化满足临床需求，在解释质量和计算效率上均优于现有方法。</span><br>
<span id='abs_en'>English: DeltaSHAP is a novel explainable AI algorithm tailored for online patient monitoring, addressing clinical needs by explaining prediction changes with real-time efficiency and outperforming existing methods in both explanation quality and computational speed.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1534, <a href='https://arxiv.org/pdf/2507.02314.pdf' target='_blank'>https://arxiv.org/pdf/2507.02314.pdf</a></span>   <span><a href='https://github.com/Jaeihk/MAGIC-Anomaly-generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>JaeHyuck Choi, MinJun Kim, JeHyeong Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02314">MAGIC: Mask-Guided Diffusion Inpainting with Multi-Level Perturbations and Context-Aware Alignment for Few-Shot Anomaly Generation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Few-shot anomaly generation is emerging as a practical solution for augmenting the scarce anomaly data in industrial quality control settings. An ideal generator would meet three demands at once, namely (i) keep the normal background intact, (ii) inpaint anomalous regions to tightly overlap with the corresponding anomaly masks, and (iii) generate anomalous regions in a semantically valid location, while still producing realistic, diverse appearances from only a handful of real examples. Existing diffusion-based methods usually satisfy at most two of these requirements: global anomaly generators corrupt the background, whereas mask-guided ones often falter when the mask is imprecise or misplaced. We propose MAGIC--Mask-guided inpainting with multi-level perturbations and Context-aware alignment--to resolve all three issues. At its core, MAGIC fine-tunes a Stable Diffusion inpainting backbone that preserves normal regions and ensures strict adherence of the synthesized anomaly to the supplied mask, directly addressing background corruption and misalignment. To offset the diversity loss that fine-tuning can cause, MAGIC adds two complementary perturbation strategies: (i) Gaussian prompt-level perturbation applied during fine-tuning and inference that broadens the global appearance of anomalies while avoiding low-fidelity textual appearances, and (ii) mask-guided spatial noise injection that enriches local texture variations. Additionally, the context-aware mask alignment module forms semantic correspondences and relocates masks so that every anomaly remains plausibly contained within the host object, eliminating out-of-boundary artifacts. Under a consistent identical evaluation protocol on the MVTec-AD dataset, MAGIC outperforms previous state-of-the-arts in downstream anomaly tasks.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1535, <a href='https://arxiv.org/pdf/2507.02302.pdf' target='_blank'>https://arxiv.org/pdf/2507.02302.pdf</a></span>   <span><a href='https://github.com/dohoonkim-ai/DoMIX' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dohoon Kim, Donghun Kang, Taesup Moon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02302">DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Domain-Adaptive Pre-training (DAP) has recently gained attention for its effectiveness in fine-tuning pre-trained models. Building on this, continual DAP has been explored to develop pre-trained models capable of incrementally incorporating different domain datasets. However, existing continual DAP methods face several limitations: (1) high computational cost and GPU memory usage during training; (2) sensitivity to incremental data order; and (3) providing a single, generalized model for all end tasks, which contradicts the essence of DAP. In this paper, we propose DoMIX, a novel approach that addresses these challenges by leveraging LoRA modules, a representative parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient and parallel domain-adaptive pre-training that is robust to domain order and effectively utilizes accumulated knowledge to provide tailored pre-trained models for specific tasks. We also demonstrate that our method can be extended beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available at https://github.com/dohoonkim-ai/DoMIX.<br>
<span id='abs_ch'>Chinese: DoMIX提出了一种利用LoRA模块的高效并行领域自适应预训练方法，解决了持续DAP中的计算成本高、领域顺序敏感和缺乏任务专用模型的问题，并可扩展至 准大语言模型微调场景。</span><br>
<span id='abs_en'>English: DoMIX introduces an efficient and parallel domain-adaptive pre-training method using LoRA modules to overcome computational costs, domain order sensitivity, and lack of task-specific models in continual DAP, extending its applicability to standard LLM fine-tuning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1536, <a href='https://arxiv.org/pdf/2507.02200.pdf' target='_blank'>https://arxiv.org/pdf/2507.02200.pdf</a></span>   <span><a href='https://github.com/Event-AHU/ESTR-CoT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiao Wang, Jingtao Jiang, Qiang Chen, Lan Chen, Lin Zhu, Yaowei Wang, Yonghong Tian, Jin Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02200">ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Event stream based scene text recognition is a newly arising research topic in recent years which performs better than the widely used RGB cameras in extremely challenging scenarios, especially the low illumination, fast motion. Existing works either adopt end-to-end encoder-decoder framework or large language models for enhanced recognition, however, they are still limited by the challenges of insufficient interpretability and weak contextual logical reasoning. In this work, we propose a novel chain-of-thought reasoning based event stream scene text recognition framework, termed ESTR-CoT. Specifically, we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input event stream into tokens and utilize a Llama tokenizer to encode the given generation prompt. A Q-former is used to align the vision token to the pre-trained large language model Vicuna-7B and output both the answer and chain-of-thought (CoT) reasoning process simultaneously. Our framework can be optimized using supervised fine-tuning in an end-to-end manner. In addition, we also propose a large-scale CoT dataset to train our framework via a three stage processing (i.e., generation, polish, and expert verification). This dataset provides a solid data foundation for the development of subsequent reasoning-based large models. Extensive experiments on three event stream STR benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the effectiveness and interpretability of our proposed framework. The source code and pre-trained models will be released on https://github.com/Event-AHU/ESTR-CoT.<br>
<span id='abs_ch'>中文: 本文提出了一种新颖的事件流场景文本识别框架ESTR-CoT，通过引入思维链推理机制提升了解释能力和上下文逻辑，在多个基准数据集上的实验充分验证了其有效性。English: This paper introduces ESTR-CoT, a novel event stream scene text recognition framework that integrates chain-of-thought reasoning to enhance interpretability and contextual logic, validated by extensive experiments on benchmark datasets.Paperid:1183,https://arxiv.org/pdf/2507.02199.pdfGitHub</span><br>
<span id='abs_en'>English: This paper introduces ESTR-CoT, a novel event stream scene text recognition framework that integrates chain-of-thought reasoning to enhance interpretability and contextual logic, validated by extensive experiments on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1537, <a href='https://arxiv.org/pdf/2507.02199.pdf' target='_blank'>https://arxiv.org/pdf/2507.02199.pdf</a></span>   <span><a href='https://github.com/wenquanlu/huginn-latent-cot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenquan Lu, Yuechuan Yang, Kyle Lee, Yanshu Li, Enqi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02199">Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-thought (CoT) reasoning has enabled transformer-based language models to excel at complex mathematics and multi-step planning. However, in standard decoder-only architectures, these reasoning steps are externalized in natural language, improving interpretability at the cost of efficiency. To capture reasoning that is not easily represented in words, many works have explored recurrent architectures that aim to internalize reasoning in latent space, potentially supporting latent CoT. In this paper, we investigate whether such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer that reuses layers at inference time without increasing parameter count. We examine the model's internal behavior on arithmetic tasks using a suite of probing techniques including the Logit Lens and Coda Lens. Our findings reveal limited evidence of interpretable latent CoT by tracking rank trajectories of final and intermediate result tokens. Furthermore, we uncover significant probing inconsistencies across recurrent blocks, where the interpretability of hidden states depends heavily on both the layer index and the decoding method. Finally, we empirically show that increasing recurrence depth yields only marginal gains and falls well short of models that explicitly externalize reasoning steps. The code is available at https://github.com/wenquanlu/huginn-latent-cot.<br>
<span id='abs_ch'>中文: 本 究探究深度循环Transformer模型Huginn-3.5B在算术任务中是否形成潜在思维链推理，发现可解释证据有限且增 循环深度仅带来微弱性能提升，远不及显式推理模型。</span><br>
<span id='abs_en'>English: This study investigates whether Huginn-3.5B, a depth-recurrent Transformer, develops latent chain-of-thought reasoning during arithmetic tasks, finding limited interpretable evidence and only marginal performance gains from increased recurrence depth compared to explicit reasoning models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1538, <a href='https://arxiv.org/pdf/2507.02199.pdf' target='_blank'>https://arxiv.org/pdf/2507.02199.pdf</a></span>   <span><a href='https://github.com/wenquanlu/huginn-latent-cot' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenquan Lu, Yuechuan Yang, Kyle Lee, Yanshu Li, Enqi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.02199">Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Chain-of-thought (CoT) reasoning has enabled transformer-based language models to excel at complex mathematics and multi-step planning. However, in standard decoder-only architectures, these reasoning steps are externalized in natural language, improving interpretability at the cost of efficiency. To capture reasoning that is not easily represented in words, many works have explored recurrent architectures that aim to internalize reasoning in latent space, potentially supporting latent CoT. In this paper, we investigate whether such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer that reuses layers at inference time without increasing parameter count. We examine the model's internal behavior on arithmetic tasks using a suite of probing techniques including the Logit Lens and Coda Lens. Our findings reveal limited evidence of interpretable latent CoT by tracking rank trajectories of final and intermediate result tokens. Furthermore, we uncover significant probing inconsistencies across recurrent blocks, where the interpretability of hidden states depends heavily on both the layer index and the decoding method. Finally, we empirically show that increasing recurrence depth yields only marginal gains and falls well short of models that explicitly externalize reasoning steps. The code is available at https://github.com/wenquanlu/huginn-latent-cot.<br>
<span id='abs_ch'>中文: 本 究探究深度循环Transformer模型Huginn-3.5B在算术任务中是否形成潜在思维链推理，发现可解释证据有限且增 循环深度仅带来微弱性能提升，远不及显式推理模型。</span><br>
<span id='abs_en'>English: This study investigates whether Huginn-3.5B, a depth-recurrent Transformer, develops latent chain-of-thought reasoning during arithmetic tasks, finding limited interpretable evidence and only marginal performance gains from increased recurrence depth compared to explicit reasoning models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1539, <a href='https://arxiv.org/pdf/2507.01903.pdf' target='_blank'>https://arxiv.org/pdf/2507.01903.pdf</a></span>   <span><a href='https://github.com/LightChen233/Awesome-AI4Research' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qiguang Chen, Mingda Yang, Libo Qin, Jinhao Liu, Zheng Yan, Jiannan Guan, Dengyun Peng, Yiyan Ji, Hanjing Li, Mengkang Hu, Yimeng Zhang, Yihao Liang, Yuhang Zhou, Jiaqi Wang, Zhi Chen, Wanxiang Che
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01903">AI4Research: A Survey of Artificial Intelligence for Scientific Research</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Recent advancements in artificial intelligence (AI), particularly in large language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated remarkable capabilities in complex domains such as logical reasoning and experimental coding. Motivated by these advancements, numerous studies have explored the application of AI in the innovation process, particularly in the context of scientific research. These AI technologies primarily aim to develop systems that can autonomously conduct research processes across a wide range of scientific disciplines. Despite these significant strides, a comprehensive survey on AI for Research (AI4Research) remains absent, which hampers our understanding and impedes further development in this field. To address this gap, we present a comprehensive survey and offer a unified perspective on AI4Research. Specifically, the main contributions of our work are as follows: (1) Systematic taxonomy: We first introduce a systematic taxonomy to classify five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key research gaps and highlight promising future directions, focusing on the rigor and scalability of automated experiments, as well as the societal impact. (3) Abundant applications and resources: Finally, we compile a wealth of resources, including relevant multidisciplinary applications, data corpora, and tools. We hope our work will provide the research community with quick access to these resources and stimulate innovative breakthroughs in AI4Research.<br>
<span id='abs_ch'>人工智能在大型语言模型等领域的最新进展已能支持自主科 ，但缺乏全面综述阻碍了发展；本 究通过提出系统分类法、指明新方向并整合丰富资源，填补了这一空白，旨在推动科 人工智能的创新突 。</span><br>
<span id='abs_en'>Recent advances in AI, especially in large language models, have enabled autonomous scientific research, but the lack of a comprehensive survey hinders progress; this work fills that gap by providing a systematic taxonomy, identifying new frontiers, and compiling abundant resources to foster innovation in AI for Research.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1540, <a href='https://arxiv.org/pdf/2507.01790.pdf' target='_blank'>https://arxiv.org/pdf/2507.01790.pdf</a></span>   <span><a href='https://github.com/ethahtz/vlm_conflicting_info_processing' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianze Hua, Tian Yun, Ellie Pavlick
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01790">How Do Vision-Language Models Process Conflicting Information Across Modalities?</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>AI models are increasingly required to be multimodal, integrating disparate input streams into a coherent state representation on which subsequent behaviors and actions can be based. This paper seeks to understand how such models behave when input streams present conflicting information. Focusing specifically on vision-language models, we provide inconsistent inputs (e.g., an image of a dog paired with the caption "A photo of a cat") and ask the model to report the information present in one of the specific modalities (e.g., "What does the caption say / What is in the image?"). We find that models often favor one modality over the other, e.g., reporting the image regardless of what the caption says, but that different models differ in which modality they favor. We find evidence that the behaviorally preferred modality is evident in the internal representational structure of the model, and that specific attention heads can restructure the representations to favor one modality over the other. Moreover, we find modality-agnostic "router heads" which appear to promote answers about the modality requested in the instruction, and which can be manipulated or transferred in order to improve performance across datasets and modalities. Together, the work provides essential steps towards identifying and controlling if and how models detect and resolve conflicting signals within complex multimodal environments.<br>
<span id='abs_ch'>中文: 本 究探讨了视觉语言模型如何处理图像与文本模态间的冲突输入，发现模型常偏向某一模态，特定注意力头可调控这种偏好，并实现跨模态性能优化。</span><br>
<span id='abs_en'>English: This study investigates how vision-language models handle conflicting inputs between image and text modalities, revealing that models often favor one modality over the other, with specific attention heads influencing this preference and enabling control over modality prioritization.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1541, <a href='https://arxiv.org/pdf/2507.01702.pdf' target='_blank'>https://arxiv.org/pdf/2507.01702.pdf</a></span>   <span><a href='https://github.com/Lbotirx/AdamMeme' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixin Chen, Hongzhan Lin, Kaixin Li, Ziyang Luo, Zhen Ye, Guang Chen, Zhiyong Huang, Jing Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01702">AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The proliferation of multimodal memes in the social media era demands that multimodal Large Language Models (mLLMs) effectively understand meme harmfulness. Existing benchmarks for assessing mLLMs on harmful meme understanding rely on accuracy-based, model-agnostic evaluations using static datasets. These benchmarks are limited in their ability to provide up-to-date and thorough assessments, as online memes evolve dynamically. To address this, we propose AdamMeme, a flexible, agent-based evaluation framework that adaptively probes the reasoning capabilities of mLLMs in deciphering meme harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive evaluations by iteratively updating the meme data with challenging samples, thereby exposing specific limitations in how mLLMs interpret harmfulness. Extensive experiments show that our framework systematically reveals the varying performance of different target mLLMs, offering in-depth, fine-grained analyses of model-specific weaknesses. Our code is available at https://github.com/Lbotirx/AdamMeme.<br>
<span id='abs_ch'>中文: AdamMeme框架作为一种基于智能体的自适应评估工具，通过多智能体协作和迭代更新数据，动态评估多模态大语言模型在识别有害表情包方面的推理能力。</span><br>
<span id='abs_en'>English: The AdamMeme framework is introduced as an adaptive, agent-based evaluation tool that dynamically assesses multimodal large language models' reasoning abilities in detecting harmful memes through iterative updates and multi-agent collaboration.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1542, <a href='https://arxiv.org/pdf/2507.01634.pdf' target='_blank'>https://arxiv.org/pdf/2507.01634.pdf</a></span>   <span><a href='https://github.com/HVision-NKU/DepthAnythingAC' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyuan Sun, Modi Jin, Bowen Yin, Qibin Hou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01634">Depth Anything at Any Condition</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present Depth Anything at Any Condition (DepthAnything-AC), a foundation monocular depth estimation (MDE) model capable of handling diverse environmental conditions. Previous foundation MDE models achieve impressive performance across general scenes but not perform well in complex open-world environments that involve challenging conditions, such as illumination variations, adverse weather, and sensor-induced distortions. To overcome the challenges of data scarcity and the inability of generating high-quality pseudo-labels from corrupted images, we propose an unsupervised consistency regularization finetuning paradigm that requires only a relatively small amount of unlabeled data. Furthermore, we propose the Spatial Distance Constraint to explicitly enforce the model to learn patch-level relative relationships, resulting in clearer semantic boundaries and more accurate details. Experimental results demonstrate the zero-shot capabilities of DepthAnything-AC across diverse benchmarks, including real-world adverse weather benchmarks, synthetic corruption benchmarks, and general benchmarks.
  Project Page: https://ghost233lism.github.io/depthanything-AC-page
  Code: https://github.com/HVision-NKU/DepthAnythingAC<br>
<span id='abs_ch'>中文总结：DepthAnything-AC是一种基础单目深度估计模型，通过 监督一致性正则化和空间距离约束处理多 化环境条件，在各类基准测试中展现出卓越的零 本能力。</span><br>
<span id='abs_en'>English Summary: DepthAnything-AC is a foundation monocular depth estimation model that handles diverse environmental conditions through unsupervised consistency regularization and a Spatial Distance Constraint, demonstrating strong zero-shot performance across various benchmarks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1543, <a href='https://arxiv.org/pdf/2507.01631.pdf' target='_blank'>https://arxiv.org/pdf/2507.01631.pdf</a></span>   <span><a href='https://github.com/Ellimac0/Snake-NeRF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Camille Billouard, Dawa Derksen, Alexandre Constantin, Bruno Vallet
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01631">Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D reconstruction from multiview satellite imagery. However, state-of-the-art NeRF methods are typically constrained to small scenes due to the memory footprint during training, which we study in this paper. Previous work on large-scale NeRFs palliate this by dividing the scene into NeRFs. This paper introduces Snake-NeRF, a framework that scales to large scenes. Our out-of-core method eliminates the need to load all images and networks simultaneously, and operates on a single device. We achieve this by dividing the region of interest into NeRFs that 3D tile without overlap. Importantly, we crop the images with overlap to ensure each NeRFs is trained with all the necessary pixels. We introduce a novel $2\times 2$ 3D tile progression strategy and segmented sampler, which together prevent 3D reconstruction errors along the tile edges. Our experiments conclude that large satellite images can effectively be processed with linear time complexity, on a single GPU, and without compromise in quality.<br>
<span id='abs_ch'>中文: Snake-NeRF是一种可扩展的框架，通过将场景划分为 重 的3D区块并在单GPU上高效处理，实现了卫星图像的大规模三维重建且不损失质量。</span><br>
<span id='abs_en'>English: Snake-NeRF is a scalable framework that enables large-scale 3D reconstruction from satellite imagery by dividing scenes into non-overlapping 3D tiles and processing them efficiently on a single GPU without quality loss.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1544, <a href='https://arxiv.org/pdf/2507.01630.pdf' target='_blank'>https://arxiv.org/pdf/2507.01630.pdf</a></span>   <span><a href='https://github.com/YuxiaoWang-AI/P3HOT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxiao Wang, Yu Lei, Zhenao Wei, Weiying Xue, Xinyu Jiang, Nan Zhuang, Qi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01630">Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The task of Human-Object conTact (HOT) detection involves identifying the specific areas of the human body that are touching objects. Nevertheless, current models are restricted to just one type of image, often leading to too much segmentation in areas with little interaction, and struggling to maintain category consistency within specific regions. To tackle this issue, a HOT framework, termed \textbf{P3HOT}, is proposed, which blends \textbf{P}rompt guidance and human \textbf{P}roximal \textbf{P}erception. To begin with, we utilize a semantic-driven prompt mechanism to direct the network's attention towards the relevant regions based on the correlation between image and text. Then a human proximal perception mechanism is employed to dynamically perceive key depth range around the human, using learnable parameters to effectively eliminate regions where interactions are not expected. Calculating depth resolves the uncertainty of the overlap between humans and objects in a 2D perspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss (RJLoss) has been created as a new loss to inhibit abnormal categories in the same area. A new evaluation metric called ``AD-Acc.'' is introduced to address the shortcomings of existing methods in addressing negative samples. Comprehensive experimental results demonstrate that our approach achieves state-of-the-art performance in four metrics across two benchmark datasets. Specifically, our model achieves an improvement of \textbf{0.7}$\uparrow$, \textbf{2.0}$\uparrow$, \textbf{1.6}$\uparrow$, and \textbf{11.0}$\uparrow$ in SC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated dataset. The sources code are available at https://github.com/YuxiaoWang-AI/P3HOT.<br>
<span id='abs_ch'>中文：提出的P3HOT框架通过结合提示引导和人体邻近感知技术，提升了人-物接触检测的区域关注度和交互准确性，在基准数据集上实现了多项指 的领先性能。</span><br>
<span id='abs_en'>English: The proposed P3HOT framework enhances human-object contact detection by integrating prompt guidance and human proximal perception to improve regional focus and interaction accuracy, achieving state-of-the-art results across multiple metrics on benchmark datasets.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1545, <a href='https://arxiv.org/pdf/2507.01504.pdf' target='_blank'>https://arxiv.org/pdf/2507.01504.pdf</a></span>   <span><a href='https://github.com/RAufschlaeger/cRID' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Robert AufschlÃ¤ger, Youssef Shoeb, Azarm Nowzad, Michael Heigl, Fabian Bally, Martin Schramm
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01504">Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The collection and release of street-level recordings as Open Data play a vital role in advancing autonomous driving systems and AI research. However, these datasets pose significant privacy risks, particularly for pedestrians, due to the presence of Personally Identifiable Information (PII) that extends beyond biometric traits such as faces. In this paper, we present cRID, a novel cross-modal framework combining Large Vision-Language Models, Graph Attention Networks, and representation learning to detect textual describable clues of PII and enhance person re-identification (Re-ID). Our approach focuses on identifying and leveraging interpretable features, enabling the detection of semantically meaningful PII beyond low-level appearance cues. We conduct a systematic evaluation of PII presence in person image datasets. Our experiments show improved performance in practical cross-dataset Re-ID scenarios, notably from Market-1501 to CUHK03-np (detected), highlighting the framework's practical utility. Code is available at https://github.com/RAufschlaeger/cRID.<br>
<span id='abs_ch'>中文摘要：本文提出cRID跨模态框架，通过检测可文本描述的个人身份信息并利用可解释特征分析，在街景图像中增强隐私保护并提升行人重识别性能。</span><br>
<span id='abs_en'>English Summary: The paper introduces cRID, a cross-modal framework that enhances privacy protection in street-level imagery by detecting textual describable PII and improving person re-identification through interpretable feature analysis.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1546, <a href='https://arxiv.org/pdf/2507.01401.pdf' target='_blank'>https://arxiv.org/pdf/2507.01401.pdf</a></span>   <span><a href='https://github.com/LL-AC/AAcls' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huanwen Liang, Jingxian Xu, Yuanji Zhang, Yuhao Huang, Yuhan Zhang, Xin Yang, Ran Li, Xuedong Deng, Yanjun Liu, Guowei Tao, Yun Wu, Sheng Zhao, Xinru Gao, Dong Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01401">Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Fetal abdominal malformations are serious congenital anomalies that require accurate diagnosis to guide pregnancy management and reduce mortality. Although AI has demonstrated significant potential in medical diagnosis, its application to prenatal abdominal anomalies remains limited. Most existing studies focus on image-level classification and rely on standard plane localization, placing less emphasis on case-level diagnosis. In this paper, we develop a case-level multiple instance learning (MIL)-based method, free of standard plane localization, for classifying fetal abdominal anomalies in prenatal ultrasound. Our contribution is three-fold. First, we adopt a mixture-of-attention-experts module (MoAE) to weight different attention heads for various planes. Secondly, we propose a medical-knowledge-driven feature selection module (MFS) to align image features with medical knowledge, performing self-supervised image token selection at the case-level. Finally, we propose a prompt-based prototype learning (PPL) to enhance the MFS. Extensively validated on a large prenatal abdominal ultrasound dataset containing 2,419 cases, with a total of 24,748 images and 6 categories, our proposed method outperforms the state-of-the-art competitors. Codes are available at:https://github.com/LL-AC/AAcls.<br>
<br>
<div id='section'>Paperid: <span id='pid'>1547, <a href='https://arxiv.org/pdf/2507.01050.pdf' target='_blank'>https://arxiv.org/pdf/2507.01050.pdf</a></span>   <span><a href='https://github.com/allacnobug/Detoxification-of-Text' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jing Yu, Yibo Zhao, Jiapeng Zhu, Wenming Shao, Bo Pang, Zhao Zhang, Xiang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01050">Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The widespread dissemination of toxic content on social media poses a serious threat to both online environments and public discourse, highlighting the urgent need for detoxification methods that effectively remove toxicity while preserving the original semantics. However, existing approaches often struggle to simultaneously achieve strong detoxification performance, semantic preservation, and robustness to out-of-distribution data. Moreover, they typically rely on costly, manually annotated parallel corpora while showing poor data efficiency. To address these challenges, we propose a two-stage training framework that jointly optimizes for data efficiency, semantic preservation, and model generalization. We first perform supervised fine-tuning on a small set of high-quality, filtered parallel data to establish a strong initialization. Then, we leverage unlabeled toxic inputs and a custom-designed reward model to train the LLM using Group Relative Policy Optimization. Experimental results demonstrate that our method effectively mitigates the trade-offs faced by previous work, achieving state-of-the-art performance with improved generalization and significantly reduced dependence on annotated data. Our code is available at: https://github.com/allacnobug/Detoxification-of-Text.<br>
<span id='abs_ch'>中文摘要：本 究提出了一种新颖的两阶段训练框架，通过监督微调和强化学 的结合，有效解决了现有文本去毒方法在毒性消除、语义保留和数据效率方面的局限，实现了更优的综合性能。</span><br>
<span id='abs_en'>English Summary: This study introduces a novel two-stage training framework that effectively addresses the limitations of existing text detoxification methods by achieving superior toxicity removal, semantic preservation, and data efficiency through supervised fine-tuning and reinforcement learning.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1548, <a href='https://arxiv.org/pdf/2507.01040.pdf' target='_blank'>https://arxiv.org/pdf/2507.01040.pdf</a></span>   <span><a href='https://github.com/egretwAlker/c-opt-clifford-layers' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianxiang Xia, Max Neuwinger, Lin Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01040">Fast Clifford Neural Layers</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra into neural networks. In this project we focus on optimizing the inference of 2/3D Clifford convolutional layers and multivector activation layers for one core CPU performance.
  Overall, by testing on a real network block involving Clifford convolutional layers and multivector activation layers, we observe that our implementation is 30% faster than standard PyTorch implementation in relatively large data + network size (>L2 cache).
  We open source our code base at https://github.com/egretwAlker/c-opt-clifford-layers<br>
<span id='abs_ch'>Chinese: Clifford神经层通过将克利福德代数引入神经网络改进了偏微分方程建模，在大规模数据和网络场景下比 准PyTorch实现快30%。</span><br>
<span id='abs_en'>English: Clifford Neural Layers enhance PDE modeling by integrating Clifford Algebra into neural networks, achieving a 30% speed improvement over standard PyTorch in large-scale data scenarios.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1549, <a href='https://arxiv.org/pdf/2507.01006.pdf' target='_blank'>https://arxiv.org/pdf/2507.01006.pdf</a></span>   <span><a href='https://github.com/zai-org/GLM-V' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>V Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Bin Chen, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiale Zhu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingdao Liu, Mingde Xu, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Tianyu Tong, Wenkai Li, Wei Jia, Xiao Liu, Xiaohan Zhang, Xin Lyu, Xinyue Fan, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yanzi Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuting Wang, Yu Wang, Yuxuan Zhang, Zhao Xue, Zhenyu Hou, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, Jie Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01006">GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>We present GLM-4.1V-Thinking and GLM-4.5V, a family of vision-language models (VLMs) designed to advance general-purpose multimodal understanding and reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. We then propose Reinforcement Learning with Curriculum Sampling (RLCS) to unlock the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document interpretation. In a comprehensive evaluation across 42 public benchmarks, GLM-4.5V achieves state-of-the-art performance on nearly all tasks among open-source models of similar size, and demonstrates competitive or even superior results compared to closed-source models such as Gemini-2.5-Flash on challenging tasks including Coding and GUI Agents. Meanwhile, the smaller GLM-4.1V-9B-Thinking remains highly competitive-achieving superior results to the much larger Qwen2.5-VL-72B on 29 benchmarks. We open-source both GLM-4.1V-9B-Thinking and GLM-4.5V. Code, models and more information are released at https://github.com/zai-org/GLM-V.<br>
<span id='abs_ch'>中文: GLM-4.1V-Thinking和GLM-4.5V视觉语言模型通过以推理为 心的训练框架和课程采 强化学 ，在42个基准测试中取得顶尖性能，超越同类开源模型并与主流闭源模型相媲美。</span><br>
<span id='abs_en'>English: The GLM-4.1V-Thinking and GLM-4.5V vision-language models achieve state-of-the-art performance across 42 benchmarks through a reasoning-centric training framework and Reinforcement Learning with Curriculum Sampling, surpassing similar-sized open-source models and competing with leading closed-source models.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1550, <a href='https://arxiv.org/pdf/2507.00979.pdf' target='_blank'>https://arxiv.org/pdf/2507.00979.pdf</a></span>   <span><a href='https://github.com/HahmDY/causal_influence_prompting.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongyoon Hahm, Woogyeol Jin, June Suk Choi, Sungsoo Ahn, Kimin Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00979">Enhancing LLM Agent Safety via Causal Influence Prompting</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>As autonomous agents powered by large language models (LLMs) continue to demonstrate potential across various assistive tasks, ensuring their safe and reliable behavior is crucial for preventing unintended consequences. In this work, we introduce CIP, a novel technique that leverages causal influence diagrams (CIDs) to identify and mitigate risks arising from agent decision-making. CIDs provide a structured representation of cause-and-effect relationships, enabling agents to anticipate harmful outcomes and make safer decisions. Our approach consists of three key steps: (1) initializing a CID based on task specifications to outline the decision-making process, (2) guiding agent interactions with the environment using the CID, and (3) iteratively refining the CID based on observed behaviors and outcomes. Experimental results demonstrate that our method effectively enhances safety in both code execution and mobile device control tasks.<br>
<span id='abs_ch'>中文: 本文提出CIP技术，通过 果影响图识别和减轻自主智能体决策风险，在代 执行和移动设备控制任务中有效提升了安全性。English: This paper introduces CIP, a novel technique using causal influence diagrams to enhance the safety of autonomous agents by identifying and mitigating risks in decision-making, with experimental validation in code execution and mobile control tasks.Paperid:1245,https://arxiv.org/pdf/2507.00953.pdfGitHub</span><br>
<span id='abs_en'>English: This paper introduces CIP, a novel technique using causal influence diagrams to enhance the safety of autonomous agents by identifying and mitigating risks in decision-making, with experimental validation in code execution and mobile control tasks.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1551, <a href='https://arxiv.org/pdf/2507.00953.pdf' target='_blank'>https://arxiv.org/pdf/2507.00953.pdf</a></span>   <span><a href='https://github.com/zjuKeLiu/RiFold' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Liu, Shuaike Shen, Hao Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00953">From Sentences to Sequences: Rethinking Languages in Biological System</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The paradigm of large language models in natural language processing (NLP) has also shown promise in modeling biological languages, including proteins, RNA, and DNA. Both the auto-regressive generation paradigm and evaluation metrics have been transferred from NLP to biological sequence modeling. However, the intrinsic structural correlations in natural and biological languages differ fundamentally. Therefore, we revisit the notion of language in biological systems to better understand how NLP successes can be effectively translated to biological domains. By treating the 3D structure of biomolecules as the semantic content of a sentence and accounting for the strong correlations between residues or bases, we highlight the importance of structural evaluation and demonstrate the applicability of the auto-regressive paradigm in biological language modeling. Code can be found at \href{https://github.com/zjuKeLiu/RiFold}{github.com/zjuKeLiu/RiFold}<br>
<span id='abs_ch'>中文: 自然语言处理中的大语言模型在生物序列建模中展现出潜力，但需通过将生物分子三维结构视为语义内容并强调结构评估，来适应其内在结构差异。</span><br>
<span id='abs_en'>English: Large language models from NLP show potential in biological sequence modeling, but require adaptation to account for intrinsic structural differences by treating 3D biomolecular structures as semantic content and emphasizing structural evaluation.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1552, <a href='https://arxiv.org/pdf/2507.00914.pdf' target='_blank'>https://arxiv.org/pdf/2507.00914.pdf</a></span>   <span><a href='https://github.com/usail-hkust/Awesome-Urban-LLM-Agents' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jindong Han, Yansong Ning, Zirui Yuan, Hang Ni, Fan Liu, Tengfei Lyu, Hao Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00914">Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The long-standing vision of intelligent cities is to create efficient, livable, and sustainable urban environments using big data and artificial intelligence technologies. Recently, the advent of Large Language Models (LLMs) has opened new ways toward realizing this vision. With powerful semantic understanding and reasoning capabilities, LLMs can be deployed as intelligent agents capable of autonomously solving complex problems across domains. In this article, we focus on Urban LLM Agents, which are LLM-powered agents that are semi-embodied within the hybrid cyber-physical-social space of cities and used for system-level urban decision-making. First, we introduce the concept of urban LLM agents, discussing their unique capabilities and features. Second, we survey the current research landscape from the perspective of agent workflows, encompassing urban sensing, memory management, reasoning, execution, and learning. Third, we categorize the application domains of urban LLM agents into five groups: urban planning, transportation, environment, public safety, and urban society, presenting representative works in each group. Finally, we discuss trustworthiness and evaluation issues that are critical for real-world deployment, and identify several open problems for future research. This survey aims to establish a foundation for the emerging field of urban LLM agents and to provide a roadmap for advancing the intersection of LLMs and urban intelligence. A curated list of relevant papers and open-source resources is maintained and continuously updated at https://github.com/usail-hkust/Awesome-Urban-LLM-Agents.<br>
<span id='abs_ch'>中文摘要：大型语言模型作为智能代理正应用于城市系统级决策，通过其在语义理解和推理方面的强大能力，推动城市规划、交通、环境等领域的智能化发展。</span><br>
<span id='abs_en'>English Summary: Large Language Models (LLMs) are emerging as intelligent agents for system-level urban decision-making, enabling advancements in urban intelligence through their semantic understanding and reasoning capabilities across various city domains.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1553, <a href='https://arxiv.org/pdf/2507.00880.pdf' target='_blank'>https://arxiv.org/pdf/2507.00880.pdf</a></span>   <span><a href='https://github.com/XuRuihan/NNFormer' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/XuRuihan/NNFormer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruihan Xu, Haokui Zhang, Yaowei Wang, Wei Zeng, Shiliang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00880">NN-Former: Rethinking Graph Structure in Neural Architecture Representation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>The growing use of deep learning necessitates efficient network design and deployment, making neural predictors vital for estimating attributes such as accuracy and latency. Recently, Graph Neural Networks (GNNs) and transformers have shown promising performance in representing neural architectures. However, each of both methods has its disadvantages. GNNs lack the capabilities to represent complicated features, while transformers face poor generalization when the depth of architecture grows. To mitigate the above issues, we rethink neural architecture topology and show that sibling nodes are pivotal while overlooked in previous research. We thus propose a novel predictor leveraging the strengths of GNNs and transformers to learn the enhanced topology. We introduce a novel token mixer that considers siblings, and a new channel mixer named bidirectional graph isomorphism feed-forward network. Our approach consistently achieves promising performance in both accuracy and latency prediction, providing valuable insights for learning Directed Acyclic Graph (DAG) topology. The code is available at https://github.com/XuRuihan/NNFormer.<br>
<span id='abs_ch'>Chinese: 本 究提出了一种新型神经预测器，融合图神经网络与Transformer的优势，通过引入兄弟节点和双向图同构前馈网络来增强拓扑学 ，在神经网络架构的准确性和延迟预测方面均取得优异性能。</span><br>
<span id='abs_en'>English: This study introduces a novel neural predictor that combines Graph Neural Networks and transformers to enhance topology learning by incorporating sibling nodes and a bidirectional graph isomorphism feed-forward network, achieving superior accuracy and latency predictions for neural architectures.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1554, <a href='https://arxiv.org/pdf/2507.00790.pdf' target='_blank'>https://arxiv.org/pdf/2507.00790.pdf</a></span>   <span><a href='https://github.com/AMAP-ML/LD-RPS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huaqiu Li, Yong Wang, Tongwen Huang, Hailang Huang, Haoqian Wang, Xiangxiang Chu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00790">LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Unified image restoration is a significantly challenging task in low-level vision. Existing methods either make tailored designs for specific tasks, limiting their generalizability across various types of degradation, or rely on training with paired datasets, thereby suffering from closed-set constraints. To address these issues, we propose a novel, dataset-free, and unified approach through recurrent posterior sampling utilizing a pretrained latent diffusion model. Our method incorporates the multimodal understanding model to provide sematic priors for the generative model under a task-blind condition. Furthermore, it utilizes a lightweight module to align the degraded input with the generated preference of the diffusion model, and employs recurrent refinement for posterior sampling. Extensive experiments demonstrate that our method outperforms state-of-the-art methods, validating its effectiveness and robustness. Our code and data are available at https://github.com/AMAP-ML/LD-RPS.<br>
<span id='abs_ch'>中文: 本文提出了一种新颖、 需数据集的统一图像恢复方法，利用预训练的潜在扩散模型，结合多模态理解提供语义先验，并通过循环细化在任务 关条件下超越现有最优方法。</span><br>
<span id='abs_en'>English: This paper introduces a novel, dataset-free unified image restoration method using a pretrained latent diffusion model, which incorporates multimodal understanding for semantic priors and recurrent refinement to outperform state-of-the-art approaches.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1555, <a href='https://arxiv.org/pdf/2507.00726.pdf' target='_blank'>https://arxiv.org/pdf/2507.00726.pdf</a></span>   <span><a href='https://github.com/krafton-ai/Chess-R1' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongyoon Hwang, Hojoon Lee, Jaegul Choo, Dongmin Park, Jongho Park
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00726">Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>While reinforcement learning (RL) for large language models (LLMs) has shown promise in mathematical reasoning, strategic reasoning for LLMs using RL remains largely unexplored. We investigate whether LLMs can develop strategic reasoning capabilities through RL in chess. To this end, we leverage a chess-pretrained action-value network to provide dense reward on the LLM's output move quality, which can be seen as a form of knowledge distillation. Our experiments show that our distillation-based dense rewards often outperform sparse binary rewards. However, surprisingly, all models plateau far below expert levels. We provide SFT and RL ablations on chess reasoning training and find evidence that this limitation stems from a deficit in the pretrained models' internal understanding of chess-a deficit which RL alone may not be able to fully overcome. The code is available at https://github.com/krafton-ai/Chess-R1.<br>
<span id='abs_ch'>中文摘要：本 究通过使用国际象棋预训练网络提供的密集奖励进行强化学 ，探索提升大语言模型在国际象棋中的策略推理能力，发现虽优于稀疏奖励，但 模型内在缺陷，性能仍远低于专家水平。</span><br>
<span id='abs_en'>English Summary: This study explores enhancing large language models' strategic reasoning in chess through reinforcement learning with dense rewards from a chess-pretrained network, finding improved performance over sparse rewards but persistent limitations below expert levels due to inherent model deficits.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1556, <a href='https://arxiv.org/pdf/2507.00665.pdf' target='_blank'>https://arxiv.org/pdf/2507.00665.pdf</a></span>   <span><a href='https://github.com/xzy-101/SAFER-code' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sihang Li, Wei Shi, Ziyuan Xie, Tao Liang, Guojun Ma, Xiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00665">SAFER: Probing Safety in Reward Models with Sparse Autoencoder</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement learning from human feedback (RLHF) is a key paradigm for aligning large language models (LLMs) with human values, yet the reward models at its core remain largely opaque. In this work, we present sparse Autoencoder For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting and improving reward models through mechanistic analysis. Leveraging Sparse Autoencoders (SAEs), we uncover human-interpretable features in reward model activations, enabling insight into safety-relevant decision-making. We apply SAFER to safety-oriented preference datasets and quantify the salience of individual features by activation differences between chosen and rejected responses. Using these feature-level signals, we design targeted data poisoning and denoising strategies. Experiments show that SAFER can precisely degrade or enhance safety alignment with minimal data modification, without sacrificing general chat performance. Our approach contributes to interpreting, auditing and refining reward models in high-stakes LLM alignment tasks. Our codes are available at https://github.com/xzy-101/SAFER-code. \textit{This paper discusses topics related to large language model safety and may include discussions or examples that highlight potential risks or unsafe outcomes.}<br>
<span id='abs_ch'>中文：SAFER框架通过稀疏自编 器解析并改进强化学 人类反馈中的奖励模型，利用针对性数据策略精准调控安全对齐效果，且不影响通用聊天性能。</span><br>
<span id='abs_en'>English: The SAFER framework utilizes sparse autoencoders to interpret and enhance reward models in RLHF, enabling targeted data manipulation that precisely adjusts safety alignment without compromising general performance.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1557, <a href='https://arxiv.org/pdf/2507.00660.pdf' target='_blank'>https://arxiv.org/pdf/2507.00660.pdf</a></span>   <span><a href='https://github.com/crs524/MTCNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rusi Chen, Yuanting Yang, Jiezhi Yao, Hongning Song, Ji Zhang, Yongsong Zhou, Yuhao Huang, Ronghao Yang, Dan Jia, Yuhan Zhang, Xing Tao, Haoran Dou, Qing Zhou, Xin Yang, Dong Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00660">MTCNet: Motion and Topology Consistency Guided Learning for Mitral Valve Segmentationin 4D Ultrasound</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Mitral regurgitation is one of the most prevalent cardiac disorders. Four-dimensional (4D) ultrasound has emerged as the primary imaging modality for assessing dynamic valvular morphology. However, 4D mitral valve (MV) analysis remains challenging due to limited phase annotations, severe motion artifacts, and poor imaging quality. Yet, the absence of inter-phase dependency in existing methods hinders 4D MV analysis. To bridge this gap, we propose a Motion-Topology guided consistency network (MTCNet) for accurate 4D MV ultrasound segmentation in semi-supervised learning (SSL). MTCNet requires only sparse end-diastolic and end-systolic annotations. First, we design a cross-phase motion-guided consistency learning strategy, utilizing a bi-directional attention memory bank to propagate spatio-temporal features. This enables MTCNet to achieve excellent performance both per- and inter-phase. Second, we devise a novel topology-guided correlation regularization that explores physical prior knowledge to maintain anatomically plausible. Therefore, MTCNet can effectively leverage structural correspondence between labeled and unlabeled phases. Extensive evaluations on the first largest 4D MV dataset, with 1408 phases from 160 patients, show that MTCNet performs superior cross-phase consistency compared to other advanced methods (Dice: 87.30%, HD: 1.75mm). Both the code and the dataset are available at https://github.com/crs524/MTCNet.<br>
<span id='abs_ch'>中文: 提出的运动-拓扑引导一致性网络（MTCNet）通过跨相位运动一致性学 和拓扑正则化方法，在仅需稀疏 注的情况下实现了对四维二尖瓣超声的精准分割，并在大规模临床数据上展现出优越性能。</span><br>
<span id='abs_en'>English: The proposed Motion-Topology guided consistency network (MTCNet) addresses challenges in 4D mitral valve ultrasound segmentation by leveraging cross-phase motion consistency and topology regularization, achieving superior performance with only sparse annotations on a large clinical dataset.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1558, <a href='https://arxiv.org/pdf/2507.00485.pdf' target='_blank'>https://arxiv.org/pdf/2507.00485.pdf</a></span>   <span><a href='https://github.com/azure-123/PNAct' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiran Guo, Guanjun Liu, Ziyuan Zhou, Ling Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00485">PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Reinforcement Learning (RL) is widely used in tasks where agents interact with an environment to maximize rewards. Building on this foundation, Safe Reinforcement Learning (Safe RL) incorporates a cost metric alongside the reward metric, ensuring that agents adhere to safety constraints during decision-making. In this paper, we identify that Safe RL is vulnerable to backdoor attacks, which can manipulate agents into performing unsafe actions. First, we introduce the relevant concepts and evaluation metrics for backdoor attacks in Safe RL. It is the first attack framework in the Safe RL field that involves both Positive and Negative Action sample (PNAct) is to implant backdoors, where positive action samples provide reference actions and negative action samples indicate actions to be avoided. We theoretically point out the properties of PNAct and design an attack algorithm. Finally, we conduct experiments to evaluate the effectiveness of our proposed backdoor attack framework, evaluating it with the established metrics. This paper highlights the potential risks associated with Safe RL and underscores the feasibility of such attacks. Our code and supplementary material are available at https://github.com/azure-123/PNAct.<br>
<span id='abs_ch'>Chinese: 本文揭示了安全强化学 易受后门攻击的风险，通过提出正负动作 本（PNAct）框架，在正常条件下维持智能体性能的同时诱导其执行危险动作。English: This paper reveals that Safe Reinforcement Learning (Safe RL) is susceptible to backdoor attacks through a novel Positive and Negative Action sample (PNAct) framework, which manipulates agents into unsafe actions while maintaining performance under normal conditions.Paperid:1277,https://arxiv.org/pdf/2507.00480.pdfGitHub</span><br>
<span id='abs_en'>English: This paper reveals that Safe Reinforcement Learning (Safe RL) is susceptible to backdoor attacks through a novel Positive and Negative Action sample (PNAct) framework, which manipulates agents into unsafe actions while maintaining performance under normal conditions.Paperid:1277,https://arxiv.org/pdf/2507.00480.pdfGitHub</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1559, <a href='https://arxiv.org/pdf/2507.00440.pdf' target='_blank'>https://arxiv.org/pdf/2507.00440.pdf</a></span>   <span><a href='https://github.com/causal-graph/CGR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujia Yin, Tianyi Qu, Zihao Wang, Yifan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00440">A Recipe for Causal Graph Regression: Confounding Effects Revisited</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Through recognizing causal subgraphs, causal graph learning (CGL) has risen to be a promising approach for improving the generalizability of graph neural networks under out-of-distribution (OOD) scenarios. However, the empirical successes of CGL techniques are mostly exemplified in classification settings, while regression tasks, a more challenging setting in graph learning, are overlooked. We thus devote this work to tackling causal graph regression (CGR); to this end we reshape the processing of confounding effects in existing CGL studies, which mainly deal with classification. Specifically, we reflect on the predictive power of confounders in graph-level regression, and generalize classification-specific causal intervention techniques to regression through a lens of contrastive learning. Extensive experiments on graph OOD benchmarks validate the efficacy of our proposals for CGR. The model implementation and the code are provided on https://github.com/causal-graph/CGR.<br>
<span id='abs_ch'>Chinese: 本 究通过对比学 将 果干预技术从分类任务推广到回归任务，提出了 果图回归方法，在分布外图学 基准上的大量实验验证了其有效性。English: This work introduces causal graph regression (CGR) by adapting causal intervention techniques from classification to regression through contrastive learning, enhancing generalizability in out-of-distribution graph scenarios as validated by extensive experiments.Paperid:1282,https://arxiv.org/pdf/2507.00430.pdfGitHub</span><br>
<span id='abs_en'>English: This work introduces causal graph regression (CGR) by adapting causal intervention techniques from classification to regression through contrastive learning, enhancing generalizability in out-of-distribution graph scenarios as validated by extensive experiments.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1560, <a href='https://arxiv.org/pdf/2507.00061.pdf' target='_blank'>https://arxiv.org/pdf/2507.00061.pdf</a></span>   <span><a href='https://github.com/Kuan2vn/smooth\_distill' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hoang-Dieu Vu, Duc-Nghia Tran, Quang-Tu Pham, Hieu H. Pham, Nicolas Vuillerme, Duc-Tan Tran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00061">Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>This paper introduces Smooth-Distill, a novel self-distillation framework designed to simultaneously perform human activity recognition (HAR) and sensor placement detection using wearable sensor data. The proposed approach utilizes a unified CNN-based architecture, MTL-net, which processes accelerometer data and branches into two outputs for each respective task. Unlike conventional distillation methods that require separate teacher and student models, the proposed framework utilizes a smoothed, historical version of the model itself as the teacher, significantly reducing training computational overhead while maintaining performance benefits. To support this research, we developed a comprehensive accelerometer-based dataset capturing 12 distinct sleep postures across three different wearing positions, complementing two existing public datasets (MHealth and WISDM). Experimental results show that Smooth-Distill consistently outperforms alternative approaches across different evaluation scenarios, achieving notable improvements in both human activity recognition and device placement detection tasks. This method demonstrates enhanced stability in convergence patterns during training and exhibits reduced overfitting compared to traditional multitask learning baselines. This framework contributes to the practical implementation of knowledge distillation in human activity recognition systems, offering an effective solution for multitask learning with accelerometer data that balances accuracy and training efficiency. More broadly, it reduces the computational cost of model training, which is critical for scenarios requiring frequent model updates or training on resource-constrained platforms. The code and model are available at https://github.com/Kuan2vn/smooth\_distill.<br>
<span id='abs_ch'>中文: Smooth-Distill是一种自蒸馏框架，通过统一的CNN架构同时进行人体活动识别和 感器位置检测，在保持高精度的同时显著降低了计算成本。</span><br>
<span id='abs_en'>English: Smooth-Distill is a self-distillation framework that efficiently performs human activity recognition and sensor placement detection using a unified CNN architecture, reducing computational costs while maintaining high accuracy.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1561, <a href='https://arxiv.org/pdf/2507.00043.pdf' target='_blank'>https://arxiv.org/pdf/2507.00043.pdf</a></span>   <span><a href='https://github.com/myigitavci/MR-CLIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehmet Yigit Avci, Pedro Borges, Paul Wright, Mehmet Yigitsoy, Sebastien Ourselin, Jorge Cardoso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00043">MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Accurate interpretation of Magnetic Resonance Imaging scans in clinical systems is based on a precise understanding of image contrast. This contrast is primarily governed by acquisition parameters, such as echo time and repetition time, which are stored in the DICOM metadata. To simplify contrast identification, broad labels such as T1-weighted or T2-weighted are commonly used, but these offer only a coarse approximation of the underlying acquisition settings. In many real-world datasets, such labels are entirely missing, leaving raw acquisition parameters as the only indicators of contrast. Adding to this challenge, the available metadata is often incomplete, noisy, or inconsistent. The lack of reliable and standardized metadata complicates tasks such as image interpretation, retrieval, and integration into clinical workflows. Furthermore, robust contrast-aware representations are essential to enable more advanced clinical applications, such as achieving modality-invariant representations and data harmonization. To address these challenges, we propose MR-CLIP, a multimodal contrastive learning framework that aligns MR images with their DICOM metadata to learn contrast-aware representations, without relying on manual labels. Trained on a diverse clinical dataset that spans various scanners and protocols, MR-CLIP captures contrast variations across acquisitions and within scans, enabling anatomy-invariant representations. We demonstrate its effectiveness in cross-modal retrieval and contrast classification, highlighting its scalability and potential for further clinical applications. The code and weights are publicly available at https://github.com/myigitavci/MR-CLIP.<br>
<span id='abs_ch'>中文: MR-CLIP是一种多模态框架，通过将磁共振图像与DICOM元数据对齐来学 对比度感知表征， 需人工 注即可实现跨模态检索和对比度分类等稳健应用。</span><br>
<span id='abs_en'>English: MR-CLIP is a multimodal framework that aligns MRI images with DICOM metadata to learn contrast-aware representations, enabling robust applications like cross-modal retrieval and contrast classification without manual labels.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1562, <a href='https://arxiv.org/pdf/2507.00037.pdf' target='_blank'>https://arxiv.org/pdf/2507.00037.pdf</a></span>   <span><a href='https://github.com/AndrewSpano/neuron-interpolation-model-fusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Phoomraphee Luenam, Andreas Spanopoulos, Amit Sant, Thomas Hofmann, Sotiris Anagnostidis, Sidak Pal Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00037">Model Fusion via Neuron Interpolation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Model fusion aims to combine the knowledge of multiple models by creating one representative model that captures the strengths of all of its parents. However, this process is non-trivial due to differences in internal representations, which can stem from permutation invariance, random initialization, or differently distributed training data. We present a novel, neuron-centric family of model fusion algorithms designed to integrate multiple trained neural networks into a single network effectively regardless of training data distribution. Our algorithms group intermediate neurons of parent models to create target representations that the fused model approximates with its corresponding sub-network. Unlike prior approaches, our approach incorporates neuron attribution scores into the fusion process. Furthermore, our algorithms can generalize to arbitrary layer types. Experimental results on various benchmark datasets demonstrate that our algorithms consistently outperform previous fusion techniques, particularly in zero-shot and non-IID fusion scenarios. The code is available at https://github.com/AndrewSpano/neuron-interpolation-model-fusion.<br>
<span id='abs_ch'>Chinese: 本文提出了一种以神经元为中心的模型融合方法，通过分组神经元并引入归 评分，将多个神经网络有效整合为单一模型，在多种场景下优于现有技术，且代 已公开。</span><br>
<span id='abs_en'>English: This paper introduces a neuron-centric model fusion method that effectively integrates multiple neural networks into a single model by grouping neurons and incorporating attribution scores, outperforming prior techniques across diverse scenarios with publicly available code.</span><br>
<br>
<div id='section'>Paperid: <span id='pid'>1563, <a href='https://arxiv.org/pdf/2507.00025.pdf' target='_blank'>https://arxiv.org/pdf/2507.00025.pdf</a></span>   <span><a href='https://github.com/WonderSeven/FNSDA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiexin Qin, Hong Yan, Haoliang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00025">Generalizing to New Dynamical Systems via Frequency Domain Adaptation</a></div></span></div><div id='abs'>Abstract:<br><span id='abs'>Learning the underlying dynamics from data with deep neural networks has shown remarkable potential in modeling various complex physical dynamics. However, current approaches are constrained in their ability to make reliable predictions in a specific domain and struggle with generalizing to unseen systems that are governed by the same general dynamics but differ in environmental characteristics. In this work, we formulate a parameter-efficient method, Fourier Neural Simulator for Dynamical Adaptation (FNSDA), that can readily generalize to new dynamics via adaptation in the Fourier space. Specifically, FNSDA identifies the shareable dynamics based on the known environments using an automatic partition in Fourier modes and learns to adjust the modes specific for each new environment by conditioning on low-dimensional latent systematic parameters for efficient generalization. We evaluate our approach on four representative families of dynamic systems, and the results show that FNSDA can achieve superior or competitive generalization performance compared to existing methods with a significantly reduced parameter cost. Our code is available at https://github.com/WonderSeven/FNSDA.<br>
<span id='abs_ch'>中文摘要：本 究提出FNSDA方法，通过傅里叶空间自适应实现动力学系统的高效泛化，在四个典型动态系统中以显著降低的参数成本达到优越的泛化性能。</span><br>
<span id='abs_en'>English Summary: The study introduces FNSDA, a parameter-efficient method that generalizes to unseen dynamical systems by adapting Fourier modes, achieving superior performance with reduced parameters across four dynamic systems.</span><br>
<br>
